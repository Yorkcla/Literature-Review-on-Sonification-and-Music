"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"AIFHRPQ7","journalArticle","2010","Park, Sihwa; Kim, Seunghun; Lee, Samuel; Yeo, Woon Seung","Online Map Interface for Creative and Interactive","","","","10.5281/zenodo.1177877","","In this paper, we discuss the musical potential of COMPath - an online map based music-making tool - as a noveland unique interface for interactive music composition andperformance. COMPath provides an intuitive environmentfor creative music making by sonification of georeferenceddata. Users can generate musical events with simple andfamiliar actions on an online map interface; a set of local information is collected along the user-drawn route andthen interpreted as sounds of various musical instruments.We discuss the musical interpretation of routes on a map,review the design and implementation of COMPath, andpresent selected sonification results with focus on mappingstrategies for map-based composition.","2010","2023-07-31 07:24:19","2023-07-31 07:24:19","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XMMIZ7DI","journalArticle","2007","le Groux, Sylvain; Manzolli, Jonatas; Verschure, Paul F.","VR-RoBoser : Real-Time Adaptive Sonification of Virtual Environments Based on Avatar Behavior","","","","10.5281/zenodo.1177101","","Until recently, the sonification of Virtual Environments had often been reduced to its simplest expression. Too often soundscapes and background music are predetermined, repetitive and somewhat predictable. Yet, there is room for more complex and interesting sonification schemes that can improve the sensation of presence in a Virtual Environment. In this paper we propose a system that automatically generates original background music in real-time called VR-RoBoser. As a test case we present the application of VR-RoBoser to a dynamic avatar that explores its environment. We show that the musical events are directly and continuously generated and influenced by the behavior of the avatar in three-dimensional virtual space, generating a context dependent sonification.","2007","2023-07-31 07:24:19","2023-07-31 07:24:19","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EK697C6P","journalArticle","2005","Bowen, Adam","Soundstone: A {3-D} Wireless Music Controller","","","","10.5281/zenodo.1176711","","Soundstone is a small wireless music controller that tracks movement and gestures, and maps these signals to characteristics of various synthesized and sampled sounds. It is intended to become a general-purpose platform for exploring the sonification of movement, with an emphasis on tactile (haptic) feedback.","2005","2023-07-31 07:24:19","2023-07-31 07:24:19","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3IFJ73XB","journalArticle","2008","Hadjakos, Aristotelis; Aitenbichler, Erwin; Mühlhäuser, Max","The Elbow Piano : Sonification of Piano Playing Movements","","","","10.5281/zenodo.1179553","","The Elbow Piano distinguishes two types of piano touch: a touchwith movement in the elbow joint and a touch without. A playednote is first mapped to the left or right hand by visual tracking.Custom-built goniometers attached to the player's arms are usedto detect the type of touch. The two different types of touchesare sonified by different instrument sounds. This gives theplayer an increased awareness of his elbow movements, which isconsidered valuable for piano education. We have implementedthe system and evaluated it with a group of music students.","2008","2023-07-31 07:24:19","2023-07-31 07:24:19","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QZ5NUKHC","journalArticle","2018","Véron-Delor, Lauriane; Pinto, Serge; Eusebio, Alexandre; Velay, Jean-Luc; Danna, Jérémy; Aramaki, Mitsuko; Davies, Matthew E. P.; Kronland-Martinet, Richard; Ystad, Sølvi","Music and Musical Sonification for the Rehabilitation of Parkinsonian Dysgraphia: Conceptual Framework","Music Technology with Swing","","","","","Music has been shown to enhance motor control in patients with Parkinson’s disease (PD). Notably, musical rhythm is perceived as an external auditory cue that helps PD patients to better control movements. The rationale of such effects is that motor control based on auditory guidance would activate a compensatory brain network that minimizes the recruitment of the defective pathway involving the basal ganglia. Would associating music to movement improve its perception and control in PD? Musical sonification consists in modifying in real-time the playback of a preselected music according to some movement parameters. The validation of such a method is underway for handwriting in PD patients. When confirmed, this study will strengthen the clinical interest of musical sonification in motor control and (re)learning in PD.","2018","2023-07-31 07:24:19","2023-07-31 07:24:19","","312-326","","","11265","","Music Technology with Swing","","","","","","","","","","","","","","","Place: Cham Publisher: Springer International Publishing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3Z786NM2","journalArticle","2016","Walus, Bartlomiej P.; Pauletto, Sandra; Mason-Jones, Amanda","Sonification and music as support to the communication of alcohol-related health risks to young people: Study design and results","Journal on Multimodal User Interfaces","","","10.1007/s12193-016-0220-0","","Excessive consumption of alcohol has been recognised as a significant risk factor impacting the health of young people. Effective communication of such risk is considered to be one key step to improve behaviour. We evaluated an innovative multimedia intervention that utilised audio (sonification—using sound to display data—and music) and interactivity to support the visual communication of alcohol health risk data. A 3-arm pilot experiment was undertaken. The trial measures included health knowledge, alcohol risk perception and user experience of the intervention. Ninetysix subjects participated in the experiment. At 1 month follow-up, alcohol knowledge and alcohol risk perception improved significantly in the whole sample. However, there was no difference between the intervention groups that experienced (1) visual presentation with interactivity (VI-Exp group) and, (2) visual presentation with audio (sonification and music) and interactivity (VAI-Exp group), when compared to the control group which experienced a (3) visual only presentation (V-Cont group). Participants reported enjoying the presentations and found them educational. The majority of participants indicated that the audio, music and sonification helped to convey the information well, and, although a larger sample size is needed to fully establish the effectiveness of the different interventions, this study provides a useful model for future similar studies.","2016","2023-07-31 07:24:19","2023-07-31 07:24:19","","235-246","","3","10","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IYX4LA5Z","journalArticle","2012","Huang, Chih-Fang; Lu, Hsiang-Pin; Ren, Jenny","Algorithmic approach to sonification of classical Chinese poetry","Multimedia Tools and Applications","","","10.1007/s11042-011-0856-4","","The classical Chinese poetry is a remarkable form of art in traditional Chinese character. However, it is difficult for people who are unfamiliar with ancient Chinese to experience the artistic content of the poetry. In this study, a sonification scheme, Tx2Ms (Text-to-Music), is proposed to extract the poetry features between lines in verses; moreover, dynamics and interval relations are modeled to map those features to the movement of multi-dimensional musical elements such as durations. This conversion is based on poetry intonation and acoustic analysis of the pronunciations of poems; and a stochastic compositional algorithm is created by applying a Markov chain. In addition, the best pentatonic mode for a specific poem is recommended according to the formants analysis. Therefore, the sonification of classical Chinese poetry not only provides a novel way for people to appreciate Chinese poetry but also enriches the state of mind and imagery in the delivery process, and the experiment results show that the proposed system is successfully accepted by most people.","2012","2023-07-31 07:24:19","2023-07-31 07:24:19","","489-518","","2","61","","Multimedia Tools and Applications","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AIQLSY8F","journalArticle","2020","Newbold, Joseph; Gold, Nicolas E.; Bianchi-Berthouze, Nadia","Movement sonification expectancy model: leveraging musical expectancy theory to create movement-altering sonifications","Journal on Multimodal User Interfaces","","","10.1007/s12193-020-00322-2","","Abstract When designing movement sonifications, their effect on people’s movement must be considered. Recent work has shown how real-time sonification can be designed to alter the way people move. However, the mechanisms through which these sonifications alter people’s expectations of their movement is not well explained. This is especially important when considering musical sonifications, to which people bring their own associations and musical expectation, and which can, in turn, alter their perception of the sonification. This paper presents a Movement Expectation Sonification Model, based on theories of motor-feedback and expectation, to explore how musical sonification can impact the way people perceive their movement. Secondly, we present a study that validates the predictions of this model by exploring how harmonic stability within sonification interacts with contextual cues in the environment to impact movement behaviour and perceptions. We show how musical expectancy can be built to either reward or encourage movement, and how such an effect is mediated through the presence of additional cues. This model offers a way for sonification designers to create movement sonifications that not only inform movement but can be used to encourage progress and reward successes.","2020","2023-07-31 07:24:19","2023-07-31 07:24:19","","153-166","","2","14","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VGP725LW","journalArticle","2008","Chemseddine, Maher; Noirhomme-Fraiture, Monique; Forbrig, Peter; Paternò, Fabio; Pejtersen, Annelise Mark","Complex and Dynamic Data Representation by Sonification","Human-Computer Interaction Symposium","","","","","So far, data representation has been based on visuals. The huge size of data verges on overuse of the visual capability. Thus, there is a need to reduce the almost exclusive use of visual techniques to represent data in order to increase our perception bandwidth. In this research, we aim to solve this problem by integrating the audio component. More precisely, we are interested in representing data using musical melodies. This paper presents a model for music elements based on human emotion, to express alert messages displayed by computer network monitoring.","2008","2023-07-31 07:24:19","2023-07-31 07:24:19","","195-200","","","272","","Human-Computer Interaction Symposium","","","","","","","","","","","","","","","Place: Boston, MA Publisher: Springer US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DKTTMX4D","journalArticle","2012","Eacott, John","Flood Tide: sonification as musical performance—an audience perspective","AI & SOCIETY","","","10.1007/s00146-011-0338-2","","The number of events and artifacts described as sonification has increased considerably in recent years with some works making a bridge between the representation of data and artistic expression. FloodTide which sonifies the flow of tidal water is such a work and has achieved a relatively high profile attracting good audiences for its 10 performances to date. It is not entirely obvious however what it is that attracts audiences and whether it is effective at representing the data being sonified. This paper aims to address these issues and is based on a discussion group in which these and other questions are considered.","2012","2023-07-31 07:24:19","2023-07-31 07:24:19","","189-195","","2","27","","AI & SOCIETY","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3DCLNSS3","journalArticle","2019","Polo, Antonio; Sevillano, Xavier","Musical Vision: an interactive bio-inspired sonification tool to convert images into music","Journal on Multimodal User Interfaces","","","10.1007/s12193-018-0280-4","","Musical Vision is a highly flexible, interactive and bio-inspired sonification tool that translates color images into harmonic polyphonic music by mimicking the human visual system in terms of its field of vision and photosensitive sensors. Putting the user at the center of the sonification process, Musical Vision allows the interactive design of fully configurable mappings between the color space and the MIDI instruments and audio pitch spaces to tailor the music rendering results to the application needs. Moreover, Musical Vision incorporates a harmonizer capable of introducing the necessary modifications to create melodies using harmonic chords. Above all else, Musical Vision is an extremely flexible system that the user can interactively configure to convert an image into either a few seconds or a several minutes long musical piece. Thus, it can be used, for instance, with trans-artistic purposes like the conversion of a painting into music, for augmenting vision with music, or for learning musical skills such as sol-fa. To evaluate the proposed sonification tool, we conducted a pilot user study, in which twelve volunteers were tested to interpret images containing geometric patterns from music rendered by Musical Vision. Results show that even those users with no musical education background were able to achieve nearly 70% accuracy in multiple choice tests after less than 25 min training. Moreover, users with some musical education were capable of accurately “drawing by ear” the images from no other stimuli than the sonifications.","2019","2023-07-31 07:24:19","2023-07-31 07:24:19","","231-243","","3","13","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H3AHJJJR","journalArticle","2019","Maes, Pieter-Jan; Lorenzoni, Valerio; Six, Joren","The SoundBike: musical sonification strategies to enhance cyclists’ spontaneous synchronization to external music","Journal on Multimodal User Interfaces","","","10.1007/s12193-018-0279-x","","The spontaneous tendency of people to synchronize their movements to music is a powerful mechanism useful for the development of strategies for tempo adaptation of simple repetitive movements. In the current article, we contribute to such strategies—applied to cycling—by introducing a new strategy based on the sonification of cyclists’ motor rhythm. For that purpose, we developed the SoundBike, a stationary bike equipped with sensors that allows interactive sonification of cyclists’ motor rhythm using two distinct but compatible sonification methods. One is based on the principle of step sequencers, which are frequently used for electronic music production. The other is based on the Kuramoto model, allowing automatic and continuous phase alignment of beat-annotated music pieces to cyclists’ motor rhythm, i.e., pedal cadence. Apart from an in-depth presentation of the technical aspects of the SoundBike, we present an experimental study in which we investigated whether the SoundBike could enhance spontaneous synchronization of cyclists to external music. The results of this experiment suggest that sonification of cyclists’ motor rhythm may increase their tendency to synchronize to external music, and helps to keep a more stable pedal cadence, compared to the condition of having external music only (without sonification). Although the results are preliminary and should be followed-up by additional experiments to become more conclusive, SoundBike seems anyhow a promising interactive sonification device to assist motor learning and adaptation in the field of sports and motor rehabilitation.","2019","2023-07-31 07:24:19","2023-07-31 07:24:19","","155-166","","3","13","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ASUZD6HP","journalArticle","2019","Wolf, KatieAnna E.; Fiebrink, Rebecca","Personalised interactive sonification of musical performance data","Journal on Multimodal User Interfaces","","","10.1007/s12193-019-00294-y","","In this article, we describe methods and consequences for giving audience members interactive control over the real-time sonification of performer movement data in electronic music performance. We first briefly describe how to technically implement a musical performance in which each audience member can interactively construct and change their own individual sonification of performers’ movements, heard through headphones on a personal WiFi-enabled device, while also maintaining delay-free synchronization between performer movements and sound. Then, we describe two studies we conducted in the context of live musical performances with this technology. These studies have allowed us to examine how providing audience members with the ability to interactively sonify performer actions impacted their experiences, including their perceptions of their own role and engagement with the performance. These studies also allowed us to explore how audience members with different levels of expertise with sonification and sound, and different motivations for interacting, could be supported and influenced by different sonification interfaces. This work contributes to a better understanding of how providing interactive control over sonification may alter listeners’ experiences, of how to support everyday people in designing and using bespoke sonifications, and of new possibilities for musical performance and participation.","2019","2023-07-31 07:24:19","2023-07-31 07:24:19","","245-265","","3","13","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YQ3B9ET9","journalArticle","2022","Frid, Emma; Bresin, Roberto","Perceptual Evaluation of Blended Sonification of Mechanical Robot Sounds Produced by Emotionally Expressive Gestures: Augmenting Consequential Sounds to Improve Non-verbal Robot Communication","International Journal of Social Robotics","","","10.1007/s12369-021-00788-4","","Abstract This paper presents two experiments focusing on perception of mechanical sounds produced by expressive robot movement and blended sonifications thereof. In the first experiment, 31 participants evaluated emotions conveyed by robot sounds through free-form text descriptions. The sounds were inherently produced by the movements of a NAO robot and were not specifically designed for communicative purposes. Results suggested no strong coupling between the emotional expression of gestures and how sounds inherent to these movements were perceived by listeners; joyful gestures did not necessarily result in joyful sounds. A word that reoccurred in text descriptions of all sounds, regardless of the nature of the expressive gesture, was “stress”. In the second experiment, blended sonification was used to enhance and further clarify the emotional expression of the robot sounds evaluated in the first experiment. Analysis of quantitative ratings of 30 participants revealed that the blended sonification successfully contributed to enhancement of the emotional message for sound models designed to convey frustration and joy. Our findings suggest that blended sonification guided by perceptual research on emotion in speech and music can successfully improve communication of emotions through robot sounds in auditory-only conditions.","2022","2023-07-31 07:24:19","2023-07-31 07:24:19","","357-372","","2","14","","International Journal of Social Robotics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K8AZPL8T","journalArticle","2020","Aldana Blanco, Andrea Lorena; Grautoff, Steffen; Hermann, Thomas","ECG sonification to support the diagnosis and monitoring of myocardial infarction","Journal on Multimodal User Interfaces","","","10.1007/s12193-020-00319-x","","Abstract This paper presents the design and evaluation of four sonification methods to support monitoring and diagnosis in Electrocardiography (ECG). In particular we focus on an ECG abnormality called ST-elevation which is an important indicator of a myocardial infarction. Since myocardial infarction represents a life-threatening condition it is of essential value to detect an ST-elevation as early as possible. As part of the evaluated sound designs, we propose two novel sonifications: (i) Polarity sonification , a continuous parameter-mapping sonification using a formant synthesizer and (ii) Stethoscope sonification , a combination of the ECG signal and a stethoscope recording. The other two designs, (iii) the water ambience sonification and the (iv) morph sonification , were presented in our previous work about ECG sonification (Aldana Blanco AL, Steffen G, Thomas H (2016) In: Proceedings of Interactive Sonification Workshop (ISon). Bielefeld, Germany). The study evaluates three components across the proposed sonifications (1) detection performance, meaning if participants are able to detect a transition from healthy to unhealthy states, (2) classification accuracy, that evaluates if participants can accurately classify the severity of the pathology, and (3) aesthetics and usability (pleasantness, informativeness and long-term listening). The study results show that the polarity design had the highest accuracy rates in the detection task whereas the stethoscope sonification obtained the better score in the classification assignment. Concerning aesthetics, the water ambience sonification was regarded as the most pleasant. Furthermore, we found a significant difference between sound/music experts and non-experts in terms of the error rates obtained in the detection task using the morph sonification and also in the classification task using the stethoscope sonification . Overall, the group of experts obtained lower error rates than the group of non-experts, which means that further training could improve accuracy rates and, particularly for designs that rely mainly on pitch variations, additional training is needed in the non-experts group.","2020","2023-07-31 07:24:19","2023-07-31 07:24:19","","207-218","","2","14","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C9JCVRNE","journalArticle","2021","Raglio, Alfredo; Panigazzi, Monica; Colombo, Roberto; Tramontano, Marco; Iosa, Marco; Mastrogiacomo, Sara; Baiardi, Paola; Molteni, Daniele; Baldissarro, Eleonora; Imbriani, Chiara; Imarisio, Chiara; Eretti, Laura; Hamedani, Mehrnaz; Pistarini, Caterina; Imbriani, Marcello; Mancardi, Gian Luigi; Caltagirone, Carlo","Hand rehabilitation with sonification techniques in the subacute stage of stroke","Scientific Reports","","","10.1038/s41598-021-86627-y","","After a stroke event, most survivors suffer from arm paresis, poor motor control and other disabilities that make activities of daily living difficult, severely affecting quality of life and personal independence. This randomized controlled trial aimed at evaluating the efficacy of a music-based sonification approach on upper limbs motor functions, quality of life and pain perceived during rehabilitation. The study involved 65 subacute stroke individuals during inpatient rehabilitation allocated into 2 groups which underwent usual care dayweek) respectively of standard upper extremity motor rehabilitation or upper extremity treatment with sonification techniques. The Fugl-Meyer Upper Extremity Scale, Box and Block Test and the Modified Ashworth Scale were used to perform motor assessment and the McGill Quality of Life-it and the Numerical Pain Rating Scale to assess quality of life and pain. The assessment was performed at baseline, after 2 weeks, at the end of treatment and at follow-up (1 month after the end of treatment). Total scores of the Fugl-Meyer Upper Extremity Scale (primary outcome measure) and hand and wrist sub scores, manual dexterity scores of the affected and unaffected limb in the Box and Block Test, pain scores of the Numerical Pain Rating Scale (secondary outcomes measures) significantly improved in the sonification group compared to the standard of care group (time*group interaction < 0.05). Our findings suggest that music-based sonification sessions can be considered an effective standardized intervention for the upper limb in subacute stroke rehabilitation.","2021","2023-07-31 07:24:19","2023-07-31 07:24:19","","7237","","1","11","","Scientific Reports","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J7GN5AEP","journalArticle","2012","Grond, Florian; Hermann, Thomas","Aesthetic strategies in sonification","AI & SOCIETY","","","10.1007/s00146-011-0341-7","","Sound can be listened to in various ways and with different intentions. Multiple factors influence how and what we perceive when listening to sound. Sonification, the acoustic representation of data, is in essence just sound. It functions as sonification only if we make sure to listen attentively in order to access the abstract information it contains. This is difficult to accomplish since sound always calls the listener’s attention to concrete—whether natural or musical—points of references. Important aspects determining how we listen to sonification are discussed in this paper: elicited sounds, repeated sounds, conceptual sounds, technologically mediated sounds, melodic sounds, familiar sounds, multimodal sounds and vocal sounds. We discuss how these aspects help the listener engage with the sound, but also how they can become points of reference in and of themselves. The various sonic qualities employed in sonification can potentially open but also risk closing doors to the accessibility and perceptibility of the sonified data.","2012","2023-07-31 07:24:19","2023-07-31 07:24:19","","213-222","","2","27","","AI & SOCIETY","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XAP85UJN","journalArticle","2010","Vogt, Katharina; Pirrò, David; Kobenz, Ingo; Höldrich, Robert; Eckel, Gerhard; Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","PhysioSonic - Evaluated Movement Sonification as Auditory Feedback in Physiotherapy","Auditory Display","","","10.1007/978-3-642-12439-6_6","","We detect human body movement interactively via a tracking system. This data is used to synthesize sound and transform sound files (music or text). A subject triggers and controls sound parameters with his or her movement within a pre-set range of motion. The resulting acoustic feedback enhances new modalities of perception and the awareness of the body movements. It is ideal for application in physiotherapy and other training contexts.","2010","2023-07-31 07:24:19","2023-07-31 07:24:19","","103-120","","","","","Auditory Display","","","","","","","","","","","","","","","Publisher: Springer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JBQGG9HL","journalArticle","2012","Knees, Peter; Pohle, Tim; Widmer, Gerhard","Sound/tracks: artistic real-time sonification of train journeys","Journal on Multimodal User Interfaces","","","10.1007/s12193-011-0089-x","","We present an application of sonification in an artistic context, namely to augment the visual impressions of train journeys. While sonification and auditory displays are typically used as means to present data and to inform the user, our project sound/tracks aims at enhancing the visual experience of looking out of a moving train’s window at the passing landscape by adding a sound dimension. This allows for reflecting upon the visual impressions and deepening the state of contemplation. To this end, sound/tracks translates the perceived movement of the scenery and other visual impressions, such as passing trains, into music. The continuously changing view outside the window is captured with a camera and transformed into instantaneously played music. The application can be run on mobile phones with a built-in camera and on laptops with a Web-cam. The paper proposes and discusses different sonification approaches and presents different application scenarios.","2012","2023-07-31 07:24:19","2023-07-31 07:24:19","","87-93","","1","6","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A3R4X7VZ","journalArticle","2017","Dyer, J. F.; Stapleton, P.; Rodger, M. W. M.","Advantages of melodic over rhythmic movement sonification in bimanual motor skill learning","Experimental Brain Research","","","10.1007/s00221-017-5047-8","","n important question for skill acquisition is whether and how augmented feedback can be designed to improve the learning of complex skills. Auditory information triggered by learners’ actions, movement sonification, can enhance learning of a complex bimanual coordination skill, specifically polyrhythmic bimanual shape tracing. However, it is not clear whether the coordination of polyrhythmic sequenced movements is enhanced by auditory-specified timing information alone or whether more complex sound mappings, such as melodic sonification, are necessary. Furthermore, while short-term retention of bimanual coordination performance has been shown with movement sonification training, longer term retention has yet to be demonstrated. In the present experiment, participants learned to trace a diamond shape with one hand while simultaneously tracing a triangle with the other to produce a sequenced 4:3 polyrhythmic timing pattern. Two groups of participants received real-time auditory feedback during training: melodic sonification (individual movements triggered a separate note of a melody) and rhythmic sonification (each movement triggered a percussive sound), while a third control group received no augmented feedback. Task acquisition and performance in immediate retention were superior in the melodic sonification group as compared to the rhythmic sonification and control group. In a 24-h retention phase, a decline in performance in the melodic sonification group was reversed by brief playback of the target pattern melody. These results show that melodic sonification of movement can provide advantages over augmented feedback which only provides timing information by better structuring the sequencing of timed actions, and also allow recovery of complex target patterns of movement after training. These findings have important implications for understanding the role of augmented perceptual information in skill learning, as well as its application to real-world training or rehabilitation scenarios.","2017","2023-07-31 07:24:19","2023-07-31 07:24:19","","3129-3140","","10","235","","Experimental Brain Research","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YPTKCM5G","journalArticle","2009","Kildal, Johan; Gross, Tom; Gulliksen, Jan; Kotzé, Paula; Oestreicher, Lars; Palanque, Philippe; Prates, Raquel Oliveira; Winckler, Marco","Aspects of Auditory Perception and Cognition for Usable Display Resolution in Data Sonification","Human-Computer Interaction – INTERACT 2009","","","","","Sonification of data via the mapping of values to frequency of sound is an auditory data analysis technique commonly used to display graph information. The goal for any form of graph is to display numerical information with accuracy and neutrality while exploiting perceptual and cognitive processes. Conveying information in frequency of sound is subject to aspects of pitch perception, largely overlooked to date, that can influence these properties of auditory graphing. This paper identifies some of these aspects and describes potential design limitations and opportunities derived from the musical nature of auditory data representations.","2009","2023-07-31 07:24:19","2023-07-31 07:24:19","","467-470","","","5726","","Human-Computer Interaction – INTERACT 2009","","","","","","","","","","","","","","","Place: Berlin, Heidelberg Publisher: Springer Berlin Heidelberg","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RU2PLIHJ","journalArticle","2017","Dyer, John; Stapleton, Paul; Rodger, Matthew","Transposing musical skill: sonification of movement as concurrent augmented feedback enhances learning in a bimanual task","Psychological Research","","","10.1007/s00426-016-0775-0","","Concurrent feedback provided during acquisition can enhance performance of novel tasks. The ‘guidance hypothesis’ predicts that feedback provision leads to dependence and poor performance in its absence. However, appropriately structured feedback information provided through sound (‘sonification’) may not be subject to this effect. We test this directly using a rhythmic bimanual shape-tracing task in which participants learned to move at a 4:3 timing ratio. Sonification of movement and demonstration was compared to two other learning conditions: (1) Sonification of task demonstration alone and (2) completely silent practice (control). Sonification of movement emerged as the most effective form of practice, reaching significantly lower error scores than control. Sonification of solely the demonstration, which was expected to benefit participants by perceptually unifying task requirements, did not lead to better performance than control. Good performance was maintained by participants in the Sonification condition in an immediate retention test without feedback, indicating that the use of this feedback can overcome the guidance effect. On a 24-h retention test, performance had declined and was equal between groups. We argue that this and similar findings in the feedback literature are best explained by an ecological approach to motor skill learning which places available perceptual information at the highest level of importance.","2017","2023-07-31 07:24:19","2023-07-31 07:24:19","","850-862","","4","81","","Psychological Research","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WACMABFA","journalArticle","2023","Toffa, O. K.; Mignotte, M.","Dataset and semantic based-approach for image sonification","Multimedia Tools and Applications","","","10.1007/s11042-022-12914-z","","This paper presents an image-audio dataset and a mid-level image sonification system that strives to help visually impaired users understand the semantic content of an image and access visual information via a combination of semantic audio and an easily decodable audio generated in real time, both triggered by sliding, taping, holding actions when the users explore the image on a touch screen or with a pointer. Firstly, we segmented the original image using a label fusion model and based on the user position in the image, a sonified signal is generated using musical notes and meaningful visual information within the active region like the color and the luminance, then the gradient and the texture. Secondly, we integrated the semantic understanding of the image into our model using DeepLab semantic segmentation of the image and created a dataset of audio and images aligned on the 20 classes of the PASCAL VOC 2012 dataset. The dataset of images are organized based on color, gradient, texture for low-level sonification and on semantic content with sounds for mid-level sonification. Thirdly, in order to provide both types of information in a complementary way, the slide, tap and hold actions of a touch screen are incorporated in the model. The semantic audio providing a brief description of the visual object is played on slide action, the generated signal with color details of the object on the tap action, gradient and texture of the object on hold action. Finally, we validated our sonification model on the provided dataset during a pilot study and the subjects were generally able to identify the objects in the image, the color of the objects and even provide a general description of the scene of the image. Our system could be useful to visually impaired persons in a photo sharing application using a smartphone or for painting art description in a digital museum.","2023","2023-07-31 07:24:19","2023-07-31 07:24:19","","1505-1518","","1","82","","Multimedia Tools and Applications","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3DWZYY4U","journalArticle","2022","Vishnevsky, Andrey; Abbas, Nadezda; Rocha, Alvaro; Adeli, Hojjat; Dzemyda, Gintautas; Moreira, Fernando","Sonification of Information Security Incidents in an Organization Using a Multistep Cooperative Game Model","Information Systems and Technologies","","","10.1007/978-3-031-04826-5_30","","This work is devoted to the development of computer attacks detection tool with a sound interface. Information security tools transmit visual signals to characterize the behavior of violators, but various types of conflict processes could be expressed by the plot of musical compositions. This approach could be used to encode the interaction of a protective computer system with an attacker in a harmonious way. In presented work, as an example of the conflict situation, was used a stochastic multistep cooperative game between the units of the attacked organization. An attempt to express audibly the stability of the cooperative game is made. It was realized with the help of the musical harmony of several voices of musical instruments. The program code for calculating the positional consistency of the proposed game-theoretic model and fragments of musical compositions for voicing the state of the protected organization are also proposed. Combining the basics of musical composition with game-theoretic modeling could offer a number of new possibilities for creating an ergonomic auditory human-computer interfaces.","2022","2023-07-31 07:24:19","2023-07-31 07:24:19","","306-314","","","","","Information Systems and Technologies","","","","","","","","","","","","","","","Publisher: Springer International Publishing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GWFWLB5U","journalArticle","2019","Rönnberg, Niklas","Sonification supports perception of brightness contrast","Journal on Multimodal User Interfaces","","","10.1007/s12193-019-00311-0","","In complex visual representations, there are several possible challenges for the visual perception that might be eased by adding sound as a second modality (i.e. sonification). It was hypothesized that sonification would support visual perception when facing challenges such as simultaneous brightness contrast or the Mach band phenomena. This hypothesis was investigated with an interactive sonification test, yielding objective measures (accuracy and response time) as well as subjective measures of sonification benefit. In the test, the participant’s task was to mark the vertical pixel line having the highest intensity level. This was done in a condition without sonification and in three conditions where the intensity level was mapped to different musical elements. The results showed that there was a benefit of sonification, with higher accuracy when sonification was used compared to no sonification. This result was also supported by the subjective measurement. The results also showed longer response times when sonification was used. This suggests that the use and processing of the additional information took more time, leading to longer response times but also higher accuracy. There were no differences between the three sonification conditions.","2019","2023-07-31 07:24:19","2023-07-31 07:24:19","","373-381","","4","13","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EHZ7BZJE","journalArticle","2006","Rutkowski, Tomasz M.; Vialatte, Francois; Cichocki, Andrzej; Mandic, Danilo P.; Barros, Allan Kardec; Gabrys, Bogdan; Howlett, Robert J.; Jain, Lakhmi C.","Auditory Feedback for Brain Computer Interface Management – An EEG Data Sonification Approach","Knowledge-Based Intelligent Information and Engineering Systems","","","10.1007/11893011_156","","An auditory feedback for Brain Computer Interface (BCI) applications is proposed. This is achieved based on the so-called sonification of the mental states of humans, captured by Electro-Encephalogram (EEG) recordings. Two time-frequency signal decomposition techniques, the Bump Modelling and Empirical Mode Decomposition (EMD), are used to map the EEG recordings onto musical scores. This auditory feedback proves to have extremely high potential in the development of on-line BCI interfaces. Examples based on the responses from visual stimuli support the analysis.","2006","2023-07-31 07:24:19","2023-07-31 07:24:19","","1232-1239","","","","","Knowledge-Based Intelligent Information and Engineering Systems","","","","","","","","","","","","","","","Publisher: Springer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KF3N3THJ","journalArticle","2019","Lorenzoni, Valerio; Van Den Berghe, Pieter; Maes, Pieter-Jan; De Bie, Tijl; De Clercq, Dirk; Leman, Marc","Design and validation of an auditory biofeedback system for modification of running parameters","Journal on Multimodal User Interfaces","","","10.1007/s12193-018-0283-1","","Real-time auditory feedback during sports activities is becoming increasingly popular in view of opportunities for monitoring and movement (re)training in ecological environments. However, the design of an effective feedback strategy is difficult. In this paper, we present a methodical approach to the design of an auditory feedback strategy for running gait modification of recreational runners, using distortion of a musical baseline. First tests were conducted to select the best performing auditory distortion signal in terms of clarity and level perception, and to derive the relative perception curve. This was found to be pink noise with an exponential response curve. Further tests were carried out to determine the just noticeable difference of this signal in actual running conditions. Finally, validation tests were performed to examine if the real-time auditory biofeedback, combined with music, could alter the runner’s steps per minute (SPM) during treadmill-based running. The results show that our sonification strategy can alter the mean running SPM in a clear and non-disturbing way, and that our noise-based continuous feedback approach performs better than standard verbal instructions. Even though some of the participants did not respond effectively to the feedback, a large majority of the participants rated the feedback system as pleasant and indicated that they would use such system to improve their running style.","2019","2023-07-31 07:24:19","2023-07-31 07:24:19","","167-180","","3","13","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RU7WNUJ3","journalArticle","2017","Temple, Mark D.","An auditory display tool for DNA sequence analysis","BMC Bioinformatics","","","10.1186/s12859-017-1632-x","","Background: DNA Sonification refers to the use of an auditory display to convey the information content of DNA sequence data. Six sonification algorithms are presented that each produce an auditory display. These algorithms are logically designed from the simple through to the more complex. Three of these parse individual nucleotides, nucleotide pairs or codons into musical notes to give rise to 4, 16 or 64 notes, respectively. Codons may also be parsed degenerately into 20 notes with respect to the genetic code. Lastly nucleotide pairs can be parsed as two separate frames or codons can be parsed as three reading frames giving rise to multiple streams of audio. Results: The most informative sonification algorithm reads the DNA sequence as codons in three reading frames to produce three concurrent streams of audio in an auditory display. This approach is advantageous since start and stop codons in either frame have a direct affect to start or stop the audio in that frame, leaving the other frames unaffected. Using these methods, DNA sequences such as open reading frames or repetitive DNA sequences can be distinguished from one another. These sonification tools are available through a webpage interface in which an input DNA sequence can be processed in real time to produce an auditory display playable directly within the browser. The potential of this approach as an analytical tool is discussed with reference to auditory displays derived from test sequences including simple nucleotide sequences, repetitive DNA sequences and coding or non-coding genes. Conclusion: This study presents a proof-of-concept that some properties of a DNA sequence can be identified through sonification alone and argues for their inclusion within the toolkit of DNA sequence browsers as an adjunct to existing visual and analytical tools.","2017","2023-07-31 07:24:19","2023-07-31 07:24:19","","221","","1","18","","BMC Bioinformatics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FU8BCDVK","journalArticle","2018","O’Brien, Benjamin; Juhas, Brett; Bieńkiewicz, Marta; Pruvost, Laurent; Buloup, Frank; Bringnoux, Lionel; Bourdin, Christophe; Aramaki, Mitsuko; Davies, Matthew E. P.; Kronland-Martinet, Richard; Ystad, Sølvi","Considerations for Developing Sound in Golf Putting Experiments","Music Technology with Swing","","","","","This chapter presents the core interests and challenges of using sound for learning motor skills and describes the development of sonification techniques for three separate golf-putting experiments. These studies are part of the ANR SoniMove project, which aims to develop new Human Machine Interfaces (HMI) that provide gestural control of sound in the areas of sports and music. After a brief introduction to sonification and sound-movement studies, the following addresses the ideas and sound synthesis techniques developed for each experiment.","2018","2023-07-31 07:24:19","2023-07-31 07:24:19","","338-358","","","11265","","Music Technology with Swing","","","","","","","","","","","","","","","Place: Cham Publisher: Springer International Publishing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9QXNX7U8","journalArticle","2022","Hamilton-Fletcher, Giles; Alvarez, James; Obrist, Marianna; Ward, Jamie","SoundSight: a mobile sensory substitution device that sonifies colour, distance, and temperature","Journal on Multimodal User Interfaces","","","10.1007/s12193-021-00376-w","","Abstract Depth, colour, and thermal images contain practical and actionable information for the blind. Conveying this information through alternative modalities such as audition creates new interaction possibilities for users as well as opportunities to study neuroplasticity. The ‘SoundSight’ App ( www.SoundSight.co.uk ) is a smartphone platform that allows 3D position, colour, and thermal information to directly control thousands of high-quality sounds in real-time to create completely unique and responsive soundscapes for the user. Users can select the specific sensor input and style of auditory output, which can be based on anything—tones, rainfall, speech, instruments, or even full musical tracks. Appropriate default settings for image-sonification are given by designers, but users still have a fine degree of control over the timing and selection of these sounds. Through utilising smartphone technology with a novel approach to sonification, the SoundSight App provides a cheap, widely accessible, scalable, and flexible sensory tool. In this paper we discuss common problems encountered with assistive sensory tools reaching long-term adoption, how our device seeks to address these problems, its theoretical background, its technical implementation, and finally we showcase both initial user experiences and a range of use case scenarios for scientists, artists, and the blind community.","2022","2023-07-31 07:24:19","2023-07-31 07:24:19","","107-123","","1","16","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3Y63BWBN","journalArticle","2013","Schmitz, Gerd; Mohammadi, Bahram; Hammer, Anke; Heldmann, Marcus; Samii, Amir; Münte, Thomas F; Effenberg, Alfred O","Observation of sonified movements engages a basal ganglia frontocortical network","BMC Neuroscience","","","10.1186/1471-2202-14-32","","Abstract Background Producing sounds by a musical instrument can lead to audiomotor coupling, i.e. the joint activation of the auditory and motor system, even when only one modality is probed. The sonification of otherwise mute movements by sounds based on kinematic parameters of the movement has been shown to improve motor performance and perception of movements. Results Here we demonstrate in a group of healthy young non-athletes that congruently (sounds match visual movement kinematics) vs. incongruently (no match) sonified breaststroke movements of a human avatar lead to better perceptual judgement of small differences in movement velocity. Moreover, functional magnetic resonance imaging revealed enhanced activity in superior and medial posterior temporal regions including the superior temporal sulcus, known as an important multisensory integration site, as well as the insula bilaterally and the precentral gyrus on the right side. Functional connectivity analysis revealed pronounced connectivity of the STS with the basal ganglia and thalamus as well as frontal motor regions for the congruent stimuli. This was not seen to the same extent for the incongruent stimuli. Conclusions We conclude that sonification of movements amplifies the activity of the human action observation system including subcortical structures of the motor loop. Sonification may thus be an important method to enhance training and therapy effects in sports science and neurological rehabilitation.","2013","2023-07-31 07:24:19","2023-07-31 07:24:19","","32","","1","14","","BMC Neuroscience","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GNXZCZUV","journalArticle","2007","DeWitt, Anna; Bresin, Roberto; Paiva, Ana C. R.; Prada, Rui; Picard, Rosalind W.; Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","Sound Design for Affective Interaction","Affective Computing and Intelligent Interaction","","","","","Different design approaches contributed to what we see today as the prevalent design paradigm for Human Computer Interaction; though they have been mostly applied to the visual aspect of interaction. In this paper we presented a proposal for sound design strategies that can be used in applications involving affective interaction. For testing our approach we propose the sonification of the Affective Diary, a digital diary with focus on emotions, affects, and bodily experience of the user. We applied results from studies in music and emotion to sonic interaction design. This is one of the first attempts introducing different physics-based models for the real-time complete sonification of an interactive user interface in portable devices.","2007","2023-07-31 07:24:19","2023-07-31 07:24:19","","523-533","","","4738","","Affective Computing and Intelligent Interaction","","","","","","","","","","","","","","","Place: Berlin, Heidelberg Publisher: Springer Berlin Heidelberg","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5HXZE4NH","journalArticle","2019","Ghai, Shashank; Ghai, Ishan","Effects of (music-based) rhythmic auditory cueing training on gait and posture post-stroke: A systematic review & dose-response meta-analysis","Scientific Reports","","","10.1038/s41598-019-38723-3","","Gait dysfunctions are common post-stroke. Rhythmic auditory cueing has been widely used in gait rehabilitation for movement disorders. However, a consensus regarding its influence on gait and postural recovery post-stroke is still warranted. A systematic review and meta-analysis was performed to analyze the effects of auditory cueing on gait and postural stability post-stroke. Nine academic databases were searched according to PRISMA guidelines. The eligibility criteria for the studies were a) studies were randomized controlled trials or controlled clinical trials published in English, German, Hindi, Punjabi or Korean languages b) studies evaluated the effects of auditory cueing on spatiotemporal gait and/or postural stability parameters post-stroke c) studies scored ≥4 points on the PEDro scale. Out of 1,471 records, 38 studies involving 968 patients were included in this present review. The review and meta-analyses revealed beneficial effects of training with auditory cueing on gait and postural stability. A training dosage of 20–45 minutes session, for 3–5 times a week enhanced gait performance, dynamic postural stability i.e. velocity (Hedge’s g: 0.73), stride length (0.58), cadence (0.75) and timed-up and go test (−0.76). This review strongly recommends the incorporation of rhythmic auditory cueing based training in gait and postural rehabilitation, post-stroke.","2019","2023-07-31 07:24:19","2023-07-31 07:24:19","","2183","","1","9","","Scientific Reports","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ASK5W3I8","journalArticle","2021","Takabatake, Kazuhiko; Kunii, Naoto; Nakatomi, Hirofumi; Shimada, Seijiro; Yanai, Kei; Takasago, Megumi; Saito, Nobuhito","Musical Auditory Alpha Wave Neurofeedback: Validation and Cognitive Perspectives","Applied Psychophysiology and Biofeedback","","","10.1007/s10484-021-09507-1","","Abstract Neurofeedback through visual, auditory, or tactile sensations improves cognitive functions and alters the activities of daily living. However, some people, such as children and the elderly, have difficulty concentrating on neurofeedback for a long time. Constant stressless neurofeedback for a long time may be achieved with auditory neurofeedback using music. The primary purpose of this study was to clarify whether music-based auditory neurofeedback increases the power of the alpha wave in healthy subjects. During neurofeedback, white noise was superimposed on classical music, with the noise level inversely correlating with normalized alpha wave power. This was a single-blind, randomized control crossover trial in which 10 healthy subjects underwent, in an assigned order, normal and random feedback (NF and RF), either of which was at least 4 weeks long. Cognitive functions were evaluated before, between, and after each neurofeedback period. The secondary purpose was to assess neurofeedback-induced changes in cognitive functions. A crossover analysis showed that normalized alpha-power was significantly higher in NF than in RF; therefore, music-based auditory neurofeedback facilitated alpha wave induction. A composite category-based analysis of cognitive functions revealed greater improvements in short-term memory in subjects whose alpha-power increased in response to NF. The present study employed a long period of auditory alpha neurofeedback and achieved successful alpha wave induction and subsequent improvements in cognitive functions. Although this was a pilot study that validated a music-based alpha neurofeedback system for healthy subjects, the results obtained are encouraging for those with difficulty in concentrating on conventional alpha neurofeedback. Trial registration: 2018077NI, date of registration: 2018/11/27","2021","2023-07-31 07:24:19","2023-07-31 07:24:19","","323-334","","4","46","","Applied Psychophysiology and Biofeedback","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TPYAQ5UP","journalArticle","2015","Sun, Yuanjing; Jeon, Myounghoon; Marcus, Aaron","Lyricon (Lyrics + Earcons) Improves Identification of Auditory Cues","Design, User Experience, and Usability: Users and Interactions","","","","","Auditory researchers have developed various non-speech cues in designing auditory user interfaces. A preliminary study of “lyricons” (lyrics + earcons [1]) has provided a novel approach to devising auditory cues in electronic products, by combining the concurrent two layers of musical speech and earcons (short musical motives). An experiment on sound-function meaning mapping was conducted between earcons and lyricons. It demonstrated that lyricons significantly more enhanced the relevance between the sound and the meaning compared to earcons. Further analyses on error type and confusion matrix show that lyricons showed a higher identification rate and a shorter mapping time than earcons. Factors affecting auditory cue identification and application directions of lyricons are discussed.","2015","2023-07-31 07:24:19","2023-07-31 07:24:19","","382-389","","","9187","","Design, User Experience, and Usability: Users and Interactions","","","","","","","","","","","","","","","Place: Cham Publisher: Springer International Publishing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P92K5UUP","journalArticle","2018","Black, David; Hahn, Horst K.; Kikinis, Ron; Wårdell, Karin; Haj-Hosseini, Neda","Auditory display for fluorescence-guided open brain tumor surgery","International Journal of Computer Assisted Radiology and Surgery","","","10.1007/s11548-017-1667-5","","Purpose—Protoporphyrin (PpIX) fluorescence allows discrimination of tumor and normal brain tissue during neurosurgery. A handheld fluorescence (HHF) probe can be used for spectroscopic measurement of 5-ALA-induced PpIX to enable objective detection compared to visual evaluation of fluorescence. However, current technology requires that the surgeon either views the measured values on a screen or employs an assistant to verbally relay the values. An auditory feedback system was developed and evaluated for communicating measured fluorescence intensity values directly to the surgeon. Methods—The auditory display was programmed to map the values measured by the HHF probe to the playback of tones that represented three fluorescence intensity ranges and one error signal. Ten persons with no previous knowledge of the application took part in a laboratory evaluation. After a brief training period, participants performed measurements on a tray of 96 wells of liquid fluorescence phantom and verbally stated the perceived measurement values for each well. The latency and accuracy of the participants’ verbal responses were recorded. The long-term memorization of sound function was evaluated in a second set of 10 participants 2–3 and 712 days after training. Results—The participants identified the played tone accurately for 98% of measurements after training. The median response time to verbally identify the played tones was 2 pulses. No correlation was found between the latency and accuracy of the responses, and no significant correlation with the musical proficiency of the participants was observed on the function responses. Responses for the memory test were 100% accurate. Conclusion—The employed auditory display was shown to be intuitive, easy to learn and remember, fast to recognize, and accurate in providing users with measurements of fluorescence intensity or error signal. The results of this work establish a basis for implementing and further evaluating auditory displays in clinical scenarios involving fluorescence guidance and other areas for which categorized auditory display could be useful.","2018","2023-07-31 07:24:19","2023-07-31 07:24:19","","25-35","","1","13","","International Journal of Computer Assisted Radiology and Surgery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H5TKWRVQ","journalArticle","2009","Bologna, Guido; Deville, Benoît; Pun, Thierry; Mira, José; Ferrández, José Manuel; Álvarez, José R.; De La Paz, Félix; Toledo, F. Javier","Blind Navigation along a Sinuous Path by Means of the See ColOr Interface","Bioinspired Applications in Artificial and Natural Computation","","","","","The See ColOr interface transforms a small portion of a coloured video image into sound sources represented by spatialized musical instruments. This interface aims at providing visually impaired people with a capability of perception of the environment. In this work, the purpose is to verify the hypothesis that it is possible to use sounds from musical instruments to replace colour. Compared to state of the art devices, a quality of the See ColOr interface is that it allows the user to receive a feed-back auditory signal from the environment and its colours, promptly. An experiment based on a head mounted camera has been performed. Specifically, this experiment is related to outdoor navigation for which the purpose is to follow a sinuous path. Our participants successfully went along a red serpentine path for more than 80 meters.","2009","2023-07-31 07:24:19","2023-07-31 07:24:19","","235-243","","","5602","","Bioinspired Applications in Artificial and Natural Computation","","","","","","","","","","","","","","","Place: Berlin, Heidelberg Publisher: Springer Berlin Heidelberg","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LM4PY69H","journalArticle","2014","Kiriella, Dawpadee B.; Kumari, Shyama C.; Ranasinghe, Kavindu C.; Jayaratne, Lakshman","Music Training Interface for Visually Impaired through a Novel Approach to Optical Music Recognition","GSTF Journal on Computing (JoC)","","","10.7603/s40601-013-0045-6","","Abstract Some inherited barriers which limits the human abilities can be surprisingly win through technology. This research focuses on defining a more reliable and a controllable interface for visually impaired people to read and study eastern music notations which are widely available in printed format. One of another concept behind was that differently-abled people should be assisted in a way which they can proceed interested tasks in an independent way. The research provide means to continue on researching the validity of using a controllable auditory interface instead using Braille music scripts converted with the help of 3 rd parties. The research further summarizes the requirements aroused by the relevant users, design considerations, evaluation results on user feedbacks of proposed interface.","2014","2023-07-31 07:24:19","2023-07-31 07:24:19","","45","","4","3","","GSTF Journal on Computing (JoC)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NTC63DAU","journalArticle","1998","Rigas, Dimitrios I.; Alty, James L.; Johnson, Hilary; Nigay, Lawrence; Roast, Christopher","How Can Multimedia Designers Utilize Timbre?","People and Computers XIII","","","","","When musical sound is required during development of auditory or multimedia interfaces, designers often need to utilize different musical voices or timbre (usually produced via a multiple timbre synthesizer or a sound card) in order to communicate information. Currently, there is a limited set of guidelines assisting multimedia designers to select appropriate timbre. This paper reports a set of recall and recognition experiments on timbres produced by a multiple timbre synthesizer. Results indicate that a number of instruments were successfully recalled and recognized. A set of empirically derived guidelines are suggested to assist multimedia designers in selecting timbre.","1998","2023-07-31 07:24:19","2023-07-31 07:24:19","","273-286","","","","","People and Computers XIII","","","","","","","","","","","","","","","Place: London Publisher: Springer London","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NI4NENJ8","journalArticle","2011","Rutkowski, Tomasz M.; Cooper, Eric W.; Kryssanov, Victor V.; Ogawa, Hitoshi; Brewster, Stephen","Auditory Brain-Computer/Machine-Interface Paradigms Design","Haptic and Audio Interaction Design","","","","","The paper discusses novel and interesting, from users’ point of view, design of auditory brain-computer/machine interfaces (BCI/ BMI) utilizing human auditory responses. Two concepts of auditory stimuli BCI/BMI are presented. The first paradigm is based on steady-state tonal or musical stimuli yielding satisfactory EEG response classification for several seconds long stimuli. The second discussed paradigm is based on spatial sound localization and the brain evoked responses estimation, requiring shorter than a second stimuli presentation. In conclusion the preliminary results are discussed and suggestions for further applications are drawn.","2011","2023-07-31 07:24:19","2023-07-31 07:24:19","","110-119","","","6851","","Haptic and Audio Interaction Design","","","","","","","","","","","","","","","Place: Berlin, Heidelberg Publisher: Springer Berlin Heidelberg","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MQ5Y784Z","journalArticle","1999","Barrass, Stephen; Kramer, Gregory","Using sonification","Multimedia Systems","","","10.1007/s005300050108","","The idea behind sonification is that synthetic non-verbal sounds can represent numerical data and provide support for information processing activities of many different kinds. This article describes some of the ways that sonification has been used in assistive technologies, remote collaboration, engineering analyses, scientific visualisations, emergency services and aircraft cockpits. Approaches for designing sonifications are surveyed, and issues raised by the existing approaches and applications are outlined. Relations are drawn to other areas of knowledge where similar issues have also arisen, such as human-computer interaction, scientific visualisation, and computer music. At the end is a list of resources that will help you delve further into the topic.","1999","2023-07-31 07:24:19","2023-07-31 07:24:19","","23-31","","1","7","","Multimedia Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YLAP7WTV","journalArticle","2018","Matinfar, Sasan; Nasseri, M. Ali; Eck, Ulrich; Kowalsky, Michael; Roodaki, Hessam; Navab, Navid; Lohmann, Chris P.; Maier, Mathias; Navab, Nassir","Surgical soundtracks: automatic acoustic augmentation of surgical procedures","International Journal of Computer Assisted Radiology and Surgery","","","10.1007/s11548-018-1827-2","","Purpose Advances in sensing and digitalization enable us to acquire and present various heterogeneous datasets to enhance clinical decisions. Visual feedback is the dominant way of conveying such information. However, environments rich with many sources of information all presented through the same channel pose the risk of over stimulation and missing crucial information. The augmentation of the cognitive field by additional perceptual modalities such as sound is a workaround to this problem. A major challenge in auditory augmentation is the automatic generation of pleasant and ergonomic audio in complex routines, as opposed to overly simplistic feedback, to avoid alarm fatigue. Methods In this work, without loss of generality to other procedures, we propose a method for aural augmentation of medical procedures via automatic modification of musical pieces. Results Evaluations of this concept regarding recognizability of the conveyed information along with qualitative aesthetics show the potential of our method. Conclusion In this paper, we proposed a novel sonification method for automatic musical augmentation of tasks within surgical procedures. Our experimental results suggest that these augmentations are aesthetically pleasing and have the potential to successfully convey useful information. This work opens a path for advanced sonification techniques in the operating room, in order to complement traditional visual displays and convey information more efficiently.","2018","2023-07-31 07:24:19","2023-07-31 07:24:19","","1345-1355","","9","13","","International Journal of Computer Assisted Radiology and Surgery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2LWL39BX","journalArticle","2018","Seiça, Mariana; Lopes, Rui; Martins, Pedro; Cardoso, F. Amílcar; Aramaki, Mitsuko; Davies, Matthew E. P.; Kronland-Martinet, Richard; Ystad, Sølvi","Sonifying Twitter’s Emotions Through Music","Music Technology with Swing","","","","","Sonification is a scientific field that seeks to explore the potential of sound as an instrument to convey and interpret data. Its techniques have been developing significantly with the growth of technology and supporting hardware and software, which have spread in our daily environment. This allowed the establishment of new communication tools to share information, opinion and feelings as part of our daily routine. The aim of this project was to unite the social media phenomena with sonification, using Twitter data to extract user’s emotions and translate them into musical compositions. The focus was to explore the potential of music in translating data as personal and subjective as human emotions, developing a musically complex and captivating mapping based on the rules of Western Music. The music is accompanied by a simple visualization, which results in emotions being heard and seen with the corresponding tweets, in a multimodal experience that represents Twitter’s emotional reality. The mapping was tested through an online survey, and despite a few misunderstandings, the results were generally positive, expressing the efficiency and impact of the developed system.","2018","2023-07-31 07:24:19","2023-07-31 07:24:19","","586-608","","","11265","","Music Technology with Swing","","","","","","","","","","","","","","","Place: Cham Publisher: Springer International Publishing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6F7AL4KP","journalArticle","2020","Roddy, Stephen; Bridges, Brian","Mapping for meaning: the embodied sonification listening model and its implications for the mapping problem in sonic information design","Journal on Multimodal User Interfaces","","","10.1007/s12193-020-00318-y","","This is a theoretical paper that considers the mapping problem, a foundational issue which arises when designing a sonification, as it applies to sonic information design. We argue that this problem can be addressed by using models from the field of embodied cognitive science, including embodied image schema theory, conceptual metaphor theory and conceptual blends, and from research which treats sound and musical structures using these models, when mapping data to sound. However, there are currently very few theoretical frameworks for applying embodied cognition principles in a sonic information design context. This article describes one such framework, the Embodied Sonification Listening Model, which provides a theoretical description of sonification listening in terms of Conceptual Metaphor Theory.","2020","2023-07-31 07:24:19","2023-07-31 07:24:19","","143-151","","2","14","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JXDHYIQQ","journalArticle","2001","Johannsen, Gunnar","Auditory Displays in Human–Machine Interfaces of Mobile Robots for Non-Speech Communication with Humans","Journal of Intelligent and Robotic Systems","","","10.1023/A:1013953213049","","Auditory displays are developed and investigated for mobile service robots in a human–machine environment. The service robot domain was chosen as an example for future use of auditory displays within multimedia process supervision and control applications in industrial, transportation, and medical systems. The design of directional sounds and of additional sounds for robot states as well as the design of more complicated robot sound tracks are explained. Basic musical elements and robot-movement sounds are combined. Experimental studies on the auditory perception of directional sounds as well as of sound tracks for the predictive display of intended robot trajectories in a simulated supermarket scenario are described.","2001","2023-07-31 07:24:19","2023-07-31 07:24:19","","161-169","","2","32","","Journal of Intelligent and Robotic Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MUZYGURP","journalArticle","2006","Vialatte, François B.; Cichocki, Andrzej; King, Irwin; Wang, Jun; Chan, Lai-Wan; Wang, DeLiang; Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","Sparse Bump Sonification: A New Tool for Multichannel EEG Diagnosis of Mental Disorders; Application to the Detection of the Early Stage of Alzheimer’s Disease","Neural Information Processing","","","","","This paper investigates the use of sound and music as a means of representing and analyzing multichannel EEG recordings. Specific focus is given to applications in early detection and diagnosis of early stage of Alzheimer’s disease. We propose here a novel approach based on multi channel sonification, with a time-frequency representation and sparsification process using bump modeling. The fundamental question explored in this paper is whether clinically valuable information, not available from the conventional graphical EEG representation, might become apparent through an audio representation. Preliminary evaluation of the obtained music score – by sample entropy, number of notes, and synchronous activity – incurs promising results.","2006","2023-07-31 07:24:19","2023-07-31 07:24:19","","92-101","","","4234","","Neural Information Processing","","","","","","","","","","","","","","","Place: Berlin, Heidelberg Publisher: Springer Berlin Heidelberg","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MBHP3J7T","journalArticle","2022","Bruder, Alexandra L.; Rothwell, Clayton D.; Fuhr, Laura I.; Shotwell, Matthew S.; Edworthy, Judy Reed; Schlesinger, Joseph J.","The Influence of Audible Alarm Loudness and Type on Clinical Multitasking","Journal of Medical Systems","","","10.1007/s10916-021-01794-9","","In high-consequence industries such as health care, auditory alarms are an important aspect of an informatics system that monitors patients and alerts providers attending to multiple concurrent tasks. Alarms levels are unnecessarily high and alarm signals are uninformative. In a laboratory-based task setting, we studied 25 anesthesiology residents’ responses to auditory alarms in a multitasking paradigm comprised of three tasks: patient monitoring, speech perception/intelligibility, and visual vigilance. These tasks were in the presence of background noise plus/minus music, which served as an attention-diverting stimulus. Alarms signified clinical decompensation and were either conventional alarms or a novel informative auditory icon alarm. Both alarms were presented at four different levels. Task performance (accuracy and response times) were analyzed using logistic and linear mixed-effects regression. Salient findings were 1), the icon alarm had similar performance to the conventional alarm at a +2 dB signal-to-noise-ratio (SNR) (accuracy: OR 1.21 (95% CI 0.88, 1.67), response time: 0.04 s at 2 dB (95% CI: –0.16, 0.24), which is a much lower level than current clinical environments; 2) the icon alarm was associated with 27% greater odds (95% CI: 18%, 37%) of correctly addressing the vigilance task, regardless of alarm SNR, suggesting crossmodal/multisensory multitasking benefits; and 3) compared to the conventional alarm, the icon alarm was associated with an absolute improvement in speech perception of 4% in the presence of an attention-diverting auditory stimulus (p = 0.031). These findings suggest that auditory icons can provide multitasking benefits in cognitively demanding clinical environments.","2022","2023-07-31 07:24:19","2023-07-31 07:24:19","","5","","1","46","","Journal of Medical Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y8KUQ62W","journalArticle","2006","Chen, Xiaoyu; Tremaine, Marilyn; Lutz, Robert; Chung, Jae-woo; Lacsina, Patrick","AudioBrowser: a mobile browsable information access for the visually impaired","Universal Access in the Information Society","","","10.1007/s10209-006-0019-y","","Although a large amount of research has been conducted on building interfaces for the visually impaired that allows users to read web pages and generate and access information on computers, little development addresses two problems faced by the blind users. First, sighted users can rapidly browse and select information they find useful, and second, sighted users can make much useful information portable through the recent proliferation of personal digital assistants (PDAs). These possibilities are not currently available for blind users. This paper describes an interface that has been built on a standard PDA and allows its user to browse the information stored on it through a combination of screen touches coupled with auditory feedback. The system also supports the storage and management of personal information so that addresses, music, directions, and other supportive information can be readily created and then accessed anytime and anywhere by the PDA user. The paper describes the system along with the related design choices and design rationale. A user study is also reported.","2006","2023-07-31 07:24:19","2023-07-31 07:24:19","","4-22","","1","5","","Universal Access in the Information Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QNQNZ4ZT","journalArticle","2015","Huang, Chih-Fang; Nien, Wei-Po","A sonification system based on geographic and meteorologic data","2015 8th International Conference on Ubi-Media Computing (UMEDIA)","","","10.1109/UMEDIA.2015.7297465","","Most algorithmic composition system is implemented based on the input of music parameters, which needs professional music and technology training to complete the automated composition. This paper proposed an innovated way called Geographic and Meteorologic Data Sonification System (GMDSS), to perform the data mappings into music for various devices such as hand phone or other hand-hold devices, and the driver can retrieve the environment information in real time to let people concentrate on driving without the need of paying attention to the long-term complicated data visualization. The mapping relationship between geographic/meteorologic data and music features is discussed, and the result shows the implementation of GIS and weather data which can compose the proper correspondent music accordingly.","2015","2023-07-31 07:24:19","2023-07-31 07:24:19","","259-262","","","","","2015 8th International Conference on Ubi-Media Computing (UMEDIA)","","","","","","","","","","","","","","","Publisher: IEEE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DAB6AJMW","journalArticle","2020","Zahray, Lisa; Savery, Richard; Syrkett, Liana; Weinberg, Gil","Robot Gesture Sonification to Enhance Awareness of Robot Status and Enjoyment of Interaction","2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","","","10.1109/RO-MAN47096.2020.9223452","","We present a divergent approach to robotic soniﬁcation with the goal of improving the quality and safety of human-robot interactions. Soniﬁcation (turning data into sound) has been underutilized in robotics, and has broad potential to convey robotic movement and intentions to users without requiring visual engagement. We design and evaluate six different soniﬁcations of movements for a robot with four degrees of freedom. Our soniﬁcation techniques include a direct mapping from each degree of freedom to pitch and timbre changes, emotion-based sound mappings, and velocity-based mappings using different types of sounds such as motors and music. We evaluate these soniﬁcations using metrics for ease of use, enjoyment/appeal, and conveyance of movement information. Based on our results, we make recommendations to inform decisions for future robot soniﬁcation design. We suggest that when using soniﬁcation to improve safety of human-robot collaboration, it is necessary not only to convey sufﬁcient information about movements, but also to convey that information in a pleasing and even social way to to enhance the human-robot relationship.","2020","2023-07-31 07:24:19","2023-07-31 07:24:19","","978-985","","","","","2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","","","","","","","","","","","","","","","Publisher: IEEE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W4EGFIWH","journalArticle","2019","Colombo, R.; Raglio, A.; Panigazzi, M.; Mazzone, A.; Bazzini, G.; Imarisio, C.; Molteni, D.; Caltagirone, C.; Imbriani, M.","The SonicHand Protocol for Rehabilitation of Hand Motor Function: A Validation and Feasibility Study","IEEE Transactions on Neural Systems and Rehabilitation Engineering","","","10.1109/TNSRE.2019.2905076","","Musical sonification therapy is a new technique that can reinforce conventional rehabilitation treatments by increasing therapy intensity and engagement through challenging and motivating exercises. The aim of this paper is to evaluate the feasibility and validity of the SonicHand protocol, a new training and assessment method for the rehabilitation of hand function. The study was conducted in 15 healthy individuals and 15 stroke patients. The feasibility of implementation of the training protocol was tested in stroke patients only, who practiced a series of exercises concurrently to music sequences produced by specific movements. The assessment protocol evaluated hand motor performance during pronation/supination, wrist horizontal flexion/extension, and hand grasp without sonification. From hand position data, 15 quantitative parameters were computed evaluating mean velocity, movement smoothness, and angular excursions of hand/fingers. We validated this assessment in terms of its ability to discriminate between patients and healthy subjects, test-retest reliability and concurrent validity with the upper limb section of the Fugl-Meyer scale (FM), the functional independence measure (FIM), and the Box and Block Test (BBT). All patients showed a good understanding of the assigned tasks and were able to correctly execute the proposed training protocol, confirming its feasibility. A moderateto-excellent intraclass correlation coefficient was found in 8/15 computed parameters. The moderate-to-strong correlation was found between the measured parameters and the clinical scales. The SonicHand training protocol is feasible and the assessment protocol showed good to excellent between-group discrimination ability, reliability, and concurrent validity, thus enabling the implementation of new personalized and motivating training programs employing sonification for the rehabilitation of hand function.","2019","2023-07-31 07:24:19","2023-07-31 07:24:19","","664-672","","4","27","","IEEE Transactions on Neural Systems and Rehabilitation Engineering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EJJX8RRP","journalArticle","2012","Dailly, Anabel Immoos; Sigrist, Roland; Kim, Yeongmi; Wolf, Peter; Erckens, Hendrik; Cerny, Joachim; Luft, Andreas; Gassert, Roger; Sulzer, James","Can simple error sonification in combination with music help improve accuracy in upper limb movements?","2012 4th IEEE RAS & EMBS International Conference on Biomedical Robotics and Biomechatronics (BioRob)","","","10.1109/BioRob.2012.6290908","","While repetitive training is widely regarded to be a useful rehabilitation strategy, such training requires motivation that may be lacking. In order to improve motivation in a potentially inexpensive and simple manner, we introduce in this proof-of-concept study a combination of error sonification and music for upper limb training. Twelve healthy participants trained a figure tracing task for the upper limb, six receiving feedback in terms of error sonification and music and six without receiving feedback in the control group. The error-sonified feedback group decreased its amount of error significantly compared to the control group. Thus this particular paradigm can help teach planar reaching movements. Eventually this paradigm may become simple and useful enough to enhance existing therapeutic intervention in stroke rehabilitation.","2012","2023-07-31 07:24:19","2023-07-31 07:24:19","","1423-1427","","","","","2012 4th IEEE RAS & EMBS International Conference on Biomedical Robotics and Biomechatronics (BioRob)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LVD3NHZV","journalArticle","2019","Giariskanis, Fotis; Parthenios, Panagiotis; Mania, Katerina","ARCHIMUSIC3D: Multimodal Playful Transformations between Music and Refined Urban Architectural Design","2019 11th International Conference on Virtual Worlds and Games for Serious Applications (VS-Games)","","","10.1109/VS-Games.2019.8864525","","The commonly used 3D architectural design tools for urban environments fail to capture aspects of an urban design which are aesthetic as well as functional. This paper describes an innovative multimodal user interface through which an urban designer can work on the music transcription of a specific urban environment applying music compositional rules and filters in order to identify discordant entities, highlight imbalanced parts and make design corrections. The proposed platform offers sonification of an Urban Virtual Environment (UVE), simulating a real-world cityscape, offering visual interpretation and musically playful modification of its soundscape. The system presented offers: The ability to view and convert an urban street to music (ready to play) based on a specific grammar of converting architectural elements to musical elements; secondly, the ability to transform this music in order to `harmonize' it based on musical rules as if composing and playing music; and finally, the prospect of converting back the aesthetically and harmonically “corrected” musical piece to a newly refined street or urban design, visualized in 3D. The presented platform comprises of three scenes, which compile the three main parts of the system's multimodal interface; e.g., the 3D scene, the Digital Audio Workstation (DAW) scene and the TouchOSC mobile controller. The purpose of this paper is to assist architects and urban designers in 1) identifying urban dissonances, 2) refining their design using musical rules and 3) interactively presenting the output both visually and acoustically.","2019","2023-07-31 07:24:19","2023-07-31 07:24:19","","1-4","","","","","2019 11th International Conference on Virtual Worlds and Games for Serious Applications (VS-Games)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V4C2UU2G","journalArticle","2015","Turchet, Luca; Bresin, Roberto","Effects of Interactive Sonification on Emotionally Expressive Walking Styles","IEEE Transactions on Affective Computing","","","10.1109/TAFFC.2015.2416724","","This paper describes two experiments conducted to investigate the role of sonically simulated ground materials in modulating both production and recognition of walks performed with emotional intentions. The results of the first experiment showed that the involved auditory feedbacks affected the pattern of emotional walking in different ways, although such an influence manifested itself in more than one direction. The results of the second experiment showed the absence of an influence of the sound conditions on the recognition of the emotions from acoustic information alone. Similar results were found in both experiments for musically-trained and untrained participants. Our results suggest that tempo and sound level are two acoustical features important in both production and recognition of emotions in walking. In addition, the similarities of the presented results with those reported in the music performance domain, as well as the absence of an influence of musical expertise lend support to the “motor origin hypothesis of emotional expression in music” according to which a motor origin for the expression of emotions is common in all those domains of human activity that result in the generation of an acoustical signal.","2015","2023-07-31 07:24:19","2023-07-31 07:24:19","","152-164","","2","6","","IEEE Transactions on Affective Computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HGYD829R","journalArticle","2004","Matta, S.; Kumar, D.K.; Yu, Xinghuo; Burry, M.","An approach for image sonification","First International Symposium on Control, Communications and Signal Processing, 2004.","","","10.1109/ISCCSP.2004.1296321","","This paper presents a new approach for image to sound mapping. The proposed method utilizes the music parameters such as pitch and rhythm to support translation of images into sounds. Many people have tried image-to-sound mapping or data-to-sound mapping and failed to prove the useful results and many people haven't followed the principles of psychoacoustics in implementing image to sound conversion methods. The important bottleneck in these kinds of experiments is that humans can't remember the normal sounds as compared to music. A method is developed to overcome this bottleneck by utilizing musical parameters. Most of the available tools have been tested on the participants and it has been discovered that the technology available to convert data streams into sounds was not sufficient and needed an improvement.","2004","2023-07-31 07:24:19","2023-07-31 07:24:19","","431-434","","","","","First International Symposium on Control, Communications and Signal Processing, 2004.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FAAKGWJE","journalArticle","2000","Ballora, M.; Pennycook, B.; Ivanov, P.Ch.; Goldberger, A.; Glass, L.","Detection of obstructive sleep apnea through auditory display of heart rate variability","Computers in Cardiology 2000. Vol.27 (Cat. 00CH37163)","","","10.1109/CIC.2000.898630","","A data set of interbeat interval times is read into a music software synthesis program to become the basis of a ""soundtrack"" or auditory display. Here the authors present examples of the sonifications, and discuss potential advantages in listening to, as opposed to viewing, heart rate variability data. This method treats the diagnosis of obstructive sleep apnea as a problem of orchestration and melody.","2000","2023-07-31 07:24:19","2023-07-31 07:24:19","","739-740","","","","","Computers in Cardiology 2000. Vol.27 (Cat. 00CH37163)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NI6Z59IT","journalArticle","2018","Maçãs, Catarina; Martins, Pedro; Machado, Penousal","Consumption as a Rhythm: A Multimodal Experiment on the Representation of Time-Series","2018 22nd International Conference Information Visualisation (IV)","","","10.1109/iV.2018.00093","","Through Data Visualisation and Sonification models, we present a study of multimodal representations to characterise the Portuguese consumption patterns, which were gathered from Portuguese hypermarkets and supermarkets over the course of two years. We focus on the rhythmic nature of the data to create and discuss audio and visual representations that highlight disruptions and sudden changes in the normal consumption patterns. For this study, we present two distinct visual and audio representations and discuss their strengths and limitations.","2018","2023-07-31 07:24:19","2023-07-31 07:24:19","","504-509","","","","","2018 22nd International Conference Information Visualisation (IV)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2BCCJ4XZ","journalArticle","2006","Kori, H.; Tezuka, T.; Tanaka, K.","Ranking of Regional Blogs by Suitability for Sonification","22nd International Conference on Data Engineering Workshops (ICDEW'06)","","","10.1109/ICDEW.2006.125","","Media content presented to a vehicle driver is mainly auditory, since visual content is distracting and viewing it increases the risk of an accident. Music and radio are thus commonly listened to while driving. However, these types of content rarely reflect regional characteristics and are therefore not well suited for tourists who want to get information about the region they are visiting. We have developed the Blog Car Radio system that presents blog entries in auditory style using sonification (speech synthesis). Blog entries are obtained from blog search engines, selected by distances from the vehicle’s current location, and ranked based on their suitability for sonification and relevance to the userspecified category. By using Blog Car Radio, a driver can obtain local information with only a small amount of distraction. In this paper, we particularly discussed the method to rank text contents which is suitable for sonification.","2006","2023-07-31 07:24:19","2023-07-31 07:24:19","","x132-x132","","","","","22nd International Conference on Data Engineering Workshops (ICDEW'06)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N7MQAPJZ","journalArticle","2004","Hermann, T.; Ritter, H.","Sound and meaning in auditory data display","Proceedings of the IEEE","","","10.1109/JPROC.2004.825904","","Auditory data display is an interdisciplinary field linking auditory perception research, sound engineering, data mining, and human-computer interaction in order to make semantic contents of data perceptually accessible in the form of (nonverbal) audible sound. For this goal it is important to understand the different ways in which sound can encode meaning. We discuss this issue from the perspectives of language, music, functionality, listening modes, and physics, and point out some limitations of current techniques for auditory data display, in particular when targeting high-dimensional data sets. As a promising, potentially very widely applicable approach, we discuss the method of model-based sonification (MBS) introduced recently by the authors and point out how its natural semantic grounding in the physics of a sound generation process supports the design of sonifications that are accessible even to untrained, everyday listening. We then proceed to show that MBS also facilitates the design of an intuitive, active navigation through ""acoustic aspects"", somewhat analogous to the use of successive two-dimensional views in three-dimensional visualization. Finally, we illustrate the concept with a first prototype of a ""tangible"" sonification interface which allows us to ""perceptually map"" sonification responses into active exploratory hand motions of a user, and give an outlook on some planned extensions.","2004","2023-07-31 07:24:19","2023-07-31 07:24:19","","730-741","","4","92","","Proceedings of the IEEE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4C49LPJN","journalArticle","2003","Osmanovic, N.; Hrustemovic, N.; Myler, H.R.","A testbed for auralization of graphic art","Annual Technical Conference IEEE Region 5, 2003","","","10.1109/REG5.2003.1199709","","Sonification is the use of non-speech audio to convey information and auralization, a similar but distinct area, is an aural metaphor for visualization - the process by which objects and scenes are interpreted by the human visual system. A fundamental goal of this study is to produce a mechanism by which the visually impaired can experience the artistic content of images using an alternate modality, hearing. In this project, the auralization of images using music is explored. A software algorithm to map pixel data from images to corresponding sound sequences is presented. This conversion is based on a frequency relationship that exists between color and sound in human perception. For example, the frequency of the G tone at 392 Hz, 40 times redoubled (octaved), delivers the frequency of the color red (431 /spl times/ 10/sup 12/ Hz). In addition, the volume of the mapped sounds is used to represent color intensity.","2003","2023-07-31 07:24:19","2023-07-31 07:24:19","","45-49","","","","","Annual Technical Conference IEEE Region 5, 2003","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7NBL5AK2","journalArticle","2001","Rigas, D.; Memery, D.; Yu, H.","Experiments in using structured musical sound, synthesised speech and environmental stimuli to communicate information: is there a case for integration and synergy?","Proceedings of 2001 International Symposium on Intelligent Multimedia, Video and Speech Processing. ISIMP 2001 (IEEE Cat. No.01EX489)","","","10.1109/ISIMP.2001.925434","","This paper describes three sets of experiments in auditory information processing using structured musical sound, synthesised speech and environmental stimuli or special effects. The first experiment examines auditory information processing of small sequences of rhythmic musical tones. The second experiment examines auditory information processing of some environmental sounds. The third experiment examines auditory information processing when sound and synthesised speech are simultaneously presented. The results of this investigation aim to help multimedia and user interface developers to design auditory messages that incorporate structured musical stimuli, synthesised speech and environmental sounds or special effects. These type of auditory messages can complement visual displays or communicate information on their own in auditory interfaces. The results of these initial experiments indicate a prima facie case for integrating various types of auditory stimuli in one single message. The simultaneous presentation of sound and speech was easily recognised and interpreted. The paper concludes with some initial practical guidelines for designers and a discussion of further work.","2001","2023-07-31 07:24:19","2023-07-31 07:24:19","","465-468","","","","","Proceedings of 2001 International Symposium on Intelligent Multimedia, Video and Speech Processing. ISIMP 2001 (IEEE Cat. No.01EX489)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"94GLN9L6","journalArticle","2021","Rönnberg, Niklas","Sonification for Conveying Data and Emotion","Proceedings of the 16th International Audio Mostly Conference","","","10.1145/3478384.3478387","","In the present study a sonification of running data was evaluated. The aim of the sonification was to both convey information about the data and convey a specific emotion. The sonification was evaluated in three parts, firstly as an auditory graph, secondly together with additional text information, and thirdly together with an animated visualization, with a total of 150 responses. The results suggest that the sonification could convey an emotion similar to that intended, but at the cost of less good representation of the data. The addition of visual information supported understanding of the sonification, and the auditory representation of data. The results thus suggest that it is possible to design sonification that is perceived as both interesting and fun, and convey an emotional impression, but that there may be a trade off between musical experience and clarity in sonification.","2021","2023-07-31 07:24:19","2023-07-31 07:24:19","","56–63","","","","","Proceedings of the 16th International Audio Mostly Conference","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NJUVR8BP","journalArticle","2008","Last, Mark; Gorelik, Anna","Using sonification for mining time series data","Proceedings of the 9th International Workshop on Multimedia Data Mining: held in conjunction with the ACM SIGKDD 2008","","","10.1145/1509212.1509220","","In recent years, there is a growing interest in mining time series databases by both automated and interactive tools. In this paper, we present an interactive methodology for mining of time series data using a novel sonification technique which uses some important properties of time series and tonal music to achieve effective (accurate) and efficient (fast) results. We have created an experimental website, where participants were asked to perform some basic data exploration and mining tasks by listening to a musical display of several time series. The initial results indicate that the proposed methodology for musical representation of data allows, on one hand, to efficiently perform some decision-making tasks ""on the fly"" - by only listening to some short music examples, and on the other hand, it provides an alternative data representation for blind or visually impaired users or users who are due to their professional or personal activities (e.g., driving) cannot use their sense of vision for watching a visual display of data, but still need to get some important time-based information by using their other senses.","2008","2023-07-31 07:24:19","2023-07-31 07:24:19","","63–72","","","","","Proceedings of the 9th International Workshop on Multimedia Data Mining: held in conjunction with the ACM SIGKDD 2008","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QYDN2M5H","journalArticle","2022","Joo, Woohun","Graphic-to-Sound Sonification for Visual and Auditory Communication Design","Proceedings of the 17th International Audio Mostly Conference","","","10.1145/3561212.3561214","","I designed two sonification platforms designed for visual/auditory communication design studies and audiovisual art. The purpose of this study was to examine whether test participants can associate visuals and sound without any prior training and sonification approaches in this paper can be utilized as an interactive musical expression. The platform for the communication design study was developed first and the artistic audiovisual platform with the same sonification methodology followed next. In this paper, I introduce the (former) sonification platform designed for the image-to-sound association studies, their sonification methodologies, and present the study results. The object-oriented sonification method that I newly developed describes each shape sonically. The five image-sound association studies were conducted to see whether people can successfully associate sounds and fundamental shapes (i.e., a circle, a triangle, a square, lines, curves, and other custom shapes). Regardless of age and educational background, the correct answer rate was high.","2022","2023-07-31 07:24:19","2023-07-31 07:24:19","","1–6","","","","","Proceedings of the 17th International Audio Mostly Conference","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BY6YQ9A3","journalArticle","2017","Pigrem, Jon; Barthet, Mathieu","Datascaping: Data Sonification as a Narrative Device in Soundscape Composition","Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences","","","10.1145/3123514.3123537","","Soundscape composition is an art form that has grown from acoustic ecology and soundscape studies. Current practices foster a wide range of approaches, from the educational and documentary function of the world soundscape project (WSP) to the creation of imaginary sonic worlds supported by theories of acousmatic and electroacoustic music. Sonification is the process of rendering audio in response to data, and is often used in scenarios where visual representations of data are impractical. The field of auditory display has grown in isolation to soundscape composition, however fosters conceptual similarities in its representation of information in sonic form. This paper investigates the use of data sonification as a narrative tool in soundscape composition. A soundscape has been created using traditional concrete sounds (fixed media recorded sound objects), augmented with sonified real-time elements. An online survey and listening experiment was conducted, which asked participants to rate the soundscape on its ability to communicate specific detail with regard to environmental and social elements contained within. Research data collected shows a strong ability in participants to decode and comprehend additional layers of narrative information communicated through the soundscape.","2017","2023-07-31 07:24:19","2023-07-31 07:24:19","","1–8","","","","","Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MJU5B4XT","journalArticle","2023","Guarese, Renan; Zambetta, Fabio; van Schyndel, Ron","Evaluating micro-guidance sonification methods in manual tasks for Blind and Visually Impaired people","Proceedings of the 34th Australian Conference on Human-Computer Interaction","","","10.1145/3572921.3572929","","This paper presents a user evaluation of seven sonification methods in two-dimensional (2D) manual micro-guidance tasks, which can be used as building blocks for spatialized audio in Mixed and Virtual Reality to model next-generation guidance aids for the Blind and Visually Impaired (BVI). The methods were tested in comparable interactive sonifications of 2D positions in a series of hand-navigation assessments with BVI and blindfolded sighted users, to validate the different approaches in environments without any visual feedback. Results highlighted that alternation and spatiality can be useful resources in sonified guidance, and that users accustomed to faster-than-regular audio speed replay tend to have more precise performances, while musical literacy only had a performance effect on methods highly dependent on aural skills. Ultimately, this work corroborates the notion that sonification may help BVI users perform better in day-to-day manual micro-guidance tasks such as retrieving items from a pantry, handling kitchen appliances, and properly discarding trash.","2023","2023-07-31 07:24:20","2023-07-31 07:24:20","","260–271","","","","","Proceedings of the 34th Australian Conference on Human-Computer Interaction","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4LTI749C","journalArticle","2008","Pohle, Tim; Knees, Peter; Widmer, Gerhard","Sound/tracks: real-time synaesthetic sonification and visualisation of passing landscapes","Proceedings of the 16th ACM international conference on Multimedia","","","10.1145/1459359.1459440","","When travelling on a train, many people enjoy looking out of the window at the landscape passing by. We present sound/tracks, an application that translates the perceived movement of the scenery and other visual impressions, such as passing trains, into music. The continuously changing view outside the window is captured with a camera and translated into MIDI events that are replayed instantaneously. This allows for a reflection of the visual impression, adding a sound dimension to the visual experience and deepening the state of contemplation. The application is intended to be run on both mobile phones (with built-in camera) and on laptops (with a connected Web-cam). We propose and discuss different approaches to translating the video signal into an audio stream, present different application scenarios, and introduce a method to visualise the dynamics of complete train journeys by ""re-transcribing"" the captured video frames used to generate the music.","2008","2023-07-31 07:24:20","2023-07-31 07:24:20","","599–608","","","","","Proceedings of the 16th ACM international conference on Multimedia","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K8DXCXPZ","journalArticle","2021","Sugawa, Moe; Furukawa, Taichi; Chernyshov, George; Hynds, Danny; Han, Jiawen; Padovani, Marcelo; Zheng, Dingding; Marky, Karola; Kunze, Kai; Minamizawa, Kouta","Boiling Mind: Amplifying the Audience-Performer Connection through Sonification and Visualization of Heart and Electrodermal Activities","Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction","","","10.1145/3430524.3440653","","In stage performances, an invisible wall in front of the stage often weakens the connections between the audience and performers. To amplify this performative connection, we present the concept ”Boiling Mind”. Our design concept is based on streaming sensor data related to heart and electrodermal activities from audience members and integrating this data into staging elements, such as visual projections, music, and lighting. Thus, the internal states of the audience directly influence the staging. Artists can have a more direct perception of the inner reactions of audience members and can create physical expressions in response to them. In this paper, we present the wearable sensing system as well as design considerations of mapping heart and electrodermal activity to changes in the staging elements. We evaluated our design and setup over three live performances.","2021","2023-07-31 07:24:20","2023-07-31 07:24:20","","1–10","","","","","Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"33ZS27SQ","journalArticle","2021","Winters, R. Michael; Walker, Bruce N.; Leslie, Grace","Can You Hear My Heartbeat?: Hearing an Expressive Biosignal Elicits Empathy","Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems","","","10.1145/3411764.3445545","","Interfaces designed to elicit empathy provide an opportunity for HCI with important pro-social outcomes. Recent research has demonstrated that perceiving expressive biosignals can facilitate emotional understanding and connection with others, but this work has been largely limited to visual approaches. We propose that hearing these signals will also elicit empathy, and test this hypothesis with sounding heartbeats. In a lab-based within-subjects study, participants (N = 27) completed an emotion recognition task in different heartbeat conditions. We found that hearing heartbeats changed participants’ emotional perspective and increased their reported ability to “feel what the other was feeling.” From these results, we argue that auditory heartbeats are well-suited as an empathic intervention, and might be particularly useful for certain groups and use-contexts because of its musical and non-visual nature. This work establishes a baseline for empathic auditory interfaces, and offers a method to evaluate the effects of future designs.","2021","2023-07-31 07:24:20","2023-07-31 07:24:20","","1–11","","","","","Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3MD364U3","journalArticle","2014","Rovithis, Emmanouel; Mniestris, Andreas; Floros, Andreas","Educational audio game design: sonification of the curriculum through a role-playing scenario in the audio game 'Kronos'","Proceedings of the 9th Audio Mostly: A Conference on Interaction With Sound","","","10.1145/2636879.2636902","","Audio-Games (AGs) are electronic games that feature partially or completely auditory interfaces to express the game's plot and mechanics. The required concentration on sonic information makes AGs a suitable medium not only for entertainment, but also for education on (and not limited to) music and sound studies curricula. This paper presents a novel educational AG entitled Kronos that implements a role-playing scenario to facilitate the sonification of the relevant curriculum and to create an educational platform that combines an audio-based gaming environment with a musical instrument. In that process a methodology suggested by the authors has been used. The sonic symbols assigned to create the game's narrative content will be explained and future developments will be mentioned.","2014","2023-07-31 07:24:20","2023-07-31 07:24:20","","1–6","","","","","Proceedings of the 9th Audio Mostly: A Conference on Interaction With Sound","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S9CKLXSP","journalArticle","2016","Chernyshov, George; Chen, Jiajun; Lai, Yenchin; Noriyasu, Vontin; Kunze, Kai","Ambient Rhythm: Melodic Sonification of Status Information for IoT-enabled Devices","Proceedings of the 6th International Conference on the Internet of Things","","","10.1145/2991561.2991564","","In this paper we explore how to embed status information of IoT-enabled devices in the acoustic atmosphere using melodic ambient sounds while limiting obtrusiveness for the user. The user can use arbitrary sound samples to represent the devices he wants to monitor. Our system combines these sound samples into a melodic ambient rhythm that contains information on all the processes or variables that user is monitoring. We focus on continuous rather than binary information (e.g. ""monitoring progress status"" rather then ""new message received""). We evaluate our system in a machine monitoring scenario focusing on 5 distinct machines/processes to monitor with 6 priority levels for each. 9 participants use our system to monitor these processes with an up to 92.44% detection rate, if several levels are combined. Participants had no previous experience with this or similar systems and had only 5-10 minute training session before the tests.","2016","2023-07-31 07:24:20","2023-07-31 07:24:20","","1–6","","","","","Proceedings of the 6th International Conference on the Internet of Things","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CU9BKNVH","journalArticle","2010","Dulyan, Aram; Edmonds, Ernest","AUXie: initial evaluation of a blind-accessible virtual museum tour","Proceedings of the 22nd Conference of the Computer-Human Interaction Special Interest Group of Australia on Computer-Human Interaction","","","10.1145/1952222.1952280","","Remotely accessible audio-based virtual tours can offer great utility for blind or vision impaired persons, eliminating the difficulties posed by travel to unfamiliar locations, and allowing truly independent exploration. This paper draws upon sonification techniques used in previous implementations of audio-based 3D environments to develop a prototype of blind-accessible virtual tours specifically tailored to the needs of cultural sites. A navigable 3D world is presented using spatially positioned musical earcons, accompanied by synthesised speech descriptions and navigation aids. The worlds are read from X3D models enhanced with metadata to identify and describe the rooms and exhibits, thus enabling an audio modality for existing 3D worlds and simplifying the tour creation process. The prototype, named AUXie, was evaluated by 11 volunteers with total blindness to establish a proof of concept and identify the problematic aspects of the interface. The positive response obtained confirmed the validity of the approach and yielded valuable insight into how such tours can be further improved.","2010","2023-07-31 07:24:20","2023-07-31 07:24:20","","272–275","","","","","Proceedings of the 22nd Conference of the Computer-Human Interaction Special Interest Group of Australia on Computer-Human Interaction","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CX6B3C68","journalArticle","2008","Dodiya, Janki; Alexandrov, Vassil N.","Use of auditory cues for wayfinding assistance in virtual environment: music aids route decision","Proceedings of the 2008 ACM symposium on Virtual reality software and technology","","","10.1145/1450579.1450615","","This paper addresses the crucial problem of wayfinding assistance in the Virtual Environments (VEs). A number of navigation aids such as maps, agents, trails and acoustic landmarks are available to support the user for navigation in VEs, however it is evident that most of the aids are visually dominated. This work-in-progress describes a sound based approach that intends to assist the task of 'route decision' during navigation in a VE using music. Furthermore, with use of musical sounds it aims to reduce the cognitive load associated with other visually as well as physically dominated tasks. To achieve these goals, the approach exploits the benefits provided by music to ease and enhance the task of wayfinding, whilst making the user experience in the VE smooth and enjoyable.","2008","2023-07-31 07:24:20","2023-07-31 07:24:20","","171–174","","","","","Proceedings of the 2008 ACM symposium on Virtual reality software and technology","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JDHWSJR7","journalArticle","2011","Jylhä, Antti; Erkut, Cumhur","Auditory feedback in an interactive rhythmic tutoring system","Proceedings of the 6th Audio Mostly Conference: A Conference on Interaction with Sound","","","10.1145/2095667.2095683","","We present the recent developments in the design of audio-visual feedback in iPalmas, the interactive Flamenco rhythm tutor. Based on evaluation of the original implementation, we have re-designed the interface to better support the user in learning and performing rhythmic patterns. The system measures the performance parameters of the user and provides auditory feedback on the performance with different sounds corresponding to different performance attributes. The design of these sounds is informed by several attributes derived from the evaluation. We propose informative, non-intrusive. and archetypal sounds to be used in the system.","2011","2023-07-31 07:24:20","2023-07-31 07:24:20","","109–115","","","","","Proceedings of the 6th Audio Mostly Conference: A Conference on Interaction with Sound","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TNMQSFVE","journalArticle","2009","Kim, Si-Jung; Thangjitham, Jennifer; Winchester, Woodrow","Assessing the efficacy of a mixed-modal auditory display system for enhancing auditory sensation","Proceedings of the 47th Annual Southeast Regional Conference","","","10.1145/1566445.1566520","","This paper presents the design and evaluation of a new type of mixed-modal auditory display for enhancing auditory sensation. The purpose of this study is to determine whether or not a light apparatus in which LED lights are installed in front of a speaker panel and alternating lights based on sound frequency and intensity can support user awareness of the auditory source. Five different auditory sources were used, and a user study with 20 participants revealed that all auditory sources were better recognized when listened to with the light apparatus. The music and the emergency alert auditory source were best represented by the light apparatus. The experiment showed synchronized visual and audio representation enhanced the user's auditory sensation. Findings suggest that the light apparatus could be useful when auditory signal displays are not universally applicable because people who suffer from hearing loss are unable to use them effectively. It is expected that the result of this study could contribute to the design of hearing aids, emergency alarm device/mechanisms, communication trust or other assistive technologies that seek to provide context to received data.","2009","2023-07-31 07:24:20","2023-07-31 07:24:20","","1–2","","","","","Proceedings of the 47th Annual Southeast Regional Conference","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V3C8S926","journalArticle","2017","Feltham, Frank; Loke, Lian","Felt Sense through Auditory Display: A Design Case Study into Sound for Somatic Awareness while Walking","Proceedings of the 2017 ACM SIGCHI Conference on Creativity and Cognition","","","10.1145/3059454.3059461","","Walking is an everyday act that we, as humans, often take for granted. To walk requires the synergy of somatosensory, neurological and physiological processes for us to move at a regular pace by lifting and setting down each foot in turn. It can be argued that walking is also a source of creativity and exploration when conducted as an intentional act of somatic or self-awareness. This design case study aims to explore the kinds of somatic awareness and aesthetic engagement of walking apparent through the introduction of a pressure mediated sound generating surface with a group of Feldenkrais movement practitioners. These explorations reveal that there is an awareness of tempo and rhythm during the step cycle. This awareness takes on an internal focus as shifts in attention and bodily organization. Another key finding is that exploration and play are enabled due to the rich timbral qualities of the pressure mediated auditory feedback. The significance and contribution in this work is in the implications it has for the design of technologies that support kinaesthetic awareness through aesthetic and exploratory strategies.","2017","2023-07-31 07:24:20","2023-07-31 07:24:20","","287–298","","","","","Proceedings of the 2017 ACM SIGCHI Conference on Creativity and Cognition","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SI657UWR","journalArticle","2011","Encelle, Benoît; Ollagnier-Beldame, Magali; Pouchot, Stéphanie; Prié, Yannick","Annotation-based video enrichment for blind people: a pilot study on the use of earcons and speech synthesis","The proceedings of the 13th international ACM SIGACCESS conference on Computers and accessibility","","","10.1145/2049536.2049560","","Our approach to address the question of online video accessibility for people with sensory disabilities is based on video annotations that are rendered as video enrichments during the playing of the video. We present an exploratory work that focuses on video accessibility for blind people with audio enrichments composed of speech synthesis and earcons (i.e. nonverbal audio messages). Our main results are that earcons can be used together with speech synthesis to enhance understanding of videos; that earcons should be accompanied with explanations; and that a potential side effect of earcons is related to video rhythm perception.","2011","2023-07-31 07:24:20","2023-07-31 07:24:20","","123–130","","","","","The proceedings of the 13th international ACM SIGACCESS conference on Computers and accessibility","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QA8JS4PS","journalArticle","2018","Lorenzoni, Valerio; Maes, Pieter-Jan; Van den Berghe, Pieter; De Clercq, Dirk; de Bie, Tijl; Leman, Marc","A biofeedback music-sonification system for gait retraining","Proceedings of the 5th International Conference on Movement and Computing","","","10.1145/3212721.3212843","","Auditory feedbacks are becoming increasingly popular in sports providing opportunities for monitoring and gait (re)training in ecological environments. We present the design process of a sonification strategy for modification of running parameters. The sonification provides real-time feedback of the performance through introduction of distortion of a baseline music track. The music BPM is continuously matched to the runners' cadence. The noise-based continuous feedback was able to significantly alter the mean running cadence in a non-instructed and non-disturbing way and performed better than standard verbal instructions. Although some of the participants did not respond effectively to the feedback, a large majority of the participants positively rated the feedback system in terms of pleasantness and motivation.","2018","2023-07-31 07:24:20","2023-07-31 07:24:20","","1–5","","","","","Proceedings of the 5th International Conference on Movement and Computing","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H5TCDCKH","journalArticle","2019","Ronnberg, Niklas","Musical Elements in Sonification Support Visual Perception","Proceedings of the 31st European Conference on Cognitive Ergonomics","","","10.1145/3335082.3335097","","Visual representations of data are commonly used to communicate research results. However, such representations might introduce several possible challenges for the human visual perception system, for example in perceiving brightness levels. Sonification, adding sound to the visual representation, might be used to overcome these challenges. As sonification provides additional information, sonification could be useful in supporting interpretations of a visual perception. In the present study, usefulness in terms of accuracy of sonification was investigated with an interactive sonification test. In the experiment, participants were asked to identify the highest brightness level in a monochrome visual representation. The task was performed in four conditions, one with no sonification and three with different sonification settings. The results show that sonification is useful, as measured by higher task accuracy.","2019","2023-07-31 07:24:20","2023-07-31 07:24:20","","114–117","","","","","Proceedings of the 31st European Conference on Cognitive Ergonomics","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GVUMFSSK","journalArticle","2016","Kolykhalova, Ksenia; Alborno, Paolo; Camurri, Antonio; Volpe, Gualtiero","A serious games platform for validating sonification of human full-body movement qualities","Proceedings of the 3rd International Symposium on Movement and Computing","","","10.1145/2948910.2948962","","In this paper we describe a serious games platfrom for validating sonification of human full-body movement qualities. This platform supports the design and development of serious games aiming at validating (i) our techniques to measure expressive movement qualities, and (ii) the mapping strategies to translate such qualities in the auditory domain, by means of interactive sonification and active music experience. The platform is a part of a more general framework developed in the context of the EU ICT H2020 DANCE ""Dancing in the dark"" Project n.645553 that aims at enabling the perception of nonverbal artistic whole-body experiences to visual impaired people.","2016","2023-07-31 07:24:20","2023-07-31 07:24:20","","1–5","","","","","Proceedings of the 3rd International Symposium on Movement and Computing","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q7ABVMVT","journalArticle","2017","Blanco, Andrea Lorena Aldana; Grautoff, Steffen; Hermann, Thomas","CardioSounds: Real-time Auditory Assistance for Supporting Cardiac Diagnostic and Monitoring","Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences","","","10.1145/3123514.3123542","","This paper presents a real-time sonification system for Electrocardiography (ECG) monitoring and diagnostic. We introduce two novel sonification designs: (a) Auditory magnification loupe, a method to sonify important beat-to-beat variations when doing sports activities, and (b) ST-segment water ambience sonification, which aims to assist clinicians in the diagnostic process by building a soundscape that exhibits ECG signal abnormalities as the analysed signal deviates from a healthy ECG. The proposed methods were designed to assist users to unobtrusively monitor their own (or their patients') heart signal in situations when a visual-only representation is not convenient for the proper fulfilment of a given task. Using CardioSounds users receive auditory feedback in order to monitor important heart rhythm disturbances (e.g. Arrhythmia) or pathologies due to a blocking of the heart's vessels.","2017","2023-07-31 07:24:20","2023-07-31 07:24:20","","1–4","","","","","Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2EAMIFZ5","journalArticle","2020","Winters, R. Michael; Koziej, Stephanie","An auditory interface for realtime brainwave similarity in dyads","Proceedings of the 15th International Audio Mostly Conference","","","10.1145/3411109.3411147","","We present a case-study in the development of a""hyperscanning"" auditory interface that transforms realtime brainwave-similarity between interacting dyads into music. Our instrument extends reality in face-to-face communication with a musical stream reflecting an invisible socio-neurophysiological signal. This instrument contributes to the historical context of brain-computer interfaces (BCIs) applied to art and music, but is unique because it is contingent on the correlation between the brainwaves of the dyad, and because it conveys this information using entirely auditory feedback. We designed the instrument to be i) easy to understand, ii) relatable and iii) pleasant for members of the general public in an exhibition context. We present how this context and user group led to our choice of EEG hardware, inter-brain similarity metric, and our auditory mapping strategy. We discuss our experience following four public exhibitions, as well as future improvements to the instrument design and user experience.","2020","2023-07-31 07:24:20","2023-07-31 07:24:20","","261–264","","","","","Proceedings of the 15th International Audio Mostly Conference","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RI9BWUUK","journalArticle","2022","Liu, Wanyu; Magalhaes, Michelle Agnes; Mackay, Wendy E.; Beaudouin-Lafon, Michel; Bevilacqua, Frédéric","Motor Variability in Complex Gesture Learning: Effects of Movement Sonification and Musical Background","ACM Transactions on Applied Perception","","","10.1145/3482967","","With the increasing interest in movement sonification and expressive gesture-based interaction, it is important to understand which factors contribute to movement learning and how. We explore the effects of movement sonification and users’ musical background on motor variability in complex gesture learning. We contribute an empirical study in which musicians and non-musicians learn two gesture sequences over three days, with and without movement sonification. Results show the interlaced interaction effects of these factors and how they unfold in the three-day learning process. For gesture 1, which is fast and dynamic with a direct “action-sound” sonification, movement sonification induces higher variability for both musicians and non-musicians on day 1. While musicians reduce this variability to a similar level as no auditory feedback condition on day 2 and day 3, non-musicians remain to have significantly higher variability. Across three days, musicians also have significantly lower variability than non-musicians. For gesture 2, which is slow and smooth with an “action-music” metaphor, there are virtually no effects. Based on these findings, we recommend future studies to take into account participants’ musical background, consider longitudinal study to examine these effects on complex gestures, and use awareness when interpreting the results given a specific design of gesture and sound.","2022","2023-07-31 07:24:20","2023-07-31 07:24:20","","2:1–2:21","","1","19","","ACM Transactions on Applied Perception","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"44X24KW2","journalArticle","2023","Clark, Matthew; Doryab, Afsaneh","Sounds of Health: Using Personalized Sonification Models to Communicate Health Information","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","","","10.1145/3570346","","This paper explores the feasibility of using sonification in delivering and communicating health and wellness status on personal devices. Ambient displays have proven to inform users of their health and wellness and help them to make healthier decisions, yet, little technology provides health assessments through sounds, which can be even more pervasive than visual displays. We developed a method to generate music from user preferences and evaluated it in a two-step user study. In the first step, we acquired general healthiness impressions from each user. In the second step, we generated customized melodies from music preferences in the first step to capture participants' perceived healthiness of those melodies. We deployed our surveys for 55 participants to complete on their own over 31 days. We analyzed the data to understand commonalities and differences in users' perceptions of music as an expression of health. Our findings show the existence of clear associations between perceived healthiness and different music features. We provide useful insights into how different musical features impact the perceived healthiness of music, how perceptions of healthiness vary between users, what trends exist between users' impressions, and what influences (or does not influence) a user's perception of healthiness in a melody. Overall, our results indicate validity in presenting health data through personalized music models. The findings can inform the design of behavior management applications on personal and ubiquitous devices.","2023","2023-07-31 07:24:20","2023-07-31 07:24:20","","206:1–206:31","","4","6","","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QEM6E9EC","journalArticle","2008","Andersen, Tue Haste; Zhai, Shumin","“Writing with music”: Exploring the use of auditory feedback in gesture interfaces","ACM Transactions on Applied Perception","","","10.1145/1773965.1773968","","We investigate the use of auditory feedback in pen-gesture interfaces in a series of informal and formal experiments. Initial iterative exploration showed that gaining performance advantage with auditory feedback was possible using absolute cues and state feedback after the gesture was produced and recognized. However, gaining learning or performance advantage from auditory feedback tightly coupled with the pen-gesture articulation and recognition process was more difficult. To establish a systematic baseline, Experiment 1 formally evaluated gesture production accuracy as a function of auditory and visual feedback. Size of gestures and the aperture of the closed gestures were influenced by the visual or auditory feedback, while other measures such as shape distance and directional difference were not, supporting the theory that feedback is too slow to strongly influence the production of pen stroke gestures. Experiment 2 focused on the subjective aspects of auditory feedback in pen-gesture interfaces. Participants' rating on the dimensions of being wonderful and stimulating was significantly higher with musical auditory feedback. Several lessons regarding pen gestures and auditory feedback are drawn from our exploration: a few simple functions such as indicating the pen-gesture recognition results can be achieved, gaining performance and learning advantage through tightly coupled process-based auditory feedback is difficult, pen-gesture sets and their recognizers can be designed to minimize visual dependence, and people's subjective experience of gesture interaction can be influenced using musical auditory feedback. These lessons may serve as references and stepping stones toward future research and development in pen-gesture interfaces with auditory feedback.","2008","2023-07-31 07:24:20","2023-07-31 07:24:20","","17:1–17:24","","3","7","","ACM Transactions on Applied Perception","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AXUMRKPJ","journalArticle","2017","Rector, Kyle; Salmon, Keith; Thornton, Dan; Joshi, Neel; Morris, Meredith Ringel","Eyes-Free Art: Exploring Proxemic Audio Interfaces For Blind and Low Vision Art Engagement","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","","","10.1145/3130958","","Engagement in the arts1 is an important component of participation in cultural activities, but remains a largely unaddressed challenge for people with sensory disabilities. Visual arts are generally inaccessible to people with visual impairments due to their inherently visual nature. To address this, we present Eyes-Free Art, a design probe to explore the use of proxemic audio for interactive sonic experiences with 2D art work. The proxemic audio interface allows a user to move closer and further away from a painting to experience background music, a novel sonification, sound effects, and a detailed verbal description. We conducted a lab study by creating interpretations of five paintings with 13 people with visual impairments and found that participants enjoyed interacting with the artwork. We then created a live installation with a visually impaired artist to iterate on this concept to account for multiple users and paintings. We learned that a proxemic audio interface allows for people to feel immersed in the artwork. Proxemic audio interfaces are similar to visual because they increase in detail with closer proximity, but are different because they need a descriptive verbal overview to give context. We present future research directions in the space of proxemic audio interactions.","2017","2023-07-31 07:24:20","2023-07-31 07:24:20","","93:1–93:21","","3","1","","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4IIRVSV2","journalArticle","2016","Yu, Bin; Hu, Jun; Funk, Mathias; Feijs, Loe","A Study on User Acceptance of Different Auditory Content for Relaxation","Proceedings of the Audio Mostly 2016","","","10.1145/2986416.2986418","","The use of auditory interface at the relaxation-assisted interactive system is becoming increasingly popular. This study aims to investigate the effects of different types of auditory content on the subjective relaxation experience. The participants listened to fifteen sound samples from five categories: (a) nature white noise, (b) natural soundscape, (c) ambient music, (d) instrumental music, (e) instrumental music mixed with the natural soundscape. These auditory contents were selected or designed specifically for assisting relaxation. The study measured the subjective relaxation rating after listening to each sample and interviewed the listeners to understand what causes the differences in relaxation experience. The results indicate that the instrumental music and the combination of nature soundscape and music might be a better auditory content or audio form to induce relaxation compared to the ambient music, pure natural soundscape, and nature white noise. The findings of this study can be used in the design of musical and auditory display in many interactive systems for stress mitigation and relaxation exercises.","2016","2023-07-31 07:24:20","2023-07-31 07:24:20","","69–76","","","","","Proceedings of the Audio Mostly 2016","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8C5MSH66","journalArticle","2019","Winters, R. Michael; Joshi, Neel; Cutrell, Edward; Morris, Meredith Ringel","Strategies for Auditory Display of Social Media","Ergonomics in Design","","","10.1177/1064804618788098","","Social media is an overwhelmingly visual medium, and we ask the simple question: How can the data and images of social media posts be transformed into something as meaningful and vivid in the auditory sense? Such a design would be useful for eyes-free browsing and could enhance the existing visual media. Our strategy first uses artificial intelligence systems to transform low-level input data into high-level sociocultural features. These features are then conveyed using a multifactored temporal design that uses speech, sonification, auditory scenes, and music.","2019","2023-07-31 07:24:20","2023-07-31 07:24:20","","11-15","","1","27","","Ergonomics in Design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XLQXGNAH","journalArticle","2002","Flowers, John H.; Grafel, Douglas C.","Perception of Sonified Daily Weather Records","Proceedings of the Human Factors and Ergonomics Society Annual Meeting","","","10.1177/154193120204601711","","Human participants performed a perceptual task in which they sorted auditory (musical) displays of one-month long daily weather summaries (temperature, rainfall, and snowfall) based on perceived similarity of weather patterns. Displays for fifteen winter months and fifteen summer months were sorted by separate groups of participants. Each group sorted two sets of displays that varied in presentation speed. Multidimensional scaling analyses indicated that these displays were effective in conveying weather features important for climate comparisons for both winter and summer months, and that the faster (7.1 sec) displays were more effective than the slower (14.2 sec) displays. These results show that sonification can be an effective tool for exploring multivariate time series data, but that optimization of such displays may require consideration of the temporal constraints of auditory sensory memory and working memory.","2002","2023-07-31 07:24:20","2023-07-31 07:24:20","","1579-1583","","17","46","","Proceedings of the Human Factors and Ergonomics Society Annual Meeting","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9CHC45BV","journalArticle","2021","Šabić, Edin; Chen, Jing; MacDonald, Justin A.","Toward a Better Understanding of In-Vehicle Auditory Warnings and Background Noise","Human Factors","","","10.1177/0018720819879311","","ObjectiveThe effectiveness of three types of in-vehicle warnings was assessed in a driving simulator across different noise conditions.BackgroundAlthough there has been much research comparing different types of warnings in auditory displays and interfaces, many of these investigations have been conducted in quiet laboratory environments with little to no consideration of background noise. Furthermore, the suitability of some auditory warning types, such as spearcons, as car warnings has not been investigated.MethodTwo experiments were conducted to assess the effectiveness of three auditory warnings (spearcons, text-to-speech, auditory icons) with different types of background noise while participants performed a simulated driving task.ResultsOur results showed that both the nature of the background noise and the type of auditory warning influenced warning recognition accuracy and reaction time. Spearcons outperformed text-to-speech warnings in relatively quiet environments, such as in the baseline noise condition where no music or talk-radio was played. However, spearcons were not better than text-to-speech warnings with other background noises. Similarly, the effectiveness of auditory icons as warnings fluctuated across background noise, but, overall, auditory icons were the least efficient of the three warning types.ConclusionOur results supported that background noise can have an idiosyncratic effect on a warning’s effectiveness and illuminated the need for future research into ameliorating the effects of background noise.ApplicationThis research can be applied to better present warnings based on the anticipated auditory environment in which they will be communicated.","2021","2023-07-31 07:24:20","2023-07-31 07:24:20","","312-335","","2","63","","Human Factors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9WK2MNEG","journalArticle","2016","Lacherez, Philippe; Donaldson, Liam; Burt, Jennifer S.","Do Learned Alarm Sounds Interfere With Working Memory?","Human Factors","","","10.1177/0018720816662733","","Objective:To assess whether identifying (or ignoring) learned alarm sounds interferes with performance on a task involving working memory.Background:A number of researchers have suggested that auditory alarms could interfere with working memory in complex task environments, and this could serve as a caution against their use. Changing auditory information has been shown to interfere with serial recall, even when the auditory information is to be ignored. However, previous researchers have not examined well-learned patterns, such as familiar alarms.Method:One group of participants learned a set of alarms (either a melody, a rhythmic pulse, or a spoken nonword phrase) and subsequently undertook a digits-forward task in three conditions (no alarms, identify the alarm, or ignore the alarm). A comparison group undertook the baseline and ignore conditions but had no prior exposure to the alarms.Results:All alarms interfered with serial recall when participants were asked to identify them; however, only the nonword phrase interfered with recall when ignored. Moreover, there was no difference between trained and untrained participants in terms of recall performance when ignoring the alarms, suggesting that previous training does not make alarms less ignorable.Conclusion:Identifying any alarm sound may interfere with immediate working memory; however, spoken alarms may interfere even when ignored.Application:It is worth considering the importance of alarms in environments requiring high working memory performance and in particular avoiding spoken alarms in such environments.","2016","2023-07-31 07:24:20","2023-07-31 07:24:20","","1044-1051","","7","58","","Human Factors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LK7WHVLU","journalArticle","2013","Walker, Bruce N.; Lindsay, Jeffrey; Nance, Amanda; Nakano, Yoko; Palladino, Dianne K.; Dingler, Tilman; Jeon, Myounghoon","Spearcons (Speech-Based Earcons) Improve Navigation Performance in Advanced Auditory Menus","Human Factors","","","10.1177/0018720812450587","","Objective: The goal of this project is to evaluate a new auditory cue, which the authors call spearcons, in comparison to other auditory cues with the aim of improving auditory menu navigation. Background: With the shrinking displays of mobile devices and increasing technology use by visually impaired users, it becomes important to improve usability of non-graphical user interface (GUI) interfaces such as auditory menus. Using nonspeech sounds called auditory icons (i.e., representative real sounds of objects or events) or earcons (i.e., brief musical melody patterns) has been proposed to enhance menu navigation. To compensate for the weaknesses of traditional nonspeech auditory cues, the authors developed spearcons by speeding up a spoken phrase, even to the point where it is no longer recognized as speech. Method: The authors conducted five empirical experiments. In Experiments 1 and 2, they measured menu navigation efficiency and accuracy among cues. In Experiments 3 and 4, they evaluated learning rate of cues and speech itself. In Experiment 5, they assessed spearcon enhancements compared to plain TTS (text to speech: speak out written menu items) in a two-dimensional auditory menu. Results: Spearcons outperformed traditional and newer hybrid auditory cues in navigation efficiency, accuracy, and learning rate. Moreover, spearcons showed comparable learnability as normal speech and led to better performance than speech-only auditory cues in two-dimensional menu navigation. Conclusion: These results show that spearcons can be more effective than previous auditory cues in menu-based interfaces. Application: Spearcons have broadened the taxonomy of nonspeech auditory cues. Users can benefit from the application of spearcons in real devices.","2013","2023-07-31 07:24:20","2023-07-31 07:24:20","","157-182","","1","55","","Human Factors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y3N7ZE9K","journalArticle","2000","Blattner, Meera M","A Design for Auditory Display","Proceedings of the Human Factors and Ergonomics Society Annual Meeting","","","10.1177/154193120004400159","","Auditory messages in home, office, and medical environments consist of little more than beeps, bells, and buzzers. Warning signals are particularly inappropriate for these environments because they startle, annoy, but do not inform listeners as to the source of the difficulty. As home automation becomes prevalent, unpleasant sounds will not be tolerated. Earcons are musical sounds that are easy to learn; yet designed to inform listeners of the status of automated systems. We discuss problems such as simultaneous sounds and levels of urgency below.","2000","2023-07-31 07:24:20","2023-07-31 07:24:20","","219-222","","1","44","","Proceedings of the Human Factors and Ergonomics Society Annual Meeting","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LFYF7YFQ","journalArticle","2013","Ho, Anson; Burns, Catherine","Music as an Auditory Display: Interaction Effects of Mode and Tempo on Perceived Urgency","Proceedings of the Human Factors and Ergonomics Society Annual Meeting","","","10.1177/1541931213571256","","Currently, there are very few guidelines on parameters needed to create an effective auditory display. Auditory displays can be intrusive and may not be used effectively if they are poorly designed. However, music is often in our environments as ambient noise and, instead of being intrusive, can be perceived as making the environment calmer and more productive. We present the initial steps of exploring the option of using music as a medium to develop an auditory display capable of conveying normal state information and warning information. An important feature that may impact the effectiveness of auditory warnings is perceived urgency: the impression of urgency that a sound evokes on a listener. To explore whether music could convey urgency as needed for auditory warnings, we evaluated four different musical phrases that varied in time and key signature as a method of measuring the effects of mode and tempo on perceived urgency. The effectiveness of the study was tested with twenty subjects split into a two by two factorial design: gender (male vs. female) and musical experience (experienced vs. non-experienced). The applications of this research can help develop concrete guidelines when designing effective auditory displays in order to improve users’ performance when dealing with complex interfaces.","2013","2023-07-31 07:24:20","2023-07-31 07:24:20","","1149-1153","","1","57","","Proceedings of the Human Factors and Ergonomics Society Annual Meeting","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IG6H7QG7","journalArticle","2012","Duarte, Emília; Rebelo, Francisco; Teles, Júlia; Wogalter, Michael S.","A Personalized Speech Warning Facilitates Compliance in an Immersive Virtual Environment","Proceedings of the Human Factors and Ergonomics Society Annual Meeting","","","10.1177/1071181312561427","","The effect of a personalized technology-based warning on compliance was assessed using an immersive virtual environment (IVE). Sixty university students performed an end-of-day routine security check in the IVE. Participants were asked to search for and activate safety-related devices, which involved entering several rooms. Just prior to abandoning the first room, participants were incidentally exposed to a posted warning (mandatory to disconnect the music generator) consisting of either a personal warning (i.e., a speech message with the participant’s first name) or an impersonal warning (i.e., a auditory beep signal). Compliance was determined by observing whether or not the participants pressed the button-switch as directed by the warning. Results reveal that compliance rate was significantly greater when the warning was personalized. No significant gender differences were found. Implications of these results are discussed in terms of the benefits of effective warnings.","2012","2023-07-31 07:24:20","2023-07-31 07:24:20","","2045-2049","","1","56","","Proceedings of the Human Factors and Ergonomics Society Annual Meeting","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5AZJYL32","journalArticle","2009","Sanderson, Penelope M.","Auditory alarms for medical equipment: How do we ensure they convey their meanings?","Proceedings of the Human Factors and Ergonomics Society Annual Meeting","","","10.1177/154193120905300422","","For systems to be effective and to earn their users' trust, their signals must be readily interpreted. An international standard IEC 60601–1–8 was released in 2005 that provides guidelines on how to make auditory alarms on medical electrical equipment more recognisable and discriminable. Since the release of the standard, there have been concerns about the adequacy of its recommendations and, in particular, its proposal that manufacturers should use melodies to distinguish alarms from different sources. The melodies presented in the standard are just suggestions, but the standard does not indicate how an acceptable set of melodies can be established. Moreover, the standard does not require that developers perform thorough testing with representative users before implementing any melodies. The paper reviews studies performed over the last few years that demonstrate that the melodies suggested in IEC 60601–1–8 are ineffective. The paper also critiques suggestions that have been put forward for alternative alarm sounds, using speech synthesis techniques, better urgency mapping, and so on. Finally, criteria for future design and evaluation efforts are indicated.","2009","2023-07-31 07:24:20","2023-07-31 07:24:20","","264-268","","4","53","","Proceedings of the Human Factors and Ergonomics Society Annual Meeting","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9J52GPGH","journalArticle","2012","Gresham-Lancaster, Scot; Sinclair, Peter","Sonification and Acoustic Environments","Leonardo Music Journal","","","","","Sonification can allow us to connect sound and/or music via data to the environment; in another sense, by ""displaying"" data through sound, sonification participates in creating our acoustic environment. The authors consider here the significance of certain aspects of this relationship.","2012","2023-07-31 07:24:20","2023-07-31 07:24:20","","67-71","","","22","","Leonardo Music Journal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W87HPEQS","journalArticle","2004","Ballora, Mark; Pennycook, Bruce; Ivanov, Plamen C.; Glass, Leon; Goldberger, Ary L.","Heart Rate Sonification: A New Approach to Medical Diagnosis","Leonardo","","","","","Ever since 1819, when Theophile Laënnec first put a block of wood to a patient's chest in order to listen to her heartbeat, physicians have used auscultation to help diagnose cardiopulmonary disorders. Here the authors describe a novel diagnostic method based in music technology. Digital musicsynthesis software is used to transform the sequence of time intervals between consecutive heartbeats into an electroacoustic soundtrack. The results show promise as a diagnostic tool and also provide the basis of an interesting musical soundscape.","2004","2023-07-31 07:24:20","2023-07-31 07:24:20","","41-46","","1","37","","Leonardo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GFX82K6N","journalArticle","2016","Barrett, Natasha","Interactive Spatial Sonification of Multidimensional Data for Composition and Auditory Display","Computer Music Journal","","","","","This article presents a new approach to interactive spatial sonification of multidimensional data as a tool for spatial sound synthesis, for composing temporal-spatial musical materials, and as an auditory display for scientists to analyze multidimensional data sets in time and space. The approach applies parameter-mapping sonification and is currently implemented in an application called Cheddar, which was programmed in Max/MSP. Cheddar sonifies data in real time, where the user can modify a wide variety of temporal, spatial, and sonic parameters during the listening process, and thus more easily uncover patterns and processes in the data than when applying non-real-time, noninteractive techniques. The design draws on existing literature concerning perception and acoustics, and it applies the author's practical experience in acousmatic composition, spectromorphology, and sound semantics, while addressing accuracy, flexibility, and ease of use. Although previous sonification applications have addressed some degree of real-time control and spatialization, this approach integrates space and sound in an interactive framework. Spatial information is sonified in high-order 3-D ambisonics, where the user can interactively move the virtual listening position to reveal details easily missed from fixed or noninteractive spatial views. Sounds used as input to the sonification take advantage of the rich spectra and extramusical attributes of acoustic sources, which, although previously theorized, are investigated here in a practical context thoroughly tested alongside acoustic and psychoacoustic considerations. Furthermore, when using Cheddar, no specialized knowledge of programming, acoustics, or psychoacoustics is required. These approaches position Cheddar at the junction between science and art. With one application serving both disciplines, the patterns and processes of science are more fluently appropriated into music or sound art, and vice versa for scientific research, science public outreach, and education.","2016","2023-07-31 07:24:20","2023-07-31 07:24:20","","47-69","","2","40","","Computer Music Journal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ICA5J9XW","journalArticle","2014","Winters, R. Michael; Wanderley, Marcelo M.","Sonification of Emotion: Strategies and results from the intersection with music","Organised Sound","","","10.1017/S1355771813000411","","Emotion is a word not often heard in sonification, though advances in affective computing make the data type imminent. At times the relationship between emotion and sonification has been contentious due to an implied overlap with music. This paper clarifies the relationship, demonstrating how it can be mutually beneficial. After identifying contexts favourable to auditory display of emotion, and the utility of its development to research in musical emotion, the current state of the field is addressed, reiterating the necessary conditions for sound to qualify as a sonification of emotion. With this framework, strategies for display are presented that use acoustic and structural cues designed to target select auditory-cognitive mechanisms of musical emotion. Two sonifications are then described using these strategies to convey arousal and valence though differing in design methodology: one designed ecologically, the other computationally. Each model is sampled at 15-second intervals at 49 evenly distributed points on the AV space, and evaluated using a publically available tool for computational music emotion recognition. The computational design performed 65 times better in this test, but the ecological design is argued to be more useful for emotional communication. Conscious of these limitations, computational design and evaluation is supported for future development.","2014","2023-07-31 07:24:20","2023-07-31 07:24:20","","60-69","","1","19","","Organised Sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D3P8JQFT","journalArticle","2019","Rönnberg, Niklas","Musical sonification supports visual discrimination of color intensity","Behaviour & Information Technology","","","10.1080/0144929X.2019.1657952","","Visual representations of data introduce several possible challenges for the human visual perception system in perceiving brightness levels. Overcoming these challenges might be simplified by adding sound to the representation. This is called sonification. As sonification provides additional information to the visual information, sonification could be useful in supporting the visual perception. In the present study, usefulness (in terms of accuracy and response time) of sonification was investigated with an interactive sonification test. In the test, participants were asked to identify the highest brightness level in a monochrome visual representation. The task was performed in four conditions, one with no sonification and three with different sonification settings. The results show that sonification is useful, as measured by higher task accuracy, and that the participant's musicality facilitates the use of sonification with better performance when sonification was used. The results were also supported by subjective measurements, where participants reported an experienced benefit of sonification.","2019","2023-07-31 07:24:20","2023-07-31 07:24:20","","1028-1037","","10","38","","Behaviour & Information Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GJSTNZLF","journalArticle","2022","Nadri, Chihab; Anaya, Chairunisa; Yuan, Shan; Jeon, Myounghoon","From Visual Art to Music: Sonification Can Adapt to Painting Styles and Augment User Experience","International Journal of Human–Computer Interaction","","","10.1080/10447318.2022.2091210","","Advances in the fields of data processing and sonification have been applied to transcribe a variety of visual experiences into an auditory format. Although image sonification examples exist, the application of these principles to visual art has not been examined thoroughly. We sought to develop and evaluate a set of guidelines for the sonification of visual artworks. Through conducting expert interviews (N = 11), we created an initial sonification algorithm that accounts for art style, lightness, and color diversity to modulate the sonified output in terms of tempo and pitch. This algorithm was evaluated through user evaluations (N = 22). User study responses supported expert interview findings, the notion that sonification can be designed to match the experience of viewing an artwork, and showed interesting interaction effects among art styles, visual components, and musical parameters. We suggest the proposed guidelines can augment visitor experiences at art exhibits and provide the basis for further experimentation.","2022","2023-07-31 07:24:20","2023-07-31 07:24:20","","1-13","","0","0","","International Journal of Human–Computer Interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q53BRWN5","journalArticle","2005","Ekdale, A. A.; Tripp, Alan C.","Paleontological Sonification: Letting Music Bring Fossils to Your Ears","Journal of Geoscience Education","","","10.5408/1089-9995-53.3.271","","Sonification is the process of translating any type of data into sound. In paleontology, it is possible to render various aspects of fossil shapes, such as cephalopod suture patterns or brachiopod commissure lines, as a series of musical tones that can be recognized easily by the human ear. Paleontological applications of sonification might enable auditory perception of morphologic patterns in fossils that may or may not be visually apparent. Some simple classroom demonstrations can help students understand the potential of using sound to identify different types of fossils with their eyes closed (i.e., using their ears alone).","2005","2023-07-31 07:24:20","2023-07-31 07:24:20","","271-280","","3","53","","Journal of Geoscience Education","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8SZRDBLN","journalArticle","2018","Yu, Bin; Funk, Mathias; Hu, Jun; Feijs, Loe","Unwind: a musical biofeedback for relaxation assistance","Behaviour & Information Technology","","","10.1080/0144929X.2018.1484515","","Unwind is a musical biofeedback interface which combines nature sounds and sedative music into a form of New-Age music for relaxation exercises. The nature sounds respond to the user’s physiological data, functioning as an informative layer for biofeedback display. The sedative music aims to induce calmness and evoke positive emotions. UnWind incorporates the benefits of biofeedback and sedative music to facilitate deep breathing, moderate arousal, and promote mental relaxation. We evaluated Unwind in a 2 × 2 factorial experiment with music and biofeedback as independent factors. Forty young adults performed the relaxation exercise under one of the following conditions after experiencing a stressful task: Nature sounds only (NS), Nature sounds with music (NM), and Auditory biofeedback with nature sounds (NSBFB), and UnWind musical biofeedback (NMBFB). The results revealed a significant interaction effect between music and biofeedback on the improvement of heart rate variability. The combination of music and nature sounds also showed benefits in lowering arousal and reducing self-report anxiety. We conclude with a discussion of UnWind for biofeedback and the wider potential of blending nature sounds with music as a musical interface.","2018","2023-07-31 07:24:20","2023-07-31 07:24:20","","800-814","","8","37","","Behaviour & Information Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6CVYG9HG","journalArticle","2019","Ballatore, Andrea; Gordon, David; Boone, Alexander P.","Sonifying data uncertainty with sound dimensions","Cartography and Geographic Information Science","","","10.1080/15230406.2018.1495103","","The communication of data uncertainty is a crucial problem in data science, information visualization, and geographic information science (GIScience). Effective ways to communicate the uncertainty of data enables data consumers to interpret the data as intended by the producer, reducing the possibilities of misinterpretation. In this article, we report on an empirical investigation of how sound can be used to convey information about data uncertainty in an intuitive way. To answer the research question How intuitive are sound dimensions to communicate uncertainty? we carry out a cognitive experiment, where participants were asked to interpret the certainty/uncertainty level in two sounds A and B (N = 33). We produce sound stimuli by varying sound dimensions, including loudness, duration, location, pitch, register, attack, decay, rate of change, noise, timbre, clarity, order, and harmony. In the stimuli, both synthetic and natural sounds are used to allow comparison. The experiment results identify three sound dimensions (loudness, order, and clarity) as significantly more intuitive to communicate uncertainty, providing guidelines for sonification and information visualization practitioners.","2019","2023-07-31 07:24:20","2023-07-31 07:24:20","","385-400","","5","46","","Cartography and Geographic Information Science","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EZT6UGCG","journalArticle","2008","Balakrishnan, G.; Sainarayanan, G.","Stereo Image Processing Procedure for Vision Rehabilitation","Applied Artificial Intelligence","","","10.1080/08839510802226777","","This article presents a review on vision-aided systems and proposes an approach for visual rehabilitation using stereo vision technology. The proposed system utilizes stereo vision, image processing methodology, and a sonification procedure to support blind mobilization. The developed system includes wearable computer, stereo cameras as vision sensor, and stereo earphones, all molded in a helmet. The image of the scene in front of the visually handicapped is captured by the vision sensors. The captured images are processed to enhance the important features in the scene in front for mobilization assistance. The image processing is designed as a model of human vision by identifying the obstacles and their depth information. The processed image is mapped onto musical stereo sound for the blind's understanding of the scene in front. The developed method has been tested in the indoor and outdoor environments and the proposed image processing methodology is found to be effective for object identification.","2008","2023-07-31 07:24:20","2023-07-31 07:24:20","","501-522","","6","22","","Applied Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D7PB3UZX","journalArticle","2015","Larsson, Pontus; Niemand, Mathias","Using Sound to Reduce Visual Distraction from In-vehicle Human–Machine Interfaces","Traffic Injury Prevention","","","10.1080/15389588.2015.1020111","","Objective: Driver distraction and inattention are the main causes of accidents. The fact that devices such as navigation displays and media players are part of the distraction problem has led to the formulation of guidelines advocating various means for minimizing the visual distraction from such interfaces. However, although design guidelines and recommendations are followed, certain interface interactions, such as menu browsing, still require off-road visual attention that increases crash risk. In this article, we investigate whether adding sound to an in-vehicle user interface can provide the support necessary to create a significant reduction in glances toward a visual display when browsing menus.Methods: Two sound concepts were developed and studied; spearcons (time-compressed speech sounds) and earcons (musical sounds). A simulator study was conducted in which 14 participants between the ages of 36 and 59 took part. Participants performed 6 different interface tasks while driving along a highway route. A 3 × 6 within-group factorial design was employed with sound (no sound /earcons/spearcons) and task (6 different task types) as factors. Eye glances and corresponding measures were recorded using a head-mounted eye tracker. Participants’ self-assessed driving performance was also collected after each task with a 10-point scale ranging from 1 = very bad to 10 = very good. Separate analyses of variance (ANOVAs) were conducted for different eye glance measures and self-rated driving performance.Results: It was found that the added spearcon sounds significantly reduced total glance time as well as number of glances while retaining task time as compared to the baseline (= no sound) condition (total glance time M = 4.15 for spearcons vs. M = 7.56 for baseline, p =.03). The earcon sounds did not result in such distraction-reducing effects. Furthermore, participants ratings of their driving performance were statistically significantly higher in the spearcon conditions compared to the baseline and earcon conditions (M = 7.08 vs. M = 6.05 and M = 5.99 respectively, p =.035 and p =.002).Conclusions: The spearcon sounds seem to efficiently reduce visual distraction, whereas the earcon sounds did not reduce distraction measures or increase subjective driving performance. An aspect that must be further investigated is how well spearcons and other types of auditory displays are accepted by drivers in general and how they work in real traffic.","2015","2023-07-31 07:24:20","2023-07-31 07:24:20","","S25-S30","","sup1","16","","Traffic Injury Prevention","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E3B3NW8G","journalArticle","1997","Stevens, Robert D.; Edwards, Alistair D.N.; Harling, Philip A.","Access to Mathematics for Visually Disabled Students Through Multimodal Interaction","Human–Computer Interaction","","","10.1080/07370024.1997.9667240","","Mathematics relies on visual forms of communication and is thus largely inaccessible to people who cannot communicate in this manner because of visual disabilities. This article outlines the Mathtalk project, which addressed this problem by using computers to produce multimodal renderings of mathematical information. This example is unusual in that it is essential to use multiple modalities because of the nature and the difficulty of the application. In addition, the emphasis is on nonvisual (and hence novel) modalities. Crucial to designing a usable auditory interface to algebra notation is an understanding of the differences between visual and listening reading, particularly those aspects that make the former active and the latter passive. A discussion of these differences yields the twin themes of compensation for lack of external memory and provision of control over information flow. These themes were addressed by: the introduction of prosody to convey algebraic structure in synthetically spoken expressions; the provision of structure-based browsing functions; and the use of a prosody-based musical glance based on algebra earcons. The addition of prosody, when compared to a traditional method of presenting spoken algebra, was experimentally shown to increase the recovery of algebraic structure, enhance the retention of content, and reduce mental workload. These three factors can be said to compensate for the lack of an adequate external memory. Evaluations showed that the browsing functions and associated command language gave the fast and accurate control over information flow that is necessary for active reading. The algebra earcon was experimentally shown to convey the presence, location, and size of algebraic constructs within an expression in a manner that might be used as a rapid glance. Finally, an evaluation of the integrated components showed that the design principles derived from the Mathtalk program can give a more usable, active reading of algebra notation than that possible with traditional methods.","1997","2023-07-31 07:24:20","2023-07-31 07:24:20","","47-92","","1-2","12","","Human–Computer Interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N34ZK2FP","journalArticle","2018","Axon, Louise; Goldsmith, Michael; Creese, Sadie","Sonification Mappings: Estimating Effectiveness, Polarities and Scaling in an Online Experiment","J. Audio Eng. Soc","","","","","Sonification is a technique to present data arrays as sound, thereby taking advantage of the human ability to hear patterns that might otherwise not be apparent. Mappings from parameters of data to parameters of sound form the basis of parameter-mapping sonification. The choice of mappings and their design can influence both the utility of the sonification system and the ability of users to interpret the sounds. In this article the authors demonstrate the use of a time-efficient methodology with an experimental online platform for assessing mappings. Experiments explored the effectiveness of various mappings, and the discussions explore the implications of each approach. Based on the responses of 100 participants in an online Magnitude Estimation experiment, the effectiveness of 16 data-sound mappings was explored. Results showed that mappings involving certain sound parameters were generally effective, while those using other sound parameters varied in their effectiveness. In some cases the ability to interpret mappings and the polarities with which they were perceived varied among individuals using them. The mappings that used the tempo parameter were generally perceived effectively, while those using other sound parameters varied. Exploratory observations suggest that differences among participants might be related to different levels of musical experience.","2018","2023-07-31 07:24:20","2023-07-31 07:24:20","","1016–1032","","12","66","","J. Audio Eng. Soc","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X6MUDGUA","journalArticle","2012","Bujacz, Michal; Skulimowski, Piotr; Strumillo, Pawel","Naviton—A Prototype Mobility Aid for Auditory Presentation of Three-Dimensional Scenes to the Visually Impaired","J. Audio Eng. Soc","","","","","To augment the task of navigation and orientation of blind individuals, a new travel aid uses 3D scene sonification to present information about the environment using nonverbal audio. The model is composed of two classes of objects: obstacles and planes. The algorithm uses scene image segmentation, personalized spatial audio, musical tones, and sonar-like sound patterns. Individually measured head-related transfer functions were used to provide users with the illusion of sounds originating from the locations of sonified scene elements. Using a segmented and parametric description overcomes the sensory mismatch between visual and auditory perception. In a pilot study using both blind and sighted volunteers, subjects were able to utilize the prototype for spatial orientation and obstacle avoidance after a few minutes of training, attaining 90% accuracy in estimating the direction and depth of obstacles.","2012","2023-07-31 07:24:20","2023-07-31 07:24:20","","696–708","","9","60","","J. Audio Eng. Soc","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C5YMA6J4","journalArticle","2012","Schaffert, Nina; Gehret, Reiner; Mattes, Klaus","Modeling the Rowing Stroke Cycle Acoustically","J. Audio Eng. Soc","","","","","Because elite athletes require an unconscious and automated sense of time, and because sound is especially appropriate for conveying timing information, acoustic feedback can be especially useful in training of rowers. In the context of human movement, rhythm is a time accurate sequence of motor actions. Rhythm and synchronization are inseparable within a moving context. An auditory feedback signal based on boat acceleration helps rowers control their activities, and this sonified data can be stored in an audio file for later training and analysis. The improved sensitivity to the time-critical nature of the rowing cycle yielded an improved synchronization among the crew, as well as an improvement of individual athlete’s rowing technique.","2012","2023-07-31 07:24:20","2023-07-31 07:24:20","","551–560","","7/8","60","","J. Audio Eng. Soc","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R8GWQJR8","journalArticle","2012","Stewart, Rebecca; Sandler, Mark","Spatial Auditory Display in Music Search and Browsing Applications","J. Audio Eng. Soc","","","","","User interfaces for searching and browsing collections of music often use nonaudio for presenting information about the contents of the collection. This study reviews the literature to unify the various ways in which auditory spatialization can be used to augment the presentation of data. The authors examined 22 user interfaces that use such concepts as auditory icons, perceived location, amplitude panning, and a usability evaluation. Commonalities among the designs are discussed including the chosen spatialization approaches and evaluation methods.","2012","2023-07-31 07:24:20","2023-07-31 07:24:20","","936–946","","11","60","","J. Audio Eng. Soc","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F4R3JNF6","journalArticle","2018","Stolfi, Ariane; Sokolovskis, Janis; Goródscy, Fábio; Iazzetta, Fernando; Barthet, Mathieu","Audio Semantics: Online Chat Communication in Open Band Participatory Music Performances","J. Audio Eng. Soc","","","","","Technology-mediated audience participation is an emergent topic in creative music technology with a blurred distinction between audience and performers. This paper analyzes communication patterns occurring in the online chat of the Open Band system for participatory live music performance. In addition to acting as a multi-user messaging tool, the chat system also serves as a control interface for the sonification of textual messages from the audience. Open Band performances have been presented at various festivals and conferences since 2016. Its web-based platform enables collective “sound dialogues” that are opened to everyone regardless of musical skills. Drawing on interactive participatory art and networked music performance, the system aims to provide engaging social experiences in colocated music-making situations. The authors collected data from four public performances including over 3,000 anonymous messages sent by audiences. After presenting the design of the system, the authors analyzed the semantic content of messages using thematic and statistical methods. Findings show how different sonification mechanisms alter the nature of the communication between participants who articulate both linguistic and musical self-expression. One of the design goals was to provide a platform for free audience expression as a web “agora.” The various themes that emerged from the analyses endorse this idea, as participants felt free to discuss subjects ranging from love to political opinions.","2018","2023-07-31 07:24:20","2023-07-31 07:24:20","","910–921","","11","66","","J. Audio Eng. Soc","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8U5QJY8K","journalArticle","2020","Liew, Kongmeng; Lindborg, PerMagnus","A Sonification of Cross-Cultural Differences in Happiness-Related Tweets","J. Audio Eng. Soc","","","","","Sonification can be defined as any technique that translates data into non-speech sound with a systematic, describable, and reproducible method, in order to reveal or facilitate communication, interpretation, or discovery of meaning that is latent in the data. This paper describes an approach for communicating cross-cultural differences in sentiment data through sonification, which is a powerful technique for the translation of patterns into sounds that are understandable, accessible, and musically pleasant. A machine-learning classifier was trained on sentiment information of two samples of Tweets from Singapore and New York with the keyword of ""happiness."" Positive-valence words that relate to the concept of happiness showed stronger influences on the classifier than negative words. For mapping, Tweet frequency differences of the semantic variable ""anticipation"" affected tempo, positive-affected pitch, and joy-affected loudness, while ""trust"" affected rhythmic regularity. The authors evaluated sonification of the original data from the two cities, together with a control condition generated from random mappings in a listening experiment. Results suggest that the original was rated as significantly more pleasant.","2020","2023-07-31 07:24:20","2023-07-31 07:24:20","","25–33","","1/2","68","","J. Audio Eng. Soc","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X7USLUEX","journalArticle","2013","Kostek, Bożena","Auditory Display From The Music Technology Perspective","","","","","","This paper presents some applications of Auditory Displays (AD) in the domain of music technology. First, the scope of music technology and auditory display areas are shortly outlined. Then, the research trends and system solutions within the fields of music technology, music information retrieval and music recommendation are discussed. Finally, an example of an auditory display that facilities music annotation process based on gaze tracking is shown.","2013","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2ISVCUZA","journalArticle","2019","Li, Grace; Walker, Bruce N.","Mixed speech and non-speech auditory displays: impacts of design, learning, and individual differences in musical engagement","","","","","","Information presented in auditory displays is often spread across multiple streams to make it easier for listeners to distinguish between different sounds and changes in multiple cues. Due to the limited resources of the auditory sense and the fact that they are often untrained compared to the visual senses, studies have tried to determine the limit to which listeners are able to monitor different auditory streams while not compromising performance in using the displays. This study investigates the difference between non-speech auditory displays, speech auditory displays, and mixed displays; and the effects of the different display designs and individual differences on performance and learnability. Results showed that practice with feedback significantly improves performance regardless of the display design and that individual differences such as active engagement in music and motivation can predict how well a listener is able to learn to use these displays. Findings of this study contribute to understanding how musical experience can be linked to usability of auditory displays, as well as the capability of humans to learn to use their auditory senses to overcome visual workload and receive important information.","2019","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7Z25XF8N","journalArticle","2007","Mauney, Lisa M.; Walker, Bruce N.","Individual Differences and the Field of Auditory Display: Past Research, A Present Study, and an Agenda for the Future","","","","","","There has been some interest in the study of individual differences in the field of auditory displays, but we argue that there is a much greater potential than has been realized, to date. Relevant types of individual differences that may be applicable to interpreting auditory information include perceptual abilities, cognitive abilities, musical abilities, and learning styles. There are many measures of these individual differences available; however, they have not been thoroughly utilized in the auditory display arena. We discuss several types of individual differences relevant to auditory displays. We then present some examples of past research, along with the results of a current investigation of individual differences in auditory displays. Finally, we propose an agenda as to what research and tests should be used to further study this area.","2007","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3I5DDDFF","journalArticle","2005","Alty, James L.; Rigas, Dimitrios; Vickers, Paul","Music and speech in auditory interfaces: When is one mode more appropriate than another?","","","","","","A number of experiments, which have been carried out using non-speech auditory interfaces, are reviewed and the advantages and disadvantages of each are discussed. The possible advantages of using non-speech audio media such as music are discussed – richness of the representations possible, the aesthetic appeal, and the possibilities of such interfaces being able to handle abstraction and consistency across the interface.","2005","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QFSJ2JXV","journalArticle","2002","Neuhoff, J. G.; Knight, R.; Wayand, J.","Pitch change, sonification, and musical expertise: Which way is up?","","","","","","Frequency change is one of the most widely used acoustic dimensions in auditory display, and pitch perception is among the most widely researched topics in audition. Nonetheless, there is little research on the appropriate mapping and scaling of information to acoustic frequency in sonification. Here, we show that musical training is a contributing factor to the mapping, scaling, and conceptual relationships that exist between the information to be sonified and its acoustic representation. In Experiment 1, three groups of listeners that varied in musical expertise moved a slider to indicate the amount of pitch change that they heard in ten non-standard musical intervals. Listeners with more musical training showed greater slider movement in response to pitch change than musical novices, but not in response to brightness in a visual control condition. Novices also made significantly more errors in identifying the direction of pitch change for intervals that were well above discrimination thresholds. Experiment 2 showed that the errors by novices were due primarily to conceptual errors in labeling `rising' and `falling' pitch with a small but significant number of perceptual discrimination errors. The results suggest that musical training is an important factor in the mapping, scaling, and conceptual relationships used in sonification.","2002","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZECLA6G7","journalArticle","2002","Joseph, A. J.; Lodha, S. K.","Musart: Musical audio transfer function real-time toolkit","","","","","","This work describes the design and implementation of a sonification toolkit. MUSART (MUSical Audio transfer function Realtime Toolkit) is a sonification toolkit which produces musical sound maps that are played in real-time. Register, pitch, timbre, chords, duration, silence, loudness, beats, and panning are the musical concepts used to create melodic sound maps. Univariate and multivariate data sets are sonified using various sound parameter combinations and music tracks. Users have the flexibility to create personalized auditory displays by mapping each data dimension of a data set to one or more sound parameters. MUSART is designed to be flexible so that it can be used with many applications. In this work, we use musical auditory maps to explore seismic volumes used for detecting areas for drilling oil.","2002","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QA5QSC9Q","journalArticle","2007","Fagerlon, Johan","Expressive Musical Warning Signs","","","","","","Warning signals are often very simple and monotone sounds. This paper focuses on taking a more musical approach to the design of warnings and alarms than has been the case in the past. We present an experimental pilot study in which we explore the possibilities of using short musical pieces as warning signals in a vehicle cab. In the study, 18 experienced drivers experienced five different driving scenarios with different levels of urgency. Each scenario was presented together with an auditory icon, a traditional abstract warning sound, and a musical warning sound designed in collaboration with a composer. The test was carried out in an “audio-only” environment. Drivers were required to rate the perceived urgency, annoyance and appropriateness for every sound. They also had a chance to talk freely about the different warning signals. The results indicate interestingly that drivers may be able to understand the intended meaning of musical warning signals. It seems like the musical warning signals may prove useful primarily in situations of low and medium levels of urgency.","2007","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3YZ4VLTV","journalArticle","2005","Childs, Edward","Auditory graphs of real-time data","","","","","","The advantages of auditory display for monitoring real-time data are discussed. Parallels between the structures of real-time data and music are emphasized as potentially fruitful areas of research. The Accentus LLC design philosophy is described, followed by several examples of auditory graphs. Areas of future research are recommended.","2005","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"44GC39SQ","journalArticle","2018","Tislar, Kay; Duford, Zackery; Nelson, Brittany; Peabody, Madeline; Jeon, Myounghoon","Examining the learnability of auditory displays: Music, earcons, spearcons, and lyricons","","","","","","Auditory displays are a useful platform to convey information to users for a variety of reasons. The present study sought to examine the use of different types of sounds that can be used in auditory displays—music, earcons, spearcons, and lyricons—to determine which sounds have the highest learnability when presented in sequences. Participants were self-trained on sound meanings and then asked to recall meanings after listening to sequences of varying lengths. The relatedness of sounds and their attributed meanings, or the intuitiveness of the sounds, was also examined. The results show that participants were able to learn and recall lyricons and spearcons the best, and related meaning is an important contributing variable to learnability and memorability of all sound types. This should open the door for future research and experimentation of lyricons and spearcons presented in auditory streams.","2018","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UP8ZZETW","journalArticle","2011","Preti, Constanza; Schubert, Emery","Sonification of Emotions II: Live music in a pediatric hospital","","","","","","In this paper, we argue that music can be used to sonify emotions. Further, we propose that the ‘sonification of emotion’ conceptualization can explain some aspects of the practice of playing music in hospitals. Music can be constantly adjusted to reflect (but more accurately, we argue, sonify) various aspects of the emotional situations unfolding in a hospital, namely the interaction between the child/patient, their caregivers and the hospital staff. The case study of a musical interaction between a musician and a child/patient is presented and led to the development of a Cycle of Sonification model, where the musician collects complex environmental cues and then portrays them through the music to adjust the emotional ‘temperature’. That is, the emotion of the environment is reflected (sonified) by the music, but additionally the music is also calibrated so as to allow the regulation of the emotional mood (including distraction) in the otherwise stressful environment. As well as adjusting the mood, the music provides a ‘reading’ or measure of emotion through the auditory, non-speech mode, consistent with some pertinent definitions of sonification","2011","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EGNPCTTX","journalArticle","1997","Lodha, Suresh K.; Beahan, John; Heppe, Travis; Joseph, Abigail; Zane-Ulman, Brett","MUSE: A musical data sonification toolkit","","","","","","Data sonification is the representation of data using sound. Last year we presented a flexible, interactive and portable data sonification toolkit called LISTEN, that allows mapping of data to several sound parameters such as pitch, volume, timbre and duration [20]. One of the potential drawbacks of LISTEN is that since the sounds generated are non-musical, they can be fatiguing when exploring large data sets over extended periods of time. A primary goal in the design of MUSE – a MUsical Sonification Environment – is to map scientific data to musical sounds. The challenge is to ensure that the data meanings are preserved and brought out by these mappings. MUSE provides flexible data mappings to musical sounds using parameters such as pitch (melody), rhythm, tempo, volume, timbre and harmony. MUSE is written in C++ for the SGI platform and works with the freely available sound specification software CSound developed at MIT. We have applied MUSE to map uncertainty in some scientific data sets to musical sounds.","1997","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E87NC9F3","journalArticle","1996","Vickers, Paul; Alty, James L.","CAITLIN: A musical problem auralisation tool to assist novice programmers with debugging","","","","","","In the field of auditory display relatively little work has focused on the use of sound to aid program debugging. In this paper, we describe CAITLIN , a pre-processor for Turbo Pascal programs that musically auralises programs with a view to assisting novice programmers with locating errors in their code. A discussion follows of an experiment which showed that programmers could use the musical feedback to visualise and describe program structure. We then present conclusions and a discussion of future work.","1996","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YQIXKGI2","journalArticle","2012","Oswald, David","Non-Speech Audio-Semiotics: A Review and Revision of Auditory Icon and Earcon Theory","","","","","","The aim of this paper is to develop a theory and taxonomy of auditory signs, based on semiotics. For more than two decades, the discourse on non-speech audio interfaces has been dominated by a dichotomy between auditory icons, which are based on everyday hearing, and earcons, which are based on musical hearing. The corresponding theory behind these concepts has to be revised for several reasons. First, the authors of these theories partly use semiotic concepts and terminology, but not always in a correct way. Second, the classification of auditory icons as ""iconic"", and earcons as ""abstract"" is too simple and based on the questionable premise that everyday sounds are per se iconic and musical motives are per se abstract and symbolic. Third, this widespread idea ignores the crucial role of the user in the process of perception. In addition, the users' perception of visual and auditory signs in computer interfaces is fundamentally different today, from how it was in the early years of graphical user interfaces — the time when the first auditory interfaces and the corresponding theories were developed.","2012","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SWHSWJGW","journalArticle","2011","Schubert, Emery; Ferguson, Sam; Farrar, Natasha; McPherson, Gary E.","Sonification of Emotion I: Film Music","","","","","","This paper discusses the uses of sound to provide information about emotion. The review of the literature suggests that music is able to communicate and express a wide variety of emotions. The novel aspect of the present study is a reconceptualisation of this literature by considering music as having the capacity to sonify emotions. A study was conducted in which excerpts of non-vocal film music were selected to sonify six putative emotions. Participants were then invited to identify which emotions each excerpt sonified. The results demonstrate a good specificity of emotion sonification, with errors attributable to selection of emotions close in meaning to the target (excited confused with happy, but not with sad, for example). While ‘sonification’ of emotions has been applied in opera and film for some time, the present study allows a new way of conceptualizing the ability of sound to communicate affect through music. Philosophical and psychological implications are considered.","2011","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TG4KTS7E","journalArticle","2006","Margounakis, D.; Politis, D.","Converting images to music using their colour properties","","","","","","Music is associated to colors since ancient years. Different mappings between attributes of sound and images allow the efficient conversion between the two types of media. The proposed method for converting images to music using the concept of chromaticism provides the area of computer music with a parameterized environment for audio-visual presentations. The auditory display of colour images may bring the different ways that a listener perceives a musical piece (because of colour transitions) to light. A design template for chromatic synthesis is described. A short example, based on a graphical digital icon, demonstrates the preliminary results.","2006","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TZSZ7HR7","journalArticle","2008","Jung, Ralf","Ambience for Auditory Displays: Embedded Musical Instruments As Peripheral Audio Cues","","","","","","From alarm signals and data sonification to multimodal interfaces, auditory displays are omnipresent in our everyday life and they become more and more popular. But there are some challenges we have to meet because of the differentness of the auditory sense compared to the visual sense. Usually, audio notification signals are limited to simple warning cues and system feedback that are in most cases intrusive be- cause they differ from the environmental noise. That has the effect that people present in the room could be distracted from their current tasks because they cannot “close their ears.” To prevent the disturbing effect of traditional notification signals we developed the novel concept of non-speech audio notification embedded in ambient soundscapes to provide multi-user notification in a more discreet and non-disturbing way. Instead of using well-known non-speech cues like auditory icons and earcons, we decided to compose and record peripheral soundscapes and notification instruments by ourselves towards a more aesthetic approach. In this paper, we give an overview of our location-aware system with two applications (PAAN, AeMN) and sketch a real life scenario in a wine department of a supermarket. We will also present findings from a user study and provide a small collection of notification instruments and soundscapes as audio samples.","2008","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2MJ86TLR","journalArticle","2012","Schmele, Timothy; Gomez, Imanol","Exploring 3D audio for brain sonification","","","","","","Brain activity data, measured by functional Magnetic Resonance Imaging (fMRI), produces extremely high dimensional, sparse and noisy signals which are difficult to visualize, monitor and analyze. The use of spatial music can be particularly appropriate to represent its contained patterns. The literature describes several research done on sonifying neuroimaging data as well as different techniques to use spatialization as a musical language. In this paper, we discuss an artistic approach to fMRI sonification exploiting new compositional paradigms in spatial music. There fore, we have consider the brain activity as audio base material of a the spatial musical composition. Our approach attempts to explore the aesthetic potential of brain sonification not by transforming the data beyond the recognizable, and presenting the data as direct as possible.","2012","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WGN87MZ9","journalArticle","2018","Rönnberg, Niklas; Löwgren, Jonas","Photone: Exploring modal synergy in photographic images and music","","","","","","We present Photone, an interactive installation combining photographic images and musical sonification. An image is displayed, and a dynamic musical score is generated based on the overall color properties of the image and the color value of the pixel under the cursor. Hence, the music changes as the user moves the cursor. This simple approach turns out to have interesting experiential qualities in use. The composition of images and music invites the user to explore the combination of hues and textures, and musical sounds. We characterize the resulting experience in Photone as one of modal synergy where visual and auditory output combine holistically with the chosen interaction technique. This tentative finding is potentially relevant to further research in auditory displays and multimodal interaction.","2018","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NR9QWUVY","journalArticle","2012","Gossmann, Joachim","A perspective on the limited potential for simultaneity in auditory display","","","","","","The auditory environment is frequently described as a juxtaposition between an array of pre-disclosed auditory streams and a process of attentional selection. The orientation of attentional selection toward environmental streams is characterized by a differentiation in the types of stream: Speech, music and sound effects are only three examples in an open polymorphism of what could be described as perceptual strategies through which we access the sounding world. The differentiable-simultaneous manifold of environmental streams allows perceptual participation only in a certain number of streams at the same time - only one speaking voice, one sense of ""harmony"", a single ""rhythm"", and so forth: We propose a re-basing of sonfication strategies not on the definition of external mechanisms, but on the definition and application of new modal strategies that are circumscribed and accessible through 'what is not possible to perceive at the same time'.","2012","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9HPCA5FF","journalArticle","2014","Jeon, Myounghoon; Sun, Yuanjing","Design and Evaluation of Lyricons (Lyrics + Earcons) for Semantic and Aesthetic Improvements of Auditory Cues","","","","","","Auditory researchers have developed various non-speech cues in designing auditory user interfaces. A preliminary study of “Lyricons” (lyrics + earcons) has provided a novel approach to devising auditory cues in electronic products, by combining the concurrent two layers of musical speech and earcons (short musical motives). The purpose of the present study is to introduce iterative design processes and to validate the effectiveness of lyricons compared to earcons, whether people can more intuitively grasp functions that lyricons imply than those of earcons. Results favor lyricons over earcons. Future work and practical application directions are also discussed.","2014","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BAZWX7N3","journalArticle","2018","Middleton, Jonathan; Hakulinen, Jaakko; Tiitinen, Katariina; Hella, Juhu; Keskinen, Tuuli; Huuskonen, Pertti; Linna, Juhani; Turunen, Markku; Ziat, Mounia; Raisamo, Roope","Sonification with musical characteristics: a path guided by user engagement","","","","","","Sonification with musical characteristics can engage users, and this dynamic carries value as a mediator between data and human perception, analysis, and interpretation. A user engagement study has been designed to measure engagement levels from conditions within primarily melodic, rhythmic, and chordal contexts. This paper reports findings from the melodic portion of the study, and states the challenges of using musical characteristics in sonifications via the perspective of form and function – a long standing debate in Human-Computer Interaction. These results can guide the design of more complex sonifications of multivariable data suitable for real life use.","2018","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G529VJZW","journalArticle","2013","Avissar, Daniel; Leider, Colby; Bennett, Hristopher; Gailey, Robert","An audio game app using interactive Movement sonification for Targeted posture control","","","","","","Interactive movement sonification has been gaining validity as a technique for biofeedback and auditory data mining in research and development for gaming, sports, and physiotherapy. Naturally, the harvesting of kinematic data over recent years has been a function of an increased availability of more portable, high-precision sensory technologies, such as smart phones, and dynamic real time programming environments, such as Max/MSP. Whereas the overlap of motor skill coordination and acoustic events has been a staple to musical pedagogy, musicians and music engineers have been surprisingly less involved than biomechanical, electrical, and computer engineers in research efforts in these fields. Thus, this paper proposes a prototype for an accessible virtual gaming interface that uses music and pitch training as positive reinforcement in the accomplishment of target postures.","2013","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TD43PFRU","book","2015","Walus, Bartlomiej P.; Pauletto, Sandra; Mason-Jones, Amanda","Evaluating the use of sonification and music to support the communication of alcohol health risk to young people: Initial results","","","","","","The interdisciplinary research project, Using Sonification to COmmunicate public health Risk data (SCORe), aims to experimentally test how sonification, interactivity in combination with music, could increase the communicative potential of a visual presentation directed to young people and focused on health risk data of alcohol consumption. Specifically, we are studying how this type of presentation can support engagement with health information, and the effective interpretation and recall of data. In order to explore the possible influence of sound in understanding important health risk messages, a 3-arm pilot randomised control (participant-blinded) trial was designed. We compared a visual presentation augmented by sonification, music and interaction with a simple visual presentation and a visual presentation augmented by simple user interaction. This paper describes the most complex of the three health presentations (the audio-visual and interactive presentation) and presents initial findings that relate to this presentation only.","2015","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UXFH2K2S","journalArticle","2008","Polotti, Pietro; Benzi, Carlo","Rhetorical Schemes for Audio Communication","","","","","","The application of rhetorical techniques to the use of non-verbal sound in the interaction between humans and technologies is the core idea of this paper. We present our ideas at a general level and illustrate an exploratory case based on the application of rhetorical schemes to the sonification of computer operating system events. Both cases of musical sounds and everyday sounds are investigated. This work is intended as a preliminary study aiming at motivating a larger scale and more rigorous research about the potentiality of the use of rhetoric in the domain of Auditory Display (AD) and Sonic Interaction Design (SID).","2008","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YV6P2FVA","journalArticle","2019","Kleinberger, Rebecca; George, Stefanakis; Franjou, Sebastian","Speech companions: Evaluating the effects of musically modulated auditory feedback on the voice","","","","","","Changing the way one hears one's own voice, for instance by adding delay or shifting the pitch in real-time, can alter vocal qualities such as speed, pitch contour, or articulation. We created new types of auditory feedback called Speech Companions that generate live musical accompaniment to the spoken voice. Our system generates harmonized chorus effects layered on top of the speakerﾒs voice that change chord at each pseudo-beat detected in the spoken voice. The harmonization variations follow predetermined chord progressions. For the purpose of this study we generated two versions: one following a major chord progression and the other one following a minor chord progression. We conducted an evaluation of the effects of the feedback on speakers and we present initial findings assessing how different musical modulations might potentially affect the emotions and mental state of the speaker as well as semantic content of speech, and musical vocal parameters.","2019","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8ZHDAAQR","journalArticle","2013","Henry, Ashley G.; Bruce, Carrie M.; Winton, Riley J.; Walker, Bruce N.","Sonification Mapping Configurations: Pairings Of Real-Time Exhibits And Sound","","","","","","Visitors to aquariums typically rely on their vision to interact with live exhibits that convey rich descriptive and aesthetic visual information. However, some visitors may prefer or need to have an alternative interpretation of the exhibitÕs visual scene to improve their experience. Musical sonification has been explored as an interpretive strategy for this purpose and related work provides some guidance for sonification design, yet more empirical work on developing and validating the music-to-visual scene mappings needs to be completed. This paper discusses work to validate mappings that were developed through an investigation of musician performances for two specific live animal exhibits at the Georgia Aquarium. In this proposed study, participants will provide feedback on musical mapping examples which will help inform design of a real-time sonification system for aquarium exhibits. Here, we describe our motivation, methods, and expected contributions.","2013","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RWCP2LK3","journalArticle","2021","Roddy, Stephen; Bridges, Brian","The design of a smart city sonification system using a conceptual blending and musical framework, web audio and deep learning techniques","","","","","","This paper describes an auditory display system for smart city data for Dublin City, Ireland. It introduces and describes the different layers of the system and outlines how they operate individually and interact with one another. The system uses a deep learning model called a variational autoencoder to generate musical content to represent data points. Further data-to-sound mappings are introduced via parameter mapping sonification techniques during sound synthesis and post-processing. Conceptual blending and music theory provide frameworks, which govern the design of the system. The paper ends with a discussion of the design process that contextualizes the contribution, highlighting the interdisciplinary nature of the project, which spans data analytics, music composition and human-computer interaction.","2021","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8PUZ88VM","journalArticle","2017","Fox, K. Michael; Stewart, Jeremy; Hamilton, Rob","madBPM: Musical and Auditory Display for Biological Predictive Modeling","","","","","","The modeling of biological data can be carried out using structured sound and musical process in conjunction with integrated visualizations. With a future goal of improving the speed and accuracy of techniques currently in use for the production of synthetic high value chemicals through the greater understanding of data sets, the madBPM project couples real-time audio synthesis and visual rendering with a highly flexible data-ingestion engine. Each component of the madBPM system is modular, allowing for customization of audio, visual and data-based processing.","2017","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QQAPJY82","journalArticle","2016","Landry, Steven; Sun, Yuangjing; Slade, Darnishia; Jeon, Myounghoon","Tempo-Fit Heart Rate App: Using Heart Rate Sonification As Exercise Performance Feedback","","","","","","Physical inactivity is a worldwide issue causing a variety of health problems. Exploring novel ways to encourage people to engage in physical activity is a topic at the forefront of research for countless stakeholders. Based upon a review of the literature, a pilot study, and exit interviews, we propose an app prototype that utilizes music tempo manipulation to guide users into a target heart rate zone during an exercise session. A study was conducted with 26 participants in a fifteen-minute cycling session using different sonification mappings and combinations of audiovisual feedback based on the user's current heart rate. Results suggest manipulating the playback speed of music in real time based on heart rate zone departures can be an effective motivational tool for increasing or decreasing activity levels of the listener. Participants vastly preferred prescriptive sonifications mappings over descriptive mappings, due to people's natural inclination to follow the tempo of music.","2016","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IA985QFZ","journalArticle","2011","Mealla, Sebastian; Bosi, Mathieu; Jorda, Sergi; Valjamae, Aleksander","Sonification of Brain and Body Signals in Collaborative Tasks Using a Tabletop Musical Interface","","","","","","Physiological Computing has been applied in different disciplines, and is becoming popular and widespread in Human-Computer In- teraction, due to device miniaturization and improvements in real- time processing. However, most of the studies on physiology- based interfaces focus on single-user systems, while their use in Computer-Supported Collaborative Work (CSCW) is still emerg- ing. The present work explores how sonification of human brain and body signals can enhance user experience in collaborative mu- sic composition. For this task, a novel multimodal interactive sys- tem is built using a musical tabletop interface (Reactable) and a hybrid Brain-Computer Interface (BCI). The described system al- lows performers to generate and control sounds using their own or their fellow team member’s physiology. Recently, we assessed this physiology-based collaboration system in a pilot experiment. Dis- cussion on the results and future work on new sonifications will be accompanied by practical demonstration during the conference.","2011","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"68B32FTN","journalArticle","2011","Tissberger, Johann P.; Wersenyi, Gyorgy","Sonification Solutions for Body Movements in Rehabilitation of Locomotor Disorders","","","","","","One of the recent fields of sonification focuses on the sonification of body movements in sports or rehabilitation. This is usually some kind of monitoring of real-time measurement data and auditory feedback for the patient. This paper presents two sonification approaches in medicine: a balancing coordination system and a robot for moving the legs after serious injuries of the lower body parts. These two systems are evaluated and compared based on the method of sonification, and transmission and analysis of the auditory information. Finally, a supposed method for using musical notes and measures is presented, and a selection method for the length of sonification based on the initial time interval is suggested.","2011","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X5QWFZ2P","journalArticle","2013","Lukasik, Ewa; Materski, Michal","Sonification Of A Virtual Model Of The Old Rare Musical Instrument","","","","","","The paper describes a project whose goal was to enable users realistically interact with a 3D virtual model of a historical musical instrument – the clavichord attributed to the famous 18th century maker. A challenge of enabling the user to play the virtual copy of the instrument was resolved by using the dynamic MIDI keyboard. The prerecorded clavichord sound samples are controlled by the software using the MIDI commands. Playing the real keyboard is synchronized with the movement of virtual keys and other parts of sound generating mechanism. The sound effects, characteristic for the clavichord Tragen der Tonne and Bebung may be imitated, which gives the user the impression of playing the real instrument.","2013","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PGHUHSLX","journalArticle","2001","Barra, Maria; Cillo, Tania; de Santis, Antonio; Petrillo, Umberto Ferraro; Negro, Alberto; Scarano, Vittoro; Matlock, Teenie; Maglio, Paul P.","Personal webmelody: Customized sonification of web servers","","","","","","This paper presents Personal WebMelody, a sonified web server that informs its administrator of both normal and abnormal operation through background music. It allows customization and full integration of system-generated music representing web server activity with external music sources (audio CD, MP3, etc) selected by the administrator. Our sonification technique works by associating MIDI or WAV sound tracks with web server events. In an attempt to enable the webmaster to listen to such system-generated music for a long period without becoming fatigued, we introduce the opportunity of mixing an external music source with systemgenerated music. In this way, the administrator can hear the status of the web server while listening to his or her preferred music. We present an empirical study that shows how our web server sonification can convey useful information efficiently.","2001","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KUPF85BS","journalArticle","2004","Ciardi, F. C.","sMax: A multimodal toolkit for stock market data sonification","","","","","","In this work, we present sMax a multimodal toolkit for stock market data sonification. Unlike most research focusing their effort primarily on the sonification of single stock information, sMax provides an auditory display for the user to monitor parallel distributed data. sMax uses a set of Java and Max modules to map real time stock market information into recognizable musical patterns. One of the main design goals of the toolkit is to allow low-latency controls over real-time data sonification. Because of its object-oriented architecture, sMax can be easily extended by the user when additional functionality is required. The project outcomes range from the creation of art installations to auditory display for mobile computing devices. We present the theoretical background, and the structure of the program.","2004","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"78PGRZPX","journalArticle","2018","Worrall, David","Sonification: A Prehistory","","","","","","The idea that sound can convey information predates the modern era, and certainly the computational present. Data sonification can be broadly described as the creation, study and use of the non-speech aural representation of information to convey information. As a field of contemporary enquiry and practice, data sonification is young, interdisciplinary and evolving; existing in parallel to the field of data visualization. Drawing on older practices such as auditing, and the use of information messaging in music, this paper provides an historical understanding of how sound and its representational deployment in communicating information has changed. In doing so, it aims to encourage a critical awareness of some of the socio- cultural as well as technical assumptions often adopted in sonifying data, especially those that have been developed in the context of Western music of the last half-century or so.","2018","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9D7XU8XJ","book","2009","Beilharz, Kirsty; Ferguson, Sam","An interface and framework design for interactive aesthetic sonification","","","","","","This paper describes the interface design of our AeSon (Aesthetic Sonification) Toolkit motivated by user-centred customisation of the aesthetic representation and scope of the data. The interface design is developed from 3 premises that distinguish our approach from more ubiquitous sonification methodologies. Firstly, we prioritise interaction both from the perspective of changing scale, scope and presentation of the data and the user's ability to reconfigure spatial panning, modality, pitch distribution, critical thresholds and granularity of data examined. The user, for the majority of parameters, determines their own listening experience for real-time data sonification, even to the extent that the interface can be used for live data-driven performance, as well as traditional information analysis and examination. Secondly, we have explored the theories of Tufte, Fry and other visualization and information design experts to find ways in which principles that are successful in the field of information visualization may be translated to the domain of sonification. Thirdly, we prioritise aesthetic variables and controls in the interface, derived from musical practice, aesthetics in information design and responses to experimental user evaluations to inform the design of the sounds and display. In addition to using notions of meter, beat, key or modality and emphasis drawn from music, we draw on our experiments that evaluated the effects of spatial separation in multivariate data presentations.","2009","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UFR568S5","journalArticle","2019","Seica, Mariana; Martins, Pedro; Roque, Licinio; Cardoso, F. Amilcar","A sonification experience to portray the sounds of portuguese consumption habits","","","","","","The stimuli for consumption is present in everyday life, where major retail companies play a role in providing a large range of products every single day. Using sonification techniques, we present a listening experiment of Portuguese consumption habits in the course of ten days, gathered from a Portuguese retail company. We focused on how to represent this time-series data as a musical piece that would engage the listenerﾒs attention and promote an active listening attitude, exploring the influence of aesthetics in the perception of auditory displays. Through a phenomenological approach, ten participants were interviewed to gather perceptions evoked by the piece, and how the consumption variations were understood. The tested composition revealed relevant associations about the data, with the consumption context indirectly present throughout the emerging themes: from the idea of everyday life, routine and consumption peaks to aesthetic aspects as the passage of time, frenzy and consumerism. Documentary, movie imagery and soundtrack were also perceived. Several musical aspects were also mentioned, as the constant, steady rhythm and the repetitive nature of the composition, and sensations such as pleasantness, satisfaction, annoyance, boredom and anxiety. These collected topics convey the incessant feeling and consumption needs which portray our present society, offering new paths for comprehending musical sound perception and consequent exploration.","2019","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PCSTQI8G","journalArticle","2011","Nguyen, Vinh Xuan","Tonal DisCo: Dissonance and Consonance in a Gaming Engine","","","","","","Whilst there are several existing toolkits specifically designed for sonification, there has been little investigation into the utilization of computer game engines for sonification. This paper will demonstrate the implementation of a real time game engine for the purpose of sonification and discuss the opportunities and limitations. An important aspect which is lacking in existing sonification toolkits is the ability to sonify streaming data in real-time. Gaming engines not only offer the potential to do this but also offer the ability to visualize data in 3D and in real-time. The sound design of an art exhibition is used as a case study to demonstrate the potential of a computer game editor/sandbox used for visualization and sonification. For the exhibition real-world objects were tracked inside a gallery space and represented in the virtual environment of a computer game, which was displayed on a projector screen. Their movement was sonified into musical form to convey their steady/consistent movement as consonant and their agitated/inconsistent movement as dissonant. Tonal “DisCo” is used to describe their dissonance and consonance rating in both musical tone and visual color. Although the sonification of data into musical structure distorts the accuracy of absolute data values, it does maintain the relationship between data values. This loss of resolution is counteracted by an increase in clarity of data relationships. This case study appropriates the single ratio scale of pitch into both an interval scale of tone and an ordinal scale of octaves in order to express interrelationships.","2011","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TIU5JN5D","journalArticle","2017","Tsuchiya, Takahiko; Freeman, Jason","Spectral Parameter Encoding: Towards a Framework for Functional-Aesthetic Sonification","","","","","","Auditory-display research has had a largely unsolved challenge of balancing functional and aesthetic considerations. While functional designs tend to reduce musical expressivity for the fidelity of data, aesthetic or musical sound organization arguably has a potential for representing multi-dimensional or hierarchical data structure with enhanced perceptibility. Existing musical designs, however, generally employ nonlinear or interpretive mappings that hinder the assessment of functionality. The authors propose a framework for designing expressive and complex sonification using small timescale musical hierarchies, such as the harmony and timbral structures, while maintaining data integrity by ensuring a close-to-the-original recovery of the encoded data utilizing descriptive analysis by a machine listener.","2017","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VBGKYTAA","journalArticle","2008","Bologna, Guido; Deville, Benoit; Pun, Thierry","Pairing Colored Socks and Following a Red Serpentine With Sounds of Musical Instruments","","","","","","The See ColOr interface transforms a small portion of a colored video image into sound sources represented by spatialized musical instruments. This interface aims at providing visually impaired people with a capability of perception of the environment. As a first step of this on-going project, the purpose is to verify the hypothesis that it is possible to use sounds from musical instruments to replace color. Compared to state of the art devices, a quality of the See ColOr interface is that it allows the user to receive a feed-back auditory signal from the environment and its colors, promptly. Two experiments based on a head mounted camera have been performed. The first experiment pertaining to object manipulation is based on the pairing of colored socks, while the second experiment is related to outdoor navigation with the goal of following a colored serpentine. The “socks” experiment demonstrated that seven blindfolded individuals were able to accurately match pairs of colored socks. The same participants successfully followed a red serpentine for more than 80 meters.","2008","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5EQ8CMW7","journalArticle","2002","Upson, R.","Educational sonification exercises: Pathways for mathematics and musical achievement","","","","","","This paper reports developments in the use of sonifications and sonification software for educational purposes. Adolescent subjects received training in Cartesian graphing over several sessions with sonification software and a sonification-enhanced curriculum. The project attracted students with low linguistic and logical-mathematical capabilities. Students were engaged by musical composition activities, but they remained anxious about traditional mathematics activities. Though students' mathematical abilities improved only slightly according to a traditional mathematical assessment, this project demonstrated the students' increased comfort level with the subject of mathematics and an increased understanding of the concepts within their own set of linguistics.","2002","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"36I8JDA6","book","2009","Vogt, Katharina; Pirro, David; Kobenz, Ingo; Höldrich, Robert; Eckel, Gerhard","Physiosonic - movement sonification as auditory feedback","","","","","","We detect human body movement interactively via a tracking sys- tem. This data is used to synthesize sound and transform sound files (music or text). A subject triggers and controls sound param- eters with his or her movement within a pre-set range of motion. The resulting acoustic feedback enhances new modalities of per- ception and the awareness of the body movements. It is ideal for application in physiotherapy and other training contexts. The sounds we use depend on the context and aesthetic pref- erences of the subject. On the one hand, metaphorical sounds are used to indicate the leaving of the range of motion or to make un- intended movements aware. On the other hand, sound material like music or speech is played as intuitive means and motivating feedback to address humans. The sound material is transformed in order to indicate deviations from the target movement. With this sonification approach, subjects perceive the sounds they have cho- sen themselves in undistorted playback as long as they perform the training task appropriately. Our main premises are a simple map- ping of movement to sound and common sense metaphors, that both enhance the understanding for the subject.","2009","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A8BATR4Q","journalArticle","2013","Vogt, Katharina; Goudarzi, Visda; Parncutt, Richard","Empirical Aesthetic Evaluation Of Sonifications","","","","","","This paper discusses three experiments on the aesthetic evaluation of different sonifications. The effects of training and understanding of the auditory display on its aesthetic appealing were tested. Results showed no significant effect, but a trend towards less acceptance due to longer exposure to the sounds in general. Furthermore, there might be effects of musical ability and gender that should be further explored.","2013","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KDXVKYHX","journalArticle","2004","Walker, Bruce N.; Mauney, Lisa M.","Individual differences, cognitive abilities, and the interpretation of auditory graphs","","","","","","Auditory graphs exploit pattern recognition in the auditory system, but questions remain about the relationship between cognitive abilities, demographics, and sonification interpretation. Subjects completed a magnitude estimation task relating sound dimensions to data dimensions. Subjects also completed a working memory task (2-back task) and a spatial reasoning task (Raven's Progressive Matrices) to assess cognitive abilities. Demographics, such as gender, age, handedness, and musical experience, were also reported and included in the analysis. A stepwise multiple regression analysis was performed to determine the relationship between the independent (cognitive abilities and demographics) and dependent (individual slopes and R-squared values) variables. The regression analysis indicates some support for most of the predictor variables, especially predicting R-squared values. The 2-back task does not seem to contribute significantly to the interpretation of sonifications and auditory graphs. However, Raven's and many of the demographic variables do show predictive value for interpretation of auditory graphs.","2004","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q2LYLLHC","journalArticle","2019","Falk, Courtney; Dykstra, Josiah","Sonification with music for cybersecurity situational awareness","","","","","","Cyber defenders work in stressful, information-rich, and highstakes environments. While other researchers have considered sonification for security operations centers (SOCs), the mappings of network events to sound parameters have produced aesthetically unpleasing results. This paper proposes a novel sonification process for transforming data about computer network traffic into music. The musical cues relate to notable network events in such a way as to minimize the amount of training time a human listener would need in order to make sense of the cues. We demonstrate our technique on a dataset of 708 million authentication events over nine continuous months from an enterprise network. We illustrate a volume-centric approach in relation to the amplitude of the input data, and also a volumetric approach mapping the input data signal into the number of notes played. The resulting music prioritizes aesthetics over bandwidth to balance performance with adoption.","2019","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MA2N8WX2","journalArticle","2021","Kantan, Prithvi Ravi; Spaich, Erika G.; Dahl, Sofia","A metaphor-based technical framework for musical sonification in movement rehabilitation","","","","","","Interactive sonification has increasingly shown potential as a means of biofeedback to aid motor learning in movement rehabilitation. However, this application domain faces challenges related to the design of meaningful, task-relevant mappings as well as aesthetic qualities of the sonic feedback. A recent mapping design approach is that of using conceptual metaphors based on image schemata and embodied music cognition. In this work, we developed a framework to facilitate the design and real-time exploration of rehabilitation-tailored mappings rooted in a specific set of music-based conceptual metaphors. The outcome was a prototype system integrating wireless inertial measurement, flexible real-time mapping control and physical modelling-based musical sonification. We focus on the technical details of the system, and demonstrate mappings that we created through it for two exercises. These will be iteratively honed and evaluated in upcoming usercentered studies. We believe our framework can be a useful tool in musical sonification design for motor learning applications.","2021","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4FR7X5NK","journalArticle","2021","Yang, Jing; Roth, Andreas","Musical features modification for less intrusive delivery of popular notification sounds","","","","","","Less intrusive information delivery has been a popular research topic for auditory displays. While most research has addressed this issue by creating new notification cues such as rendering ambient soundscapes or modifying background music, we present a novel method to gently deliver artificial notification sounds that have been commonly used in digital devices and for popular applications. We propose to play a notification sound by embedding it into the music that a user is listening to, after changing the musical timbre, amplitude, tempo, and octave of the notification to match these features of the music. To implement this concept, we extend a melody extraction algorithm for notification timbre transfer, and we present a pipeline that algorithmically selects a proper time spot and harmoniously embeds the notification into music. To validate our design concept, we present a user study comparing our method with the standard method of playing notification sounds on digital devices. Through an extensive analysis of 96 tasks performed by 32 participants, we demonstrate that our method can deliver notification sounds in a less intrusive but adequately noticeable manner and is preferred by most participants.","2021","2023-07-31 07:24:20","2023-07-31 07:24:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HSVD8FRJ","journalArticle","2006","Orzessek, B.; Falkner, M.","Sonification of autonomic rhythms in the frequency spectrum of heart rate variability","","","","","","This poster presents some of the work currently being done at the Paracelsus Clinic in Switzerland on heart rate variability biofeedback with a real time auditory display. Heart rate variability biofeedback is an important diagnostic and therapeutic tool in the work with a wide variety of chronic disorders. We use a proprietary building-block type laboratory computer program that is linked via MIDI to a software sequencer with a VST virtual instrument library. Beyond the sonification of RR intervals as discrete numbers, the development of new techniques became necessary in order to be able to sonify the dynamic, wave-like structure of autonomic rhythms in the frequency spectrum of HRV, what we call ”heartmusic”. The fact that patients can hear their inner autonomic activity as music in real time and so work with elements of their own autonomous rhythmic oscillations, may also add an important new dimension to this field in the future.","2006","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WKVZLRXH","journalArticle","2019","Nadri, Chihab; Anaya, Chairunisa; Yuan, Shan; Jeon, Myounghoon","Preliminary guidelines on the sonification of visual artworks: Linking music, sonification & visual arts","","","","","","Sonification and data processing algorithms have advanced over the years to reach practical applications in our everyday life. Similarly, image processing techniques have improved over time. While a number of image sonification methods have already been developed, few have delved into potential synergies through the combined use of multiple data and image processing techniques. Additionally, little has been done on the use of image sonification for artworks, as most research has been focused on the transcription of visual data for people with visual impairments. Our goal is to sonify paintings reflecting their art style and genre to improve the experience of both sighted and visually impaired individuals. To this end, we have designed initial sonifications for paintings of abstractionism and realism, and conducted interviews with visual and auditory experts to improve our mappings. We believe the recommendations and design directions we have received will help develop a multidimensional sonification algorithm that can better transcribe visual art into appropriate music.","2019","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3YYCDYAN","journalArticle","2003","England, David; Salces, Fausto J. Sainz; Vickers, Paul","Household appliances control device for the elderly","","","","","","An evaluation of musical earcons was carried out to see whether they are an effective and efficient method of delivering information about household appliances to elderly people. A test was carried out to explore the ability of the elderly subjects in remembering and learning the musical earcons. This test indicated a poor rate of recognition of the earcons. A second test that included the presentation of information in three modes (audio, visual and multimodal) was performed to determine which modality was preferred to deliver certain types of information among this group. We hypothesized that the multimodal interface would be the best in terms of speed and accuracy of response, and this was supported by the data. The results showed the need for a redesign of the earcons.","2003","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N7L7VRCR","journalArticle","2021","Fiedler, Brett L.; Walker, Bruce N.; Moore, Emily B.","To sonify or not to sonify? Educator perceptions of auditory display in interactive simulations","","","","","","With the growing presence of auditory display in popular learning tools, it is beneficial to researchers to consider not only the perceptions of the students who use the tools, but the educators who include the tools in their curriculum. We surveyed over 4000 educators to investigate educator perceptions and preferences across four interactive physics simulations for the presence and qualities of non-speech auditory display, as well as surveying users' selfrated musical sophistication as potentially predictive of auditory display preference. We find that the majority of teachers preferred the simulations with auditory display and consistently rated aspects of the experience using simulations with sound positively over the without-sound variants. We also identify simulation design features that align with trends in educator ratings. We did not find the measured musical sophistication to be a predictor of auditory display preference.","2021","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RQ7REQBC","journalArticle","2002","Johannsen, G.","Auditory display of directions and states for mobile systems","","","","","","Auditory displays for mobile systems, such as service robots, have been developed. The design of directional sounds and of additional sounds for robot states (e.g., Heavy Load), as well as the design of more complicated robot sound tracks are explained. Basic musical elements and robot movement sounds have been combined. Two experimental studies, on the understandability of the directional sounds and the robot state sounds as well as on the auditory perception of intended robot trajectories in a simulated supermarket scenario, are described. Subjective evaluations of sound characteristics such as urgency, expressiveness, and annoyance have been performed by non-musicians and musicians. These experimental results are compared with the diagrams which have been computed with two wavelet techniques for time-frequency analyses.","2002","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X4GMN96R","journalArticle","2018","Khan, Ridwan Ahmed; Jeon, Myounghoon; Yoon, Tejin","“Musical Exercise” for people with visual impairments: A preliminary study with the blindfolded","","","","","","Performing independent physical exercise is critical to maintain one's good health, but it is specifically hard for people with visual impairments. To address this problem, we have developed a Musical Exercise platform for people with visual impairments so that they can perform exercise in a good form consistently. We designed six different conditions, including blindfolded or visual without audio conditions, and blindfolded or visual with two different types of audio feedback (continuous vs. discrete) conditions. Eighteen sighted participants participated in the experiment, by doing two exercises - squat and wall sit with all six conditions. The results show that Musical Exercise is a usable exercise assistance system without any adverse effect on exercise completion time or perceived workload. Also, the results show that with a specific sound design (i.e., discrete), participants in the blindfolded condition can do exercise as consistently as participants in the non-blindfolded condition. This implies that not all sounds equally work and thus, care is required to refine auditory displays. Potentials and limitations of Musical Exercise and future works are discussed with the results.","2018","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FCGK5FCQ","book","2015","Rutz, Hanns Holger; Vogt, Katharina; Höldrich, Robert","The SysSon platform: A computer music perspective of sonification","","","","","","We introduce SysSon, a platform for the development and application of sonification. SysSon aims to be an integrative system that serves different types of users, from domain scientists to sonification researchers to composers and sound artists. It therefore has an open nature capable of addressing different usage scenarios. We have used SysSon both in workshops with climatologists and sonification researchers and as the engine to run a real-time sound installation based on climate data. The paper outlines the architecture and design decisions made, showing how a sonification system can be conceived as a collection of specialised abstractions that sit atop a general computer music environment. We report on our experience with SysSon so far and make suggestions about future improvements.","2015","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"95N7IGTN","journalArticle","2017","Newbold, Joseph W.; Bianchi-Berthouze, Nadia; Gold, Nicolas E.","Musical Expectancy in Squat Sonification For People Who Struggle With Physical Activity","","","","","","Physical activity is important for a healthy lifestyle. However, it can be hard to stay engaged with exercise and this can often lead to avoidance. Sonification has been used to support physical activity through the optimisation/correction of movement. Though previous work has shown how sonification can improve movement execution and motivation, the specific mechanisms of motivation have yet to be investigated in the context of challenging exercises. We investigate the role of music expectancy as a way to leverage people’s implicit and embodied understanding of music within movement sonification to provide information on technique while also motivating continuation of movement and rewarding its completion. The paper presents two studies showing how this musically informed sonification can be used to support the squat movement. The results show how musical expectancy impacted people’s perception of their own movement, in terms of reward, motivation and movement behaviour and the way in which they moved.","2017","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LNQS2EGG","journalArticle","2018","Tsuchiya, Takahiko; Freeman, Jason","A study of exploratory analysis in melodic sonification with structural and durational time scales","","","","","","Melodic sonification is one of the most common methods of sonification: data modulates the pitch of an audio synthesizer over time. This simple sonification, however, still raises questions about how we listen to a melody and perceive the motions and patterns characterized by the underlying data. We argue that analytical listening to such melodies may focus on different ranges of the melody at different times and discover the pitch (and data) relationships gradually over time and after repeated listening. To examine such behaviors in real-time listening to a melodic sonification, we conducted a user study employing interactive time and pitch resolution controls for the user. The study also examines the relationships of these changing time and pitch resolutions to perceived musicality. The results indicate a stronger general relationship between the time progression and the use of time-resolution control to analyze data characteristics, while the pitch resolution controls tend to have more correlation with subjective perceptions of musicality.","2018","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8RP2TJGN","journalArticle","2004","Roeber, N.; Masuch, M.","Interacting with sound: An interaction paradigm for virtual auditory worlds","","","","","","The visual and the auditory field of perception respond on different input signals from our environment. Thus, interacting with worlds solely trough sound is a very challenging task. This paper discusses methods and techniques for sonification and interaction in virtual auditory worlds. In particular, it describes auditory elements such as speech, sound and music and discusses their application in diverse auditory situations, as well as interaction techniques for assisted sonification. The work is motivated by the development of a framework for the interactive exploration of auditory environments which will be used to evaluate the later discussed techniques. The main focus for the design of this framework is the use in narrative environments for auditory games, but also for general purpose auditory user interfaces and communication processes.","2004","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KI6Z2JP6","journalArticle","2004","Coyle, Eugene; Cullen, C.","Orchestration within the sonification of basic data sets","","","","","","The use of sonification as a means of representing and analysing data has become a growing field of research in recent years and as such has become a far more accepted means of working with data. Existing work carried out as part of this research has focused primarily on the sonification of DNA/RNA sequences and their subsequent protein structures for the purposes of analysis. This sonification work raised many questions as regards the need for sequences to be set to music in a standard manner so that different strands could be analysed by comparison, and hence the orchestration and instrumentation used became of great importance. The basic principles of sonification can be rapidly extended to include many different data elements within a single rendering, and thus the importance of orchestration grows accordingly. Existing work on the use of rhythmic parsing within a sonification had suggested that far more information could be represented when orchestrated in a rhythmic manner than when simply reconstituted in single musical block. The principle was further extended to include the allocation specific instruments and pitches within rhythmic patterns so that each sonic event would convey the data it was intended to represent. To this end a fictional database of employees in a company was created as a means of developing the principles required for more effective sonification through orchestration. The employee database was intended as a means of using a straightforward data set to analyse the effect of basic changes in instrumentation and orchestration rather than the data itself. The allocation of chord intervals or melodies to different data elements allowed the data to be represented in different ways at output in order that these differences would eventually highlight some form of framework for effective sonification of data sets with multiple elements.","2004","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BV3BNAA4","book","2015","Tsuchiya, Takahiko; Freeman, Jason; Lerner, Lee W.","Data-to-music API: Real-time data-agnostic sonification with musical structure models","","","","","","In sonification methodologies that aim to represent the underlying data accurately, musical or artistic approaches are often dismissed as being not transparent, likely to distort the data, not generalizable, or not reusable for different data types. Scientific applications for sonification have been, therefore, hesitant to use approaches guided by artistic aesthetics and musical expressivity. All sonifications, however, may have musical effects on listeners, as our trained ears with daily exposure to music tend to naturally distinguish musical and non-musical sound relationships, such as harmony, rhythmic stability, or timbral balance. This study proposes to take advantage of the musical effects of sonification in a systematic manner. Data may be mapped to high-level musical parameters rather than to one-to-one low-level audio parameters. An approach to create models that encapsulate modulatable musical structures is proposed in the context of the new DataTo- Music JavaScript API. The API provides an environment for rapid development of data-agnostic sonification applications in a web browser, with a model-based modular musical structure system. The proposed model system is compared to existing sonification frameworks as well as music theory and composition models. Also, issues regarding the distortion of original data, transparency, and reusability of musical models are discussed.","2015","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E9HGC6DE","journalArticle","2005","Lemmens, Paul M. C.","Using the major and minor mode to create affectively-charged earcons","","","","","","The importance of the structured fabrication of auditory (feedback) signals like earcons is common knowledge in the ICAD community. To create such structured families of earcons musical transformations like rhythm or pitch (and many others) are usually employed. However, one important transformation in Western tonal music, that of the distinction between major and minor mode, to our knowledge, has not been exploited, despite the fact that the affective connotation of the major and minor mode might be useful for research into auditory signals for affective human– computer interfaces. The present study investigated whether the transformation to major or minor mode can be used to create affectively–charged earcons for use in affective– computing research [1]. The affective–congruency effect that we obtained provides evidence that the processing of affective information can interfere with making rational, cognitive decisions. We argue that the transformation to the major or minor mode is suitable to create affectively–charged earcons and that it is important to ensure affective correspondence in computer interfaces to be able to realize optimal performance levels.","2005","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8WIZL2W7","journalArticle","2006","Palomaki, H.","Meanings conveyed by simple auditory rhythms","","","","","","In this article we concentrate on perception of non-musical rhythm. The purpose of this study has been to find possible meanings related to simple auditory rhythms. Meanings were examined using semantic scales. 26 subjects rated nine different rhythm samples according to adjective pair scales. We also identify some preliminary design suggestions as to how rhythm can be used in sonification and discuss duration limitation when composing earcons.","2006","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H8G3I6E8","journalArticle","2019","Savery, Richard; Ayyagari, Madhukesh; May, Keenan; Walker, Bruce N.","Soccer sonification: Enhancing viewer experience","","","","","","We present multiple approaches to soccer sonification, focusing on enhancing the experience for a general audience. For this work, we developed our own soccer data set through computer vision analysis of footage from a tactical overhead camera. This data-set included X, Y, coordinates for the ball and players throughout, as well as passes, steals and goals. After a divergent creation process, we developed four main methods of sports sonification for entertainment. For the Tempo Variation and Pitch Variation methods, tempo or pitch is operationalized to demonstrate ball and player movement data. The Key Moments method features only pass, steal and goal data, while the Musical Moments method takes existing music and attempts to align the track with important data points. Evaluation was done using a combination of qualitative focus groups and quantitative surveys, with 36 participants completing hour long sessions. Results indicated an overall preference for the Pitch Variation and Musical Moments methods, and revealed a robust trade-off between usability and enjoyability.","2019","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6DXJUAS6","book","2015","Perez-Lopez, Andres","3DJ: A supercollider framework for real-time sound spatialization","","","","","","The field of real time sound spatizalization is recently receiving much attention, as suggested by the large number of proposals appeared in last years - both from software spatialization frameworks and from hardware spatialization interfaces. However, most of the proposed works do not take into account the existing knowledge in Human Computer Interaction Design, which causes them to remain in a simplified approach. We propose a theoretical basis for real-time spatialization design from a holistic perspective, based on the Digital Musical Instruments theory, and use it to provide a comparative review of recent proposals. Furthermore, we develop our own state-of-the-art software spatialization system, 3Dj, which may help in the task of design and evaluation of new proposals for real-time sound spatialization in the fields of interactive performance, data sonification or virtual environments.","2015","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4HYUS9PQ","journalArticle","1998","Fernstrom, Mikael; McNamara, Caolan","After direct manipulation - direct sonification","","","","","","The effectiveness of providing multiple-stream audio to support browsing on a computer was investigated through the iterative development and evaluation of a series of sonic browser prototypes. The data set used was a database containing music. Interactive sonification1 was provided in conjunction with simplified human-computer interaction sequences. It was investigated to what extent interactive sonification with multiple-stream audio could enhance browsing tasks, compared to interactive sonification with single-stream audio support. With ten users it was found that with interactive multiple-stream audio the users could accurately complete the browsing tasks significantly faster than those who had single-stream audio support.","1998","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SHLKHL6S","journalArticle","2012","McLachlan, Ross; McGee-Lennon, Marilyn; Brewster, Stephen","The sound of musicons: investigating the design of musically derived audio cues","","","","","","Musicons (brief samples of well-known music used in auditory interface design) have been shown to be memorable and easy to learn. However, little is known about what actually makes a good Musicon and how they can be created. This paper reports on an empirical user study (N=15) to explore the recognition rate and preference ratings for a set of Musicons that were created by allowing users to self-select 5 second sections from (a) a selection of their own music and (b) a set of control tracks. It was observed that sampling a 0.5 second Musicon from a 5-second musical section resulted in easily identifiable and well liked Musicons. Qualitative analysis highlighted some of the underlying properties of the musical sections that resulted in ‘good’ Musicons. A preliminary set of guidelines is presented that provides a greater understanding of how to create effective and identifiable Musicons for future auditory interfaces.","2012","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P5WBXSMS","journalArticle","1996","Back, Maribeth; Des, D.","Micro-narratives in sound design: Context, character, and caricature in waveform manipulation","","","","","","This paper reviews sound design techniques used in professional audio for music and theater and proposes a conceptual approach to the construction of audio based in narrative structure. The sound designer does not attempt to replicate ""real"" sounds; the task is rather to create the impression of a real sound in a listener's mind. In this attempt to create a sound in the listener's mind, the sound designer is aided by user expectations based upon cultural experience as well as physical experience. Practical sound manipulation techniques are discussed in view of their usefulness in matching a listener's mental model of a sound. Narrative aspects of audio design in computational environments are also delineated. Some keywords involved in this paper are sound design, auditory display, multimodal interaction, interface design, narrative, sonic narrative, micro-narrative, and audio.","1996","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SLTSJ87E","journalArticle","2021","Frid, Emma; Orini, Michele; Martinelli, Giampaolo; Chew, Elaine","Mapping inter-cardiovascular time-frequency coherence to harmonic tension in sonification of ensemble interaction between a COVID-19 patient and the medical team","","","","","","This paper presents exploratory work on sonic and visual representations of heartbeats of a COVID-19 patient and a medical team. The aim of this work is to sonify heart signals to reflect how a medical team comes together during a COVID-19 treatment, i.e. to highlight other aspects of the COVID-19 pandemic than those usually portrayed through sonification, which often focuses on the number of cases. The proposed framework highlights synergies between sound and heart signals through mapping between timefrequency coherence (TFC) of heart signals and harmonic tension and dissonance in music. Results from a listening experiment suggested that the proposed mapping between TFC and harmonic tension was successful in terms of communicating low versus high coherence between heart signals, with an overall accuracy of 69%, which was significantly higher than chance. In the light of the performed work, we discuss how links between heart- and sound signals can be further explored through sonification to promote understanding of aspects related to cardiovascular health.","2021","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V6L5XMDN","journalArticle","2021","Falkenberg, Kjetil; Frid, Emma; Eriksson, Martin Ljungdahl; Otterbring, Tobias; Daunfeldt, Sven-Olov","Auditory notification of customer actions in a virtual retail environment: Sound design, awareness and attention","","","","","","In this paper, we introduce sonification as a less intrusive method for preventing shoplifting. Music and audible alerts are common in retail, and auditory monitoring of a store can aid clerks and reduce losses. Despite these potential advantages, sonification of interaction with goods in retail is an undeveloped field. We conducted an experiment focusing on peripheral auditory notifications in a virtual retail environment, evaluating aspects such as awareness and attention, sound design and noticeability, and localization of event sounds. Results highlighted behavioral differences depending on whether users were informed about the presence of auditory notification sounds or not. The alerts did not cause distraction or annoyance and we suggest that the findings give a promising starting point for future studies and investigations focused on improving the auditory environments in retail.","2021","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U5VW7XRQ","journalArticle","2014","Rouat, Jean; Lescal, Damien; Wood, Sean","Handheld Device for Substitution From Vision to Audition","","","","","","Sensorial substitution has great potential in rehabilitation, education, games, and in the creation of music and art. Current technologies allow us to develop sensorial substitution and sonification systems that would not have been imaginable two decades ago. It is desirable to let a large audience use and test sonification systems to provide feedback and improve their design. Handheld devices like smartphones or tablets include network connectivity (WIFI and/or Cellular radio) that can be used to transmit anonymous information about the configuration and strategies adopted by users. It is now feasible to obtain feedback from any user of substitution and sonification technology and not only from a limited number of subjects in the laboratory. Testing in the field with a large number of users is now possible thanks to telecommunication networks and machine learning tools to analyze big data. This work presents a handheld implementation of a simple video sonification system designed to test the acceptability of vision to audition substitution systems and in the near future to provide feedback from users. A first beta version was publicly released in November 2013 as an iOS application for large scale testing. The extended abstract introduces the interface and the underlying technology.","2014","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DSCWHTXR","book","2010","Pun, Thierry; Deville, Benoit; Bologna, Guido","Sonification of Colour and Depth in a Mobility Aid for Blind People","","","","","","The See Color interface transforms a small portion of a colored video image into sound sources represented by spatialized musical instruments. Basically, the conversion of colors into sounds is achieved by quantization of the HSL color system. Our purpose is to provide visually impaired individuals with a capability of perception of the environment in real time. In this work the novelty is the simultaneous sonification of color and depth, depth being coded by sound rhythm. Our sonification model is illustrated by several experiments, such as: (1) detecting an open door in order to go out from the office; (2) walking in a hallway and looking for a blue cabinet; (3) walking in a hallway and looking for a red tee shirt; (4) moving outside and avoiding a parked car. Videos with sounds of experiments are available on http://www.youtube.com/guidobologna.","2010","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DE7JHJRL","journalArticle","2021","Morales, Esteban; James, Kedrick; Horst, Rachel; Takeda, Yuya; Yung, Effiam","The sound of our words: Singling, a textual sonification software","","","","","","As visualization struggles to grasp the intricate and temporal networks of meaning found in textual data, sonification emerges as a creative and effective way of representing language. Accordingly, this paper seeks to introduce Singling, a textual sonification software that allows users to create and manipulate auditory representations of a text's lexicogrammatical properties. To achieve this, we first present Singling's main features and interface. We then discuss an example of using this sonification software to explore—both analytically and aesthetically—three different poems. Overall, this paper seeks to introduce researchers, educators, and artists to the many possibilities of Singling and the practice of textual sonification, which includes data analysis, multimodal and collaborative narrative creation, and musical performance to name a few.","2021","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"84Y8MV32","book","2009","Gygi, Brian; Shafiro, Valeriy","From signal to substance and back: Insights from environmental sound research to auditory display design","","","","","","A persistent concern in the field of auditory display design has been how to effectively use environmental sounds, which are naturally occurring familiar non-speech, non-musical sounds. Environmental sounds represent physical events in the everyday world, and thus they have a semantic content that enables learning and recognition. However, unless used appropriately, their functions in auditory displays may cause problems. One of the main considerations in using environmental sounds as auditory icons is how to ensure the identifiability of the sound sources. The identifiability of an auditory icon depends on both the intrinsic acoustic properties of the sound it represents, and on the semantic fit of the sound to its context, i.e., whether the context is one in which the sound naturally occurs or would be unlikely to occur. Relatively recent research has yielded some insights into both of these factors. A second major consideration is how to use the source properties to represent events in the auditory display. This entails parameterizing the environmental sounds so the acoustics will both relate to source properties familiar to the user and convey meaningful new information to the user. Finally, particular considerations come into play when designing auditory displays for special populations, such as hearing impaired listeners who may not have access to all the acoustic information available to a normal hearing listener, or to elderly or other individuals whose cognitive resources may be diminished. Some guidelines for designing displays for these populations will be outlined.","2009","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5AYZ97I3","journalArticle","2007","Davison, Benjamin K.; Walker, Bruce N.","Sonification Sandbox Reconstruction: Software Standard for Auditory Graphs","","","","","","We report on an overhaul to the Sonification Sandbox. The Sonification Sandbox provides a cross-platform, flexible tool for converting tabular information into a descriptive auditory graph. It is implemented in Java, using the Java Sound API to generate MIDI output. An improved modular code structure provides a strong user interface and model framework for auditory graph representation and manipulation. A researcher can integrate part or the entire program into a different experimental implementation. The upgraded Sonification Sandbox provides a rich description of the auditory graph representation that can be saved or exported into various file formats. This description includes data representations of pitch, timbre, polarity, pan, and volume, along with graph contexts analogous to visual graph axes. Applications for the Sonification Sandbox include experimentation with various sonification techniques, data analytics beyond visualization, science education, auditory display for the blind, and musical interpretation of data.","2007","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IAGZUJG8","journalArticle","2000","Hankinson, John CK; Edwards, Alistair DN","Musical phrase-structured audio communication","","","","","","It has previously been shown that musical grammars can impose structural constraints upon the design of earcons, thereby providing a grammatical basis to earcon combinations. In this paper, more complex structural combinations are explored, based upon linguistic phrases. By mapping between a musical grammar and a linguistic grammar, musical phrases can be generated which correspond to linguistic sentences. A large number of unique meanings can be presented in this way based upon a simple musical vocabulary. This is of great value to auditory designers. A user study has been undertaken which reveals that users can recognize these complex auditory phrases after a small amount of training.","2000","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"88VXVHND","journalArticle","2021","Elmquist, Elias; Ejdbo, Malin; Bock, Alexander; Rönnberg, Niklas","OpenSpace sonification: complementing visualization of the solar system with sound","","","","","","Data visualization software is commonly used to explore outer space in a planetarium environment, where the visuals of the software is typically accompanied with a narrator and supplementary background music. By letting sound take a bigger role in these kinds of presentations, a more informative and immersive experience can be achieved. The aim of the present study was to explore how sonification can be used as a complement to the visualization software OpenSpace to convey information about the Solar System, as well as increasing the perceived immersiveness for the audience in a planetarium environment. This was investigated by implementing a sonification that conveyed planetary properties, such as the size and orbital period of a planet, by mapping this data to sonification parameters. With a user-centered approach, the sonification was designed iteratively and evaluated in both an online and planetarium environment. The results of the evaluations show that the participants found the sonification informative and interesting, which suggest that sonification can be beneficially used as a complement to visualization in a planetarium environment.","2021","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KTXMNW4D","journalArticle","2002","McGregor, I.; Crerar, A.; Benyon, D.; Macaulay, C.","Soundfields and soundscapes: Reifying auditory communities","","","","","","This paper reports progress towards mapping workplace soundscapes. In order to design auditory interfaces that integrate effectively with workplace environments, we need a detailed understanding of the way in which end users inhabit these environments, and in particular, how they interact with the existing auditory environment. Our work concentrates first on mapping the physical soundfield, then overlaying this with a representation of the soundscape as experienced by its active participants. The ultimate aim of this work is to develop an interactive soundscape-mapping tool, analogous to the modeling tools available to architects. Such a tool would be of use to designers of physical, augmented and virtual environments and usable without professional musical or acoustical expertise.","2002","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GI43UIL5","journalArticle","2007","Palladino, Dianne K.; Walker, Bruce N.","Learning Rates for Auditory Menus Enhanced with Spearcons Versus Earcons","","","","","","Increasing the usability of menus on small electronic devices is essential due to their increasing proliferation and decreasing physical sizes in the marketplace. Auditory menus are being studied as an enhancement to the menus on these devices. This study compared the learning rates for earcons (hierarchical representations of menu locations using musical tones) and spearcons (compressed speech) as potential candidates for auditory menu enhancement. We found that spearcons outperformed earcons significantly in rate of learning. We also found evidence that spearcon comprehension was enhanced by a brief training cycle, and that participants considered the process of learning spearcons much easier than the same process using earcons. Since the efficiency of learning and the perceived ease of use of auditory menus will increase the likelihood they are embraced by those who need them, this paper presents compelling evidence that spearcons may be the superior choice for such applications.","2007","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DRRJ64WP","journalArticle","2001","Bonebright, Terri L.; Nees, Mike A.; Connerley, Tayla T.; McCain, Glenn R.","Testing the effectiveness of sonified graphs for education: A programmatic research project","","","","","","This programmatic research project builds on results from research on data sonification and from studies investigating comprehension of visual graphs. The purpose of the project is to explore the effectiveness of using sonified graphs of real data sets from disciplines to which students are exposed during academic courses. The primary question is whether sonified graphs can increase the comprehension of graphed data for students. The secondary question is whether stereo or monaural sonifications are most effective for graph comprehension. The third and final question of this project is whether sonified graphs with rhythm markers result in better comprehension than sonified graphs without them. The project consists of three laboratory experiments that explore whether students can match auditory representations with the correct visual graphs, whether they can comprehend graphed data sets more effectively by adding sonified components, and whether they can be trained to use sonified graphs better with practice. Results could provide new methods for teaching students with different learning styles quantitative skills in educational settings from kindergarten through college. They could also be extended to assist in teaching students with visual impairments about graphed data sets.","2001","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CLGY44BU","journalArticle","2017","Broderick, James; Duggan, Jim; Redfern, Sam","Using Auditory Display Techniques to Enhance Decision Making And Perceive Changing Environmental Data Within a 3D Virtual Game Environment","","","","","","When it comes to understanding our environment, we use all our senses. Within the study and implementation of virtual environments and systems, huge advancements in the quality of visuals and graphics have been made, but when it comes to the audio in our environment, many people have been content with very basic sound information. Video games have strived towards powerful sound design, both for player immersion and information perception. Research exists showing how we can use audio sources and waypoints to navigate environments, and how we can perceive information from audio in our surroundings. This research explores using sonification of changing environmental data and environmental objects to improve user's perception of virtual spaces and navigation within simulated environments, with case studies looking at training and for remote operation of unmanned vehicles. This would also expand into how general awareness and perception of dynamic 3D environments can be improved. Our research is done using the Unity3D game engine to create a virtual environment, within which users navigate around water currents represented both visually and through sonification of their information using Csound, a C based programming language for sound and music creation.","2017","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SGTFLLFY","journalArticle","2011","Bujacz, Michal; Skulimowski, Piotr; Strumillo, Pawel","Sonification of 3D Scenes Using Personalized Spatial Audio to Aid Visually Impaired Persons","","","","","","The research presented concerns the development of a sonification algorithm for representation of 3D scenes for use in an electronic travel aid (ETA) for visually impaired persons. The proposed sonar-like algorithm utilizes segmented 3D scene images, personalized spatial audio and musical sound patterns. The use of segmented and parametrically described 3D scenes allowed to overcome the large sensory mismatch between visual and auditory perception. Utilization of individually measured head related transfer functions (HRTFs), enabled the application of illusions of virtual sound sources. The selection of sounds used was based on surveys with blind volunteers. A number of sonification schemes, dubbed sound codes, were proposed, assigning sound parameters to segmented object parameters. The sonification algorithm was tested in virtual reality using software simulation along with studies of virtual sound source localization accuracy. Afterwards, trials in controlled real environments were made using a portable ETA prototype, with participation of both blind and sighted volunteers. Successful trials demonstrated that it is possible to quickly learn and efficiently use the proposed sonification algorithm to aid spatial orientation and obstacle avoidance.","2011","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9BERTEV7","journalArticle","2018","Quinton, Michael; McGregor, Iain; Benyon, David","Investigating effective methods of designing sonifications","","","","","","This study aims to provide an insight into effective sonification design. There are currently no standardized design methods, allowing a creative development approach. Sonifcation has been implemented in many different applications from scientific data representation to novel styles of musical expression. This means that methods of practice can vary a greatly. The indistinct line between art and science might be the reason why sonification is still sometimes deemed by scientists with a degree of scepticism. Some well established practitioners argue that it is poor design that renders sonifications meaningless, in-turn having an adverse effect on acceptance. To gain a deeper understanding about sonification research and development 11 practitioners were interviewed. They were asked about methods of sonification design and their insights. The findings present information about sonification research and development, and a variety of views regarding sonification design practice.","2018","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JKM46G6P","journalArticle","2007","Reissel, L. M.; Pai, Dinesh K.","High-Resolution Analysis and Resynthesis of Environmental Impact Sounds","","","","","","Impact sounds produced by everyday objects are an important source of information about contact interactions in virtual environments and auditory displays. Impact signals also provide a rich class of real and synthetic percussive musical sounds. However, their perceptually acceptable resynthesis and modification requires accurate estimation of mode parameters, which has proved difficult using traditional methods. In this paper we describe some of the problems posed by impact phenomena when applying standard methods, and present a phase-constrained high-resolution algorithm which allows more accurate estimation of modes and amplitudes for impact signals. The phase-constrained algorithm is based on least squares estimation, with initial estimates obtained from a modified ESPRIT algorithm, and it produces better resynthesis results than previously used methods. We give examples with everyday object impact sounds.","2007","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VQJG28JA","journalArticle","2018","Lenzi, Sara; Gleria, Francesca","Humanising data through sound: Res Extensae and a user-centric approach to data sonification","","","","","","In this paper, starting from a case study (the mixed-media data sonification installation Res Extensae), we discuss a number of assumptions on the efficacy of sound as a means to represent and communicate numerical data. The discussion is supported by the results of a questionnaire aimed at validating our assumptions and conducted with fifteen of the participants to the experience. At the same time, we have the ambition to contribute to a wider debate on the value of data sonification. We introduce the first stage of a research on sonification as a design-driven, user-centred and multi-modal experience, in that closer to data design practices rather than to traditional composition and computer music. We describe the usage of physical objects to help users to put sounds and data into a wider context, improving the user experience and facilitating the comprehension and retention of the meaning of data.","2018","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7X36XFG5","journalArticle","2016","Dyer, John; Stapleton, Paul; Rodger, Matthew","Sonification of Movement for Motor Skill Learning in a Novel Bimanual Task: Aesthetics and Retention Strategies","","","","","","Here we report early results from an experiment designed to investigate the use of sonification for the learning of a novel perceptual-motor skill. We find that sonification which employs melody is more effective than a strategy which provides only bare timing information. We additionally show that it might be possible to �refresh' learning after performance has waned following training - through passive listening to the sound that would be produced by perfect performance. Implications of these findings are discussed in terms of general motor performance enhancement and sonic feedback design.","2016","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LXSE7C9M","journalArticle","2018","Aldana Blanco, Andrea Lorena; Grautoff, Steffen; Hermann, Thomas","CardioSounds: A portable system to sonify ECG rhythm disturbances in real-time","","","","","","CardioSounds is a portable system that allows users to measure and sonify their electrocardiogram signal in real-time. The ECG signal is acquired using the hardware platform BITalino and subsequently analyzed and sonified using a Raspberry Pi. Users can control basic features from the system (start recording, stop recording) using their smartphone. The system is meant to be used for diagnostic and monitoring of cardiac pathologies, providing users with the possibility to monitor a signal without occupying their visual attention. In this paper, we introduce a novel method, anticipatory mapping, to sonify rhythm disturbances such as Atrial Fibrillation, Atrial flutter and Ventricular Fibrillation. Anticipatory mapping enhances perception of rhythmic details without disrupting the direct perception of the actual heart beat rhythm. We test the method on selected pathological data involving three of the most known rhythm disturbances. A preliminary perception test to assess aesthetics of the sonifications and its possible use in medical scenarios shows that the anticipatory mapping method is regarded as informative discerning healthy and pathological states, however there is no agreement about a preferred sonification type.","2018","2023-07-31 07:24:21","2023-07-31 07:24:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""