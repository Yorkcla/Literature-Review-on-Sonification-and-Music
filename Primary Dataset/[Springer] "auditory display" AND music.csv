"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"CFHB45EM","bookSection","2006","Murphy, Emma; Pirhonen, Antti; McAllister, Graham; Yu, Wai","A Semiotic Approach to the Design of Non-speech Sounds","Haptic and Audio Interaction Design","978-3-540-37595-1 978-3-540-37596-8","","","http://link.springer.com/10.1007/11821731_12","In the field of auditory display there is currently a lack of theoretical support for the design of non-speech sounds as elements of a user interface. Sound design methods are often based on ad hoc choices or the personal preferences of the designer. A method is proposed in this paper based on a semiotic approach to the design of non-speech sounds. In this approach, the design process is conceptualised by referring to structural semiotics, acknowledging the unique qualities of non-speech sounds, as a mode of conveying information. This method is based on a rich use scenario presented to a design panel. A case study where the design method has been applied is presented and evaluated. Finally recommendations for a practical design method are presented supported by this empirical investigation.","2006","2023-07-05 07:32:03","2023-07-20 00:15:41","2023-07-05 07:32:03","121-132","","","4129","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11821731_12","","","","","","McGookin, David; Brewster, Stephen","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VDFT69N4","journalArticle","2016","Katz, Brian F. G.; Marentakis, Georgios","Advances in auditory display research","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-016-0226-7","http://link.springer.com/10.1007/s12193-016-0226-7","The research community on Auditory Design has been active and organized now for almost 25 years. Work in Auditory Display is pluri-disciplinary/inter-disciplinary in nature, and involves disciplines such as perception, acoustics, digital signal processing, multimodality, ergonomics, aesthetics, cognition, etc. Published works have spanned fundamental, phenomenological, theoretical, applicative, and artistic studies. Over the years, various journals have had special issues related to such work. The range of the associated journals reflects the variety of implicated domains. This current special issue presents a collection of extended works that were selected from papers presented at the 2015 International Conference on Auditory Display. We present here an overview of the selection process and the accepted papers.","2016-09","2023-07-06 00:46:54","2023-07-20 06:59:42","2023-07-06 00:46:54","191-193","","3","10","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/CCR5HKUR/Katz and Marentakis - 2016 - Advances in auditory display research.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q8Q63FT7","journalArticle","2017","Black, David; Hansen, Christian; Nabavi, Arya; Kikinis, Ron; Hahn, Horst","A Survey of auditory display in image-guided interventions","International Journal of Computer Assisted Radiology and Surgery","","1861-6410, 1861-6429","10.1007/s11548-017-1547-z","http://link.springer.com/10.1007/s11548-017-1547-z","","2017-10","2023-07-06 00:46:54","2023-07-06 00:46:54","2023-07-06 00:46:54","1665-1676","","10","12","","Int J CARS","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/3IB984G8/Black et al. - 2017 - A Survey of auditory display in image-guided inter.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VNPB49M4","bookSection","2010","Gygi, Brian; Shafiro, Valeriy","From Signal to Substance and Back: Insights from Environmental Sound Research to Auditory Display Design","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_16","","2010","2023-07-06 00:46:54","2023-07-06 00:46:54","2023-07-06 00:46:54","306-329","","","5954","","","From Signal to Substance and Back","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_16","","/Users/minsik/Zotero/storage/5NQSNLAL/Gygi and Shafiro - 2010 - From Signal to Substance and Back Insights from E.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S4Q6KQMT","journalArticle","2017","Temple, Mark D.","An auditory display tool for DNA sequence analysis","BMC Bioinformatics","","1471-2105","10.1186/s12859-017-1632-x","http://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-017-1632-x","","2017-12","2023-07-06 00:46:54","2023-07-06 00:46:54","2023-07-06 00:46:54","221","","1","18","","BMC Bioinformatics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/R9CZ7K22/Temple - 2017 - An auditory display tool for DNA sequence analysis.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KAG8HBKA","journalArticle","2012","Vazquez-Alvarez, Yolanda; Oakley, Ian; Brewster, Stephen A.","Auditory display design for exploration in mobile audio-augmented reality","Personal and Ubiquitous Computing","","1617-4909, 1617-4917","10.1007/s00779-011-0459-0","http://link.springer.com/10.1007/s00779-011-0459-0","In this paper, we compare four different auditory displays in a mobile audio-augmented reality environment (a sound garden). The auditory displays varied in the use of non-speech audio, Earcons, as auditory landmarks and 3D audio spatialization, and the goal was to test the user experience of discovery in a purely exploratory environment that included multiple simultaneous sound sources. We present quantitative and qualitative results from an initial user study conducted in the Municipal Gardens of Funchal, Madeira. Results show that spatial audio together with Earcons allowed users to explore multiple simultaneous sources and had the added benefit of increasing the level of immersion in the experience. In addition, spatial audio encouraged a more exploratory and playful response to the environment. An analysis of the participants’ logged data suggested that the level of immersion can be related to increased instances of stopping and scanning the environment, which can be quantified in terms of walking speed and head movement.","2012-12","2023-07-06 00:46:54","2023-07-21 04:54:27","2023-07-06 00:46:54","987-999","","8","16","","Pers Ubiquit Comput","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IZJMTHGZ","journalArticle","2018","Black, David; Hahn, Horst K.; Kikinis, Ron; Wårdell, Karin; Haj-Hosseini, Neda","Auditory display for fluorescence-guided open brain tumor surgery","International Journal of Computer Assisted Radiology and Surgery","","1861-6410, 1861-6429","10.1007/s11548-017-1667-5","http://link.springer.com/10.1007/s11548-017-1667-5","","2018-01","2023-07-06 00:46:54","2023-07-06 00:46:54","2023-07-06 00:46:54","25-35","","1","13","","Int J CARS","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/Q3EEV9BP/Black et al. - 2018 - Auditory display for fluorescence-guided open brai.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KNSW8EBI","journalArticle","2019","Plazak, Joseph; DiGiovanni, Daniel A.; Collins, D. Louis; Kersten-Oertel, Marta","Cognitive load associations when utilizing auditory display within image-guided neurosurgery","International Journal of Computer Assisted Radiology and Surgery","","1861-6410, 1861-6429","10.1007/s11548-019-01970-w","http://link.springer.com/10.1007/s11548-019-01970-w","Purpose The combination of data visualization and auditory display (e.g., sonification) has been shown to increase accuracy, and reduce perceived difficulty, within 3D navigation tasks. While accuracy within such tasks can be measured in real time, subjective impressions about the difficulty of a task are more elusive to obtain. Prior work utilizing electrophysiology (EEG) has found robust support that cognitive load and working memory can be monitored in real time using EEG data. Methods In this study, we replicated a 3D navigation task (within the context of image-guided surgery) while recording data pertaining to participants’ cognitive load through the use of EEG relative alpha-band weighting data. Specifically, 13 subjects navigated a tracked surgical tool to randomly placed 3D virtual locations on a CT cerebral angiography volume while being aided by visual, aural, or both visual and aural feedback. During the study EEG data were captured from the participants, and after the study a NASA TLX questionnaire was filled out by the subjects. In addition to replicating an existing experimental design on auditory display within image-guided neurosurgery, our primary aim sought to determine whether EEG-based markers of cognitive load mirrored subjective ratings of task difficulty Results Similar to existing literature, our study found evidence consistent with the hypothesis that auditory display can increase the accuracy of navigating to a specified target. We also found significant differences in cognitive working load across different feedback modalities, but none of which supported the experiments hypotheses. Finally, we found mixed results regarding the relationship between real-time measurements of cognitive workload and a posteriori subjective impressions of task difficulty. Conclusions Although we did not find a significant correlation between the subjective and physiological measurements, differences in cognitive working load were found. As well, our study further supports the use of auditory display in image-guided surgery.","2019-08","2023-07-06 00:46:54","2023-07-20 06:41:27","2023-07-06 00:46:54","1431-1438","","8","14","","Int J CARS","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YLJT4V9R","bookSection","2014","Sirkka, Anna; Fagerlönn, Johan; Lindberg, Stefan; Frimalm, Ronja","An Auditory Display to Convey Urgency Information in Industrial Control Rooms","Engineering Psychology and Cognitive Ergonomics","978-3-319-07514-3 978-3-319-07515-0","","","http://link.springer.com/10.1007/978-3-319-07515-0_53","Auditory warning signals are common features in industrial control rooms. Finding sound signals that convey higher degrees of urgency while keeping the potential for annoyance low is challenging. In the present study, evaluations were performed on four different types of auditory displays. The displays were all designed to convey three levels of urgency. The examination focused on the following questions: (1) “How reliably can the operators identify the three levels of urgency?” and (2) “How annoying do the operators find the sound signals?”. Fourteen operators participated in the study. For every signal within each auditory display, the participants were asked to rate the level of urgency and annoyance. The results show that one can design auditory displays that employ appropriate urgency mapping while the perceived annoyance is kept at a low level. The work also suggests that involving the end users in the design process could be advantageous.","2014","2023-07-06 00:46:54","2023-07-19 23:59:08","2023-07-06 00:46:54","533-544","","","8532","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-07515-0_53","","/Users/minsik/Zotero/storage/AW2IRCV4/Sirkka et al. - 2014 - An Auditory Display to Convey Urgency Information .pdf","","","","Harris, Don","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Kobsa, Alfred; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Doug; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IYRCA4S8","journalArticle","2019","Ziemer, Tim; Schultheis, Holger","Psychoacoustic auditory display for navigation: an auditory assistance system for spatial orientation tasks","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-018-0282-2","http://link.springer.com/10.1007/s12193-018-0282-2","","2019-09","2023-07-06 00:46:54","2023-07-06 00:46:54","2023-07-06 00:46:54","205-218","","3","13","","J Multimodal User Interfaces","Psychoacoustic auditory display for navigation","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HU3SZWIC","journalArticle","1999","Barrass, Stephen; Kramer, Gregory","Using sonification","Multimedia Systems","","0942-4962, 1432-1882","10.1007/s005300050108","http://link.springer.com/10.1007/s005300050108","The idea behind sonification is that synthetic non-verbal sounds can represent numerical data and provide support for information processing activities of many different kinds. This article describes some of the ways that sonification has been used in assistive technologies, remote collaboration, engineering analyses, scientific visualisations, emergency services and aircraft cockpits. Approaches for designing sonifications are surveyed, and issues raised by the existing approaches and applications are outlined. Relations are drawn to other areas of knowledge where similar issues have also arisen, such as human-computer interaction, scientific visualisation, and computer music. At the end is a list of resources that will help you delve further into the topic.","1999-01-01","2023-07-06 00:46:54","2023-07-21 04:31:20","2023-07-06 00:46:54","23-31","","1","7","","Multimedia Systems","","","","","","","","","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"93E5YCGS","journalArticle","2020","Jeon, Myounghoon; Andreopoulou, Areti; Katz, Brian F. G.","Auditory displays and auditory user interfaces: art, design, science, and research","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-020-00324-0","https://link.springer.com/10.1007/s12193-020-00324-0","For almost 3 decades, research on auditory displays and sonification has been well advanced. Now, the auditory display community has arrived at the stage of sonic information design with a more systematic, refined necessity, going beyond random mappings between the referents and sounds. Due to its innate transdisciplinary nature of auditory display, it would be difficult to unify the methods to study it. This special issue covers a diverse collection of approaches to auditory displays, involving art, design, science, and research. Accordingly, the works in the present special issue included new theories, frameworks, methods, and applications about auditory displays and auditory user interfaces. We hope that this special issue can provide the state of art of auditory display research and auditory user interface design, offering fresh inspiration and motivation to researchers and designers for their future works.","2020-06","2023-07-06 00:46:54","2023-07-20 06:59:30","2023-07-06 00:46:54","139-141","","2","14","","J Multimodal User Interfaces","Auditory displays and auditory user interfaces","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/4NH4RXWD/Jeon et al. - 2020 - Auditory displays and auditory user interfaces ar.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UMT6HJCV","bookSection","2006","Frauenberger, C; Stockman, T; Putz, V; Höldrich, R","Design Patterns for Auditory Displays","People and Computers XIX — The Bigger Picture","978-1-84628-192-1 978-1-84628-249-2","","","http://link.springer.com/10.1007/1-84628-249-7_30","","2006","2023-07-06 00:46:54","2023-07-06 00:46:54","2023-07-06 00:46:54","473-488","","","","","","","","","","","Springer London","London","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/1-84628-249-7_30","","","","","","McEwan, Tom; Gulliksen, Jan; Benyon, David","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I2D3JNY3","journalArticle","2012","Varni, Giovanna; Dubus, Gaël; Oksanen, Sami; Volpe, Gualtiero; Fabiani, Marco; Bresin, Roberto; Kleimola, Jari; Välimäki, Vesa; Camurri, Antonio","Interactive sonification of synchronisation of motoric behaviour in social active listening to music with mobile devices","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0079-z","http://link.springer.com/10.1007/s12193-011-0079-z","","2012-05","2023-07-06 00:46:54","2023-07-06 00:46:54","2023-07-06 00:46:54","157-173","","3-4","5","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BPZCAYIB","bookSection","2010","Worrall, David","Using Sound to Identify Correlations in Market Data","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_11","","2010","2023-07-06 00:46:54","2023-07-06 00:46:54","2023-07-06 00:46:54","202-218","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_11","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z5A2ELU2","journalArticle","2022","Rocchesso, Davide; Andolina, Salvatore; Ilardo, Giacomo; Palumbo, Salvatore Danilo; Galluzzo, Ylenia; Randazzo, Mario","A perceptual sound space for auditory displays based on sung-vowel synthesis","Scientific Reports","","2045-2322","10.1038/s41598-022-23736-2","https://www.nature.com/articles/s41598-022-23736-2","Abstract             When designing displays for the human senses, perceptual spaces are of great importance to give intuitive access to physical attributes. Similar to how perceptual spaces based on hue, saturation, and lightness were constructed for visual color, research has explored perceptual spaces for sounds of a given timbral family based on timbre, brightness, and pitch. To promote an embodied approach to the design of auditory displays, we introduce the Vowel–Type–Pitch (VTP) space, a cylindrical sound space based on human sung vowels, whose timbres can be synthesized by the composition of acoustic formants and can be categorically labeled. Vowels are arranged along the circular dimension, while voice type and pitch of the vowel correspond to the remaining two axes of the cylindrical VTP space. The decoupling and perceptual effectiveness of the three dimensions of the VTP space are tested through a vowel labeling experiment, whose results are visualized as maps on circular slices of the VTP cylinder. We discuss implications for the design of auditory and multi-sensory displays that account for human perceptual capabilities.","2022-11-12","2023-07-06 00:46:54","2023-07-06 00:46:54","2023-07-06 00:46:54","19370","","1","12","","Sci Rep","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/SUKCHVYJ/Rocchesso et al. - 2022 - A perceptual sound space for auditory displays bas.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BBUUU9D7","bookSection","2010","Brazil, Eoin","A Review of Methods and Frameworks for Sonic Interaction Design: Exploring Existing Approaches","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_3","","2010","2023-07-06 00:46:54","2023-07-06 00:46:54","2023-07-06 00:46:54","41-67","","","5954","","","A Review of Methods and Frameworks for Sonic Interaction Design","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_3","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TZLZ2VCK","bookSection","2010","Schaffert, Nina; Mattes, Klaus; Effenberg, Alfred O.","A Sound Design for Acoustic Feedback in Elite Sports","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_8","","2010","2023-07-06 00:46:54","2023-07-06 00:46:54","2023-07-06 00:46:54","143-165","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_8","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XIRHWXGX","journalArticle","2014","McGregor, Iain","Comparing designers’ and listeners’ experiences","AI & SOCIETY","","0951-5666, 1435-5655","10.1007/s00146-013-0489-4","http://link.springer.com/10.1007/s00146-013-0489-4","This paper compares the listening experiences of non-experts and the designers of two sound designs. To date, no such comparisons have been examined empirically, and so for ease of comparison, repertory grids were chosen to explore these experiences, which preclude the need for listener training. The results suggest that (a) it is meaningful to compare designers’ and non-experts’ listening experiences, (b) points of agreement and disagreement are readily identified and (c) the use of repertory grids is a practical means of conducting such studies. The findings further suggest that a taxonomy of sound attributes based on these experiences rather than designers’ intuition or predilection is also possible.","2014-11","2023-07-06 00:46:54","2023-07-19 11:22:38","2023-07-06 00:46:54","473-483","","4","29","","AI & Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NWQET4HW","journalArticle","2020","Temple, Mark D.","Real-time audio and visual display of the Coronavirus genome","BMC Bioinformatics","","1471-2105","10.1186/s12859-020-03760-7","https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-03760-7","Abstract                            Background               This paper describes a web based tool that uses a combination of sonification and an animated display to inquire into the SARS-CoV-2 genome. The audio data is generated in real time from a variety of RNA motifs that are known to be important in the functioning of RNA. Additionally, metadata relating to RNA translation and transcription has been used to shape the auditory and visual displays. Together these tools provide a unique approach to further understand the metabolism of the viral RNA genome. This audio provides a further means to represent the function of the RNA in addition to traditional written and visual approaches.                                         Results               Sonification of the SARS-CoV-2 genomic RNA sequence results in a complex auditory stream composed of up to 12 individual audio tracks. Each auditory motive is derived from the actual RNA sequence or from metadata. This approach has been used to represent transcription or translation of the viral RNA genome. The display highlights the real-time interaction of functional RNA elements. The sonification of codons derived from all three reading frames of the viral RNA sequence in combination with sonified metadata provide the framework for this display. Functional RNA motifs such as transcription regulatory sequences and stem loop regions have also been sonified. Using the tool, audio can be generated in real-time from either genomic or sub-genomic representations of the RNA. Given the large size of the viral genome, a collection of interactive buttons has been provided to navigate to regions of interest, such as cleavage regions in the polyprotein, untranslated regions or each gene. These tools are available through an internet browser and the user can interact with the data display in real time.                                         Conclusion               The auditory display in combination with real-time animation of the process of translation and transcription provide a unique insight into the large body of evidence describing the metabolism of the RNA genome. Furthermore, the tool has been used as an algorithmic based audio generator. These audio tracks can be listened to by the general community without reference to the visual display to encourage further inquiry into the science.","2020-12","2023-07-06 00:46:54","2023-07-06 00:46:54","2023-07-06 00:46:54","431","","1","21","","BMC Bioinformatics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/KDG7UZNJ/Temple - 2020 - Real-time audio and visual display of the Coronavi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JCRD26KP","journalArticle","2012","Huang, Chih-Fang; Lu, Hsiang-Pin; Ren, Jenny","Algorithmic approach to sonification of classical Chinese poetry","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-011-0856-4","http://link.springer.com/10.1007/s11042-011-0856-4","","2012-11","2023-07-06 00:48:35","2023-07-06 00:48:35","2023-07-06 00:48:35","489-518","","2","61","","Multimed Tools Appl","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/W5WKEIJM/Huang et al. - 2012 - Algorithmic approach to sonification of classical .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JJ22H7U7","bookSection","2010","Allali, Julien; Ferraro, Pascal; Hanna, Pierre; Robine, Matthias","Polyphonic Alignment Algorithms for Symbolic Music Retrieval","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_24","Melody is an important property for the perceptual description of Western musical pieces. A lot of applications rely on the evaluation of similarity between two melodies. While several existing techniques assume a monophonic context or extract a monophonic melody from polyphonic pieces, in this paper, we propose to consider the whole polyphonic context to evaluate the similarity without reducing to a monophonic melody. We thus propose a new model and a corresponding methodology that takes into account all the notes, even if they sound at the same time or if they overlap. Our model relies on a quotiented sequence representation of music. A quotiented sequence is a sequence graph defined with an additional equivalent relation on its vertices and such that the quotient graph is also a sequence graph. The core of the comparison method is based on an adaptation of edit-distance metrics, regularly applied in bio-informatic context. This algorithm is currently being used to evaluate the similarity between a monophonic or polyphonic query and a database of polyphonic musical pieces. First experiments show that the adaptation to polyphony does not degrade the quality of the algorithm with monophonic musical pieces. Furthermore, the results of experiments with polyphonic pieces are promising, even if they show some limitations.","2010","2023-07-06 00:48:35","2023-07-19 11:36:05","2023-07-06 00:48:35","466-482","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_24","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H8LEGUNQ","journalArticle","2021","Iber, Michael; Lechner, Patrik; Jandl, Christian; Mader, Manuel; Reichmann, Michael","Auditory augmented process monitoring for cyber physical production systems","Personal and Ubiquitous Computing","","1617-4909, 1617-4917","10.1007/s00779-020-01394-3","https://link.springer.com/10.1007/s00779-020-01394-3","Abstract             We describe two proof-of-concept approaches on the sonification of estimated operation states and conditions focusing on two scenarios: a laboratory setup of a manipulated 3D printer and an industrial setup focusing on the operations of a punching machine. The results of these studies form the basis for the development of an “intelligent” noise protection headphone as part of Cyber Physical Production Systems which provides auditorily augmented information to machine operators and enables radio communication between them. Further application areas are implementations in control rooms (equipped with multi-channel loudspeaker systems) and utilization for training purposes. As a first proof-of-concept, the data stream of error probability estimations regarding partly manipulated 3D printing processes were mapped to three sonification models, providing evidence about momentary operation states. The neural network applied indicates a high accuracy (> 93%) of the error estimation distinguishing between normal and manipulated operation states. None of the manipulated states could be identified by listening. An auditory augmentation, or sonification of these error estimations, provides a considerable benefit to process monitoring. For a second proof-of-concept, setup operations of a punching machine were recorded. Since all operations were apparently flawlessly executed, and there were no errors to be reported, we focused on the identification of operation phases. Each phase of a punching process could be algorithmically distinguished at an estimated probability rate of > 94%. In the auditory display, these phases were represented by different instrumentations of a musical piece in order to allow users to differentiate between operations auditorily.","2021-08","2023-07-06 00:48:35","2023-07-06 00:48:35","2023-07-06 00:48:35","691-704","","4","25","","Pers Ubiquit Comput","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/X39GW2DE/Iber et al. - 2021 - Auditory augmented process monitoring for cyber ph.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JGTNHD2F","bookSection","2014","Jeon, Myounghoon; Smith, Michael T.; Walker, James W.; Kuhl, Scott A.","Constructing the Immersive Interactive Sonification Platform (iISoP)","Distributed, Ambient, and Pervasive Interactions","978-3-319-07787-1 978-3-319-07788-8","","","http://link.springer.com/10.1007/978-3-319-07788-8_32","","2014","2023-07-06 00:48:35","2023-07-06 00:48:35","2023-07-06 00:48:35","337-348","","","8530","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-07788-8_32","","/Users/minsik/Zotero/storage/KB4FGLSS/Jeon et al. - 2014 - Constructing the Immersive Interactive Sonificatio.pdf","","","","Streitz, Norbert; Markopoulos, Panos","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Kobsa, Alfred; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Doug; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8HR4HYNG","bookSection","2010","Baldan, Stefano; Ludovico, Luca A.; Mauro, Davide A.","Algorithms for an Automatic Transcription of Live Music Performances into Symbolic Format","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_22","This paper addresses the problem of the real-time automatic transcription of a live music performance into a symbolic format. The source data are given by any music instrument or other device able to communicate through a performance protocol. During a performance, music events are parsed and their parameters are evaluated thanks to rhythm and pitch detection algorithms. The final step is the creation of a well-formed XML document, validated against the new international standard known as IEEE 1599. This work will shortly describe both the software environment and the XML format, but the main analysis will involve the real-time recognition of music events. Finally, a case study will be presented: PureMX, a set of Pure Data externals, able to perform the automatic transcription of MIDI events.","2010","2023-07-06 00:48:35","2023-07-19 11:36:44","2023-07-06 00:48:35","422-437","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_22","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UQY7ZFJF","journalArticle","2012","Metatla, Oussama; Bryan-Kinns, Nick; Stockman, Tony","Interactive hierarchy-based auditory displays for accessing and manipulating relational diagrams","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0067-3","http://link.springer.com/10.1007/s12193-011-0067-3","","2012-05","2023-07-06 00:48:35","2023-07-06 00:48:35","2023-07-06 00:48:35","111-122","","3-4","5","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MSBRBF5A","bookSection","2010","Wersényi, György","Auditory Representations of a Graphical User Interface for a Better Human-Computer Interaction","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_5","","2010","2023-07-06 00:48:35","2023-07-06 00:48:35","2023-07-06 00:48:35","80-102","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_5","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"732HWCE7","journalArticle","2018","Matinfar, Sasan; Nasseri, M. Ali; Eck, Ulrich; Kowalsky, Michael; Roodaki, Hessam; Navab, Navid; Lohmann, Chris P.; Maier, Mathias; Navab, Nassir","Surgical soundtracks: automatic acoustic augmentation of surgical procedures","International Journal of Computer Assisted Radiology and Surgery","","1861-6410, 1861-6429","10.1007/s11548-018-1827-2","http://link.springer.com/10.1007/s11548-018-1827-2","Purpose Advances in sensing and digitalization enable us to acquire and present various heterogeneous datasets to enhance clinical decisions. Visual feedback is the dominant way of conveying such information. However, environments rich with many sources of information all presented through the same channel pose the risk of over stimulation and missing crucial information. The augmentation of the cognitive field by additional perceptual modalities such as sound is a workaround to this problem. A major challenge in auditory augmentation is the automatic generation of pleasant and ergonomic audio in complex routines, as opposed to overly simplistic feedback, to avoid alarm fatigue. Methods In this work, without loss of generality to other procedures, we propose a method for aural augmentation of medical procedures via automatic modification of musical pieces. Results Evaluations of this concept regarding recognizability of the conveyed information along with qualitative aesthetics show the potential of our method. Conclusion In this paper, we proposed a novel sonification method for automatic musical augmentation of tasks within surgical procedures. Our experimental results suggest that these augmentations are aesthetically pleasing and have the potential to successfully convey useful information. This work opens a path for advanced sonification techniques in the operating room, in order to complement traditional visual displays and convey information more efficiently.","2018-09","2023-07-06 00:48:35","2023-07-21 07:43:23","2023-07-06 00:48:35","1345-1355","","9","13","","Int J CARS","Surgical soundtracks","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WR4CRAMU","journalArticle","2019","Sanyal, Shankha; Nag, Sayan; Banerjee, Archi; Sengupta, Ranjan; Ghosh, Dipak","Music of brain and music on brain: a novel EEG sonification approach","Cognitive Neurodynamics","","1871-4080, 1871-4099","10.1007/s11571-018-9502-4","http://link.springer.com/10.1007/s11571-018-9502-4","","2019-02","2023-07-06 00:48:35","2023-07-06 00:48:35","2023-07-06 00:48:35","13-31","","1","13","","Cogn Neurodyn","Music of brain and music on brain","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/PGVM4HXR/Sanyal et al. - 2019 - Music of brain and music on brain a novel EEG son.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X92BEUVF","bookSection","2010","Valle, Andrea; Lombardo, Vincenzo; Schirosa, Mattia","Simulating the Soundscape through an Analysis/Resynthesis Methodology","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_17","","2010","2023-07-06 00:48:35","2023-07-06 00:48:35","2023-07-06 00:48:35","330-357","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_17","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9Y8NLNWS","bookSection","1995","Levkowitz, Haim; Pickett, Ronald M.; Smith, Stuart; Torpey, Mark","An Environment and Studies for Exploring Auditory Representations of Multidimensional Data","Perceptual Issues in Visualization","978-3-642-79059-1 978-3-642-79057-7","","","http://link.springer.com/10.1007/978-3-642-79057-7_5","The field of auditory data representation has produced several intriguing proof-of-concept systems, but up until now there has been little formal research to measure the effectiveness of auditory data displays or to increase our understanding of how they work and how to improve them. Formal assessment is necessary throughout the process of developing new auditory display technologies in order to learn how to restrict the universe of possible sound attributes to those that are most effective for data representation. The capability to run quick psychometric tests to obtain quantitative figures of merit for alternative auditory representations is a requirement for auditory-display researchers engaged in the development of new technologies. For the first time, this capability is realized with a special-purpose workstation designed to generate and administer psychometric tests automatically using test patterns generated from statistically well-specified synthetic data. We describe the characteristics of one such workstation we have developed. We also describe a testing methodology we propose for the development of new auditory data displays of a type that we have been working with for the last few years. Finally, we describe a specific set of studies we are now beginning to conduct.","1995","2023-07-06 00:48:35","2023-07-21 04:45:58","2023-07-06 00:48:35","47-58","","","","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-3-642-79057-7_5","","","","","","Grinstein, Georges; Levkowitz, Haim","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V5RBXEDV","bookSection","2010","Vogt, Katharina; Pirrò, David; Kobenz, Ingo; Höldrich, Robert; Eckel, Gerhard","PhysioSonic - Evaluated Movement Sonification as Auditory Feedback in Physiotherapy","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_6","","2010","2023-07-06 00:48:35","2023-07-06 00:48:35","2023-07-06 00:48:35","103-120","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_6","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M9YIGNAV","bookSection","2008","Salter, Christopher L.; Baalman, Marije A. J.; Moody-Grigsby, Daniel","Between Mapping, Sonification and Composition: Responsive Audio Environments in Live Performance","Computer Music Modeling and Retrieval. Sense of Sounds","978-3-540-85034-2 978-3-540-85035-9","","","http://link.springer.com/10.1007/978-3-540-85035-9_17","","2008","2023-07-06 00:48:35","2023-07-06 00:48:35","2023-07-06 00:48:35","246-262","","","4969","","","Between Mapping, Sonification and Composition","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-85035-9_17","","/Users/minsik/Zotero/storage/J23RVWB2/Salter et al. - 2008 - Between Mapping, Sonification and Composition Res.pdf","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IAQXU3JM","bookSection","2013","Schubert, Emery; Ferguson, Sam; Farrar, Natasha; Taylor, David; McPherson, Gary E.","The Six Emotion-Face Clock as a Tool for Continuously Rating Discrete Emotional Responses to Music","From Sounds to Music and Emotions","978-3-642-41247-9 978-3-642-41248-6","","","http://link.springer.com/10.1007/978-3-642-41248-6_1","Recent instruments measuring continuous self-reported emotion responses to music have tended to use dimensional rating scale models of emotion such as valence (happy to sad). However, numerous retrospective studies of emotion in music use checklist style responses, usually in the form of emotion words, (such as happy, angry, sad…) or facial expressions. A response interface based on six simple sketch style emotion faces aligned into a clock-like distribution was developed with the aim of allowing participants to quickly and easily rate emotions in music continuously as the music unfolded. We tested the interface using six extracts of music, one targeting each of the six faces: ‘Excited’ (at 1 o’clock), ‘Happy’ (3), ‘Calm’ (5), ‘Sad’ (7), ‘Scared’ (9) and ‘Angry’ (11). 30 participants rated the emotion expressed by these excerpts on our ‘emotion-face-clock’. By demonstrating how continuous category selections (votes) changed over time, we were able to show that (1) more than one emotion-face could be expressed by music at the same time and (2) the emotion face that best portrayed the emotion the music conveyed could change over time, and (3) the change could be attributed to changes in musical structure. Implications for research on orientation time and mixed emotions are discussed.","2013","2023-07-06 00:48:35","2023-07-20 00:06:10","2023-07-06 00:48:35","1-18","","","7900","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-41248-6_1","","","","","","Aramaki, Mitsuko; Barthet, Mathieu; Kronland-Martinet, Richard; Ystad, Sølvi","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VBNWLU84","bookSection","2010","Frissen, Ilja; Katz, Brian F. G.; Guastavino, Catherine","Effect of Sound Source Stimuli on the Perception of Reverberation in Large Volumes","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_18","The aim of the presented research is to determine whether the perception of reverberation is dependent on the type of sound stimuli used. We quantified the discrimination thresholds for reverberations that are representative for large rooms such as concert halls (reverberation times around 1.8 s). For exponential decays, simulating an ideal simple room, thresholds are around 6% (Experiment 1). We found no difference in thresholds between a short noise burst and a male voice spoken word, suggesting that discrimination is not dependent on the type, or spectral content, of the sound source (Experiment 2). In two further experiments using a magnitude estimation paradigm we assessed the perceived amount of reverberation as a function of various types of stimuli. Whereas the discrimination of reverberant stimuli does not seem to be affected by the sound stimulus, the perceived amount of reverberation is affected. Vocal stimuli are perceived as being more reverberant than nonvocal stimuli. The results are discussed in light of current neuroscientific models of auditory processing of complex stimuli but also with respect to their consequences for the use of reverberation in auditory display.","2010","2023-07-06 00:48:35","2023-07-19 11:37:32","2023-07-06 00:48:35","358-376","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_18","","/Users/minsik/Zotero/storage/IR2GFI2L/Frissen et al. - 2010 - Effect of Sound Source Stimuli on the Perception o.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NJBRI7GL","bookSection","2010","Brock, Derek; McClimens, Brian; Wasylyshyn, Christina; Trafton, J. Gregory; McCurry, Malcolm","Evaluating the Utility of Auditory Perspective-Taking in Robot Speech Presentations","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_14","","2010","2023-07-06 00:48:35","2023-07-06 00:48:35","2023-07-06 00:48:35","266-286","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_14","","/Users/minsik/Zotero/storage/PA8KIRJM/Brock et al. - 2010 - Evaluating the Utility of Auditory Perspective-Tak.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MH44Y3PH","journalArticle","2012","Grond, Florian; Hermann, Thomas","Singing function: Exploring auditory graphs with a vowel based sonification","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0068-2","http://link.springer.com/10.1007/s12193-011-0068-2","","2012-05","2023-07-06 00:48:35","2023-07-06 00:48:35","2023-07-06 00:48:35","87-95","","3-4","5","","J Multimodal User Interfaces","Singing function","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R57VHMLA","bookSection","2008","Amatriain, Xavier; Castellanos, Jorge; Höllerer, Tobias; Kuchera-Morin, JoAnn; Pope, Stephen T.; Wakefield, Graham; Wolcott, Will","Experiencing Audio and Music in a Fully Immersive Environment","Computer Music Modeling and Retrieval. Sense of Sounds","978-3-540-85034-2 978-3-540-85035-9","","","http://link.springer.com/10.1007/978-3-540-85035-9_27","","2008","2023-07-06 00:48:35","2023-07-06 00:48:35","2023-07-06 00:48:35","380-400","","","4969","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-85035-9_27","","/Users/minsik/Zotero/storage/DA4M6CZE/Amatriain et al. - 2008 - Experiencing Audio and Music in a Fully Immersive .pdf","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2M8UAXS8","bookSection","2018","Seiça, Mariana; Lopes, Rui; Martins, Pedro; Cardoso, F. Amílcar","Sonifying Twitter’s Emotions Through Music","Music Technology with Swing","978-3-030-01691-3 978-3-030-01692-0","","","http://link.springer.com/10.1007/978-3-030-01692-0_39","Sonification is a scientific field that seeks to explore the potential of sound as an instrument to convey and interpret data. Its techniques have been developing significantly with the growth of technology and supporting hardware and software, which have spread in our daily environment. This allowed the establishment of new communication tools to share information, opinion and feelings as part of our daily routine. The aim of this project was to unite the social media phenomena with sonification, using Twitter data to extract user’s emotions and translate them into musical compositions. The focus was to explore the potential of music in translating data as personal and subjective as human emotions, developing a musically complex and captivating mapping based on the rules of Western Music. The music is accompanied by a simple visualization, which results in emotions being heard and seen with the corresponding tweets, in a multimodal experience that represents Twitter’s emotional reality. The mapping was tested through an online survey, and despite a few misunderstandings, the results were generally positive, expressing the efficiency and impact of the developed system.","2018","2023-07-06 00:48:35","2023-07-21 04:34:49","2023-07-06 00:48:35","586-608","","","11265","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-01692-0_39","","","","","","Aramaki, Mitsuko; Davies, Matthew E. P.; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZUWSY564","bookSection","2006","Hermann, Thomas; Höner, Oliver; Ritter, Helge","AcouMotion – An Interactive Sonification System for Acoustic Motion Control","Gesture in Human-Computer Interaction and Simulation","978-3-540-32624-3 978-3-540-32625-0","","","http://link.springer.com/10.1007/11678816_35","This paper introduces AcouMotion as a new hard-/software system for combining human body motion, tangible interfaces and sonification to a closed-loop human computer interface that allows non-visual motor control by using sonification (non-speech auditory displays) as major feedback channel. AcouMotion’s main components are (i) a sensor device for measuring motion parameters (ii) a computer simulation to represent the dynamical evolution of a model world, and (iii) a sonification engine which generates an auditory representation of objects and any interactions in the model world. The intended applications of AcouMotion range from new kinds of sport games that can be played without visual displays and therefore may be particularly interesting for people with visual impairment to further applications in data mining, physiotherapy and cognitive research. The first application of AcouMotion presented in this paper is Blindminton, a sport game similar to Badminton which is particularly adapted to the abilities of people with visual impairment. We describe our current system and its state of development, and we present first sound examples for interactive sonification using an early prototype. Finally, we discuss some interesting research directions based on the fact that AcouMotion binds auditory stimuli and body motion, and thus can represent a counterpart to the Eye-tracker device that exploits the binding of visual stimuli and eye-movement in cognitive research.","2006","2023-07-06 00:54:30","2023-07-20 00:09:03","2023-07-06 00:54:29","312-323","","","3881","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11678816_35","","","","","","Gibet, Sylvie; Courty, Nicolas; Kamp, Jean-François","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZGGTL53B","bookSection","2011","Merer, Adrien; Ystad, Sølvi; Kronland-Martinet, Richard; Aramaki, Mitsuko","Abstract Sounds and Their Applications in Audio and Perception Research","Exploring Music Contents","978-3-642-23125-4 978-3-642-23126-1","","","http://link.springer.com/10.1007/978-3-642-23126-1_12","","2011","2023-07-06 00:54:30","2023-07-06 00:54:30","2023-07-06 00:54:29","176-187","","","6684","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-23126-1_12","","/Users/minsik/Zotero/storage/54T52MKS/Merer et al. - 2011 - Abstract Sounds and Their Applications in Audio an.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y32EPJTU","bookSection","2009","Kildal, Johan","Aspects of Auditory Perception and Cognition for Usable Display Resolution in Data Sonification","Human-Computer Interaction – INTERACT 2009","978-3-642-03654-5 978-3-642-03655-2","","","http://link.springer.com/10.1007/978-3-642-03655-2_52","","2009","2023-07-06 00:54:30","2023-07-06 00:54:30","2023-07-06 00:54:29","467-470","","","5726","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-03655-2_52","","/Users/minsik/Zotero/storage/TI74ZZ5G/Kildal - 2009 - Aspects of Auditory Perception and Cognition for U.pdf","","","","Gross, Tom; Gulliksen, Jan; Kotzé, Paula; Oestreicher, Lars; Palanque, Philippe; Prates, Raquel Oliveira; Winckler, Marco","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SIA9QLWZ","bookSection","2010","Garavaglia, Javier Alejandro","Raising Awareness about Complete Automation of Live-Electronics: A Historical Perspective","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_23","The article raises attention to the advantages and disadvantages of complete automation in DSP processes (including the triggering of events) during the performance of interactive music. By proposing a historic summary divided in three main periods according to the technologies, methods and processes available during each of them, (including examples of key works and composers), it shows how the usage of automation in live-electronics was dependent on the development of new technologies, specially digital. Further, it explains how full automation works in two works of mine, describing the features and techniques involved. Considering those examples, the advantages and disadvantages resulting from the introduction of complete automation of DSP in live performance are finally discussed. Even though automation is not a new technique in the field, I am keen to dedicate special attention to completely automated events -including their triggering- given the impact that automation can have on performances.","2010","2023-07-06 00:54:30","2023-07-19 11:37:40","2023-07-06 00:54:29","438-465","","","5954","","","Raising Awareness about Complete Automation of Live-Electronics","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_23","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X7S2LUAM","bookSection","2007","Young-hyun, Park; Kwang-hee, Han","Information Display of Wearable Devices Through Sound Feedback of Wearable Computing","Human-Computer Interaction. Interaction Platforms and Techniques","978-3-540-73106-1","","","http://link.springer.com/10.1007/978-3-540-73107-8_132","Functions in wearable devices came to be various and specialized during the developmental process of wearable computing. However, such surfacing of new functions made it difficult and time-consuming for the devices to display the condition of their status, such as whether they are turned on or off. Moreover, mere dependency on visual display could lead to an overload of users’ visual cognition. In this research, sounds were used for relaying information feedback of the status of wearable devices. We first verified the usefulness of adding sound feedback, and next, the effect of each device-specific sound was confirmed as a sound feedback.","2007","2023-07-06 00:54:30","2023-07-20 06:32:31","2023-07-06 00:54:29","1200-1209","","","4551","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73107-8_132","","","","","","Jacko, Julie A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UTYPYE45","bookSection","2023","Martín-Gómez, Lucía; Pérez-Marcos, Javier; Rivero, Alfonso José López; Bermúdez, Giovanny Mauricio Tarazona","Drawing Music: Using Neural Networks to Compose Descriptive Music from Illustrations","New Trends in Disruptive Technologies, Tech Ethics and Artificial Intelligence","978-3-031-14858-3 978-3-031-14859-0","","","https://link.springer.com/10.1007/978-3-031-14859-0_3","The creative capacity of machines is still questioned by researchers and users alike. For this reason, computational creativity does not only focus on the development of machines for the creation of artistic content but also on the evaluation of the generated content. This works presents a system that composes polyphonic music from the drawings of a user in real time. Our proposal provides an analysis of the Fantasia film, produced in 1940 by Walt Disney and deduces the relationship between its audio and images. As part of system development, an LSTM-based Recurrent Neural Network was trained with MIDI music files and a model was obtained. As a result, the proposed system generates polyphonic music with expressive timing and dynamics by inferring chords from the user’s drawings. To assess the creative ability of the machine a Turing test was conducted and the quality of the interconnection between drawings and music was measured by another user test. Additionally, the performance of the considered classifiers is discussed.","2023","2023-07-06 00:54:30","2023-07-21 04:38:58","2023-07-06 00:54:29","30-42","","","1430","","","Drawing Music","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Advances in Intelligent Systems and Computing DOI: 10.1007/978-3-031-14859-0_3","","","","","","De La Iglesia, Daniel H.; De Paz Santana, Juan F.; López Rivero, Alfonso J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BWNPG65K","bookSection","2021","Yihang, Du; Ke, Niu; Yan, Pei; Petrovna, Gnativ Marina; Lijun, Wang","Multi-scale Evaluation of HCI Acoustic Expression in Digital Performance Space","Advances in Usability, User Experience, Wearable and Assistive Technology","978-3-030-80090-1 978-3-030-80091-8","","","https://link.springer.com/10.1007/978-3-030-80091-8_103","With the development of digitization and interaction technology, the digital performance space (DPS) based on multi-sensory channels create more abundant aesthetic experience for audience, and rationality of soundscape is an important part of DPS design. An evaluation system was established by critical incidence technique (CIT) for human-computer interactive (HCI) acoustic expression in DPS, including nine indicators under the two dimensions of sound environment and sound sequence. Three art installation works were selected from Chinese Theatre Art Interactive Area in Beijing Design Week to verify the effectiveness of evaluation system. Fifteen adults with normal hearing were recruited to the experiments. The result shows quietness should be highlighted in the work with auditory dominant information, and the work which with auxiliary information in auditory, should to focus on content foresight and consistency of auditory-visual of acoustic information. The results will provide optimization strategy in an active and passive measures for acoustic design in DPS.","2021","2023-07-06 00:54:30","2023-07-19 11:13:05","2023-07-06 00:54:29","869-879","","","275","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Networks and Systems DOI: 10.1007/978-3-030-80091-8_103","","","","","","Ahram, Tareq Z.; Falcão, Christianne S.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GIFSAZSI","journalArticle","2012","El-Shimy, Dalia; Grond, Florian; Olmos, Adriana; Cooperstock, Jeremy R.","Eyes-free environmental awareness for navigation","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0065-5","http://link.springer.com/10.1007/s12193-011-0065-5","","2012-05","2023-07-06 00:54:30","2023-07-06 00:54:30","2023-07-06 00:54:29","131-141","","3-4","5","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"32VFFRU2","journalArticle","2018","Baur, Kilian; Speth, Florina; Nagle, Aniket; Riener, Robert; Klamroth-Marganska, Verena","Music meets robotics: a prospective randomized study on motivation during robot aided therapy","Journal of NeuroEngineering and Rehabilitation","","1743-0003","10.1186/s12984-018-0413-8","https://jneuroengrehab.biomedcentral.com/articles/10.1186/s12984-018-0413-8","Background: Robots have been successfully applied in motor training during neurorehabilitation. As music is known to improve motor function and motivation in neurorehabilitation training, we aimed at integrating music creation into robotic-assisted motor therapy. We developed a virtual game-like environment with music for the arm therapy robot ARMin, containing four different motion training conditions: a condition promoting creativity (C+) and one not promoting creativity (C–), each in a condition with (V+) and without (V–) a visual display (i.e., a monitor). The visual display was presenting the game workspace but not contributing to the creative process itself. In all four conditions the therapy robot haptically displayed the game workspace. Our aim was to asses the effects of creativity and visual display on motivation. Methods: In a prospective randomized single-center study, healthy participants were randomly assigned to play two of the four training conditions, either with (V+) or without visual display (V–). In the third round, the participants played a repetition of the preferred condition of the two first rounds, this time with a new V condition (i.e., with or without visual display). For each of the three rounds, motivation was measured with the Intrinsic Motivation Inventory (IMI) in the subscales interest/enjoyment, perceived choice, value/usefulness, and man-machine-relation. We recorded the actual training time, the time of free movement, and the velocity profile and administered a questionnaire to measure perceived training time and perceived effort. All measures were analysed using linear mixed models. Furthermore, we asked if the participants would like to receive the created music piece. Results: Sixteen healthy subjects (ten males, six females, mean age: 27.2 years, standard deviation: 4.1 years) with no known motor or cognitive deficit participated. Promotion of creativity (i.e., C+ instead of C–) significantly increased the IMI-item interest/enjoyment (p = 0.001) and the IMI-item perceived choice (p = 0.010). We found no significant effects in the IMI-items man-machine relation and value/usefulness. Conditions promoting creativity (with or without visual display) were preferred compared to the ones not promoting creativity. An interaction effect of promotion of creativity and omission of visual display was present for training time (p = 0.013) and training intensity (p < 0.001). No differences in relative perceived training time, perceived effort, and perceived value among the four training conditions were found.","2018-12","2023-07-06 00:54:30","2023-07-21 07:43:39","2023-07-06 00:54:29","79","","1","15","","J NeuroEngineering Rehabil","Music meets robotics","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/Q6GTJCKH/Baur et al. - 2018 - Music meets robotics a prospective randomized stu.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9BACPKPI","bookSection","2010","Verron, Charles; Aramaki, Mitsuko; Kronland-Martinet, Richard; Pallone, Grégory","Spatialized Synthesis of Noisy Environmental Sounds","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_20","In this paper, an overview of the stochastic modeling for analysis/synthesis of noisy sounds is presented. In particular, we focused on the time-frequency domain synthesis based on the inverse fast Fourier transform (IFFT) algorithm from which we proposed the design of a spatialized synthesizer. The originality of this synthesizer remains in its one-stage architecture that efficiently combines the synthesis with 3D audio techniques at the same level of sound generation. This architecture also allowed including a control of the source width rendering to reproduce naturally diffused environments. The proposed approach led to perceptually realistic 3D immersive auditory scenes. Applications of this synthesizer are here presented in the case of noisy environmental sounds such as air swishing, sea wave or wind sound. We finally discuss the limitations but also the possibilities offered by the synthesizer to achieve sound transformations based on the analysis of recorded sounds.","2010","2023-07-06 00:54:30","2023-07-19 11:38:59","2023-07-06 00:54:29","392-407","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_20","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GUVKNHDZ","journalArticle","2016","Walus, Bartlomiej P.; Pauletto, Sandra; Mason-Jones, Amanda","Sonification and music as support to the communication of alcohol-related health risks to young people: Study design and results","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-016-0220-0","http://link.springer.com/10.1007/s12193-016-0220-0","","2016-09","2023-07-06 00:54:30","2023-07-06 00:54:30","2023-07-06 00:54:29","235-246","","3","10","","J Multimodal User Interfaces","Sonification and music as support to the communication of alcohol-related health risks to young people","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/7T7PZLM3/Walus et al. - 2016 - Sonification and music as support to the communica.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"83G37R2E","bookSection","2007","Ag. Ibrahim, Ag. Asri; Hunt, Andy","An HCI Model for Usability of Sonification Applications","Task Models and Diagrams for Users Interface Design","978-3-540-70815-5 978-3-540-70816-2","","","http://link.springer.com/10.1007/978-3-540-70816-2_18","","2007","2023-07-06 00:54:30","2023-07-06 00:54:30","2023-07-06 00:54:29","245-258","","","4385","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-70816-2_18","","","","","","Coninx, Karin; Luyten, Kris; Schneider, Kevin A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B4A2D9VH","journalArticle","2020","Roddy, Stephen; Bridges, Brian","Mapping for meaning: the embodied sonification listening model and its implications for the mapping problem in sonic information design","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-020-00318-y","http://link.springer.com/10.1007/s12193-020-00318-y","This is a theoretical paper that considers the mapping problem, a foundational issue which arises when designing a sonification, as it applies to sonic information design. We argue that this problem can be addressed by using models from the field of embodied cognitive science, including embodied image schema theory, conceptual metaphor theory and conceptual blends, and from research which treats sound and musical structures using these models, when mapping data to sound. However, there are currently very few theoretical frameworks for applying embodied cognition principles in a sonic information design context. This article describes one such framework, the Embodied Sonification Listening Model, which provides a theoretical description of sonification listening in terms of Conceptual Metaphor Theory.","2020-06","2023-07-06 00:54:30","2023-07-20 06:56:45","2023-07-06 00:54:29","143-151","","2","14","","J Multimodal User Interfaces","Mapping for meaning","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/29NLEMJR/Roddy and Bridges - 2020 - Mapping for meaning the embodied sonification lis.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NWDS48SW","bookSection","2017","Matinfar, Sasan; Nasseri, M. Ali; Eck, Ulrich; Roodaki, Hessam; Navab, Navid; Lohmann, Chris P.; Maier, Mathias; Navab, Nassir","Surgical Soundtracks: Towards Automatic Musical Augmentation of Surgical Procedures","Medical Image Computing and Computer-Assisted Intervention − MICCAI 2017","978-3-319-66184-1 978-3-319-66185-8","","","https://link.springer.com/10.1007/978-3-319-66185-8_76","Advances in sensing and digitalization enable us to acquire and present various heterogeneous datasets to enhance clinical decisions. Visual feedback is the dominant way of conveying such information. However, environments rich with many sources of information all presented through the same channel pose the risk of over stimulation and missing crucial information. The augmentation of the cognitive field by additional perceptual modalities such as sound is a workaround to this problem. A major challenge in auditory augmentation is the automatic generation of pleasant and ergonomic audio in complex routines, as opposed to overly simplistic feedback, to avoid fatigue. In this work, without loss of generality to other procedures, we propose a method for aural augmentation of ophthalmic procedures via automatic modification of musical pieces. Evaluations of this first proof of concept regarding recognizability of the conveyed information along with qualitative aesthetics show the potential of our method.","2017","2023-07-06 00:54:30","2023-07-21 04:29:05","2023-07-06 00:54:29","673-681","","","10434","","","Surgical Soundtracks","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-66185-8_76","","","","","","Descoteaux, Maxime; Maier-Hein, Lena; Franz, Alfred; Jannin, Pierre; Collins, D. Louis; Duchesne, Simon","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SL3TX393","bookSection","2010","Fagerlönn, Johan; Liljedahl, Mats","Designing a Web-Based Tool That Informs the Audio Design Process","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_4","Research on auditory displays has shown how various properties of sounds can influence perception and performance. However, a challenge for system developers is how to find signals that correspond to specific user situations and make sense within a user context. This paper presents a web-based tool called AWESOME Sound design tool. The fundamental idea with the tool is to give end users control over some aspects of the auditory stimuli and encourage them to manipulate the sound with a specific user scenario in mind. A first evaluation of the tool has been conducted in which aspects of both usefulness and usability were addressed.","2010","2023-07-06 00:54:30","2023-07-19 11:37:19","2023-07-06 00:54:29","68-79","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_4","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9YCJC2MX","bookSection","2010","Ramakrishnan, Chandrasekhar","Sonification and Information Theory","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_7","","2010","2023-07-06 00:54:30","2023-07-06 00:54:30","2023-07-06 00:54:29","121-142","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_7","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LTKE6NDQ","journalArticle","2022","Hagen, Edward H.","The Biological Roots of Music and Dance: Extending the Credible Signaling Hypothesis to Predator Deterrence","Human Nature","","1045-6767, 1936-4776","10.1007/s12110-022-09429-9","https://link.springer.com/10.1007/s12110-022-09429-9","After they diverged from panins, hominins evolved an increasingly committed terrestrial lifestyle in open habitats that exposed them to increased predation pressure from Africa’s formidable predator guild. In the Pleistocene, Homo transitioned to a more carnivorous lifestyle that would have further increased predation pressure. An effective defense against predators would have required a high degree of cooperation by the smaller and slower hominins. It is in the interest of predator and potential prey to avoid encounters that will be costly for both. A wide variety of species, including carnivores and apes and other primates, have therefore evolved visual and auditory signals that deter predators by credibly signaling detection and/or the ability to effectively defend themselves. In some cooperative species, these predator deterrent signals involve highly synchronized visual and auditory displays among group members. Hagen and Bryant (Human Nature, 14(1), 21–51, 2003) proposed that synchronized visual and auditory displays credibly signal coalition quality. Here, this hypothesis is extended to include credible signals to predators that they have been detected and would be met with a highly coordinated defensive response, thereby deterring an attack. Within-group signaling functions are also proposed. The evolved cognitive abilities underlying these behaviors were foundations for the evolution of fully human music and dance.","2022-09","2023-07-06 00:54:30","2023-07-20 05:53:59","2023-07-06 00:54:29","261-279","","3","33","","Hum Nat","The Biological Roots of Music and Dance","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VTRC2WU7","bookSection","2005","Eldridge, Alice C.","Extra-Music(ologic)al Models for Algorithmic Composition","Applications of Evolutionary Computing","978-3-540-25396-9 978-3-540-32003-6","","","http://link.springer.com/10.1007/978-3-540-32003-6_58","This paper addresses design approaches to algorithmic composition and suggests that music-theoretic tenets alone are unsuitable as prescriptive principles and could be profitably complemented by attempts to represent and recreate dynamical structures of music. Examples of ongoing work using adaptive dynamical processes for generating dynamic structures are presented.","2005","2023-07-06 00:54:30","2023-07-19 11:31:12","2023-07-06 00:54:29","557-562","","","3449","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-32003-6_58","","","","","","Rothlauf, Franz; Branke, Jürgen; Cagnoni, Stefano; Corne, David Wolfe; Drechsler, Rolf; Jin, Yaochu; Machado, Penousal; Marchiori, Elena; Romero, Juan; Smith, George D.; Squillero, Giovanni","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LEWB5JMW","journalArticle","2001","Johannsen, Gunnar","Auditory Displays in Human–Machine Interfaces of Mobile Robots for Non-Speech Communication with Humans","Journal of Intelligent and Robotic Systems","","1573-0409","10.1023/A:1013953213049","https://doi.org/10.1023/A:1013953213049","Auditory displays are developed and investigated for mobile service robots in a human–machine environment. The service robot domain was chosen as an example for future use of auditory displays within multimedia process supervision and control applications in industrial, transportation, and medical systems. The design of directional sounds and of additional sounds for robot states as well as the design of more complicated robot sound tracks are explained. Basic musical elements and robot-movement sounds are combined. Experimental studies on the auditory perception of directional sounds as well as of sound tracks for the predictive display of intended robot trajectories in a simulated supermarket scenario are described.","2001-10-01","2023-07-06 00:54:44","2023-07-06 00:54:44","2023-07-06 00:54:44","161-169","","2","32","","Journal of Intelligent and Robotic Systems","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/LG856EUC/Johannsen - 2001 - Auditory Displays in Human–Machine Interfaces of M.pdf","","","auditory displays; auditory perception; directional sounds; human–machine interfaces; mobile service robots; multimedia process control; robot sound tracks; robot-movement sounds; simulated robot environment; sound communication of states and intentions","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RVS6JQWV","bookSection","2019","Paul, Tirthankar; Vainio, Seppo; Roning, Juha","Towards Personalised, DNA Signature Derived Music via the Short Tandem Repeats (STR)","Intelligent Computing","978-3-030-01176-5 978-3-030-01177-2","","","http://link.springer.com/10.1007/978-3-030-01177-2_69","Our inheritance is defined by the genetic code, which is based on the exact order of the nucleotides A, T, C and G of the deoxyribonucleic acid (DNA) polymer in the nucleus. The personalised genetic makeup is set in meiosis in the germline cells, while the particular genetic construction is established upon fertilisation. Given the capacity to define the exact genome structure by deep sequencing in a high throughput manner in populations, this enabled establishment of gene banks for not only human, but also for a wealth of other species. Computer added programming offers capacity to transform or translate the genetic code to other modalities such as sound. Variations in the nucleotides and the repeat regions of a short sequence, the Short Tandem Repeats (STRs), serve to define the biological identity of a person, especially in 13 genomic, the combined DNA index system (CODIS) locations. The number of STR and the STR sequences were selected as the units of musical elements and programmed to a musical instrument digital interference (MIDI) file to frequencies that can be heard by the ear. The novelty of the work is that an algorithm can be expected to provide DNA-based musical communication platform, data based bio-authentication via the personal music, and to identify for example, the musical similarity score for the family members.","2019","2023-07-06 00:55:52","2023-07-20 06:34:43","2023-07-06 00:55:52","951-964","","","857","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Advances in Intelligent Systems and Computing DOI: 10.1007/978-3-030-01177-2_69","","","","","","Arai, Kohei; Kapoor, Supriya; Bhatia, Rahul","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7JGKXMNY","bookSection","2003","Tzanetakis, George","Musescape: A Tool for Changing Music Collections into Libraries","Research and Advanced Technology for Digital Libraries","978-3-540-40726-3 978-3-540-45175-4","","","http://link.springer.com/10.1007/978-3-540-45175-4_38","","2003","2023-07-06 00:55:52","2023-07-06 00:55:52","2023-07-06 00:55:52","412-421","","","2769","","","Musescape","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-45175-4_38","","","","","","Koch, Traugott; Sølvberg, Ingeborg Torvik","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N2IRMMJU","bookSection","2011","Klapuri, Anssi","Pattern Induction and Matching in Music Signals","Exploring Music Contents","978-3-642-23125-4 978-3-642-23126-1","","","http://link.springer.com/10.1007/978-3-642-23126-1_13","This paper discusses techniques for pattern induction and matching in musical audio. At all levels of music - harmony, melody, rhythm, and instrumentation - the temporal sequence of events can be subdivided into shorter patterns that are sometimes repeated and transformed. Methods are described for extracting such patterns from musical audio signals (pattern induction) and computationally feasible methods for retrieving similar patterns from a large database of songs (pattern matching).","2011","2023-07-06 00:55:52","2023-07-20 00:05:29","2023-07-06 00:55:52","188-204","","","6684","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-23126-1_13","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IGNYNJFM","bookSection","2012","Tulilaulu, Aurora; Paalasmaa, Joonas; Waris, Mikko; Toivonen, Hannu","Sleep Musicalization: Automatic Music Composition from Sleep Measurements","Advances in Intelligent Data Analysis XI","978-3-642-34155-7 978-3-642-34156-4","","","http://link.springer.com/10.1007/978-3-642-34156-4_36","","2012","2023-07-06 00:55:52","2023-07-06 00:55:52","2023-07-06 00:55:52","392-403","","","7619","","","Sleep Musicalization","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-34156-4_36","","","","","","Hollmén, Jaakko; Klawonn, Frank; Tucker, Allan","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3CQP8TR9","bookSection","2013","Marchegiani, Letizia; Fafoutis, Xenofon","A Behavioral Study on the Effects of Rock Music on Auditory Attention","Human Behavior Understanding","978-3-319-02713-5 978-3-319-02714-2","","","http://link.springer.com/10.1007/978-3-319-02714-2_2","We are interested in the distribution of top-down attention in noisy environments, in which the listening capability is challenged by rock music playing in the background. We conducted behavioral experiments in which the subjects were asked to focus their attention on a narrative and detect a specific word, while the voice of the narrator was masked by rock songs that were alternating in the background. Our study considers several types of songs and investigates how their distinct features affect the ability to segregate sounds. Additionally, we examine the effect of the subjects’ familiarity to the music.","2013","2023-07-06 00:55:52","2023-07-20 05:50:43","2023-07-06 00:55:52","15-26","","","8212","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-02714-2_2","","","","","","Salah, Albert Ali; Hung, Hayley; Aran, Oya; Gunes, Hatice","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FT2Z38IT","journalArticle","2016","Albrecht, Robert; Väänänen, Riitta; Lokki, Tapio","Guided by music: pedestrian and cyclist navigation with route and beacon guidance","Personal and Ubiquitous Computing","","1617-4909, 1617-4917","10.1007/s00779-016-0906-z","http://link.springer.com/10.1007/s00779-016-0906-z","Music listening and navigation are both common tasks for mobile device users. In this study, we integrated music listening with a navigation service, allowing users to follow the perceived direction of the music to reach their destination. This navigation interface provided users with two different guidance methods: route guidance and beacon guidance. The user experience of the navigation service was evaluated with pedestrians in a city center and with cyclists in a suburban area. The results show that spatialized music can be used to guide pedestrians and cyclists toward a destination without any prior training, offering a pleasant navigation experience. Both route and beacon guidance were deemed good alternatives, but the preference between them varied from person to person and depended on the situation. Beacon guidance was generally considered to be suitable for familiar surroundings, while route guidance was seen as a better alternative for areas that are unfamiliar or more difficult to navigate.","2016-02","2023-07-06 00:55:52","2023-07-21 04:46:05","2023-07-06 00:55:52","121-145","","1","20","","Pers Ubiquit Comput","Guided by music","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AUKTBA4S","bookSection","2014","Yamaguchi, Daiki; Kiyozaki, Mutsumi; Magatani, Kazushige","A Study of the Influence Which MP3 Formatted Sound Gives EEG of the Human","The 15th International Conference on Biomedical Engineering","978-3-319-02912-2 978-3-319-02913-9","","","https://link.springer.com/10.1007/978-3-319-02913-9_238","currently, many types of no-reversible compressed sound source, represented by MP3 (MPEG Audio Layer-3) are popular in the world and they are widely used to make the music file size smaller. The sound data created in this way has less information as compared to pre-compressed data. The objective of this study is by analyzing EEG to determine if people can recognize such difference as differences in sound. Measurement systems that can measure and analyze EEG when a subject listens to music were experimentally developed. And ten subjects were studied with this system. In this experiment, a WAVE formatted music data and a MP3 compressed music data that is made from the WAVE formatted data were prepared. Each subject was made to hear these music sources at the same volume. From the results of this experiment, clear differences were confirmed between two sound sources.","2014","2023-07-06 00:55:52","2023-07-21 05:04:26","2023-07-06 00:55:52","924-927","","","43","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: IFMBE Proceedings DOI: 10.1007/978-3-319-02913-9_238","","","","","","Goh, James","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8WE8WJQA","journalArticle","2012","Dubus, Gaël","Evaluation of four models for the sonification of elite rowing","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0085-1","http://link.springer.com/10.1007/s12193-011-0085-1","","2012-05","2023-07-06 00:55:52","2023-07-06 00:55:52","2023-07-06 00:55:52","143-156","","3-4","5","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/6B6CYVVU/Dubus - 2012 - Evaluation of four models for the sonification of .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ISDMFL4F","bookSection","2020","Inoue, Naoki; Fujimoto, Yuichiro; Plopski, Alexander; Okahashi, Sayaka; Kanbara, Masayuki; Hsu, Hsiu-Yun; Kuo, Li-Chieh; Su, Fong-Chin; Kato, Hirokazu","Effect of Display Location on Finger Motor Skill Training with Music-Based Gamification","Human Aspects of IT for the Aged Population. Healthy and Active Aging","978-3-030-50248-5 978-3-030-50249-2","","","http://link.springer.com/10.1007/978-3-030-50249-2_6","The motor control of individual fingers is an important part of daily life, but there are many people who have difficulty with it, such as elderly people and stroke patients. While continuous rehabilitation is necessary for functional recovery of finger mobility and suppression of functional deterioration, it usually requires the assistance of occupational therapists. Furthermore, the rehabilitation process can be monotonous, which makes it difficult for patients to maintain their motivation. Over a series of studies, we have developed a finger movement training system that incorporates gamification and is based on playing music using a Pressing Evaluation Training System that can measure the force exerted by each finger. One remaining problem was that patients had difficulty recognizing the fingering information, and it took some time for them to get used to locating this information quickly. In this study, we applied augmented reality (AR) technology to display each sound element as close as possible to the position of the corresponding finger so that the user could directly perceive the information for each finger while wearing the head mounted display. We conducted a user study with 10 university students to determine if the distance between the sound element display position and the location of each finger had an effect on performance. The results indicated that incorporating AR allowed the users to recognize the correct finger positions more quickly.","2020","2023-07-06 00:55:52","2023-07-20 05:50:25","2023-07-06 00:55:52","78-90","","","12208","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-50249-2_6","","","","","","Gao, Qin; Zhou, Jia","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E88EGPD3","bookSection","2014","Argo, Jessica; Ma, Minhua; Kayser, Christoph","Immersive Composition for Sensory Rehabilitation: 3D Visualisation, Surround Sound, and Synthesised Music to Provoke Catharsis and Healing","Serious Games Development and Applications","978-3-319-11622-8 978-3-319-11623-5","","","http://link.springer.com/10.1007/978-3-319-11623-5_12","There is a wide range of sensory therapies using sound, music and visual stimuli. Some focus on soothing or distracting stimuli such as natural sounds or classical music as analgesic, while other approaches emphasize the active performance of producing music as therapy. This paper proposes an immersive multi-sensory Exposure Therapy for people suffering from anxiety disorders, based on a rich, detailed surround-soundscape. This soundscape is composed to include the users’ own idiosyncratic anxiety triggers as a form of habituation, and to provoke psychological catharsis, as a non-verbal, visceral and enveloping exposure. To accurately pinpoint the most effective sounds and to optimally compose the soundscape we will monitor the participants’ physiological responses such as electroencephalography, respiration, electromyography, and heart rate during exposure. We hypothesize that such physiologically optimized sensory landscapes will aid the development of future immersive therapies for various psychological conditions, Sound is a major trigger of anxiety, and auditory hypersensitivity is an extremely problematic symptom. Exposure to stress-inducing sounds can free anxiety sufferers from entrenched avoidance behaviors, teaching physiological coping strategies and encouraging resolution of the psychological issues agitated by the sound.","2014","2023-07-06 00:55:52","2023-07-21 04:59:29","2023-07-06 00:55:52","134-149","","","8778","","","Immersive Composition for Sensory Rehabilitation","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-11623-5_12","","/Users/minsik/Zotero/storage/DG5YYQ83/Argo et al. - 2014 - Immersive Composition for Sensory Rehabilitation .pdf","","","","Ma, Minhua; Oliveira, Manuel Fradinho; Baalsrud Hauge, Jannicke","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SAY2S2ES","bookSection","2006","Camurri, Antonio; Castellano, Ginevra; Ricchetti, Matteo; Volpe, Gualtiero","Subject Interfaces: Measuring Bodily Activation During an Emotional Experience of Music","Gesture in Human-Computer Interaction and Simulation","978-3-540-32624-3 978-3-540-32625-0","","","http://link.springer.com/10.1007/11678816_30","This paper focuses on the relationship between emotions induced by musical stimuli and movement. A pilot experiment has been realized with the aim to verify whether there are correlations between the emotional characterization of music excerpts and human movement. Subjects were asked to move a laser pointer on a white wall in front of them while listening to musical excerpts classified with respect to the type of emotions they can induce. Trajectories obtained moving the laser pointer have been recorded with a video camera and have been analyzed in a static and global way by using the EyesWeb platform. Results highlight a difference between trajectories associated to music stimuli classified as “fast” and “slow”, in term of smoothness/angularity, suggesting the existence of a strong link between the emotional characterization of the musical excerpts listened to and the movement performed.","2006","2023-07-06 00:55:52","2023-07-20 00:08:53","2023-07-06 00:55:52","268-279","","","3881","","","Subject Interfaces","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11678816_30","","","","","","Gibet, Sylvie; Courty, Nicolas; Kamp, Jean-François","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6SEJEF9G","bookSection","2007","Raś, Zbigniew W.; Zhang, Xin; Lewis, Rory","MIRAI: Multi-hierarchical, FS-Tree Based Music Information Retrieval System","Rough Sets and Intelligent Systems Paradigms","978-3-540-73450-5 978-3-540-73451-2","","","http://link.springer.com/10.1007/978-3-540-73451-2_10","","2007","2023-07-06 00:55:52","2023-07-06 00:55:52","2023-07-06 00:55:52","80-89","","","4585","","","MIRAI","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73451-2_10","","","","","","Kryszkiewicz, Marzena; Peters, James F.; Rybinski, Henryk; Skowron, Andrzej","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NSS8GCFW","journalArticle","2007","Nagel, Frederik; Kopiez, Reinhard; Grewe, Oliver; Altenmüller, Eckart","EMuJoy: Software for continuous measurement of perceived emotions in music","Behavior Research Methods","","1554-351X, 1554-3528","10.3758/BF03193159","http://link.springer.com/10.3758/BF03193159","An adequate study of emotions in music and film should be based on the real-time measurement of selfreported data using a continuous-response method. The recording system discussed in this article reflects two important aspects of such research: First, for a better comparison of results, experimental and technical standards for continuous measurement should be taken into account, and second, the recording system should be open to the inclusion of multimodal stimuli. In light of these two considerations, our article addresses four basic principles of the continuous measurement of emotions: (1) the dimensionality of the emotion space, (2) data acquisition (e.g., the synchronization of media and the self-reported data), (3) interface construction for emotional responses, and (4) the use of multiple stimulus modalities. Researcher-developed software (EMuJoy) is presented as a freeware solution for the continuous measurement of responses to different media, along with empirical data from the self-reports of 38 subjects listening to emotional music and viewing affective pictures.","2007-05","2023-07-06 00:55:52","2023-07-19 23:33:55","2023-07-06 00:55:52","283-290","","2","39","","Behavior Research Methods","EMuJoy","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/XQ3XEAUI/Nagel et al. - 2007 - EMuJoy Software for continuous measurement of per.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HFDFQRKI","journalArticle","2012","Fabiani, Marco; Bresin, Roberto; Dubus, Gaël","Interactive sonification of expressive hand gestures on a handheld device","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0076-2","http://link.springer.com/10.1007/s12193-011-0076-2","We present here a mobile phone application called MoodifierLive which aims at using expressive music performances for the sonification of expressive gestures through the mapping of the phone’s accelerometer data to the performance parameters (i.e. tempo, sound level, and articulation). The application, and in particular the sonification principle, is described in detail. An experiment was carried out to evaluate the perceived matching between the gesture and the music performance that it produced, using two distinct mappings between gestures and performance. The results show that the application produces consistent performances, and that the mapping based on data collected from real gestures works better than one defined a priori by the authors.","2012-07","2023-07-06 00:55:52","2023-07-20 06:54:23","2023-07-06 00:55:52","49-57","","1-2","6","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UV7WRQNY","bookSection","1997","Fernström, M.; Bannon, L.","Explorations in Sonic Browsing","People and Computers XII","978-3-540-76172-3 978-1-4471-3601-9","","","http://link.springer.com/10.1007/978-1-4471-3601-9_8","This paper describes a novel browser prototype that has been designed and implemented on PC’s and soundcards. Our focus has been on the development of a usable and engaging interface which exploits both visual and aural features of the data space. The project involves state-of-the-art work in human-computer interaction and multimedia development. We are working on a data set of musical compositions, and are designing and testing the prototype with a group of musicians. This paper provides some detail on the development process, the current architecture of the system, and describes some of the problems encountered.","1997","2023-07-06 00:55:52","2023-07-21 04:41:39","2023-07-06 00:55:52","117-131","","","","","","","","","","","Springer London","London","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-1-4471-3601-9_8","","","","","","Thimbleby, Harold; O’Conaill, Brid; Thomas, Peter J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"86S8BV6J","journalArticle","2013","Sigrist, Roland; Rauter, Georg; Riener, Robert; Wolf, Peter","Augmented visual, auditory, haptic, and multimodal feedback in motor learning: A review","Psychonomic Bulletin & Review","","1069-9384, 1531-5320","10.3758/s13423-012-0333-8","http://link.springer.com/10.3758/s13423-012-0333-8","","2013-02","2023-07-06 00:55:52","2023-07-06 00:55:52","2023-07-06 00:55:52","21-53","","1","20","","Psychon Bull Rev","Augmented visual, auditory, haptic, and multimodal feedback in motor learning","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/JKN5H5IL/Sigrist et al. - 2013 - Augmented visual, auditory, haptic, and multimodal.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MGQ48K4T","bookSection","2010","Hug, Daniel","Investigating Narrative and Performative Sound Design Strategies for Interactive Commodities","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_2","","2010","2023-07-06 00:55:52","2023-07-06 00:55:52","2023-07-06 00:55:52","12-40","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_2","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TBWWWE6T","bookSection","2007","Korhonen, Hannu; Holm, Jukka; Heikkinen, Mikko","Utilizing Sound Effects in Mobile User Interface Design","Human-Computer Interaction – INTERACT 2007","978-3-540-74794-9 978-3-540-74796-3","","","http://link.springer.com/10.1007/978-3-540-74796-3_27","","2007","2023-07-06 00:55:52","2023-07-06 00:55:52","2023-07-06 00:55:52","283-296","","","4662","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-74796-3_27","","/Users/minsik/Zotero/storage/ZWXDPSGF/Korhonen et al. - 2007 - Utilizing Sound Effects in Mobile User Interface D.pdf","","","","Baranauskas, Cécilia; Palanque, Philippe; Abascal, Julio; Barbosa, Simone Diniz Junqueira","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TW3Z49TW","journalArticle","2015","Dubus, Gaël; Bresin, Roberto","Exploration and evaluation of a system for interactive sonification of elite rowing","Sports Engineering","","1369-7072, 1460-2687","10.1007/s12283-014-0164-0","http://link.springer.com/10.1007/s12283-014-0164-0","","2015-03","2023-07-06 00:55:52","2023-07-06 00:55:52","2023-07-06 00:55:52","29-41","","1","18","","Sports Eng","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6U98EHU5","bookSection","2005","Beilharz, Kirsty","Responsive Sensate Environments: Past and Future Directions","Computer Aided Architectural Design Futures 2005","978-1-4020-3460-2","","","http://link.springer.com/10.1007/1-4020-3698-1_34","","2005","2023-07-06 00:57:05","2023-07-06 00:57:05","2023-07-06 00:57:05","361-370","","","","","","Responsive Sensate Environments","","","","","Springer-Verlag","Berlin/Heidelberg","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/1-4020-3698-1_34","","","","","","Martens, Bob; Brown, Andre","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HBS7N37M","bookSection","2014","Sterkenburg, Jason; Jeon, Myounghoon; Plummer, Christopher","Auditory Emoticons: Iterative Design and Acoustic Characteristics of Emotional Auditory Icons and Earcons","Human-Computer Interaction. Advanced Interaction Modalities and Techniques","978-3-319-07229-6 978-3-319-07230-2","","","http://link.springer.com/10.1007/978-3-319-07230-2_60","","2014","2023-07-06 00:57:05","2023-07-06 00:57:05","2023-07-06 00:57:05","633-640","","","8511","","","Auditory Emoticons","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-07230-2_60","","","","","","Kurosu, Masaaki","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Kobsa, Alfred; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Doug; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SPCCNLU4","journalArticle","2022","Paté, Arthur; Farge, Gaspard; Holtzman, Benjamin K.; Barth, Anna C.; Poli, Piero; Boschi, Lapo; Karlstrom, Leif","Combining audio and visual displays to highlight temporal and spatial seismic patterns","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-021-00378-8","https://link.springer.com/10.1007/s12193-021-00378-8","Data visualization, and to a lesser extent data sonification, are classic tools to the scientific community. However, these two approaches are very rarely combined, although they are highly complementary: our visual system is good at recognizing spatial patterns, whereas our auditory system is better tuned for temporal patterns. In this article, data representation methods are proposed that combine visualization, sonification, and spatial audio techniques, in order to optimize the user’s perception of spatial and temporal patterns in a single display, to increase the feeling of immersion, and to take advantage of multimodal integration mechanisms. Three seismic data sets are used to illustrate the methods, covering different physical phenomena, time scales, spatial distributions, and spatio-temporal dynamics. The methods are adapted to the specificities of each data set, and to the amount of information that the designer wants to display. This leads to further developments, namely the use of audification with two time scales, the switch from pure audification to time-modulated noise, and the switch from pure audification to sonic icons. First user feedback from live demonstrations indicates that the methods presented in this article seem to enhance the perception of spatio-temporal patterns, which is a key parameter to the understanding of seismically active systems, and a step towards apprehending the processes that drive this activity.","2022-03","2023-07-06 00:57:05","2023-07-20 07:03:47","2023-07-06 00:57:05","125-142","","1","16","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ADLSI7W5","bookSection","2019","Rogozinsky, Gleb G.; Lyzhinkin, Konstantin; Egorova, Anna; Podolsky, Dmitry","Distributed Software Hardware Solution for Complex Network Sonification","Language, Music and Computing","978-3-030-05593-6 978-3-030-05594-3","","","http://link.springer.com/10.1007/978-3-030-05594-3_12","","2019","2023-07-06 00:57:05","2023-07-06 00:57:05","2023-07-06 00:57:05","149-160","","","943","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Communications in Computer and Information Science DOI: 10.1007/978-3-030-05594-3_12","","","","","","Eismont, Polina; Mitrenina, Olga; Pereltsvaig, Asya","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MIVXPZJH","journalArticle","2012","Grond, Florian; Hermann, Thomas","Aesthetic strategies in sonification","AI & SOCIETY","","0951-5666, 1435-5655","10.1007/s00146-011-0341-7","http://link.springer.com/10.1007/s00146-011-0341-7","","2012-05","2023-07-06 00:57:05","2023-07-06 00:57:05","2023-07-06 00:57:05","213-222","","2","27","","AI & Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NZJRYABC","journalArticle","1971","Pollack, Irwin","Discrimination of restrictions in sequentially-encoded auditory displays: Block designs","Perception & Psychophysics","","0031-5117, 1532-5962","10.3758/BF03213028","http://link.springer.com/10.3758/BF03213028","Sequential constraints, in the form represented by equal frequencies within blocks, were imposed upon finite-state statistical generators. The sequences were encoded in the form of interval-coded pulse trains and were transduced to sound by earphones. In discrimination tests, interval thresholds between the finite states provided a measure of the relative discriminability between different sequential constraints. These thresholds are shown to be quantitatively related to the difference in uncertainty in specification of the sequences. To a first approximation: Equal interstate interval thresholds are associated with equal differences in uncertainty.","1971-01","2023-07-06 00:57:05","2023-07-21 04:44:29","2023-07-06 00:57:05","57-60","","1","9","","Perception & Psychophysics","Discrimination of restrictions in sequentially-encoded auditory displays","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/ZN8TIT2P/Pollack - 1971 - Discrimination of restrictions in sequentially-enc.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WVP9LY3I","bookSection","2014","Zhang, Yuxi; Huang, Yifeng; Yue, Junwei; Zhang, Liqing","Sonification for EEG Frequency Spectrum and EEG-Based Emotion Features","Neural Information Processing","978-3-319-12642-5 978-3-319-12643-2","","","http://link.springer.com/10.1007/978-3-319-12643-2_6","","2014","2023-07-06 00:57:05","2023-07-06 00:57:05","2023-07-06 00:57:05","42-49","","","8836","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-12643-2_6","","","","","","Loo, Chu Kiong; Yap, Keem Siah; Wong, Kok Wai; Beng Jin, Andrew Teoh; Huang, Kaizhu","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AZLUGIG2","bookSection","2008","Devallez, Delphine; Rocchesso, Davide; Fontana, Federico","An Audio-Haptic Interface Concept Based on Depth Information","Haptic and Audio Interaction Design","978-3-540-87882-7 978-3-540-87883-4","","","http://link.springer.com/10.1007/978-3-540-87883-4_11","","2008","2023-07-06 00:57:05","2023-07-06 00:57:05","2023-07-06 00:57:05","102-110","","","5270","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-87883-4_11","","","","","","Pirhonen, Antti; Brewster, Stephen","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"78MA7479","bookSection","2008","Kendall, Gary S.; Ardila, Mauricio","The Artistic Play of Spatial Organization: Spatial Attributes, Scene Analysis and Auditory Spatial Schemata","Computer Music Modeling and Retrieval. Sense of Sounds","978-3-540-85034-2 978-3-540-85035-9","","","http://link.springer.com/10.1007/978-3-540-85035-9_8","Electroacoustic music lacks a definitive vocabulary for describing its spatiality. Not only does it lack a vocabulary for describing the spatial attributes of individual sound sources, it lacks a vocabulary for describing how these attributes participate in artistic design and expression. Following work by Rumsey [15] the definition of spatial attributes is examined in the broader context of auditory scene analysis. A limited number of spatial attributes are found to be adequate to characterize the individual levels of organization nested within the auditory scene (levels that for acoustic music Rumsey labels as source, ensemble, room and scene). These levels are then viewed as products of both tangible spatial relationships and auditory spatial schemata, the recurrent patterns by which listeners understand the behavior of sound in space. In electroacoustic music the interrelationship of spatial attributes and spatial schemata is often engaged in a play of perceptual grouping that blurs and confounds distinctions like source and ensemble. Our ability to describe and categorize these complex interactions depends on having clear concepts and terminology so that we can recognize the crisscrossing of boundaries and the violation of conventions in this artistic interplay.","2008","2023-07-06 00:57:05","2023-07-19 23:49:08","2023-07-06 00:57:05","125-138","","","4969","","","The Artistic Play of Spatial Organization","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-85035-9_8","","/Users/minsik/Zotero/storage/874C2RG4/Kendall and Ardila - 2008 - The Artistic Play of Spatial Organization Spatial.pdf","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QAEU6DYS","bookSection","2022","Asonitis, Tasos; Allmendinger, Richard; Benatan, Matt; Climent, Ricardo","SonOpt: Sonifying Bi-objective Population-Based Optimization Algorithms","Artificial Intelligence in Music, Sound, Art and Design","978-3-031-03788-7 978-3-031-03789-4","","","https://link.springer.com/10.1007/978-3-031-03789-4_1","We propose SonOpt, the first (open source) data sonification application for monitoring the progress of bi-objective populationbased optimization algorithms during search, to facilitate algorithm understanding. SonOpt provides insights into convergence/stagnation of search, the evolution of the approximation set shape, location of recurring points in the approximation set, and population diversity. The benefits of data sonification have been shown for various non-optimization related monitoring tasks. However, very few attempts have been made in the context of optimization and their focus has been exclusively on single-objective problems. In comparison, SonOpt is designed for biobjective optimization problems, relies on objective function values of non-dominated solutions only, and is designed with the user (listener) in mind; avoiding convolution of multiple sounds and prioritising ease of familiarizing with the system. This is achieved using two sonification paths relying on the concepts of wavetable and additive synthesis. This paper motivates and describes the architecture of SonOpt, and then validates SonOpt for two popular multi-objective optimization algorithms (NSGA-II and MOEA/D). Experience SonOpt yourself via https://github.com/tasos- a/SonOpt- 1.0 .","2022","2023-07-06 00:57:05","2023-07-19 11:33:28","2023-07-06 00:57:05","3-18","","","13221","","","SonOpt","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-031-03789-4_1","","/Users/minsik/Zotero/storage/IFXX5MMK/Asonitis et al. - 2022 - SonOpt Sonifying Bi-objective Population-Based Op.pdf","","","","Martins, Tiago; Rodríguez-Fernández, Nereida; Rebelo, Sérgio M.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IJ6RLFGU","bookSection","2010","Rubisch, Julian; Husinsky, Matthias; Raffaseder, Hannes","AllThatSounds: Associative Semantic Categorization of Audio Data","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_25","","2010","2023-07-06 00:57:05","2023-07-06 00:57:05","2023-07-06 00:57:05","483-491","","","5954","","","AllThatSounds","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_25","","/Users/minsik/Zotero/storage/Q2JYUDJG/Rubisch et al. - 2010 - AllThatSounds Associative Semantic Categorization.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4VMJAZJM","bookSection","2019","Arai, Kohei","Method for Audible Representation of Meteorological Data Derived from Remote Sensing Satellites","Intelligent Systems and Applications","978-3-030-01056-0 978-3-030-01057-7","","","http://link.springer.com/10.1007/978-3-030-01057-7_83","Method for audible representation of meteorological data derived from remote sensing satellites is proposed. Audible representation of multi-dimensional meteorological data layered wind direction and speed data derived from Earth observation satellite data is developed. Experimental results show rapidly changing meteorological data can be listened with the different musical scale (melody), harmony, rhythm and music instrument types. More importantly, weather analysts may much easily be aware of the inversely proportional relation between altitude and air temperature by hearing the multi-dimensional meteorological data.","2019","2023-07-06 00:57:05","2023-07-20 06:35:48","2023-07-06 00:57:05","1139-1149","","","869","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Advances in Intelligent Systems and Computing DOI: 10.1007/978-3-030-01057-7_83","","","","","","Arai, Kohei; Kapoor, Supriya; Bhatia, Rahul","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XNWPNUAE","bookSection","2012","Aramaki, Mitsuko; Kronland-Martinet, Richard; Ystad, Sølvi","Perceptual Control of Environmental Sound Synthesis","Speech, Sound and Music Processing: Embracing Research in India","978-3-642-31979-2 978-3-642-31980-8","","","http://link.springer.com/10.1007/978-3-642-31980-8_13","","2012","2023-07-06 00:57:05","2023-07-06 00:57:05","2023-07-06 00:57:05","172-186","","","7172","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-31980-8_13","","/Users/minsik/Zotero/storage/SMH7GMMC/Aramaki et al. - 2012 - Perceptual Control of Environmental Sound Synthesi.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer; Mohanty, Sanghamitra","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9EUXVA8W","journalArticle","2010","Valle, Andrea","Environmental Sound Synthesis, Processing, and Retrieval","EURASIP Journal on Audio, Speech, and Music Processing","","1687-4714, 1687-4722","10.1155/2010/178164","http://asmp.eurasipjournals.com/content/2010/1/178164","","2010","2023-07-06 00:57:05","2023-07-06 00:57:05","2023-07-06 00:57:05","1-3","","","2010","","EURASIP Journal on Audio, Speech, and Music Processing","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/KYD5ZQMG/Valle - 2010 - Environmental Sound Synthesis, Processing, and Ret.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6NR58K6M","journalArticle","2014","Wu, Dan; Li, Chao-yi; Liu, Jie; Lu, Jing; Yao, De-zhong","Scale-free brain ensemble modulated by phase synchronization","Journal of Zhejiang University SCIENCE C","","1869-1951, 1869-196X","10.1631/jzus.C1400199","http://link.springer.com/10.1631/jzus.C1400199","","2014-10","2023-07-06 00:57:05","2023-07-06 00:57:05","2023-07-06 00:57:05","821-831","","10","15","","J. Zhejiang Univ. - Sci. C","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2GWR8TI3","journalArticle","2009","Valente, Luis; De Souza, Clarisse Sieckenius; Feijó, Bruno","Turn off the graphics: designing non-visual interfaces for mobile phone games","Journal of the Brazilian Computer Society","","0104-6500, 1678-4804","10.1007/BF03192576","https://journal-bcs.springeropen.com/articles/10.1007/BF03192576","Abstract             Mobile phones are a widespread platform for ICT applications because they are highly pervasive in contemporary society. Hence, we can think of mobile gaming as a serious candidate to being a prominent form of entertainment in the near future. However, most games (for computers, console and mobile devices) make extensive use of the visual medium, which tends to exclude visually-impaired users from the play. While mobile gaming could potentially reach many visually-impaired users, who are very familiar with this technology, currently there seems to be only very few alternatives for this community. In an attempt to explore new interactive possibilities for such users, this work presents an initial study on non-visual interfaces for mobile phone games. It is based on Semiotic Engineering principles, emphasizing communication through aural, tactile and gestural signs, and deliberately excluding visual information. Results include a number of issues that can be incorporated to a wider research agenda on mobile gaming accessibility, both for the visually-impaired and sighted.","2009-03","2023-07-06 00:57:05","2023-07-06 00:57:05","2023-07-06 00:57:05","45-58","","1","15","","J Braz Comp Soc","Turn off the graphics","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/7P5L73AN/Valente et al. - 2009 - Turn off the graphics designing non-visual interf.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UND37FYN","bookSection","2010","Mariette, Nicholas","Navigation Performance Effects of Render Method and Head-Turn Latency in Mobile Audio Augmented Reality","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_13","This study assessed participant performance of an outdoor navigation task using a mobile audio augmented reality system. Several quantitative performance measures and one subjective measure were used to compare the perceptual efficacy of Ambisonic and VBAP binaural rendering techniques, and a range of head-turn latencies. The study extends existing indoors research on the effects of head-turn latency for seated listeners. The pilot experiment found that a source capture radius of 2 meters significantly affected the sole participant’s navigation distance efficiency compared to other radii. The main experiment, using 8 participants, found that render method significantly affected all performance measures except subjective stability rating, while head-turn latency only affected mean track curvature and subjective stability. Results also showed an interaction in which the choice of rendering method mitigated or potentiated the effects of head-turn latency on perceived source stability.","2010","2023-07-06 00:57:05","2023-07-19 11:38:17","2023-07-06 00:57:05","239-265","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_13","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QQ9I4DFN","bookSection","2012","Nymoen, Kristian; Torresen, Jim; Godøy, Rolf Inge; Jensenius, Alexander Refsum","A Statistical Approach to Analyzing Sound Tracings","Speech, Sound and Music Processing: Embracing Research in India","978-3-642-31979-2 978-3-642-31980-8","","","http://link.springer.com/10.1007/978-3-642-31980-8_11","This paper presents an experiment on sound tracing, meaning an experiment on how people relate motion to sound. 38 participants were presented with 18 short sounds, and instructed to move their hands in the air while acting as though the sound was created by their hand motion. The hand motion of the participants was recorded, and has been analyzed using statistical tests, comparing results between different sounds, between different subjects, and between different sound classes. We have identified several relationships between sound and motion which are present in the majority of the subjects. A clear distinction was found in onset acceleration for motion to sounds with an impulsive dynamic envelope compared to non-impulsive sounds. Furthermore, vertical movement has been shown to be related to sound frequency, both in terms of spectral centroid and pitch. Moreover, a significantly higher amount of overall acceleration was observed for non-pitched sounds as compared to pitched sounds.","2012","2023-07-06 00:57:05","2023-07-21 05:02:57","2023-07-06 00:57:05","120-145","","","7172","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-31980-8_11","","/Users/minsik/Zotero/storage/X2IL8W9N/Nymoen et al. - 2012 - A Statistical Approach to Analyzing Sound Tracings.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer; Mohanty, Sanghamitra","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T78539B7","bookSection","2002","Lorho, Gaëtan; Hiipakka, Jarmo; Marila, Juha","Structured Menu Presentation Using Spatial Sound Separation","Human Computer Interaction with Mobile Devices","978-3-540-44189-2 978-3-540-45756-5","","","http://link.springer.com/10.1007/3-540-45756-9_51","This paper describes a technique to support user interaction in a hierarchical menu, based on spatial sound separation. A complex menu structure is represented in space using a limited number of sound positions obtained by stereo panning or 3-D audio processing techniques. Spatial organisation of menu items can be designed in a logical way to provide navigation cues to the user, independent of the menu item nature. Two different strategies for menu presentation and interaction are described and compared in this paper. Finally, an application of this technique to the navigation in a large music collection is considered. This case study is an interesting example of usage situation for which eyes-free interaction would be useful, for instance on a portable audio player using headphones and a small remote control.","2002","2023-07-06 00:57:05","2023-07-20 05:51:07","2023-07-06 00:57:05","419-424","","","2411","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-45756-9_51","","","","","","Paternò, Fabio","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4H2QHM84","bookSection","2008","Pirhonen, Antti; Tuuri, Kai","In Search for an Integrated Design Basis for Audio and Haptics","Haptic and Audio Interaction Design","978-3-540-87882-7 978-3-540-87883-4","","","http://link.springer.com/10.1007/978-3-540-87883-4_9","","2008","2023-07-06 00:58:15","2023-07-06 00:58:15","2023-07-06 00:58:15","81-90","","","5270","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-87883-4_9","","","","","","Pirhonen, Antti; Brewster, Stephen","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HSPZQ8JW","journalArticle","1974","Cutting, James E.; Rosner, Burton S.","Categories and boundaries in speech and music*","Perception & Psychophysics","","0031-5117, 1532-5962","10.3758/BF03198588","http://link.springer.com/10.3758/BF03198588","Perceptual categories and boundaries arise when Ss respond to continuous variation on a physical dimension in a discontinuous fashion. It is more difficult to discriminate between members of the same category than to discriminate between members of different categories, even though the amount of physical difference between both pairs is the same. Speech stimuli have been the sole class of auditory signals to yield such perception; for example, each different consonant phoneme serves as a category label. Experiment I demonstrates that categories and boundaries occur for both speech and nonspeech stimuli differing in rise time. Experiment II shows that rise time cues categorical differences in both complex and simple nonspeech waveforms. Taken together, these results suggest that certain aspects of speech perception are intimately related to processes and mechanisms exploited in other domains. The many categories in speech may be based on categories that occur elsewhere in auditory perception.","1974-05","2023-07-06 00:58:15","2023-07-21 04:43:33","2023-07-06 00:58:15","564-570","","3","16","","Perception & Psychophysics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/JD2ZRUPN/Cutting and Rosner - 1974 - Categories and boundaries in speech and music.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W7D5UHU3","bookSection","2021","Paterson, Estrella; Sanderson, Penelope; Mohamed, Ismail; Salisbury, Isaac; Loeb, Robert G.; Paterson, Neil","Testing Interventions in a Medical Simulator: Challenges and Solutions","Proceedings of the 21st Congress of the International Ergonomics Association (IEA 2021)","978-3-030-74610-0 978-3-030-74611-7","","","https://link.springer.com/10.1007/978-3-030-74611-7_57","When testing medical interventions in a simulator, establishing an environment that allows findings to generalize to clinical settings; designing scenarios that are representative of clinical situations and that can be delivered consistently; and ensuring correct operation of simulator systems to ensure efficient data collection, can be challenging. We address these factors using the example of a study that tested auditory displays for the pulse oximeter conducted in a medical simulator with anesthesiologist participants. To establish fidelity of the clinical setting, the simulator was arranged as an operating room (OR) and actors trained to conduct surgery based on real cases. To ensure that scenarios were presented uniformly to each participant, we devised a novel approach: visual displays of vital signs were video-recorded and auditory displays then dubbed into the recordings that were displayed to participants via a monitor. Timing of actors’ actions was controlled by the simulator coordinator via verbal cues through earpieces according to a script that corresponded to the recorded displays. To allow for any technical malfunctions and to ensure effective data collection, we established redundant systems for recording scenarios and carried out piloting and training prior to conducting experimental sessions. Taking into account these factors, we were able to show that a novel auditory display was more effective for identifying oxygen saturation values than a standard display. Participants appeared to accept the simulator setting as natural and reacted in ways similar to behavior displayed in the OR. We conducted 20 sessions without any loss of data.","2021","2023-07-06 00:58:15","2023-07-21 04:55:25","2023-07-06 00:58:15","417-423","","","222","","","Testing Interventions in a Medical Simulator","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Networks and Systems DOI: 10.1007/978-3-030-74611-7_57","","","","","","Black, Nancy L.; Neumann, W. Patrick; Noy, Ian","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5EHGIRI7","bookSection","2006","Vialatte, François B.; Cichocki, Andrzej","Sparse Bump Sonification: A New Tool for Multichannel EEG Diagnosis of Mental Disorders; Application to the Detection of the Early Stage of Alzheimer’s Disease","Neural Information Processing","978-3-540-46484-6 978-3-540-46485-3","","","http://link.springer.com/10.1007/11893295_11","This paper investigates the use of sound and music as a means of representing and analyzing multichannel EEG recordings. Specific focus is given to applications in early detection and diagnosis of early stage of Alzheimer’s disease. We propose here a novel approach based on multi channel sonification, with a time-frequency representation and sparsification process using bump modeling. The fundamental question explored in this paper is whether clinically valuable information, not available from the conventional graphical EEG representation, might become apparent through an audio representation. Preliminary evaluation of the obtained music score – by sample entropy, number of notes, and synchronous activity – incurs promising results.","2006","2023-07-06 00:58:15","2023-07-21 04:36:58","2023-07-06 00:58:15","92-101","","","4234","","","Sparse Bump Sonification","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11893295_11","","","","","","King, Irwin; Wang, Jun; Chan, Lai-Wan; Wang, DeLiang","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NUJVURMS","journalArticle","2019","Wolf, KatieAnna E.; Fiebrink, Rebecca","Personalised interactive sonification of musical performance data","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-019-00294-y","http://link.springer.com/10.1007/s12193-019-00294-y","","2019-09","2023-07-06 00:58:15","2023-07-06 00:58:15","2023-07-06 00:58:15","245-265","","3","13","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/WQQXDA38/Wolf and Fiebrink - 2019 - Personalised interactive sonification of musical p.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TBGJTIZG","bookSection","2010","Neff, Flaithri; Mehigan, Tracey J.; Pitt, Ian","Accelerometer & Spatial Audio Technology: Making Touch-Screen Mobile Devices Accessible","Computers Helping People with Special Needs","978-3-642-14096-9 978-3-642-14097-6","","","http://link.springer.com/10.1007/978-3-642-14097-6_28","As mobile-phone design moves toward a touch-screen form factor, the visually disabled are faced with new accessibility challenges. The mainstream interaction model for touch-screen devices relies on the user having the ability to see spatially arranged visual icons, and to interface with these icons via a smooth glass screen. An inherent challenge for blind users with this type of interface is its lack of tactile feedback. In this paper we explore the concept of using a combination of spatial audio and accelerometer technology to enable blind users to effectively operate a touch-screen device. We discuss the challenges involved in representing icons using sound and we introduce a design framework that is helping us tease out some of these issues. We also outline a set of proposed user-studies that will test the effectiveness of our design using a Nokia N97. The results of these studies will be presented at ICCHP 2010.","2010","2023-07-06 00:58:15","2023-07-19 23:52:34","2023-07-06 00:58:15","170-177","","","6179","","","Accelerometer & Spatial Audio Technology","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-14097-6_28","","","","","","Miesenberger, Klaus; Klaus, Joachim; Zagler, Wolfgang; Karshmer, Arthur","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z95RLDJH","journalArticle","2020","Burdick, Kendall J.; Jorgensen, Seiver K.; Combs, Taylor N.; Holmberg, Megan O.; Kultgen, Samantha P.; Schlesinger, Joseph J.","SAVIOR ICU: sonification and vibrotactile interface for the operating room and intensive care unit","Journal of Clinical Monitoring and Computing","","1387-1307, 1573-2614","10.1007/s10877-019-00381-1","http://link.springer.com/10.1007/s10877-019-00381-1","Alarm fatigue is an issue for healthcare providers in the intensive care unit, and may result from desensitization of overbearing and under-informing alarms. To directly increase the overall identification of medical alarms and potentially contribute to a downstream decrease in the prevalence of alarm fatigue, we propose advancing alarm sonification by combining auditory and tactile stimuli to create a multisensory alarm. Participants completed four trials—two multisensory (auditory and tactile) and two unisensory (auditory). Analysis compared the unisensory trials to the multisensory trials based on the percentage of correctly identified point of change, direction of change and identity of three physiological parameters (indicated by different instruments): heart rate (drums), blood pressure (piano), blood oxygenation (guitar). A repeated-measures of ANOVA yielded a significant improvement in performance for the multisensory group compared to the unisensory group (p < 0.05). Specifically, the multisensory group had better performance in correctly identifying parameter (p < 0.05) and point of change (p < 0.05) compared to the unisensory group. Participants demonstrated a higher accuracy of identification with the use of multisensory alarms. Therefore, multisensory alarms may relieve the auditory burden of the medical environment and increase the overall quality of care and patient safety.","2020-08","2023-07-06 00:58:15","2023-07-20 06:46:47","2023-07-06 00:58:15","787-796","","4","34","","J Clin Monit Comput","SAVIOR ICU","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PZBV6PN8","bookSection","2010","Aramaki, Mitsuko; Gondre, Charles; Kronland-Martinet, Richard; Voinier, Thierry; Ystad, Sølvi","Imagine the Sounds: An Intuitive Control of an Impact Sound Synthesizer","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_21","In this paper we present a synthesizer developed for musical and Virtual Reality purposes that offers an intuitive control of impact sounds. A three layer control strategy is proposed for this purpose, where the top layer gives access to a control of the sound source through verbal descriptions, the middle layer to a control of perceptually relevant sound descriptors, while the bottom layer is directly linked to the parameters of the additive synthesis model. The mapping strategies between the parameters of the different layers are described. The synthesizer has been implemented using Max/MSP, offering the possibility to manipulate intrinsic characteristics of sounds in real-time through the control of few parameters.","2010","2023-07-06 00:58:15","2023-07-19 11:36:16","2023-07-06 00:58:15","408-421","","","5954","","","Imagine the Sounds","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_21","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3HA3BM84","bookSection","2013","Betsworth, Liam; Rajput, Nitendra; Srivastava, Saurabh; Jones, Matt","Audvert: Using Spatial Audio to Gain a Sense of Place","Human-Computer Interaction – INTERACT 2013","978-3-642-40497-9 978-3-642-40498-6","","","http://link.springer.com/10.1007/978-3-642-40498-6_35","We introduce Audvert – a system that facilitates serendipitous discovery and navigation through spatial audio; used to navigate and discover points of interest in large, unfamiliar indoor environments. Our main aim was to create a lightweight spatial audio display that can convey a sense of a place without complex point and select interactions. We conducted a preliminary study comparing two audio types to see which best suited sound localization and a study of Audvert used in a real world scenario. Our findings suggest that long continuous audio performs better than short intermittent audio for sound localisation. We also discover a change in behaviour when using the system, with a large percentage of users wanting to visit newly discovered shops after using the system. We discuss the findings and draw research conclusions.","2013","2023-07-06 00:58:15","2023-07-20 06:28:52","2023-07-06 00:58:15","455-462","","","8120","","","Audvert","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-40498-6_35","","/Users/minsik/Zotero/storage/YLWWBCSG/Betsworth et al. - 2013 - Audvert Using Spatial Audio to Gain a Sense of Pl.pdf","","","","Kotzé, Paula; Marsden, Gary; Lindgaard, Gitte; Wesson, Janet; Winckler, Marco","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JB55U9VL","bookSection","2009","Godøy, Rolf Inge","Geometry and Effort in Gestural Renderings of Musical Sound","Gesture-Based Human-Computer Interaction and Simulation","978-3-540-92864-5 978-3-540-92865-2","","","http://link.springer.com/10.1007/978-3-540-92865-2_23","As may be seen at concerts and in various everyday listening situations, people often make spontaneous gestures when listening to music. We believe these gestures are interesting to study because they may reveal important features of musical experience. In particular, hand movements may give us information on what features are perceived as salient by listeners. Based on various current ideas on embodied cognition, the aim of this paper is to argue that gestures are integral to music perception, and to present research in support of this. A conceptual model of separating geometry and effort is presented in order to better understand the variety of music-related gestures we may observe, leading up to some ideas on how to apply this conceptual model in present and future research.","2009","2023-07-06 00:58:15","2023-07-20 00:09:21","2023-07-06 00:58:15","205-215","","","5085","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-92865-2_23","","/Users/minsik/Zotero/storage/KJ6CQJNG/Godøy - 2009 - Geometry and Effort in Gestural Renderings of Musi.pdf","","","","Sales Dias, Miguel; Gibet, Sylvie; Wanderley, Marcelo M.; Bastos, Rafael","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WIVWW8SF","bookSection","2007","Jung, Ralf; Schwartz, Tim","A Location-Adaptive Human-Centered Audio Email Notification Service for Multi-user Environments","Human-Computer Interaction. HCI Intelligent Multimodal Interaction Environments","978-3-540-73108-5 978-3-540-73110-8","","","http://link.springer.com/10.1007/978-3-540-73110-8_36","In this paper, we introduce an application for a discreet notification of mobile persons in a multi-user environment. In particular we use the current user position to provide a personalized email notification with non-speech audio cues embedded in aesthetic background music. The notification is done in a peripheral way to avoid distration of other people in the surrounding.","2007","2023-07-06 00:58:15","2023-07-20 06:31:39","2023-07-06 00:58:15","340-348","","","4552","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73110-8_36","","/Users/minsik/Zotero/storage/QF6PF3DL/Jung and Schwartz - 2007 - A Location-Adaptive Human-Centered Audio Email Not.pdf","","","","Jacko, Julie A.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K48AWIBI","bookSection","2010","Dicke, Christina; Aaltonen, Viljakaisa; Billinghurst, Mark","Simulator Sickness in Mobile Spatial Sound Spaces","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_15","","2010","2023-07-06 00:58:15","2023-07-06 00:58:15","2023-07-06 00:58:15","287-305","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_15","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YNXXAV9C","bookSection","2006","Coleman, Graeme W.; Macaulay, Catriona; Newell, Alan F.","Listen to This – Using Ethnography to Inform the Design of Auditory Interfaces","Haptic and Audio Interaction Design","978-3-540-37595-1 978-3-540-37596-8","","","http://link.springer.com/10.1007/11821731_13","","2006","2023-07-06 00:58:15","2023-07-06 00:58:15","2023-07-06 00:58:15","133-144","","","4129","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11821731_13","","","","","","McGookin, David; Brewster, Stephen","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BVM9A9YC","journalArticle","2012","Ness, Steven; Reimer, Paul; Love, Justin; Schloss, W. Andrew; Tzanetakis, George","Sonophenology: A multimodal tangible interface for the sonification of phenological data at multiple time-scales","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0066-4","http://link.springer.com/10.1007/s12193-011-0066-4","The study of periodic biological processes, such as when plants flower and birds arrive in the spring is known as Phenology. In recent years this field has gained interest from the scientific community because of the applicability of this data to the study of climate change and other ecological processes. In this paper we propose the use of tangible interfaces for interactive sonification with a specific example of a multimodal tangible interface consisting of a physical paper map and tracking of fiducial markers combined with a novel drawing interface. The designed interface enables one or more users to specify point queries with the map interface and to specify time queries with the drawing interface. This allows the user to explore both time and space while receiving immediate sonic feedback of their actions. This system can be used to study and explore the effects of climate change, both as tool to be used by scientists, and as a way to educate and involve members of the general public in a dynamic way in this research.","2012-05","2023-07-06 00:58:15","2023-07-20 07:03:17","2023-07-06 00:58:15","123-129","","3-4","5","","J Multimodal User Interfaces","Sonophenology","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SPNZRKB6","bookSection","2013","Li, Xin; Kong, Dehui; Zhang, Yong","OsgAL-Based Sound Source Management in Simulation Games","Intelligence Computation and Evolutionary Computation","978-3-642-31655-5 978-3-642-31656-2","","","https://link.springer.com/10.1007/978-3-642-31656-2_97","Sound rendering is a technique which uses auditory display to communicate information to a user and provides an alternative way of visualization. Traditional 3D sound rendering techniques are lower effective when the number and types of sound sources increase. In this paper, we propose a method of 3D sound source management based on osgAL toolkit, by using sound classification and XML document structure for sound properties. We allocate buffers for necessary sound sources and cull inaudible sound sources, which reduce system’s consumption of time and space. Experiments on train-driving simulation system show that our method provides a realistic and effective simulation.","2013","2023-07-06 00:58:15","2023-07-20 06:34:32","2023-07-06 00:58:15","723-732","","","180","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Advances in Intelligent Systems and Computing DOI: 10.1007/978-3-642-31656-2_97","","","","","","Du, Zhenyu","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EEY8DGEH","journalArticle","2023","Asonitis, Tasos; Allmendinger, Richard; Benatan, Matt; Climent, Ricardo","SonOpt: understanding the behaviour of bi-objective population-based optimisation algorithms through sound","Genetic Programming and Evolvable Machines","","1389-2576, 1573-7632","10.1007/s10710-023-09451-5","https://link.springer.com/10.1007/s10710-023-09451-5","Abstract                            We present an extension of SonOpt, the first ever openly available tool for the sonification of bi-objective population-based optimisation algorithms. SonOpt has already introduced benefits on the understanding of algorithmic behaviour by proposing the use of sound as a medium for the process monitoring of bi-objective optimisation algorithms. The first edition of SonOpt utilised two different sonification paths to provide information on convergence, population diversity, recurrence of objective values across consecutive generations and the shape of the approximation set. The present extension provides further insight through the introduction of a third sonification path, which involves hypervolume contributions to facilitate the understanding of the relative importance of non-dominated solutions. Using a different sound generation approach than the existing ones, this newly proposed sonification path utilizes pitch deviations to highlight the distribution of hypervolume contributions across the approximation set. To demonstrate the benefits of SonOpt we compare the sonic results obtained from two popular population-based multi-objective optimisation algorithms, Non-Dominated Sorting Genetic Algorithm (NSGA-II) and Multi-Objective Evolutionary Algorithm based on Decomposition (MOEA/D), and use a Multi-objective Random Search (MRS) approach as a baseline. The three algorithms are applied to numerous test problems and showcase how sonification can reveal various aspects of the optimisation process that may not be obvious from visualisation alone. SonOpt is available for download at               https://github.com/tasos-a/SonOpt-2.0               .","2023-06","2023-07-06 00:58:15","2023-07-06 00:58:15","2023-07-06 00:58:15","3","","1","24","","Genet Program Evolvable Mach","SonOpt","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/CKBAR2K3/Asonitis et al. - 2023 - SonOpt understanding the behaviour of bi-objectiv.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N9QNJSG4","bookSection","2015","Ng, Patrick; Nesbitt, Keith; Blackmore, Karen","Sound Improves Player Performance in a Multiplayer Online Battle Arena Game","Artificial Life and Computational Intelligence","978-3-319-14802-1 978-3-319-14803-8","","","http://link.springer.com/10.1007/978-3-319-14803-8_13","Sound in video games is often used by developers to enhance the visual experience on screen. Despite its importance in creating presence and improving visual screen elements, sound also plays an important role in providing additional information to a player when completing various game tasks. This preliminary study focuses on the use of informative sound in the popular multiplayer online battle arena game, Dota 2. Our initial results indicate that team performance improves with the use of sound. However, mixed results with individual performances were measured, with some individual performances better with sound and some better without sound.","2015","2023-07-06 00:58:15","2023-07-19 11:34:53","2023-07-06 00:58:15","166-174","","","8955","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-14803-8_13","","","","","","Chalup, Stephan K.; Blair, Alan D.; Randall, Marcus","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7VPZNNVJ","journalArticle","2014","Chima, Ranjit S.; Ortega, Rafael; Connor, Christopher W.","Melodic algorithms for pulse oximetry to allow audible discrimination of abnormal systolic blood pressures","Journal of Clinical Monitoring and Computing","","1387-1307, 1573-2614","10.1007/s10877-014-9558-6","http://link.springer.com/10.1007/s10877-014-9558-6","An anesthesiologist must remain vigilant of the patient’s clinical status, incorporating many independent physiological measurements. Oxygen saturation and heart rate are represented by continuous audible tones generated by the pulse oximeter, a mandated monitoring device. Other important clinical parameters—notably blood pressure—lack any audible representation beyond arbitrarily-configured threshold alarms. Attempts to introduce further continuous audible tones have apparently foundered; the complexity and interaction of these tones have exceeded the ability of clinicians to interpret them. Instead, we manipulate the tonal and rhythmic structure of the accepted pulse oximeter tone pattern melodically. Three melodic algorithms were developed to apply tonal and rhythmic variations to the continuous pulse oximeter tone, dependent on the systolic blood pressure. The algorithms distort the original audible pattern minimally, to facilitate comprehension of both the underlying pattern and the applied variations. A panel of anesthesia practitioners (attending anesthesiologists, residents and nurse anesthetists) assessed these algorithms in characterizing perturbations in cardiopulmonary status. Twelve scenarios, incorporating combinations of oxygen desaturation, bradycardia, tachycardia, hypotension and hypertension, were tested. A rhythmic variation in which additional auditory information was conveyed only at halftime intervals, with every other “beat” of the pulse oximeter, was strongly favored. The respondents also strongly favored the use of musical chords over single tones. Given three algorithms of tones embedded in the pulse oximeter signal, anesthesiologists preferred a melodic tone to signal a significant change in blood pressure.","2014-12","2023-07-06 00:58:15","2023-07-20 06:46:57","2023-07-06 00:58:15","597-603","","6","28","","J Clin Monit Comput","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QSKLV77D","bookSection","1997","Duce, David A.","Theory and practice in interactionally rich distributed systems","SOFSEM'97: Theory and Practice of Informatics","978-3-540-63774-5 978-3-540-69645-2","","","http://link.springer.com/10.1007/3-540-63774-5_105","This paper explores the notion of richness in interactive systems, at the device level, the application level and the theoretical level. The paper attempts to show something of the breadth of recent research and practice, with a somewhat unintentional leaning towards the domain of scientific visualization in a broad sense. Theoretical work which may lay the foundations for a systematic understanding of interactionally rich systems is also discussed, drawing on the literature of psychology and system modelling. The paper concludes with some thoughts on future directions for both theory and practice.","1997","2023-07-06 01:00:05","2023-07-21 05:00:15","2023-07-06 01:00:05","163-182","","","1338","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-63774-5_105","","","","","","Plášil, František; Jeffery, Keith G.","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M5RWJ99B","bookSection","2004","Holland, Simon; Day, Robert; Leplâtre, Grégory; Edwards, Alistair","Mobile HCI and Sound","Mobile Human-Computer Interaction - MobileHCI 2004","978-3-540-23086-1 978-3-540-28637-0","","","http://link.springer.com/10.1007/978-3-540-28637-0_73","Sound plays an increasingly varied and vital role in mobile and ubiquitous user interaction. One reason is the limited screen real-estate available in typical mobile devices. Another reason is that many mobile devices are used in minimal attention situations These are situations in which the user has only limited attention available for the interface: the user’s eyes may be busy elsewhere; and the user may be busy avoiding the normal hazards of moving around, and engaging with real-world tasks. In many circumstances, such interactions will involve non-speech audio and gesture to afford natural means of access to information, to other people, and to services and situations in the environment.","2004","2023-07-06 01:00:05","2023-07-21 04:30:05","2023-07-06 01:00:05","527-528","","","3160","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-28637-0_73","","","","","","Brewster, Stephen; Dunlop, Mark","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6NGQXGR3","bookSection","2013","Garcia, Franco Eusébio; De Almeida Neris, Vânia Paula","Design Guidelines for Audio Games","Human-Computer Interaction. Applications and Services","978-3-642-39261-0 978-3-642-39262-7","","","http://link.springer.com/10.1007/978-3-642-39262-7_26","","2013","2023-07-06 01:00:05","2023-07-06 01:00:05","2023-07-06 01:00:05","229-238","","","8005","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-39262-7_26","","/Users/minsik/Zotero/storage/QFD7AT4G/Garcia and De Almeida Neris - 2013 - Design Guidelines for Audio Games.pdf","","","","Kurosu, Masaaki","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XKWIP2KY","bookSection","2018","Parseihian, Gaëtan; Aramaki, Mitsuko; Ystad, Sølvi; Kronland-Martinet, Richard","Exploration of Sonification Strategies for Guidance in a Blind Driving Game","Music Technology with Swing","978-3-030-01691-3 978-3-030-01692-0","","","http://link.springer.com/10.1007/978-3-030-01692-0_27","This paper explores the use of continuous auditory display for a dynamic guidance task. Through a driving game with blindfolded players, the success and the efficiency of a lane-keeping task in which no visual feedback is provided is observed. The results highlight the importance of the display information and reveal that a task-related rather than an error-related feedback should be used to enable the driver to finish the circuit. In terms of sound strategies, a first experiment explores the effect of two complex strategies (pitch and modulations) combined with a basic stereo strategy that informs the user about the distance and the direction to the target. The second experiment examines the influence of morphological sound attributes on the performance compared to the use of the spatial sound attributes alone. The results reveal the advantage of using morphological sound attributes for such kinds of applications.","2018","2023-07-06 01:00:05","2023-07-21 04:34:18","2023-07-06 01:00:05","413-428","","","11265","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-01692-0_27","","","","","","Aramaki, Mitsuko; Davies, Matthew E. P.; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K5U4XS5N","bookSection","2010","Tünnermann, René; Kolbe, Lukas; Bovermann, Till; Hermann, Thomas","Surface Interactions for Interactive Sonification","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_9","This paper presents novel interaction modes for Model-Based Sonification (MBS) via interactive surfaces. We first discuss possible interactions for MBS on a multi-touch surface. This is followed by a description of the Data Sonogram Sonification and the Growing Neural Gas Sonification Model and their implementation for the multi-touch interface. Modifications from the original sonification models such as the limited space scans are described and discussed with sonification examples. Videos showing interaction examples are provided. Furthermore, the presented system provides a basis for the implementation of known and novel sonification models. We discuss the available interaction modes with multi-touch surfaces and how these interactions can be profitably used to control spatial and non-spatial sonification models.","2010","2023-07-06 01:00:05","2023-07-19 11:38:43","2023-07-06 01:00:05","166-183","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_9","","/Users/minsik/Zotero/storage/U362RXUN/Tünnermann et al. - 2010 - Surface Interactions for Interactive Sonification.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AMSVZBWC","bookSection","2021","Sanghavi, Harsh; Jeon, Myounghoon; Nadri, Chihab; Ko, Sangjin; Sodnik, Jaka; Stojmenova, Kristina","Multimodal Takeover Request Displays for Semi-automated Vehicles: Focused on Spatiality and Lead Time","HCI in Mobility, Transport, and Automotive Systems","978-3-030-78357-0 978-3-030-78358-7","","","https://link.springer.com/10.1007/978-3-030-78358-7_22","To investigate the full potential of non-speech sounds, this study explored the effects of different multimodal takeover request displays in semi-automated vehicles. It used a mixed design - the visual and auditory notification lead time was within-subjects, whereas the auditory notification spatiality was between-subjects. The study was conducted in a motion-based driving simulator with 24 participants. All participants were engaged in four 9-min driving tasks in level-3 automated vehicle and simultaneously performed a non-driving related task (NDRT, online game). Each driving session contained three hazardous events with takeover request (in total 12 requests per user). The results showed that 3-s lead time evoked the fastest reaction time but caused high perceived workload and resulted in unsafe and non-comfortable maneuver. In terms of workload and maneuver, 7-s lead time showed better results than others. Auditory displays with directional information provided significantly better reaction times and reaction types. Subjective evaluation, on the other hand, did not show any significant differences between non-directional and directional displays. Additionally, the results showed that braking is a more common first reaction than steering, and that the NDRT did not influence the takeover request.","2021","2023-07-06 01:00:05","2023-07-20 05:43:44","2023-07-06 01:00:05","315-334","","","12791","","","Multimodal Takeover Request Displays for Semi-automated Vehicles","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-78358-7_22","","","","","","Krömker, Heidi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7ZCU8EKJ","journalArticle","2012","Polli, Andrea","Soundscape, sonification, and sound activism","AI & SOCIETY","","0951-5666, 1435-5655","10.1007/s00146-011-0345-3","http://link.springer.com/10.1007/s00146-011-0345-3","","2012-05","2023-07-06 01:00:05","2023-07-06 01:00:05","2023-07-06 01:00:05","257-268","","2","27","","AI & Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"539N4YQ8","journalArticle","2008","Amatriain, Xavier; Arumi, Pau; Garcia, David","A framework for efficient and rapid development of cross-platform audio applications","Multimedia Systems","","0942-4962, 1432-1882","10.1007/s00530-007-0109-6","http://link.springer.com/10.1007/s00530-007-0109-6","In this article, we present CLAM, a C++ software framework, that offers a complete development and research platform for the audio and music domain. It offers an abstract model for audio systems and includes a repository of processing algorithms and data types as well as all the necessary tools for audio and control input/output. The framework offers tools that enable the exploitation of all these features to easily build cross-platform applications or rapid prototypes for media processing algorithms and systems. Furthermore, included ready-to-use applications can be used for tasks such as audio analysis/synthesis, plug-in development, feature extraction or metadata annotation. CLAM represents a step forward over other similar existing environments in the multimedia domain. Nevertheless, it also shares models and constructs with many of those. These commonalities are expressed in the form of a metamodel for multimedia processing systems and a design pattern language.","2008-06","2023-07-06 01:00:05","2023-07-21 04:31:12","2023-07-06 01:00:05","15-32","","1","14","","Multimedia Systems","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YPFGQQEP","journalArticle","2005","Russo, Frank A.; Thompson, William Forde","The subjective size of melodic intervals over a two-octave range","Psychonomic Bulletin & Review","","1069-9384, 1531-5320","10.3758/BF03206445","http://link.springer.com/10.3758/BF03206445","Musically trained and untrained participants provided magnitude estimates of the size of melodic intervals. Each interval was formed by a sequence of two pitches that differed by between 50 cents (one half of a semitone) and 2,400 cents (two octaves) and was presented in a high or a low pitch register and in an ascending or a descending direction. Estimates were larger for intervals in the high pitch register than for those in the low pitch register and for descending intervals than for ascending intervals. Ascending intervals were perceived as larger than descending intervals when presented in a high pitch register, but descending intervals were perceived as larger than ascending intervals when presented in a low pitch register. For intervals up to an octave in size, differentiation of intervals was greater for trained listeners than for untrained listeners. We discuss the implications for psychophysical pitch scales and models of music perception.","2005-12","2023-07-06 01:00:05","2023-07-21 04:56:07","2023-07-06 01:00:05","1068-1075","","6","12","","Psychonomic Bulletin & Review","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/TBWHPQUY/Russo and Thompson - 2005 - The subjective size of melodic intervals over a tw.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8VHMZ9XJ","bookSection","2004","Tsukada, Koji; Yasumura, Michiaki","ActiveBelt: Belt-Type Wearable Tactile Display for Directional Navigation","UbiComp 2004: Ubiquitous Computing","978-3-540-22955-1 978-3-540-30119-6","","","http://link.springer.com/10.1007/978-3-540-30119-6_23","In this paper we propose a novel wearable interface called “ActiveBelt” that enables users to obtain multiple directional information with the tactile sense. Since the information provided by the tactile sense is relatively unobtrusive, it is suited for daily use in mobile environments. However, many existing systems don’t transmit complex information via the tactile sense. Most of them send only simple signals, such as vibration in cellular phones. ActiveBelt is a novel belt-type wearable tactile display that can transmit directional information. We have developed prototype systems and applications, evaluated system performance and usability, and demonstrated the possibility of practical use.","2004","2023-07-06 01:00:05","2023-07-21 05:08:08","2023-07-06 01:00:05","384-399","","","3205","","","ActiveBelt","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-30119-6_23","","/Users/minsik/Zotero/storage/LZFR2BWC/Tsukada and Yasumura - 2004 - ActiveBelt Belt-Type Wearable Tactile Display for.pdf","","","","Davies, Nigel; Mynatt, Elizabeth D.; Siio, Itiro","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9P85EHXM","bookSection","2010","Saranti, Anna; Eckel, Gerhard; Pirrò, David","Quantum Harmonic Oscillator Sonification","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_10","","2010","2023-07-06 01:00:05","2023-07-06 01:00:05","2023-07-06 01:00:05","184-201","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_10","","/Users/minsik/Zotero/storage/IL3WTRW9/Saranti et al. - 2010 - Quantum Harmonic Oscillator Sonification.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TDMJA5RJ","bookSection","2005","Nicol, Craig; Brewster, Stephen; Gray, Philip","A System for Manipulating Audio Interfaces Using Timbre Spaces","Computer-Aided Design of User Interfaces IV","978-1-4020-3145-8","","","http://link.springer.com/10.1007/1-4020-3304-4_29","","2005","2023-07-06 01:00:05","2023-07-06 01:00:05","2023-07-06 01:00:05","361-374","","","","","","","","","","","Springer-Verlag","Berlin/Heidelberg","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/1-4020-3304-4_29","","","","","","Jacob, Robert J.K.; Limbourg, Quentin; Vanderdonckt, Jean","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TGZJ44AV","bookSection","2014","Lockton, Dan; Bowden, Flora; Brass, Clare; Gheerawo, Rama","Powerchord: Towards Ambient Appliance-Level Electricity Use Feedback through Real-Time Sonification","Ubiquitous Computing and Ambient Intelligence. Personalisation and User Adapted Services","978-3-319-13101-6 978-3-319-13102-3","","","http://link.springer.com/10.1007/978-3-319-13102-3_10","","2014","2023-07-06 01:00:05","2023-07-06 01:00:05","2023-07-06 01:00:05","48-51","","","8867","","","Powerchord","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-13102-3_10","","","","","","Hervás, Ramón; Lee, Sungyoung; Nugent, Chris; Bravo, José","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WBKTGM69","bookSection","2003","Bornträger, Christian; Cheverst, Keith; Davies, Nigel; Dix, Alan; Friday, Adrian; Seitz, Jochen","Experiments with Multi-modal Interfaces in a Context-Aware City Guide","Human-Computer Interaction with Mobile Devices and Services","978-3-540-40821-5 978-3-540-45233-1","","","http://link.springer.com/10.1007/978-3-540-45233-1_10","In recent years there has been considerable research into the development of mobile context-aware applications. The canonical example of such an application is the context-aware tour-guide that offers city visitors information tailored to their preferences and environment. The nature of the user interface for these applications is critical to their success. Moreover, the user interface and the nature and modality of information presented to the user impacts on many aspects of the system’s overall requirements, such as screen size and network provision. Current prototypes have used a range of different interfaces developed in a largely ad-hoc fashion and there has been no systematic exploration of user preferences for information modality in mobile context-aware applications. In this paper we describe a series of experiments with multi-modal interfaces for context-aware city guides. The experiments build on our earlier research into the GUIDE system and include a series of field trials involving members of the general public. We report on the results of these experiments and extract design guid","2003","2023-07-06 01:00:05","2023-07-20 06:29:47","2023-07-06 01:00:05","116-130","","","2795","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-45233-1_10","","/Users/minsik/Zotero/storage/945X3CQ6/Bornträger et al. - 2003 - Experiments with Multi-modal Interfaces in a Conte.pdf","","","","Chittaro, Luca","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FA6KXPI9","bookSection","2009","Johannsen, Gunnar","Design of Visual and Auditory Human-Machine Interfaces with User Participation and Knowledge Support","Industrial Engineering and Ergonomics","978-3-642-01292-1 978-3-642-01293-8","","","https://link.springer.com/10.1007/978-3-642-01293-8_37","Human-machine interfaces for dynamic technical systems (such as industrial processes, vehicles, etc.) are a particularly important and complex subset of user interfaces. The design of these interfaces has become so sophisticated that it can, already for quite some time, no longer be handled in an intuitive fashion. The designer needs to possess a huge amount of multidisciplinary knowledge and experience with respect to the application domain of the respective technical process, the available automation and information technologies, the capabilities and limitations of the human users (human operators, maintenance personnel, and others), work psychological and organizational matters as well as ergonomic and cognitive engineering principles of good human-machine interface design.","2009","2023-07-06 01:00:05","2023-07-20 06:33:42","2023-07-06 01:00:05","499-509","","","","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-3-642-01293-8_37","","","","","","Schlick, Christopher M.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UPFSADF5","journalArticle","2005","Zotkin, Dmitry N.; Chi, Taishih; Shamma, Shihab A.; Duraiswami, Ramani","Neuromimetic Sound Representation for Percept Detection and Manipulation","EURASIP Journal on Advances in Signal Processing","","1687-6180","10.1155/ASP.2005.1350","https://asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.1350","","2005-12","2023-07-06 01:00:05","2023-07-06 01:00:05","2023-07-06 01:00:05","486137","","9","2005","","EURASIP J. Adv. Signal Process.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/5MWNPFHN/Zotkin et al. - 2005 - Neuromimetic Sound Representation for Percept Dete.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"APBFNC35","conferencePaper","2021","Goudarzi, Visda","Exploring a Taxonomy of Interaction in Interactive Sonification Systems","Human Interaction, Emerging Technologies and Future Applications III","978-3-030-55307-4","","10.1007/978-3-030-55307-4_22","","This paper explores a variety of existing interactive sonification systems in the context of interactive sound art. In design of interactive sonification from technological standpoint, the stress is put on studying the usability and functionality of the systems. We explore the focus towards creative aspects of interaction in both technology development and sound creation stages. In some artistic sonifications, the control is in the hand of the technology creators, in some others in the hand of the artists, and sometimes in the hand of the performers or the audience members. The numerous relations and interactions between performers, composers, technologists, data domain scientists, environment and audiences make it difficult to classify the complex phenomenon of interactive sonification. Some challenges in such systems are the ownership of technical and aesthetic components, balancing engagement and interaction among different stakeholders (domain scientist, designer, composer, spectator, etc.) and encouraging audience engagement.","2021","2023-07-06 01:00:35","2023-07-06 01:00:35","","140-145","","","","","","","Advances in Intelligent Systems and Computing","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","","","","Sonification; Interactive sonification; Auditory display; Human computer interaction; Parameter mapping","Ahram, Tareq; Taiar, Redha; Langlois, Karine; Choplin, Arnaud","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ELTPZ6A8","bookSection","2016","Eriksson, Martin Ljungdahl; Pareto, Lena","Sound Bubbles for Productive Office Work","Nordic Contributions in IS Research","978-3-319-43596-1 978-3-319-43597-8","","","http://link.springer.com/10.1007/978-3-319-43597-8_3","A growing number of organizations are moving towards more open and collaborative workplaces. In these offices workers share a common open space, often with flexible seating based on activities, so called activity-based offices. Most problems in these workplaces are related to sound. Thus, the question of how to design suitable acoustic environments, supporting both collaborative and individual work, has emerged. Noise-reduction approaches do not suffice. In this study we explored the possibility of adding context-sensitive, activity-based sound environments to enhance the office workplace. For this purpose, we developed the “sound bubble,” a prototype for individual work, sonically immersing the listener and generating a sensation of an encapsulating sonic environment. A total of 43 test subjects participated in an experience-based test using the sound bubble prototype while conducting self-selected, ordinary work tasks in their office landscape. Their behaviors during the test were observed and documented. All participants took a post-experience questionnaire about experiences working in the sound bubble, and two subjects were interviewed. The responses show that the sound bubble can enhance auditory work conditions for individual work that demands concentration.","2016","2023-07-06 01:02:06","2023-07-21 04:39:14","2023-07-06 01:02:06","29-42","","","259","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Business Information Processing DOI: 10.1007/978-3-319-43597-8_3","","","","","","Lundh Snis, Ulrika","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FBEY7BV7","bookSection","2014","Carvalho, Luiz Roberto; Cybis Pereira, Alice T.","Interface Design and Dynamic Audio","Human-Computer Interaction. Advanced Interaction Modalities and Techniques","978-3-319-07229-6 978-3-319-07230-2","","","http://link.springer.com/10.1007/978-3-319-07230-2_50","In the age of digital devices, text, image, sound, interactivity, blend themselves into a symbiotic and unique media, presenting a multifaceted specie of language called hypermedia. However, since many years ago, we have seen a notable emphasis on visual communication´s interfaces, and due to its limitations, products and services in design can often present inconsistencies when other sensory properties are relevant, as in the case of sound information. This over-emphasis on visual displays has constrained the development of interactive systems that are capable of making better use of the auditory modality. Recognizing the HCI as an integrating element of media and visual, sound and tactile metaphors, this study will demonstrate investigations that contextualize the role of sound into interactive environments by proposing an overview for the term interactive sound, suggesting its classification into direct-interactive and indirect-adaptative sounds, and pointing out its meanings and applications.","2014","2023-07-06 01:02:06","2023-07-20 06:30:18","2023-07-06 01:02:06","523-531","","","8511","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-07230-2_50","","","","","","Kurosu, Masaaki","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Kobsa, Alfred; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Doug; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4YAJCBYT","journalArticle","2023","Weaver, Alexis; Firmer, Genevieve; Motion, Alice; O’Regan, Jadey; O’Reilly, Chiara; Yeadon, Daniel","Sounding Out Science: the Sonaphor and Electronic Sound Design as a Learning Tool in Secondary Science","Postdigital Science and Education","","2524-485X, 2524-4868","10.1007/s42438-022-00321-4","https://link.springer.com/10.1007/s42438-022-00321-4","Abstract                            The divergent use of digital technologies provides an important opportunity for students to develop critical and postdigital approaches to learning. Despite the rising accessibility of music technology, creatively composed sound is a relatively underexplored educational tool compared to the musical elements of melody, rhythm, and lyrics. Sound’s ability to transfer spatial and temporal information renders it a transformative tool for teaching and learning. Embracing an interdisciplinary approach, our research explores the possibility of supplementing secondary science education with a sound-based learning tool which creatively interprets scientific concepts to increase comprehension and engagement. Building on the existing ways in which science is communicated through music and sound, we have developed the               Sonaphor               (abbreviated from ‘sonic metaphor’). This article will outline the capacity for experimental electronic sound design to increase engagement in contexts ranging from classrooms through to informal learning environments. We see potential for the               Sonaphor               as a learning tool that reignites wonder and curiosity in science; it combines learning and creativity in sound design and science, allowing learners to interact with, and create their own               Sonaphors               . Through exemplar               Sonaphors               , we highlight a proposed structure and discuss the importance of harmonious script, dialogue, and sound design. The flexibility of the digital medium and increasing ubiquity of sound recording and editing software presents an opportunity for               Sonaphors               to become ‘living’ digital objects that could be adapted by different narrators, sound designers, and artists for different cultures, languages, syllabi, and purposes that build inclusivity in science education and communication.","2023-04","2023-07-06 01:02:06","2023-07-06 01:02:06","2023-07-06 01:02:06","408-439","","2","5","","Postdigit Sci Educ","Sounding Out Science","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/IAFLZQMW/Weaver et al. - 2023 - Sounding Out Science the Sonaphor and Electronic .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YZ3G2WS8","bookSection","2015","Walker, James; Smith, Michael T.; Jeon, Myounghoon","Interactive Sonification Markup Language (ISML) for Efficient Motion-Sound Mappings","Human-Computer Interaction: Interaction Technologies","978-3-319-20915-9 978-3-319-20916-6","","","http://link.springer.com/10.1007/978-3-319-20916-6_36","","2015","2023-07-06 01:02:06","2023-07-06 01:02:06","2023-07-06 01:02:06","385-394","","","9170","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-20916-6_36","","/Users/minsik/Zotero/storage/5VI8RAWY/Walker et al. - 2015 - Interactive Sonification Markup Language (ISML) fo.pdf","","","","Kurosu, Masaaki","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XG9DPNZ7","bookSection","2013","Jeon, Myounghoon; Lee, Ju-Hwan","The Ecological AUI (Auditory User Interface) Design and Evaluation of User Acceptance for Various Tasks on Smartphones","Human-Computer Interaction. Interaction Modalities and Techniques","978-3-642-39329-7 978-3-642-39330-3","","","http://link.springer.com/10.1007/978-3-642-39330-3_6","With the rapid development of the touch screen technology, some usability issues of smartphones have been reported [1]. To tackle those user experience issues, there has been research on the use of non-speech sounds on the mobile devices [e.g., 2, 3-7]. However, most of them have focused on a single specific task of the device. Given the varying functions of the smartphone, the present study designed plausibly integrated auditory cues for diverse functions and evaluated user acceptance levels from the ecological interface design perspective. Results showed that sophisticated auditory design could change users’ preference and acceptance of the interface and the extent depended on usage contexts. Overall, participants gave significantly higher scores on the functional satisfaction and the fun scales in the sonically-enhanced smartphones than in the no-sound condition. The balanced sound design may free users from auditory pollution and allow them to use their devices more pleasantly.","2013","2023-07-06 01:02:06","2023-07-20 06:32:22","2023-07-06 01:02:06","49-58","","","8007","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-39330-3_6","","","","","","Kurosu, Masaaki","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"23DWAFIC","bookSection","2013","Rinaldi, Claudia; Santic, Marco; Pomante, Luigi; Graziosi, Fabio","Exploiting Latest Technologies for RF Sounding’s Evolution","Arts and Technology","978-3-642-37981-9 978-3-642-37982-6","","","http://link.springer.com/10.1007/978-3-642-37982-6_5","","2013","2023-07-06 01:02:06","2023-07-06 01:02:06","2023-07-06 01:02:06","33-40","","","116","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering DOI: 10.1007/978-3-642-37982-6_5","","","","","","De Michelis, Giorgio; Tisato, Francesco; Bene, Andrea; Bernini, Diego","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S2TGDHJC","journalArticle","2022","Bruder, Alexandra L.; Rothwell, Clayton D.; Fuhr, Laura I.; Shotwell, Matthew S.; Edworthy, Judy Reed; Schlesinger, Joseph J.","The Influence of Audible Alarm Loudness and Type on Clinical Multitasking","Journal of Medical Systems","","0148-5598, 1573-689X","10.1007/s10916-021-01794-9","https://link.springer.com/10.1007/s10916-021-01794-9","In high-consequence industries such as health care, auditory alarms are an important aspect of an informatics system that monitors patients and alerts providers attending to multiple concurrent tasks. Alarms levels are unnecessarily high and alarm signals are uninformative. In a laboratory-based task setting, we studied 25 anesthesiology residents’ responses to auditory alarms in a multitasking paradigm comprised of three tasks: patient monitoring, speech perception/intelligibility, and visual vigilance. These tasks were in the presence of background noise plus/minus music, which served as an attention-diverting stimulus. Alarms signified clinical decompensation and were either conventional alarms or a novel informative auditory icon alarm. Both alarms were presented at four different levels. Task performance (accuracy and response times) were analyzed using logistic and linear mixed-effects regression. Salient findings were 1), the icon alarm had similar performance to the conventional alarm at a +2 dB signal-to-noise-ratio (SNR) (accuracy: OR 1.21 (95% CI 0.88, 1.67), response time: 0.04 s at 2 dB (95% CI: –0.16, 0.24), which is a much lower level than current clinical environments; 2) the icon alarm was associated with 27% greater odds (95% CI: 18%, 37%) of correctly addressing the vigilance task, regardless of alarm SNR, suggesting crossmodal/multisensory multitasking benefits; and 3) compared to the conventional alarm, the icon alarm was associated with an absolute improvement in speech perception of 4% in the presence of an attention-diverting auditory stimulus (p = 0.031). These findings suggest that auditory icons can provide multitasking benefits in cognitively demanding clinical environments.","2022-01","2023-07-06 01:02:06","2023-07-20 06:47:55","2023-07-06 01:02:06","5","","1","46","","J Med Syst","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CEYQZWFG","journalArticle","2010","Nordahl, Rolf","Evaluating Environmental Sounds from a Presence Perspective for Virtual Reality Applications","EURASIP Journal on Audio, Speech, and Music Processing","","1687-4714, 1687-4722","10.1155/2010/426937","http://asmp.eurasipjournals.com/content/2010/1/426937","We propose a methodology to design and evaluate environmental sounds for virtual environments. We propose to combine physically modeled sound events with recorded soundscapes. Physical models are used to provide feedback to users’ actions, while soundscapes reproduce the characteristic soundmarks of an environment. In this particular case, physical models are used to simulate the act of walking in the botanical garden of the city of Prague, while soundscapes are used to reproduce the particular sound of the garden. The auditory feedback designed was combined with a photorealistic reproduction of the same garden. A between-subject experiment was conducted, where 126 subjects participated, involving six different experimental conditions, including both uni- and bimodal stimuli (auditory and visual). The auditory stimuli consisted of several combinations of auditory feedback, including static sound sources as well as self-induced interactive sounds simulated using physical models. Results show that subjects’ motion in the environment is significantly enhanced when dynamic sound sources and sound of egomotion are rendered in the environment.","2010","2023-07-06 01:02:06","2023-07-21 07:43:57","2023-07-06 01:02:06","1-10","","","2010","","EURASIP Journal on Audio, Speech, and Music Processing","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/QH6YUYQZ/Nordahl - 2010 - Evaluating Environmental Sounds from a Presence Pe.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RILXUJX5","bookSection","2015","Sun, Yuanjing; Jeon, Myounghoon","Lyricon (Lyrics + Earcons) Improves Identification of Auditory Cues","Design, User Experience, and Usability: Users and Interactions","978-3-319-20897-8 978-3-319-20898-5","","","http://link.springer.com/10.1007/978-3-319-20898-5_37","","2015","2023-07-06 01:02:06","2023-07-06 01:02:06","2023-07-06 01:02:06","382-389","","","9187","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-20898-5_37","","","","","","Marcus, Aaron","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ATKGGCAW","bookSection","2010","Larsson, Pontus","Tools for Designing Emotional Auditory Driver-Vehicle Interfaces","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_1","Auditory interfaces are often used in vehicles to inform and alert the driver of various events and hazards. When designed properly, such interfaces can e.g. reduce reaction times and increase the impression of quality of the vehicle. In this paper it is argued that emotional response is an important aspect to consider when designing auditory driver-vehicle interfaces. This paper discusses two applications developed to investigate the emotional dimensions of auditory interfaces. EarconSampler is a tool for designing and modifying earcons. It allows for creating melodic patterns of wav-snippets and adjustment of parameters such as tempo and pitch. It also contains an analysis section where sound quality parameters, urgency and emotional response to the sound is calculated / predicted. SoundMoulder is another tool which offers extended temporal and frequency modifications of earcons. The primary idea with this application is to study how users design sounds given a desired emotional response.","2010","2023-07-06 01:02:06","2023-07-19 11:38:03","2023-07-06 01:02:06","1-11","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_1","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BMI8HSFS","bookSection","2012","Shahrul Azmi, Mohd Yusof; Nor Idayu, M.; Roshidi, D.; Yaakob, A. R.; Yaacob, Sazali","Noise Robustness of Spectrum Delta (SpD) Features in Malay Vowel Recognition","Computer Applications for Communication, Networking, and Digital Contents","978-3-642-35593-6 978-3-642-35594-3","","","http://link.springer.com/10.1007/978-3-642-35594-3_38","In Malaysia, there is increasing number of speech recognition researchers focusing on developing independent speaker speech recognition systems that uses Malay Language which are noise robust and accurate. The performance of speech recognition application under adverse noisy condition often becomes the topic of interest among speech recognition researchers regardless of the languages in use. This paper present a study of noise robust capability of an improved vowel feature extraction method called Spectrum Delta (SpD). The features are extracted from both original data and noise-added data and classified using three classifiers; (i) Multinomial Logistic Regression (MLR), (ii) K-Nearest Neighbors (k-NN) and (iii) Linear Discriminant Analysis (LDA). Results show that the proposed SpD is robust towards noise and LDA performs the best in overall vowel classification compared to MLR and k-NN in terms of robustness capability especially with signal-to-noise (SNR) above 20dB.","2012","2023-07-06 01:02:06","2023-07-19 23:47:52","2023-07-06 01:02:06","270-277","","","350","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Communications in Computer and Information Science DOI: 10.1007/978-3-642-35594-3_38","","","","","","Kim, Tai-hoon; Ko, Dae-sik; Vasilakos, Thanos; Stoica, Adrian; Abawajy, Jemal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L7RPM8TN","bookSection","2009","McGookin, David; Brewster, Stephen; Priego, Pablo","Audio Bubbles: Employing Non-speech Audio to Support Tourist Wayfinding","Haptic and Audio Interaction Design","978-3-642-04075-7 978-3-642-04076-4","","","http://link.springer.com/10.1007/978-3-642-04076-4_5","We introduce the concept of Audio Bubbles - virtual spheres filled with audio that are geocentered on physical landmarks, providing navigational homing information for a user to more easily locate the landmark. We argue that the way in which tourists navigate is not well supported by traditional visual maps, and that Audio Bubbles better support the serendipitous discovery and homing behaviours exhibited in such tourist activities. We present a study comparing Audio Bubbles to a visual map in a real world navigation task. Navigation with Audio Bubbles appeared to be faster and was preferred by most of the participants. We discuss the findings and outline our future development plans.","2009","2023-07-06 01:02:06","2023-07-20 00:15:20","2023-07-06 01:02:06","41-50","","","5763","","","Audio Bubbles","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-04076-4_5","","/Users/minsik/Zotero/storage/KI5UAKPT/McGookin et al. - 2009 - Audio Bubbles Employing Non-speech Audio to Suppo.pdf","","","","Altinsoy, M. Ercan; Jekosch, Ute; Brewster, Stephen","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W5PHDBHW","bookSection","2012","Sade, Jack; Naz, Komal; Plaza, Malgorzata","Enhancing Audio Description: A Value Added Approach","Computers Helping People with Special Needs","978-3-642-31521-3 978-3-642-31522-0","","","http://link.springer.com/10.1007/978-3-642-31522-0_40","Audio Description makes films, shows and TV programs accessible to visually impaired audience. It is expensive so wide adoption of this technology is not practical. Canadian Radio-television and Telecommunications Commission requires that broadcaster describes a minimum of four hours of primetime programming a week. Production companies do not see any incentives to move beyond the required minimum. This paper investigates the possibility of making AD profitable by making a described movie, show or program attractive to all kind of audiences including visually impaired. We argue that AD can become a revenue generation product widely adopted by production companies.","2012","2023-07-06 01:02:06","2023-07-19 23:52:43","2023-07-06 01:02:06","270-277","","","7382","","","Enhancing Audio Description","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-31522-0_40","","","","","","Miesenberger, Klaus; Karshmer, Arthur; Penaz, Petr; Zagler, Wolfgang","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PF6G2MWG","journalArticle","2007","Shajahan, Peer; Irani, Pourang","One family, many voices: Can multiple synthetic voices be used as navigational cues in hierarchical interfaces?","International Journal of Speech Technology","","1381-2416, 1572-8110","10.1007/s10772-006-9000-7","http://link.springer.com/10.1007/s10772-006-9000-7","","2007-03-14","2023-07-06 01:02:06","2023-07-06 01:02:06","2023-07-06 01:02:06","1-15","","1-2","9","","Int J Speech Technol","One family, many voices","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X24NFCC3","bookSection","2018","Pedersen, Jon Ram Bruun; Serafin, Stefania; Grani, Francesco","Investigating the Role of Auditory Feedback in a Multimodal Biking Experience","Music Technology with Swing","978-3-030-01691-3 978-3-030-01692-0","","","http://link.springer.com/10.1007/978-3-030-01692-0_22","In this paper, we investigate the role of auditory feedback in a↵ecting perception of e↵ort while biking in a virtual environment. Subjects were biking on a stationary chair bike, while exposed to 3D renditions of a recumbent bike inside a virtual environment (VE). The VE simulated a park and was created in the Unity5 engine. While biking, subjects were exposed to 9 kinds of auditory feedback (3 amplitude levels with three di↵erent filters) which were continuously triggered corresponding to pedal speed, representing the sound of the wheels and bike/chain mechanics. Subjects were asked to rate the perception of exertion using the Borg RPE scale. Results of the experiment showed that most subjects perceived a di↵erence in mechanical resistance from the bike between conditions, but did not consciously notice the variations of the auditory feedback, although these were significantly varied. This points towards interesting perspectives for subliminal perception potential for auditory feedback for VR exercise purposes.","2018","2023-07-06 01:02:06","2023-07-21 04:34:31","2023-07-06 01:02:06","327-337","","","11265","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-01692-0_22","","/Users/minsik/Zotero/storage/CQESQEXV/Pedersen et al. - 2018 - Investigating the Role of Auditory Feedback in a M.pdf","","","","Aramaki, Mitsuko; Davies, Matthew E. P.; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K8QII6TR","journalArticle","2008","Bormann, Karsten","Visuals are not what they look","Virtual Reality","","1359-4338, 1434-9957","10.1007/s10055-007-0068-4","http://link.springer.com/10.1007/s10055-007-0068-4","When developing virtual environments (VEs), most effort goes into developing the visuals. For many, the ideal is to create virtual worlds of photo-realistic quality or otherwise being of high fidelity. The purpose is to make the VE seem real to the user. This paper takes a closer look at subjects’ ratings of the visuals, and of the extent to which the VE feels real to the subjects, in the context of an experiment on audio in which subjects were to perform two search tasks: the first in an ordinary, textured house; the second in a bare structure consisting almost exclusively of the barren, white walls. Audio was never relevant to the search task in the first experiment, while in the second experiment it was relevant to the search task for half of the subjects. Subjects for whom audio was irrelevant to both their search tasks rated their visual involvement as large in the barren VE as in the higher quality one. However, subjects for which audio was relevant to their search task in the second experiment saw their visual involvement plummet, while their auditory involvement surged. Finally, the extent to which the VE felt real to the subjects did not correlate with their visual involvement but instead showed a strong correlation with the extent to which the interaction felt natural.","2008-06","2023-07-06 01:02:06","2023-07-21 05:14:11","2023-07-06 01:02:06","115-123","","2","12","","Virtual Reality","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5UVEPKC4","bookSection","2013","Frisson, Christian; Keyaerts, Gauthier; Grisard, Fabien; Dupont, Stéphane; Ravet, Thierry; Zajéga, François; Colmenares Guerra, Laura; Todoroff, Todor; Dutoit, Thierry","MashtaCycle: On-Stage Improvised Audio Collage by Content-Based Similarity and Gesture Recognition","Intelligent Technologies for Interactive Entertainment","978-3-319-03891-9 978-3-319-03892-6","","","http://link.springer.com/10.1007/978-3-319-03892-6_14","In this paper we present the outline of a performance in-progress. It brings together the skilled musical practices from Belgian audio collagist Gauthier Keyaerts aka Very Mash’ta; and the realtime, content-based audio browsing capabilities of the AudioCycle and LoopJam applications developed by the remaining authors. The tool derived from AudioCycle named MashtaCycle aids the preparation of collections of stem audio loops before performances by extracting content-based features (for instance timbre) used for the positioning of these sounds on a 2D visual map. The tool becomes an embodied on-stage instrument, based on a user interface which uses a depth-sensing camera, and augmented with the public projection of the 2D map. The camera tracks the position of the artist within the sensing area to trigger sounds similarly to the LoopJam installation. It also senses gestures from the performer interpreted with the Full Body Interaction (FUBI) framework, allowing to apply sound effects based on bodily movements. MashtaCycle blurs the boundary between performance and preparation, navigation and improvisation, installations and concerts.","2013","2023-07-06 01:02:06","2023-07-20 06:35:57","2023-07-06 01:02:06","114-123","","","124","","","MashtaCycle","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering DOI: 10.1007/978-3-319-03892-6_14","","","","","","Mancas, Matei; D’ Alessandro, Nicolas; Siebert, Xavier; Gosselin, Bernard; Valderrama, Carlos; Dutoit, Thierry","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D2ZRXEZX","bookSection","2014","Bertacchini, Francesca; Bilotta, Eleonora; Carini, Manuela; Gabriele, Lorella; Pantano, Pietro; Tavernise, Assunta","Learning in the Smart City: A Virtual and Augmented Museum Devoted to Chaos Theory","New Horizons in Web Based Learning","978-3-662-43453-6 978-3-662-43454-3","","","http://link.springer.com/10.1007/978-3-662-43454-3_27","","2014","2023-07-06 01:02:06","2023-07-06 01:02:06","2023-07-06 01:02:06","261-270","","","7697","","","Learning in the Smart City","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-662-43454-3_27","","","","","","Chiu, Dickson K. W.; Wang, Minhong; Popescu, Elvira; Li, Qing; Lau, Rynson","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Kobsa, Alfred; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Doug; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XG8N6N58","bookSection","2014","Stoll, Thomas M.","Genomic: Evolving Sound Treatments Using Genetic Algorithms","Evolutionary and Biologically Inspired Music, Sound, Art and Design","978-3-662-44334-7 978-3-662-44335-4","","","http://link.springer.com/10.1007/978-3-662-44335-4_10","There are many systems for the evolution of creative musical material, that create and/or manipulate musical score data or synthesis parameters with a variety of techniques. This paper aims to add the technique of corpus-based sound sampling and processing to the list of applications used in conjunction with genetic algorithms. Genomic, a simple system for evolving sound treatment parameters, is presented, along with two simple use cases. Finally, a more complex process is outlined where sound treatment parameters are evolved and stored in a database with associated metadata for further organization and compositional use.","2014","2023-07-06 01:03:18","2023-07-20 00:04:37","2023-07-06 01:03:18","107-118","","","8601","","","Genomic","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-662-44335-4_10","","","","","","Romero, Juan; McDermott, James; Correia, João","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R4X6HWAY","bookSection","2010","Kobayashi, Yosuke; Kondo, Kazuhiro; Nakagawa, Kiyoshi","Intelligibility of HE-AAC Coded Japanese Words with Various Stereo Coding Modes in Virtual 3D Audio Space","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_12","In this paper, we investigated the influence of stereo coding on Japanese speech localized in virtual 3-D space. We encoded localized speech using joint stereo and parametric stereo modes within the HE-AAC encoder. First, we tested subjective quality of localized speech at various azimuths on the horizontal plane relative to the listener using the standard MUSHRA tests. We compared the encoded localized speech quality with various stereo encoding modes. The joint stereo mode showed significantly higher MUSHRA scores than the parametric stereo mode at azimuths of ±45 degrees. Next, the Japanese word intelligibility tests were conducted using the Japanese Diagnostic Rhyme Tests. Test speech was first localized at 0 and ±45 degrees and compared with localized speech with no coding. Parametric stereo-coded speech showed lower scores when localized at -45 degrees, but all other speech showed no difference between speech samples with no coding. Next, test speech was localized in front, while competing noise was localized at various angles. The two stereo coding modes with bit rates of 56, 32, and 24 kbps were tested. In most cases, these conditions show just as good intelligibility as speech with no encoding at all noise azimuths. This shows that stereo coding has almost no effect on the intelligibility in the bit rate range tested.","2010","2023-07-06 01:03:18","2023-07-19 11:37:56","2023-07-06 01:03:18","219-238","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_12","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7D837YJM","journalArticle","1977","Siegel, Jane A.; Siegel, William","Absolute identification of notes and intervals by musicians","Perception & Psychophysics","","0031-5117, 1532-5962","10.3758/BF03198717","http://link.springer.com/10.3758/BF03198717","Speech sounds are judged reliably and absolutely, while the judgment of nonspeech stimuli, such as tones, is thought to be unreliable and dependent on contextual cues. Here we demonstrated that the judgment of tonal stimuli may also be reliable and absolute, provided that the subjects are trained musicians, In Experiment 1, musicians with relative pitch identified 21 tonal intervals ranging from unison to major third, and the resulting identification functions were similar to those that have been previously obtained for speech. In Experiment 3, the judgment of intervals by musicians was shown to be free of context effects, since the best subjects gave virtually identical judgments to the same intervals in two stimulus contexts, Similar results were obtained in Experiments 2 and 4 for the judgment of single tones by possessors of absolute pitch. Performance with both notes and intervals by nonmusicians, however, was unreliable and greatly influenced by context. These findings suggest that musicians acquire categories for pitch that are functionally similar to phonemic categories for speech.","1977-03","2023-07-06 01:03:18","2023-07-21 04:44:37","2023-07-06 01:03:18","143-152","","2","21","","Perception & Psychophysics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/922I9UGV/Siegel and Siegel - 1977 - Absolute identification of notes and intervals by .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RISQ6PT6","journalArticle","2004","Coward, Sean W.; Stevens, Catherine J.","Extracting Meaning from Sound: Nomic Mappings, Everyday Listening, and Perceiving Object Size from Frequency","The Psychological Record","","0033-2933, 2163-3452","10.1007/BF03395478","http://link.springer.com/10.1007/BF03395478","","2004-07","2023-07-06 01:03:18","2023-07-06 01:03:18","2023-07-06 01:03:18","349-364","","3","54","","Psychol Rec","Extracting Meaning from Sound","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NN9U23XR","journalArticle","2021","Cohen, Michael; Satō, Rintarō; Noji, Ryota; Iida, Takato; Tokumitsu, Yoshiki","Directional selectivity in panoramic and pantophonic interfaces: Flashdark, Narrowcasting for Stereoscopic Photospherical Cinemagraphy, Akabeko Ensemble","The Visual Computer","","0178-2789, 1432-2315","10.1007/s00371-021-02293-1","https://link.springer.com/10.1007/s00371-021-02293-1","","2021-12","2023-07-06 01:03:18","2023-07-06 01:03:18","2023-07-06 01:03:18","3125-3137","","12","37","","Vis Comput","Directional selectivity in panoramic and pantophonic interfaces","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"REW3NU2H","bookSection","2017","Jeon, Myounghoon","Aesthetic Computing for Representation of the Computing Process and Expansion of Perceptual Dimensions: Cases for Art, Education, and Interfaces","Interactivity, Game Creation, Design, Learning, and Innovation","978-3-319-55833-2 978-3-319-55834-9","","","http://link.springer.com/10.1007/978-3-319-55834-9_36","With the advances of technologies, the application of computing to aesthetics has rapidly increased. However, little effort has been put to apply aesthetics to computing. Aesthetic Computing is an attempt to fill that research gap. The present paper revisits and elaborates its concept, and highlights “embodiment” as a new format of representation of Aesthetic Computing. The present paper also describes how embodiment can provide more opportunities for accessibility and personalization of computing across different projects – art, STEAM education, and in-vehicle interfaces. Finally, more aesthetic components in Aesthetic Computing are discussed for future research.","2017","2023-07-06 01:03:18","2023-07-20 06:37:30","2023-07-06 01:03:18","305-313","","","196","","","Aesthetic Computing for Representation of the Computing Process and Expansion of Perceptual Dimensions","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering DOI: 10.1007/978-3-319-55834-9_36","","","","","","Brooks, Anthony L.; Brooks, Eva","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EDULT8TW","bookSection","2010","Sanderson, Penelope M.","The Power and the Puzzles of Auditory Interfaces","Human-Computer Interaction","978-3-642-15230-6 978-3-642-15231-3","","","http://link.springer.com/10.1007/978-3-642-15231-3_1","Auditory interfaces are increasingly prevalent in work and everyday environments. I survey recent uses of non-speech auditory interfaces and advances in knowledge about them, highlighting research from The University of Queensland.","2010","2023-07-06 01:03:18","2023-07-20 05:55:14","2023-07-06 01:03:18","1-2","","","332","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: IFIP Advances in Information and Communication Technology DOI: 10.1007/978-3-642-15231-3_1","","/Users/minsik/Zotero/storage/KUZUEQMY/Sanderson - 2010 - The Power and the Puzzles of Auditory Interfaces.pdf","","","","Forbrig, Peter; Paternó, Fabio; Mark Pejtersen, Annelise","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QZBGSMLX","bookSection","2009","Kern, Dagmar; Marshall, Paul; Hornecker, Eva; Rogers, Yvonne; Schmidt, Albrecht","Enhancing Navigation Information with Tactile Output Embedded into the Steering Wheel","Pervasive Computing","978-3-642-01515-1 978-3-642-01516-8","","","http://link.springer.com/10.1007/978-3-642-01516-8_5","","2009","2023-07-06 01:03:18","2023-07-06 01:03:18","2023-07-06 01:03:18","42-58","","","5538","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-01516-8_5","","/Users/minsik/Zotero/storage/3CXMGZKR/Kern et al. - 2009 - Enhancing Navigation Information with Tactile Outp.pdf","","","","Tokuda, Hideyuki; Beigl, Michael; Friday, Adrian; Brush, A. J. Bernheim; Tobe, Yoshito","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WAKX2PS9","bookSection","2001","Van Scoy, Frances L.; Kawai, Takamitsu; Darrah, Marjorie; Rash, Connie","Haptic display of mathematical functions for teaching mathematics to students with vision disabilities: design and proof of concept","Haptic Human-Computer Interaction","978-3-540-42356-0 978-3-540-44589-0","","","http://link.springer.com/10.1007/3-540-44589-7_4","","2001","2023-07-06 01:03:18","2023-07-06 01:03:18","2023-07-06 01:03:18","31-40","","","2058","","","Haptic display of mathematical functions for teaching mathematics to students with vision disabilities","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-44589-7_4","","","","","","Brewster, Stephen; Murray-Smith, Roderick","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YXP4MDI3","bookSection","2013","Morrell, Martin J.; Reiss, Joshua D.","Two-Dimensional Hybrid Spatial Audio Systems with User Variable Controls of Sound Source Attributes","From Sounds to Music and Emotions","978-3-642-41247-9 978-3-642-41248-6","","","http://link.springer.com/10.1007/978-3-642-41248-6_4","This paper presents two novel hybrid spatial audio systems demonstrated for use in two-dimensional applications with their scalability to three-dimensions. The emphasis of these hybrid systems is to give further creative freedom to a composer, sound engineer or sound designer. The systems are principally based on the end result of Ambisonics spatial audio reproduction systems. Since Ambisonics systems are used primarily for temporary sound installations and exhibits, the use of B-Format can be unnecessary. Therefore these systems revert to producing channel based content rather than sound field content that is later separately decoded. The presented systems use the decoder as a real-time sound manipulation feature on a per sound source basis. A comparison is drawn between the two systems and each method is described as to how it can be used as part of a standard music production workflow.","2013","2023-07-06 01:03:18","2023-07-20 00:06:01","2023-07-06 01:03:18","58-81","","","7900","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-41248-6_4","","","","","","Aramaki, Mitsuko; Barthet, Mathieu; Kronland-Martinet, Richard; Ystad, Sølvi","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MP3AE3WK","bookSection","2012","Wu, Victor K. Y.; Campbell, Roy H.","3D Audio Interface for Rich Mobile Web Experiences","Mobile Computing, Applications, and Services","978-3-642-29335-1 978-3-642-29336-8","","","http://link.springer.com/10.1007/978-3-642-29336-8_1","We propose a novel paradigm of consuming rich web content in a mobile setting (which are often eyes-free), through a predominantly 3D audio interface. Web content, formatted in audio, is streamed via the mobile device’s network connection, and placed virtually in a 3D audio space. The user moves in the virtual space, using a variety of human computer interaction (HCI) means, such as voice input, touching, rotating, and shaking the device, as well as hand and head gesturing. We provide applications benefitting from this paradigm of rich mobile 3D audio web consumption. We provide system designs, and a system architecture for mobile 3D audio. Finally, we implement our ideas in a system prototype using the Apple iOS mobile platform.","2012","2023-07-06 01:03:18","2023-07-21 04:29:57","2023-07-06 01:03:18","1-16","","","76","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering DOI: 10.1007/978-3-642-29336-8_1","","","","","","Gris, Martin; Yang, Guang","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RQCMNST5","bookSection","2006","Sánchez, Jaime; Baloian, Nelson","Issues in Implementing Awareness in Collaborative Software for Blind People","Computers Helping People with Special Needs","978-3-540-36020-9 978-3-540-36021-6","","","http://link.springer.com/10.1007/11788713_190","","2006","2023-07-06 01:03:18","2023-07-06 01:03:18","2023-07-06 01:03:18","1318-1325","","","4061","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11788713_190","","","","","","Miesenberger, Klaus; Klaus, Joachim; Zagler, Wolfgang L.; Karshmer, Arthur I.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"INK7HGB9","bookSection","2010","Grond, Florian; Hermann, Thomas; Verfaille, Vincent; Wanderley, Marcelo M.","Methods for Effective Sonification of Clarinetists’ Ancillary Gestures","Gesture in Embodied Communication and Human-Computer Interaction","978-3-642-12552-2 978-3-642-12553-9","","","http://link.springer.com/10.1007/978-3-642-12553-9_15","","2010","2023-07-06 01:03:18","2023-07-06 01:03:18","2023-07-06 01:03:18","171-181","","","5934","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12553-9_15","","","","","","Kopp, Stefan; Wachsmuth, Ipke","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GCM79T43","bookSection","2010","Sobue, Shin-ichi; Araki, Hiroshi; Tazawa, Seiichi; Noda, Hirotomo; Kamiya, Izumi; Yamamoto, Aya; Fujita, Takeo; Higashiizumi, Ichiro; Okumura, Hayato","An Application of Lunar GIS with Visualized and Auditory Japan’s Lunar Explorer “Kaguya” Data","Advanced Techniques in Computing Sciences and Software Engineering","978-90-481-3659-9 978-90-481-3660-5","","","https://link.springer.com/10.1007/978-90-481-3660-5_27","","2010","2023-07-06 01:03:18","2023-07-06 01:03:18","2023-07-06 01:03:18","159-163","","","","","","","","","","","Springer Netherlands","Dordrecht","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-90-481-3660-5_27","","","","","","Elleithy, Khaled","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GJQG4G2V","bookSection","1998","Cohen, Michael; Herder, Jens","Symbolic representations of exclude and include for audio sources and sinks: Figurative suggestions of & and &","Virtual Environments ’98","978-3-211-83233-2 978-3-7091-7519-4","","","http://link.springer.com/10.1007/978-3-7091-7519-4_23","Shared virtual environments require generalized control of user-dependent media streams. Traditional audio mixing idioms for enabling and disabling various sources employ and functions, which, along with selectively disable or focus on respective channels. Exocentric interfaces which explicitly model not only spatial audio sources, but also location, orientation, directivity, and multiplicity of sinks, motivate the generalization of & to exclude and include, manifested for sinks as & a narrowing of stimuli by explicitly blocking out and/or concentrating on selected entities. This paper introduces figurative representations of these functions, virtual hands to be clasped over avatars’ ears and mouths, with orientation suggesting the nature of the blocking. Applications include groupware for collaboration and teaching, teleconferencing and chat spaces, and authoring and manipulation of distributed virtual environments.","1998","2023-07-06 01:03:18","2023-07-21 05:13:47","2023-07-06 01:03:18","235-242","","","","","","Symbolic representations of exclude and include for audio sources and sinks","","","","","Springer Vienna","Vienna","","","","","","DOI.org (Crossref)","","Series Title: Eurographics DOI: 10.1007/978-3-7091-7519-4_23","","","","","","Göbel, Martin; Landauer, Jürgen; Lang, Ulrich; Wapler, Matthias","Hansmann, W.; Hewitt, W. T.; Purgathofer, W.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X9L8BF7J","journalArticle","2015","Bakker, Saskia; Van Den Hoven, Elise; Eggen, Berry","Peripheral interaction: characteristics and considerations","Personal and Ubiquitous Computing","","1617-4909, 1617-4917","10.1007/s00779-014-0775-2","http://link.springer.com/10.1007/s00779-014-0775-2","In everyday life, we are able to perceive information and perform physical actions in the background or periphery of attention. Inspired by this observation, several researchers have studied interactive systems that display digital information in the periphery of attention. To broaden the scope of this research direction, a few recent studies have focused on interactive systems that can not only be perceived in the background but also enable users to physically interact with digital information in their periphery. Such peripheral interaction designs can support computing technology to fluently embed in and become a meaningful part of people’s everyday routines. With the increasing ubiquity of technology in our everyday environment, we believe that this direction is highly relevant nowadays. This paper presents an in-depth analysis of three case studies on peripheral interaction. These case studies involved the design and development of peripheral interactive systems and deployment of these systems in the real context of use for a number of weeks. Based on the insights gained through these case studies, we discuss generalized characteristics and considerations for peripheral interaction design and evaluation. The aim of the work presented in this paper is to support interaction design researchers and practitioners in anticipating and facilitating peripheral interaction with the designs they are evaluating or developing.","2015-01","2023-07-06 01:03:18","2023-07-21 04:46:17","2023-07-06 01:03:18","239-254","","1","19","","Pers Ubiquit Comput","Peripheral interaction","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/MV4DGFE9/Bakker et al. - 2015 - Peripheral interaction characteristics and consid.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P7FFQ2KT","bookSection","2006","Hermann, Thomas; Paschalidou, Stella; Beckmann, Dirk; Ritter, Helge","Gestural Interactions for Multi-parameter Audio Control and Audification","Gesture in Human-Computer Interaction and Simulation","978-3-540-32624-3 978-3-540-32625-0","","","http://link.springer.com/10.1007/11678816_37","","2006","2023-07-06 01:03:18","2023-07-06 01:03:18","2023-07-06 01:03:18","335-338","","","3881","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11678816_37","","","","","","Gibet, Sylvie; Courty, Nicolas; Kamp, Jean-François","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LY38TXA9","journalArticle","2004","Karjalainen, Matti; Erkut, Cumhur","Digital Waveguides versus Finite Difference Structures: Equivalence and Mixed Modeling","EURASIP Journal on Advances in Signal Processing","","1687-6180","10.1155/S1110865704401176","https://asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704401176","Digital waveguides and finite difference time domain schemes have been used in physical modeling of spatially distributed systems. Both of them are known to provide exact modeling of ideal one-dimensional (1D) band-limited wave propagation, and both of them can be composed to approximate two-dimensional (2D) and three-dimensional (3D) mesh structures. Their equal capabilities in physical modeling have been shown for special cases and have been assumed to cover generalized cases as well. The ability to form mixed models by joining substructures of both classes through converter elements has been proposed recently. In this paper, we formulate a general digital signal processing (DSP)-oriented framework where the functional equivalence of these two approaches is systematically elaborated and the conditions of building mixed models are studied. An example of mixed modeling of a 2D waveguide is presented.","2004-12","2023-07-06 01:03:18","2023-07-20 00:00:15","2023-07-06 01:03:18","561060","","7","2004","","EURASIP J. Adv. Signal Process.","Digital Waveguides versus Finite Difference Structures","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/RXBKQXJN/Karjalainen and Erkut - 2004 - Digital Waveguides versus Finite Difference Struct.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"259MMLSK","journalArticle","2022","Su, Isabelle; Hattwick, Ian; Southworth, Christine; Ziporyn, Evan; Bisshop, Ally; Mühlethaler, Roland; Saraceno, Tomás; Buehler, Markus J.","Interactive exploration of a hierarchical spider web structure with sound","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-021-00375-x","https://link.springer.com/10.1007/s12193-021-00375-x","3D spider webs exhibit highly intricate fiber architectures and owe their outstanding performance to a hierarchical organization that spans orders of magnitude in length scale from the molecular silk protein, to micrometer-sized fibers, and up to cm-scale web. Similarly, but in a completely different physical manifestation, music has a hierarchical structure composed of elementary sine wave building blocks that can be combined with other waveforms to create complex timbres, which are then arranged within larger-scale musical compositions. Although apparently different, spider webs and music have many similarities, as we point out in this work. Here, we propose an intuitive and interactive way to explore and visualize a 3D Cyrtophora citricola spider web geometry that has been digitally modeled with micron-scale details from full-scale laboratory experiments. We use model-based sonification to translate the web architecture into sound, allowing for aural perception and interpretation of its essential topological features. We implement this sonification using Unity3D and Max/MSP to create an interactive spider web environment in which a user travels through a virtual spider web. Each silk fiber in their field of view is sonified using different sine waves. Together, the sonified fibers create new and more complex timbres that reflects the architecture of 3D spider webs. These concepts are implemented into a spider web-based instrument for live performances, art installations and data exploration. It provides an unprecedented and creative way to immerse the composer, audience and user in an immersive multimedia experience generated by the complexity of a 3D spider web.","2022-03","2023-07-06 01:03:18","2023-07-20 07:04:41","2023-07-06 01:03:18","71-85","","1","16","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/WZ9B7IRU/Su et al. - 2022 - Interactive exploration of a hierarchical spider w.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DK7N4VED","journalArticle","2022","Misdariis, N.; Özcan, E.; Grassi, M.; Pauletto, S.; Barrass, S.; Bresin, R.; Susini, P.","Sound experts’ perspectives on astronomy sonification projects","Nature Astronomy","","2397-3366","10.1038/s41550-022-01821-w","https://www.nature.com/articles/s41550-022-01821-w","The Audible Universe project aims to create dialogue between two scientific domains investigating two distinct research objects: stars and sound. It has been instantiated within a collaborative workshop that began to mutually acculturate the two communities, by sharing and transmitting respective knowledge, skills and practices. One main outcome of this exchange was a global view on the astronomical data sonification paradigm for observing the diversity of tools, uses and users (including visually impaired people), but also the current limitations and potential methods of improvement. From this viewpoint, here we present basic elements gathered and contextualized by sound experts in their respective fields (sound perception/cognition, sound design, psychoacoustics, experimental psychology), to anchor sonification for astronomy in a more well informed, methodological and creative process.","2022-11","2023-07-06 01:03:36","2023-07-06 01:03:36","2023-07-06 01:03:36","1249-1255","","11","6","","Nat Astron","","","","","","","","en","2022 Springer Nature Limited","","","","www.nature.com","","Number: 11 Publisher: Nature Publishing Group","","/Users/minsik/Zotero/storage/KBVDW4WK/Misdariis et al. - 2022 - Sound experts’ perspectives on astronomy sonificat.pdf","","","Astronomy and astrophysics; Information theory and computation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5SGI9ULL","bookSection","2012","Chan, Shih-Han; Natkin, Stéphane; Tiger, Guillaume; Topol, Alexandre","Extensible Sound Description in COLLADA: A Unique File for a Rich Sound Design","Advances in Computer Entertainment","978-3-642-34291-2 978-3-642-34292-9","","","http://link.springer.com/10.1007/978-3-642-34292-9_11","","2012","2023-07-06 01:06:12","2023-07-06 01:06:12","2023-07-06 01:06:12","151-166","","","7624","","","Extensible Sound Description in COLLADA","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-34292-9_11","","","","","","Nijholt, Anton; Romão, Teresa; Reidsma, Dennis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"92QV4SL8","bookSection","2021","Delle Monache, Stefano; Rocchesso, Davide","Exploring Design Cognition in Voice-Driven Sound Sketching and Synthesis","Perception, Representations, Image, Sound, Music","978-3-030-70209-0 978-3-030-70210-6","","","http://link.springer.com/10.1007/978-3-030-70210-6_30","","2021","2023-07-06 01:06:12","2023-07-06 01:06:12","2023-07-06 01:06:12","465-480","","","12631","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-70210-6_30","","/Users/minsik/Zotero/storage/E9KPFG6T/Delle Monache and Rocchesso - 2021 - Exploring Design Cognition in Voice-Driven Sound S.pdf","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Aramaki, Mitsuko","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FTKNKN2H","bookSection","2005","Nguyen, Cong Phuong; Pham, Thi Ngoc Yen; Eric, Castelli","Toward a Sound Analysis System for Telemedicine","Fuzzy Systems and Knowledge Discovery","978-3-540-28331-7 978-3-540-31828-6","","","http://link.springer.com/10.1007/11540007_44","Our work is within the framework of studying a sound analysis system in a telemedicine project. The task of this system is to detect situations of distress in a patient’s room basing on sound analysis. In this paper we present our studies on the constructions of a speech/non-speech discriminator and of a speech/scream-groan discriminator. The first discriminator’s task is to distinguish speech signal from non speech signal in a room such as sounds of broken glass, door shutting, chair falling, water in toilette, etc. The second one’s task is to detect sounds of scream-groan from speech signal. Results show that these discriminators are applicable to our sound analysis system.","2005","2023-07-06 01:06:12","2023-07-20 00:07:27","2023-07-06 01:06:12","352-361","","","3614","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11540007_44","","","","","","Wang, Lipo; Jin, Yaochu","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"36QBX6ND","bookSection","2013","Thoret, Etienne; Aramaki, Mitsuko; Kronland-Martinet, Richard; Velay, Jean-Luc; Ystad, Sølvi","Reenacting Sensorimotor Features of Drawing Movements from Friction Sounds","From Sounds to Music and Emotions","978-3-642-41247-9 978-3-642-41248-6","","","http://link.springer.com/10.1007/978-3-642-41248-6_8","Even though we generally don’t pay attention to the friction sounds produced when we are writing or drawing, these sounds are recordable, and can even evoke the underlying gesture. In this paper, auditory perception of such sounds, and the internal representations they evoke when we listen to them, is considered from the sensorimotor learning point of view. The use of synthesis processes of friction sounds makes it possible to investigate the perceptual influence of each gestures parameter separately. Here, the influence of the velocity profile on the mental representation of the gesture induced by a friction sound was investigated through 3 experiments. The results reveal the perceptual relevance of this parameter, and particularly a specific morphology corresponding to biological movements, the so-called 1/3-power law. The experiments are discussed according to the sensorimotor theory and the invariant taxonomy of the ecological approach.","2013","2023-07-06 01:06:12","2023-07-20 00:06:20","2023-07-06 01:06:12","130-153","","","7900","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-41248-6_8","","/Users/minsik/Zotero/storage/NKHQTYLA/Thoret et al. - 2013 - Reenacting Sensorimotor Features of Drawing Moveme.pdf","","","","Aramaki, Mitsuko; Barthet, Mathieu; Kronland-Martinet, Richard; Ystad, Sølvi","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"23ZWJXUQ","bookSection","2006","O’Sullivan, Conor; Chang, Angela","An Activity Classification for Vibrotactile Phenomena","Haptic and Audio Interaction Design","978-3-540-37595-1 978-3-540-37596-8","","","http://link.springer.com/10.1007/11821731_14","","2006","2023-07-06 01:06:12","2023-07-06 01:06:12","2023-07-06 01:06:12","145-156","","","4129","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11821731_14","","/Users/minsik/Zotero/storage/UU3CAEQ8/O’Sullivan and Chang - 2006 - An Activity Classification for Vibrotactile Phenom.pdf","","","","McGookin, David; Brewster, Stephen","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BXM7Z5X3","journalArticle","2021","Cibrian, Franceli L.; Ley-Flores, Judith; Newbold, Joseph W.; Singh, Aneesha; Bianchi-Berthouze, Nadia; Tentori, Monica","Interactive sonification to assist children with autism during motor therapeutic interventions","Personal and Ubiquitous Computing","","1617-4909, 1617-4917","10.1007/s00779-020-01479-z","http://link.springer.com/10.1007/s00779-020-01479-z","Interactive sonification is an effective tool used to guide individuals when practicing movements. Little research has shown the use of interactive sonification in supporting motor therapeutic interventions for children with autism who exhibit motor impairments. The goal of this research is to study if children with autism understand the use of interactive sonification during motor therapeutic interventions, its potential impact of interactive sonification in the development of motor skills in children with autism, and the feas ibility of using it in specialized schools for children with autism. We conducted two deployment studies in Mexico using Go-withthe-Flow, a framework to sonify movements previously developed for chronic pain rehabilitation. In the first study, six children with autism were asked to perform the forward reach and lateral upper-limb exercises while listening to three different sound structures (i.e., one discrete and two continuous sounds). Results showed that children with autism exhibit awareness about the sonification of their movements and engage with the sonification. We then adapted the sonifications based on the results of the first study, for motor therapy of children with autism. In the next study, nine children with autism were asked to perform upper-limb lateral, cross-lateral, and push movements while listening to five different sound structures (i.e., three discrete and two continues) designed to sonify the movements. Results showed that discrete sound structures engage the children in the performance of upper-limb movements and increase their ability to perform the movements correctly. We finally propose design considerations that could guide the design of projects related to interactive sonification","2021-04","2023-07-06 01:06:12","2023-07-21 04:52:43","2023-07-06 01:06:12","391-410","","2","25","","Pers Ubiquit Comput","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/3IY33WVZ/Cibrian et al. - 2021 - Interactive sonification to assist children with a.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8W68SHFH","bookSection","2010","Geier, Matthias; Spors, Sascha; Weinzierl, Stefan","The Future of Audio Reproduction","Adaptive Multimedia Retrieval. Identifying, Summarizing, and Recommending Image and Music","978-3-642-14757-9 978-3-642-14758-6","","","http://link.springer.com/10.1007/978-3-642-14758-6_1","The introduction of new techniques for audio reproduction such as binaural technology, Wave Field Synthesis and Higher Order Ambisonics is accompanied by a paradigm shift from channel-based to object-based transmission and storage of spatial audio. The separate coding of source signal and source location is not only more efficient considering the number of channels used for reproduction by large loudspeaker arrays, it will also open up new options for a user-controlled soundfield design. The paper describes the technological change from stereophonic to array-based audio reproduction techniques and introduces a new proposal for the coding of spatial properties related to auditory objects.","2010","2023-07-06 01:06:12","2023-07-19 11:09:30","2023-07-06 01:06:12","1-17","","","5811","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-14758-6_1","","","","","","Detyniecki, Marcin; Leiner, Ulrich; Nürnberger, Andreas","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5ULZWIRQ","bookSection","2000","Harding, Chris; Kakadiaris, Ioannis; Loftin, R. Bowen","A Multimodal User Interface for Geoscientific Data Investigation","Advances in Multimodal Interfaces — ICMI 2000","978-3-540-41180-2 978-3-540-40063-9","","","http://link.springer.com/10.1007/3-540-40063-X_80","","2000","2023-07-06 01:06:12","2023-07-06 01:06:12","2023-07-06 01:06:12","615-623","","","1948","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-40063-X_80","","","","","","Tan, Tieniu; Shi, Yuanchun; Gao, Wen","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LHXD3K4L","bookSection","2003","Djennane, Safia","3D-Audio News Presentation Modeling","Universal Access Theoretical Perspectives, Practice, and Experience","978-3-540-00855-2 978-3-540-36572-3","","","http://link.springer.com/10.1007/3-540-36572-9_22","","2003","2023-07-06 01:06:12","2023-07-06 01:06:12","2023-07-06 01:06:12","280-286","","","2615","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-36572-9_22","","","","","","Carbonell, Noëlle; Stephanidis, Constantine","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LFCK9JKK","bookSection","2002","Hermann, Thomas; Nölker, Claudia; Ritter, Helge","Hand Postures for Sonification Control","Gesture and Sign Language in Human-Computer Interaction","978-3-540-43678-2 978-3-540-47873-7","","","http://link.springer.com/10.1007/3-540-47873-6_32","","2002","2023-07-06 01:06:12","2023-07-06 01:06:12","2023-07-06 01:06:12","307-316","","","2298","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-47873-6_32","","","","","","Wachsmuth, Ipke; Sowa, Timo","Goos, G.; Hartmanis, J.; Van Leeuwen, J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I74UV2Q8","journalArticle","2009","Hwang, Sungmok; Park, Youngjin; Park, Youn-sik","Analysis on perceptual sensitivity to head-related impulse responses in the median plane","Journal of Mechanical Science and Technology","","1738-494X, 1976-3824","10.1007/s12206-009-0925-z","http://link.springer.com/10.1007/s12206-009-0925-z","This study deals with the perceptual sensitivity to Head-Related Impulse Responses (HRIRs) in the median plane based on a series of subjective listening tests using a pair of headphones. First, the non-individualized HRIRs were modeled from 12 principal components (PCs) extracted from Principal Components Analysis (PCA) of the CIPIC HRTF database. The Just Noticeable Difference (JND) in weight of PCs (PCWs) at each elevation was estimated. It was not observed the common elevation-dependent tendency or PCW-dependent tendency of JND in PCWs across the five subjects who participated in the tests, and the inter-subject variation of JND in PCWs was large. The JND in HRIRs can be estimated indirectly from the JND in PCWs because the HRIRs can be represented by a linear summation of the PCs weighted by PCWs. The common elevation-dependent tendency of JND in Directional Impulse Responses (DIRs), which are the mean-subtracted HRIRs, across the five subjects can be found. The change in PCWs does not seem to contribute to our perception of sound source characteristics; however, the resulting change in HRIRs due to the change in PCWs seems to contribute. The subjects showed larger JND in DIRs in the frontal region than in the rear region. This means that our perception of sound source characteristics is more sensitive for frontal sources than rear sources.","2009-12","2023-07-06 01:06:12","2023-07-20 06:47:45","2023-07-06 01:06:12","3340-3348","","12","23","","J Mech Sci Technol","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M5L9UHBA","bookSection","2000","Semwal, Sudhanshu Kumar; Evans-Kamp, Debra Lee","Virtual Environments for Visually Impaired","Virtual Worlds","978-3-540-67707-9 978-3-540-45016-0","","","http://link.springer.com/10.1007/3-540-45016-5_25","","2000","2023-07-06 01:06:12","2023-07-06 01:06:12","2023-07-06 01:06:12","270-285","","","1834","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-45016-5_25","","","","","","Heudin, Jean-Claude","Goos, G.; Hartmanis, J.; Van Leeuwen, J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BLMQ9ZUZ","bookSection","2006","Röber, Niklas; Huber, Cornelius; Hartmann, Knut; Feustel, Matthias; Masuch, Maic","Interactive Audiobooks: Combining Narratives with Game Elements","Technologies for Interactive Digital Storytelling and Entertainment","978-3-540-49934-3 978-3-540-49935-0","","","http://link.springer.com/10.1007/11944577_36","","2006","2023-07-06 01:06:12","2023-07-06 01:06:12","2023-07-06 01:06:12","358-369","","","4326","","","Interactive Audiobooks","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11944577_36","","","","","","Göbel, Stefan; Malkewitz, Rainer; Iurgel, Ido","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U74AASR7","bookSection","2009","Mannheimer, Steve; Ferati, Mexhid; Bolchini, Davide; Palakal, Mathew","Educational Sound Symbols for the Visually Impaired","Universal Access in Human-Computer Interaction. Addressing Diversity","978-3-642-02706-2 978-3-642-02707-9","","","http://link.springer.com/10.1007/978-3-642-02707-9_12","Acoustic-based computer interactivity offers great potential [1], particularly with blind and visually impaired users [2]. At Indiana University’s School of Informatics at IUPUI, we have developed an innovative educational approach relying on “audemes,” short, nonverbal sound symbols made up of 2-5 individual sounds lasting 3-7 seconds - like expanded “earcons”[3] - to encode and prompt memory. To illustrate: An audeme for “American Civil War” includes a 3-second snippet of the song Dixie partially overlapped by a snippet of Battle Hymn of the Republic, followed by battle sounds, together lasting 5 seconds. Our focus on non-verbal sound explores the mnemonic impact of metaphoric rather than literal signification. Working for a year with BVI students, we found audemes improved encoding and long-term memory of verbal educational content, even after five months, and engaged the students in stimulating ways.","2009","2023-07-06 01:06:12","2023-07-21 05:08:24","2023-07-06 01:06:12","106-115","","","5614","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-02707-9_12","","","","","","Stephanidis, Constantine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UJ4ZZR8V","bookSection","2001","Harding, Chris; Kakadiaris, Ioannis A.; Casey, John F.; Bowen Loftin, R.","A Case Study in Multi-Sensory Investigation of Geoscientific Data","Data Visualization 2001","978-3-211-83674-3 978-3-7091-6215-6","","","http://link.springer.com/10.1007/978-3-7091-6215-6_2","","2001","2023-07-06 01:06:12","2023-07-06 01:06:12","2023-07-06 01:06:12","3-14","","","","","","","","","","","Springer Vienna","Vienna","en","","","","","DOI.org (Crossref)","","Series Title: Eurographics DOI: 10.1007/978-3-7091-6215-6_2","","","","","","Ebert, David S.; Favre, Jean M.; Peikert, Ronald","Hansmann, W.; Purgathofer, W.; Sillion, F.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BC5LH3FV","bookSection","2011","White, Sean; Feiner, Steven","Dynamic, Abstract Representations of Audio in a Mobile Augmented Reality Conferencing System","Recent Trends of Mobile Collaborative Augmented Reality Systems","978-1-4419-9844-6 978-1-4419-9845-3","","","https://link.springer.com/10.1007/978-1-4419-9845-3_12","We describe a wearable audio conferencing and information presentation system that represents individual participants and audio elements through dynamic, visual abstractions, presented on a tracked, see-through head-worn display. Our interest is in communication spaces, annotation, and data that are represented by auditory media with synchronistic or synesthetic visualizations. Representations can transition between different spatial modalities as audio elements enter and exit the wearer’s physical presence. In this chapter, we discuss the user interface and infrastructure, SoundSight, which uses the Skype Internet telephony API to support wireless conferencing, and describe our early experience using the system.","2011","2023-07-06 01:06:12","2023-07-21 04:56:49","2023-07-06 01:06:12","149-160","","","","","","","","","","","Springer New York","New York, NY","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-1-4419-9845-3_12","","","","","","Alem, Leila; Huang, Weidong","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TUIVPXEA","bookSection","2009","Olivetti Belardinelli, Marta; Federici, Stefano; Delogu, Franco; Palmiero, Massimiliano","Sonification of Spatial Information: Audio-Tactile Exploration Strategies by Normal and Blind Subjects","Universal Access in Human-Computer Interaction. Intelligent and Ubiquitous Interaction Environments","978-3-642-02709-3 978-3-642-02710-9","","","http://link.springer.com/10.1007/978-3-642-02710-9_62","","2009","2023-07-06 01:06:12","2023-07-06 01:06:12","2023-07-06 01:06:12","557-563","","","5615","","","Sonification of Spatial Information","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-02710-9_62","","/Users/minsik/Zotero/storage/NMV8TPKA/Olivetti Belardinelli et al. - 2009 - Sonification of Spatial Information Audio-Tactile.pdf","","","","Stephanidis, Constantine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6W62HP6X","bookSection","2021","Mangiron, Carme","Game Accessibility: Taking Inclusion to the Next Level","Universal Access in Human-Computer Interaction. Design Methods and User Experience","978-3-030-78091-3 978-3-030-78092-0","","","https://link.springer.com/10.1007/978-3-030-78092-0_17","This paper provides an overview of game accessibility. It describes what game accessibility is and highlights the need to combine usability and adaptability, as a “one-size-fits-all” approach is not possible due to the interactive and dynamic nature of the video game medium and the array of (dis)abilities different users may have. The main accessibility barriers different groups of users face are also presented. Then, the main focus shifts to the current state of the art in game accessibility, including existing guidelines and research carried out in this field. Finally, future perspectives are outlined, emphasizing the need for a user-centred approach and for collaboration between users, academia, and the industry.","2021","2023-07-06 01:06:12","2023-07-21 05:09:51","2023-07-06 01:06:12","269-279","","","12768","","","Game Accessibility","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-78092-0_17","","","","","","Antona, Margherita; Stephanidis, Constantine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JBM33QCV","journalArticle","2011","Stefik, Andreas; Gellenbeck, Ed","Empirical studies on programming language stimuli","Software Quality Journal","","0963-9314, 1573-1367","10.1007/s11219-010-9106-7","http://link.springer.com/10.1007/s11219-010-9106-7","Comprehending and debugging computer programs are inherently difficult tasks. The current approach to building program execution and debugging environments is to use exclusively visual stimuli on programming languages whose syntax and semantics has often been designed without empirical guidance. We present an alternative: Sodbeans, an open-source integrated development environment designed to output carefully chosen spoken auditory cues to supplement empirically evaluated visual stimuli. Originally designed for the blind, earlier work suggested that Sodbeans may benefit sighted programmers as well. We evaluate Sodbeans in two experiments. First, we report on a formal debugging experiment comparing (1) a visual debugger, (2) an auditory debugger, and (3) a multimedia debugger, which includes both visual and auditory stimuli. The results from this study indicate that while auditory debuggers on their own are significantly less effective for sighted users when compared with visual and multimedia debuggers, multimedia debuggers might benefit sighted programmers under certain circumstances. Specifically, we found that while multimedia debuggers do not provide instant usability, once programmers have some practice, their performance in answering comprehension questions improves. Second, we created and evaluated a pilot survey analyzing individual elements in a custom programming language (called HOP) to garner empirical metrics on their comprehensibility. Results showed that some of the most widely used syntax and semantics choices in commercial programming languages are extraordinarily unintuitive for novices. For example, at an aggregate level, the word for , as in a for loop, was rated reliably worse than repeat by more than 673% by novices. After completing our studies, we implemented the HOP programming language and integrated it into Sodbeans.","2011-03","2023-07-06 01:06:12","2023-07-21 05:00:51","2023-07-06 01:06:12","65-99","","1","19","","Software Qual J","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M2E924PN","bookSection","2010","Sciabica, Jean-François; Bezat, Marie-Céline; Roussarie, Vincent; Kronland-Martinet, Richard; Ystad, Sølvi","Towards Timbre Modeling of Sounds Inside Accelerating Cars","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_19","Quality investigations and design of interior car sounds constitute an important challenge for the car industry. Such sounds are complex and time-varying, inducing considerable timbre variations depending on the driving conditions. An interior car sound is indeed a mixture between several sound sources, with two main contributions, i.e. the engine noise on the one hand and the aerodynamic and tire-road noise on the other. Masking phenomena occur between these two components and should be considered when studying perceptive attributes of interior car sounds in order to identify relevant signal parameters. By combining sensory analysis and signal analysis associated with an auditory model, a relation between a reduced number of signal parameters and perceptive attributes can be found. This approach has enabled us to propose timbre descriptors based on the tristimulus criterion that reflect the dynamic behavior of a sound inside an accelerating car.","2010","2023-07-06 01:06:12","2023-07-19 11:38:33","2023-07-06 01:06:12","377-391","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_19","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2I8TMV4C","journalArticle","2016","Stahl, Benjamin; Thoshkahna, Balaji","Design and evaluation of the effectiveness of a sonification technique for real time heart-rate data","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-016-0218-7","http://link.springer.com/10.1007/s12193-016-0218-7","","2016-09","2023-07-06 01:07:23","2023-07-06 01:07:23","2023-07-06 01:07:23","207-219","","3","10","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WKQ3IP3N","bookSection","2007","Reiter, Ulrich; Jumisko-Pyykkö, Satu","Watch, Press, and Catch – Impact of Divided Attention on Requirements of Audiovisual Quality","Human-Computer Interaction. HCI Intelligent Multimodal Interaction Environments","978-3-540-73108-5 978-3-540-73110-8","","","http://link.springer.com/10.1007/978-3-540-73110-8_104","Many of today’s audiovisual application systems offer some kind of interactivity. Yet, quality assessments of these systems are often performed without taking into account the possible effects of divided attention caused by interaction or user task. We present a subjective assessment performed among 40 test subjects to investigate the impact of divided attention on the perception of audiovisual quality in interactive application systems. Test subjects were asked to rate the overall perceived audiovisual quality in an interactive 3D scene with varying degrees of interactive tasks to be performed by the subjects. As a result we found that the experienced overall quality did not vary with the degree of interaction. The results of our study make clear that in the case where interactivity is offered in an audiovisual application, it is not generally possible to technically lower the signal quality without perceptual effects.","2007","2023-07-06 01:07:23","2023-07-20 06:31:51","2023-07-06 01:07:23","943-952","","","4552","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73110-8_104","","","","","","Jacko, Julie A.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IBTL8FGS","bookSection","2011","Absar, Rafa; Guastavino, Catherine","Nonspeech Sound Design for a Hierarchical Information System","Human Centered Design","978-3-642-21752-4 978-3-642-21753-1","","","http://link.springer.com/10.1007/978-3-642-21753-1_52","","2011","2023-07-06 01:07:23","2023-07-06 01:07:23","2023-07-06 01:07:23","461-470","","","6776","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-21753-1_52","","","","","","Kurosu, Masaaki","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SS8XAYU9","bookSection","2007","Miyashita, Hisashi; Takagi, Hironobu; Sato, Daisuke; Asakawa, Chieko","Making Multimedia Internet Content Accessible and Usable","Universal Access in Human-Computer Interaction. Applications and Services","978-3-540-73282-2 978-3-540-73283-9","","","http://link.springer.com/10.1007/978-3-540-73283-9_12","Although multimedia content containing streaming media is now widely used on the World Wide Web, there exist considerable difficulties for blind users to access such content, due to its dynamic changes, keyboard inoperability, and audio interference with the speech from assistive software. In particular, the third problem of audio interference is serious for blind users, since multimedia content often contains streaming media such as video and music which continuously play sounds, and thus they cannot hear the speech, which is masked by the loud media. In this paper, we propose a new accessible browser that can directly manipulate such multimedia content. In order to control Flash contents, our browser relies on a transcoding HTTP proxy to inject special scripts into the Flash content and then manipulates the embedded streaming media and sound objects via the injected scripts. By using our browser, users can easily turn the volume up or down, play, stop, or pause the streaming media with shortcut keys. Since the users do not need to focus on buttons or sliders for these operations, they can immediately stop or fade out the intrusive media when listening to speech from assistive software.","2007","2023-07-06 01:07:23","2023-07-21 05:09:31","2023-07-06 01:07:23","98-107","","","4556","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73283-9_12","","/Users/minsik/Zotero/storage/HEU3K3MU/Miyashita et al. - 2007 - Making Multimedia Internet Content Accessible and .pdf","","","","Stephanidis, Constantine","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3HE3R4LE","bookSection","2008","Yamaguchi, Tokuo; Asai, Kazuhiro; Kitamura, Yoshifumi; Kishino, Fumio","Interactive Multimedia Contents in the IllusionHole","Entertainment Computing - ICEC 2008","978-3-540-89221-2 978-3-540-89222-9","","","http://link.springer.com/10.1007/978-3-540-89222-9_13","This paper proposes a system of interactive multimedia contents that allows multiple users to participate in a face-to-face manner and share the same time and space. It provides an interactive environment where multiple users can see and manipulate stereoscopic animation with individual sound. Two application examples are implemented; one is location-based content design and the other is user-based content design. Both effectively use a unique feature of the IllusionHole, i.e., a location-sensitive display device that provides a stereoscopic image with multiple users around the table. Keywords: 3D user interface, entertainment computi","2008","2023-07-06 01:07:23","2023-07-19 23:59:35","2023-07-06 01:07:23","116-121","","","5309","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-89222-9_13","","/Users/minsik/Zotero/storage/SBEX2DGX/Yamaguchi et al. - 2008 - Interactive Multimedia Contents in the IllusionHol.pdf","","","","Stevens, Scott M.; Saldamarco, Shirley J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6RFZEF4S","journalArticle","2007","Gygi, Brian; Kidd, Gary R.; Watson, Charles S.","Similarity and categorization of environmental sounds","Perception & Psychophysics","","0031-5117, 1532-5962","10.3758/BF03193921","http://link.springer.com/10.3758/BF03193921","Four experiments investigated the acoustical correlates of similarity and categorization judgments of environmental sounds. In Experiment 1, similarity ratings were obtained from pairwise comparisons of recordings of 50 environmental sounds. A three-dimensional multidimensional scaling (MDS) solution showed three distinct clusterings of the sounds, which included harmonic sounds, discrete impact sounds, and continuous sounds. Furthermore, sounds from similar sources tended to be in close proximity to each other in the MDS space. The orderings of the sounds on the individual dimensions of the solution were well predicted by linear combinations of acoustic variables, such as harmonicity, amount of silence, and modulation depth. The orderings of sounds also correlated significantly with MDS solutions for similarity ratings of imagined sounds and for imagined sources of sounds, obtained in Experiments 2 and 3—as was the case for free categorization of the 50 sounds (Experiment 4)—although the categorization data were less well predicted by acoustic features than were the similarity data.","2007-08","2023-07-06 01:07:23","2023-07-21 04:43:41","2023-07-06 01:07:23","839-855","","6","69","","Perception & Psychophysics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/ASNWFGG5/Gygi et al. - 2007 - Similarity and categorization of environmental sou.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"43T5WJYU","bookSection","2021","Kariyado, Yuta; Arevalo, Camilo; Villegas, Julián","Auralization of Three-Dimensional Cellular Automata","Artificial Intelligence in Music, Sound, Art and Design","978-3-030-72913-4 978-3-030-72914-1","","","https://link.springer.com/10.1007/978-3-030-72914-1_11","An auralization tool for exploring three-dimensional cellular automata is presented. This proof-of-concept allows the creation of a sound field comprising individual sound events associated with each cell in a three-dimensional grid. Each sound-event is spatialized depending on the orientation of the listener relative to the three-dimensional model. Users can listen to all cells simultaneously or in sequential slices at will. Conceived to be used as an immersive Virtual Reality (VR) scene, this software application also works as a desktop application for environments where the VR infrastructure is missing. Subjective evaluations indicate that the proposed sonification increases the perceived quality and immersability of the system with respect to a visualization-only system. No subjective differences between the sequential or simultaneous presentations were found.","2021","2023-07-06 01:07:23","2023-07-19 11:33:38","2023-07-06 01:07:23","161-170","","","12693","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-72914-1_11","","","","","","Romero, Juan; Martins, Tiago; Rodríguez-Fernández, Nereida","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZFHPFUCU","journalArticle","2012","Grond, Florian","Safety Certificate: an audification performance of high-speed trains","AI & SOCIETY","","0951-5666, 1435-5655","10.1007/s00146-011-0351-5","http://link.springer.com/10.1007/s00146-011-0351-5","","2012-05","2023-07-06 01:07:23","2023-07-06 01:07:23","2023-07-06 01:07:23","293-295","","2","27","","AI & Soc","Safety Certificate","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GEQ46RMA","bookSection","2017","Matsuo, Masaki; Miura, Takahiro; Sakajiri, Masatsugu; Onishi, Junji; Ono, Tsukasa","Inclusive Side-Scrolling Action Game Securing Accessibility for Visually Impaired People","Human-Computer Interaction – INTERACT 2017","978-3-319-68058-3 978-3-319-68059-0","","","https://link.springer.com/10.1007/978-3-319-68059-0_41","Though many computer games have recently become accessible for gamers with visual impairments, these players still face difficulty in manipulating game characters and acquiring visual information. It is true that although an increasing number of games for visually impaired people called audio games are being developed, many of these games cannot satisfy their basic needs because of the shortage of contents and are difficult for sighted people because of no visual information. Based on this situation, we have been developing accessible games for visually impaired people that feature enriched materials and multimodal information presentation. However, the needs of real-time action on accessible games remain unsolved. In this article, our objective is to develop an inclusive side scroller game with high real-time performance and accessibility functions for visually impaired people, and be available to play with more than one person including sighted persons.","2017","2023-07-06 01:07:23","2023-07-21 07:44:16","2023-07-06 01:07:23","410-414","","","10516","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-68059-0_41","","/Users/minsik/Zotero/storage/3SBWEXJX/Matsuo et al. - 2017 - Inclusive Side-Scrolling Action Game Securing Acce.pdf","","","","Bernhaupt, Regina; Dalvi, Girish; Joshi, Anirudha; K. Balkrishan, Devanuj; O’Neill, Jacki; Winckler, Marco","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YXEP4H9N","journalArticle","2012","Knees, Peter; Pohle, Tim; Widmer, Gerhard","Sound/tracks: artistic real-time sonification of train journeys","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0089-x","http://link.springer.com/10.1007/s12193-011-0089-x","","2012-07","2023-07-06 01:07:23","2023-07-06 01:07:23","2023-07-06 01:07:23","87-93","","1-2","6","","J Multimodal User Interfaces","Sound/tracks","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ARV33F5T","bookSection","2010","Tuuri, Kai","Gestural Attributions as Semantics in User Interface Sound Design","Gesture in Embodied Communication and Human-Computer Interaction","978-3-642-12552-2 978-3-642-12553-9","","","http://link.springer.com/10.1007/978-3-642-12553-9_23","","2010","2023-07-06 01:07:23","2023-07-06 01:07:23","2023-07-06 01:07:23","257-268","","","5934","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12553-9_23","","","","","","Kopp, Stefan; Wachsmuth, Ipke","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SZLVY62F","bookSection","2013","Wilkie, Sonia; Stockman, Tony","The Perception of Auditory-Visual Looming in Film","From Sounds to Music and Emotions","978-3-642-41247-9 978-3-642-41248-6","","","http://link.springer.com/10.1007/978-3-642-41248-6_21","Auditory-visual looming (the presentation of objects moving in depth towards the viewer) is a technique used in film (particularly those in 3D) to assist in drawing the viewer into the created world. The capacity of a viewer to perceptually immerse within the multidimensional world and interact with moving objects can be affected by the sounds (audio cues) that accompany these looming objects. However the extent to which sound parameters should be manipulated remains unclear. For example, the amplitude, spectral components, reverb and spatialisation can all be altered, but the degree of their alteration and the resulting perception generated need greater investigation. Building on a previous study analysing the physical properties of the sounds, we analyse people’s responses to the complex sounds which use multiple audio cues for film looming scenes, reporting which conditions elicited a faster response to contact time, causing the greatest amount of underestimation.","2013","2023-07-06 01:07:23","2023-07-20 00:06:32","2023-07-06 01:07:23","378-386","","","7900","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-41248-6_21","","","","","","Aramaki, Mitsuko; Barthet, Mathieu; Kronland-Martinet, Richard; Ystad, Sølvi","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z2Y4MB4E","bookSection","2010","Okada, Noriko; Miki, Mitsunori; Hiroyasu, Tomoyuki; Yoshimi, Masato","Classified-Chime Sound Generation Support System Using an Interactive Genetic Algorithm","Artifical Intelligence and Soft Computing","978-3-642-13231-5 978-3-642-13232-2","","","http://link.springer.com/10.1007/978-3-642-13232-2_21","This research proposes a chime sound generation support system to readily generate intercom chime sounds that are agreeable to individual persons and to associate the chime sounds with visitors. In the proposed system, an interactive genetic algorithm (IGA) is used. Based on the melodies created by users, chime sounds are automatically generated in accordance with rules. The effectiveness of the proposed system is verified by an experiment using the system.","2010","2023-07-06 01:07:23","2023-07-19 11:32:40","2023-07-06 01:07:23","173-180","","","6114","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-13232-2_21","","","","","","Rutkowski, Leszek; Scherer, Rafał; Tadeusiewicz, Ryszard; Zadeh, Lotfi A.; Zurada, Jacek M.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5A7BXSW6","bookSection","2003","Marila, Juha; Ronkainen, Sami","Time-Out in Mobile Text Input: The Effects of Learning and Feedback","Human-Computer Interaction with Mobile Devices and Services","978-3-540-40821-5 978-3-540-45233-1","","","http://link.springer.com/10.1007/978-3-540-45233-1_8","In many user interfaces with restricted input/output capabilities, a time-out is used to automatically change the UI from one mode into another. In this paper we studied the learning of time-outs and the effect of feedback on it in mobile phone text entry. The effects of three different feedback schemes (auditory/visual/no feedback) on learning of two different time-out lengths were compared. We measured the response time from the time-out occurrence to the time of user’s reaction. Error rates and the development of the response times in different schemes were used as measures of learning. We also studied if the users learned to estimate the time-out lengths, or if they just reacted to the available feedback. There were three main findings. Without feedback, response times had great variation. Auditory feedback enabled faster response times than visual. Finally, we found evidence of short-term learning, but not as much of a lasting effect.","2003","2023-07-06 01:07:23","2023-07-20 06:29:58","2023-07-06 01:07:23","91-103","","","2795","","","Time-Out in Mobile Text Input","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-45233-1_8","","","","","","Chittaro, Luca","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JXTSMSU6","bookSection","2017","Sousa, Luís; Pinto, António","MuSec: Sonification of Alarms Generated by a SIEM","Ambient Intelligence– Software and Applications – 8th International Symposium on Ambient Intelligence (ISAmI 2017)","978-3-319-61117-4 978-3-319-61118-1","","","http://link.springer.com/10.1007/978-3-319-61118-1_5","","2017","2023-07-06 01:07:23","2023-07-06 01:07:23","2023-07-06 01:07:23","32-39","","","615","","","MuSec","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Advances in Intelligent Systems and Computing DOI: 10.1007/978-3-319-61118-1_5","","/Users/minsik/Zotero/storage/XDHU2ZEB/Sousa and Pinto - 2017 - MuSec Sonification of Alarms Generated by a SIEM.pdf","","","","De Paz, Juan F.; Julián, Vicente; Villarrubia, Gabriel; Marreiros, Goreti; Novais, Paulo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QJX7ZLAZ","bookSection","2003","Bennett, David J.","Effects of Navigation and Position on Task When Presenting Diagrams to Blind People uUsing Sound","Diagrammatic Representation and Inference","978-3-540-43561-7 978-3-540-46037-4","","","http://link.springer.com/10.1007/3-540-46037-3_19","","2003","2023-07-06 01:07:23","2023-07-06 01:07:23","2023-07-06 01:07:23","161-175","","","2317","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-46037-3_19","","","","","","Hegarty, Mary; Meyer, Bernd; Narayanan, N. Hari","Goos, G.; Hartmanis, J.; Van Leeuwen, J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YMD2JNHT","journalArticle","2019","Ben-Hur, Zamir; Alon, David Lou; Rafaely, Boaz; Mehra, Ravish","Loudness stability of binaural sound with spherical harmonic representation of sparse head-related transfer functions","EURASIP Journal on Audio, Speech, and Music Processing","","1687-4722","10.1186/s13636-019-0148-x","https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-019-0148-x","In response to renewed interest in virtual and augmented reality, the need for high-quality spatial audio systems has emerged. The reproduction of immersive and realistic virtual sound requires high resolution individualized head-related transfer function (HRTF) sets. In order to acquire an individualized HRTF, a large number of spatial measurements are needed. However, such a measurement process requires expensive and specialized equipment, which motivates the use of sparsely measured HRTFs. Previous studies have demonstrated that spherical harmonics (SH) can be used to reconstruct the HRTFs from a relatively small number of spatial samples, but reducing the number of samples may produce spatial aliasing error. Furthermore, by measuring the HRTF on a sparse grid the SH representation will be order-limited, leading to constrained spatial resolution. In this paper, the effect of sparse measurement grids on the reproduced binaural signal is studied by analyzing both aliasing and truncation errors. The expected effect of these errors on the perceived loudness stability of the virtual sound source is studied theoretically, as well as perceptually by an experimental investigation. Results indicate a substantial effect of truncation error on the loudness stability, while the added aliasing seems to significantly reduce this effect.","2019-12","2023-07-06 01:07:23","2023-07-20 00:01:41","2023-07-06 01:07:23","5","","1","2019","","J AUDIO SPEECH MUSIC PROC.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/FU924GLQ/Ben-Hur et al. - 2019 - Loudness stability of binaural sound with spherica.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AGFDYFTL","bookSection","2005","McCarthy, John; Wright, Peter","Technology in Place: Dialogics of Technology, Place and Self","Human-Computer Interaction - INTERACT 2005","978-3-540-28943-2 978-3-540-31722-7","","","http://link.springer.com/10.1007/11555261_72","Ubiquitous and ambient computing – computationally enhanced built environments and portable products that aim to make computing available anytime-anywhere – has somewhat paradoxically put place at the heart of Interaction Design. In this paper, foundations are laid for a dialogical approach to place as an expression of the experienced relationship between people and space. Building on McCarthy and Wright’s dialogical conceptualisation of technology as experience, place is described in terms of the plurality of histories, interactions and meanings that characterise people’s different engagements with particular spaces. Implications of a dialogical approach to place are considered with respect to the further development within Interaction Design of concepts such as context, engagement, and interactivity.","2005","2023-07-06 01:07:23","2023-07-20 05:55:40","2023-07-06 01:07:23","914-926","","","3585","","","Technology in Place","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11555261_72","","/Users/minsik/Zotero/storage/TVNAM9N4/McCarthy and Wright - 2005 - Technology in Place Dialogics of Technology, Plac.pdf","","","","Costabile, Maria Francesca; Paternò, Fabio","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8ZZEXHA2","bookSection","2009","Zhang, Jingjing; Lotto, Beau; Bergstom, Ilias; Andreou, Lefkothea; Miyadera, Youzou; Yokoyama, Setsuo","Development of a Visualised Sound Simulation Environment: An e-Approach to a Constructivist Way of Learning","Human-Computer Interaction. Interacting in Various Application Domains","978-3-642-02582-2 978-3-642-02583-9","","","http://link.springer.com/10.1007/978-3-642-02583-9_30","In this paper, the design and implementation of a visualised sound simulation environment is presented as an initial step to further laboratory experimentation. Preliminary laboratory experiments showed a positive learning curve in human auditory perception. Learning occurred when new information was processed with relevant existing knowledge in this simulation environment. While the work towards the truth of the empirical hypothesis is still under discussion, this project has been expanded beyond the scope that was originally envisaged and the developed environment showed its potential to be adopted on mobile devices for many educational purposes. This initiative not only brings scientists and educators together, but it is also hoped that it represents a possible e-approach to a constructivist way of learning.","2009","2023-07-06 01:07:23","2023-07-20 06:32:15","2023-07-06 01:07:23","266-275","","","5613","","","Development of a Visualised Sound Simulation Environment","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-02583-9_30","","/Users/minsik/Zotero/storage/D8WKHDPA/Zhang et al. - 2009 - Development of a Visualised Sound Simulation Envir.pdf","","","","Jacko, Julie A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7BF2VXTE","bookSection","2004","Dorin, Alan","The Virtual Ecosystem as Generative Electronic Art","Applications of Evolutionary Computing","978-3-540-21378-9 978-3-540-24653-4","","","http://link.springer.com/10.1007/978-3-540-24653-4_48","This paper proposes four desirable attributes of processes to be applied in generative electronic art. By example, it then demonstrates that the virtual ecosystem in its entirety is a process with many of these desirable attributes. The paper contrasts this process with the use of cellular automata. It outlines a number of generative artworks with which the author has been involved that utilize the virtual ecosystem, and discusses their pros and cons in the context of generative art. The paper suggests means by which the application of the four desirable attributes may extend the creative possibilities for these works.","2004","2023-07-06 01:07:23","2023-07-19 11:30:56","2023-07-06 01:07:23","467-476","","","3005","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-24653-4_48","","","","","","Raidl, Günther R.; Cagnoni, Stefano; Branke, Jürgen; Corne, David Wolfe; Drechsler, Rolf; Jin, Yaochu; Johnson, Colin G.; Machado, Penousal; Marchiori, Elena; Rothlauf, Franz; Smith, George D.; Squillero, Giovanni","Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PNKUKK8N","bookSection","2009","Ferati, Mexhid; Bolchini, Davide; Mannheimer, Steve","Towards a Modeling Language for Designing Auditory Interfaces","Universal Access in Human-Computer Interaction. Applications and Services","978-3-642-02712-3 978-3-642-02713-0","","","http://link.springer.com/10.1007/978-3-642-02713-0_53","Auditory applications are systems that communicate content, navigation capabilities and functionality mainly via the aural channel, or via a combination of the aural and visual channels, and can support the user interaction in a multimodal fashion as well (e.g. through touch or speech). In this paper, we present the preliminary results of an exploratory research effort aimed at establishing a design modeling language for auditory applications, by extending an existing interactive application design model (IDM, Interactive Dialogue Model) used in the area of hypermedia and information-intensive applications. Our exploratory research capitalizes on previous experience in hypermedia modeling, aural information architectures, and design of auditory applications. We use an auditory application, the Acoustic Edutainment Interface (AEDIN), as a real case study to inform and exemplify the use of the modeling language.","2009","2023-07-06 01:08:42","2023-07-21 05:09:03","2023-07-06 01:08:42","502-511","","","5616","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-02713-0_53","","/Users/minsik/Zotero/storage/97RQBNKZ/Ferati et al. - 2009 - Towards a Modeling Language for Designing Auditory.pdf","","","","Stephanidis, Constantine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HB3SD353","bookSection","2011","Mertens, Alexander; Przybysz, Philipp; Groß, Alexander; Koch-Koerfges, David; Nick, Claudia; Kaethner, Martin; Schlick, Christopher M.","Age-Adapted Psychoacoustics: Target Group Oriented Sound Schemes for the Interaction with Telemedical Systems","Universal Access in Human-Computer Interaction. Applications and Services","978-3-642-21656-5 978-3-642-21657-2","","","https://link.springer.com/10.1007/978-3-642-21657-2_44","","2011","2023-07-06 01:08:42","2023-07-06 01:08:42","2023-07-06 01:08:42","406-415","","","6768","","","Age-Adapted Psychoacoustics","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-21657-2_44","","","","","","Stephanidis, Constantine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6PK3BGES","bookSection","2002","Baloian, Nelson; Luther, Wolfram","Visualization for the Mind’s Eye","Software Visualization","978-3-540-43323-1 978-3-540-45875-3","","","http://link.springer.com/10.1007/3-540-45875-1_28","","2002","2023-07-06 01:08:42","2023-07-06 01:08:42","2023-07-06 01:08:42","354-367","","","2269","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-45875-1_28","","/Users/minsik/Zotero/storage/98JLWYEW/Baloian and Luther - 2002 - Visualization for the Mind’s Eye.pdf","","","","Diehl, Stephan","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5GMKINYN","bookSection","2007","Duarte, Carlos; Carriço, Luís","Conveying Browsing Context Through Audio on Digital Talking Books","Universal Access in Human-Computer Interaction. Applications and Services","978-3-540-73282-2 978-3-540-73283-9","","","http://link.springer.com/10.1007/978-3-540-73283-9_30","","2007","2023-07-06 01:08:42","2023-07-06 01:08:42","2023-07-06 01:08:42","259-268","","","4556","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73283-9_30","","","","","","Stephanidis, Constantine","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2I753DTP","journalArticle","2012","Drioli, Carlo; Rocchesso, Davide","Acoustic rendering of particle-based simulation of liquids in motion","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0063-7","http://link.springer.com/10.1007/s12193-011-0063-7","","2012-05","2023-07-06 01:08:42","2023-07-06 01:08:42","2023-07-06 01:08:42","187-195","","3-4","5","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6PIRBEMK","bookSection","2012","Alharbi, Saad","Empirically Derived Guidelines for Audio-Visual E-mail Browsing","Advances in New Technologies, Interactive Interfaces and Communicability","978-3-642-34009-3 978-3-642-34010-9","","","http://link.springer.com/10.1007/978-3-642-34010-9_10","","2012","2023-07-06 01:08:42","2023-07-06 01:08:42","2023-07-06 01:08:42","104-113","","","7547","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-34010-9_10","","","","","","Cipolla-Ficarra, Francisco; Veltman, Kim; Verber, Domen; Cipolla-Ficarra, Miguel; Kammüller, Florian","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GAUDA9ZR","journalArticle","2007","Baldwin, Carryl L.","Cognitive implications of facilitating echoic persistence","Memory & Cognition","","0090-502X, 1532-5946","10.3758/BF03193314","http://link.springer.com/10.3758/BF03193314","Seventeen participants performed a tone-pattern-matching task at different presentation levels while concurrently engaged in a simulated-driving task. Presentation levels of 60, 65, and 70 dBC (SPL) were combined factorially with tone-matching delays of 2, 3, and 4 sec. Intensity had no effect on performance in single-task conditions and short-delay conditions. However, when the participants were engaged concurrently in the driving task, a significant interaction between presentation level and delay was observed. In the longest delay condition, the participants performed the tone-pattern-matching task more efficiently (more quickly and without additional errors) as presentation intensity increased. These findings demonstrate the interaction between sensory and cognitive processes and point to a direct-intensity relationship where intensity affects the persistence of echoic memory. Implications for facilitating auditory processing and improving auditory interfaces in complex systems (i.e., transportation environments), particularly for older and hearing-impaired listeners, are discussed.","2007-06","2023-07-06 01:08:42","2023-07-21 04:29:23","2023-07-06 01:08:42","774-780","","4","35","","Memory & Cognition","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/XDYW7WE4/Baldwin - 2007 - Cognitive implications of facilitating echoic pers.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NMZQGN6W","bookSection","2000","Nesbitt, Keith","Designing Multi-sensory Models for Finding Patterns in Stock Market Data","Advances in Multimodal Interfaces — ICMI 2000","978-3-540-41180-2 978-3-540-40063-9","","","http://link.springer.com/10.1007/3-540-40063-X_4","","2000","2023-07-06 01:08:42","2023-07-06 01:08:42","2023-07-06 01:08:42","24-31","","","1948","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-40063-X_4","","","","","","Tan, Tieniu; Shi, Yuanchun; Gao, Wen","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z4Z36GS9","journalArticle","1970","Cuddy, Lola L.","Training the absolute identification of pitch","Perception & Psychophysics","","0031-5117, 1532-5962","10.3758/BF03212589","http://link.springer.com/10.3758/BF03212589","Two methods for training the absolute judgment of pitch, reference trainillg and series training, were studied. Reference training concentrated during training on the identification of three reference tones in a set of nine pure tones, while series training gave equal weight during training to the idelltification of all nine tones. Results of flre- and posttraining tests, scored for the number of correct judgmellts, showed that reference training was more effective than series training for listeners wit!: musical experience. In addition. aiscriminability (a') scaling ofpreand posttest performance indicated that reference training was particular(v effective for training listeners with musical experienCe' when the nine tones of aset were grouped into three pitch classes-high, medium, and low pitch. Listeners without musical experience benefited from buill training methods, but their overall improvement was less than that for musical listeners.","1970-09","2023-07-06 01:08:42","2023-07-21 04:43:23","2023-07-06 01:08:42","265-269","","5","8","","Perception & Psychophysics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/I5AA6FDS/Cuddy - 1970 - Training the absolute identification of pitch.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QUFEEYAH","journalArticle","2016","Metatla, Oussama; Martin, Fiore; Parkinson, Adam; Bryan-Kinns, Nick; Stockman, Tony; Tanaka, Atau","Audio-haptic interfaces for digital audio workstations: A participatory design approach","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-016-0217-8","http://link.springer.com/10.1007/s12193-016-0217-8","","2016-09","2023-07-06 01:08:42","2023-07-06 01:08:42","2023-07-06 01:08:42","247-258","","3","10","","J Multimodal User Interfaces","Audio-haptic interfaces for digital audio workstations","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/8SDWULYK/Metatla et al. - 2016 - Audio-haptic interfaces for digital audio workstat.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DNCXWDHT","journalArticle","2009","Tolentino, Lisa; Birchfield, David; Megowan-Romanowicz, Colleen; Johnson-Glenberg, Mina C.; Kelliher, Aisling; Martinez, Christopher","Teaching and Learning in the Mixed-Reality Science Classroom","Journal of Science Education and Technology","","1059-0145, 1573-1839","10.1007/s10956-009-9166-2","http://link.springer.com/10.1007/s10956-009-9166-2","As emerging technologies become increasingly inexpensive and robust, there is an exciting opportunity to move beyond general purpose computing platforms to realize a new generation of K-12 technology-based learning environments. Mixed-reality technologies integrate real world components with interactive digital media to offer new potential to combine best practices in traditional science learning with the powerful affordances of audio/visual simulations. This paper introduces the realization of a learning environment called SMALLab, the Situated Multimedia Arts Learning Laboratory. We present a recent teaching experiment for high school chemistry students. A mix of qualitative and quantitative research documents the efficacy of this approach for students and teachers. We conclude that mixed-reality learning is viable in mainstream high school classrooms and that students can achieve significant learning gains when this technology is co-designed with educators.","2009-12","2023-07-06 01:08:42","2023-07-20 06:49:49","2023-07-06 01:08:42","501-517","","6","18","","J Sci Educ Technol","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SIFZIR4U","journalArticle","2006","Chen, Xiaoyu; Tremaine, Marilyn; Lutz, Robert; Chung, Jae-woo; Lacsina, Patrick","AudioBrowser: a mobile browsable information access for the visually impaired","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-006-0019-y","http://link.springer.com/10.1007/s10209-006-0019-y","Although a large amount of research has been conducted on building interfaces for the visually impaired that allows users to read web pages and generate and access information on computers, little development addresses two problems faced by the blind users. First, sighted users can rapidly browse and select information they find useful, and second, sighted users can make much useful information portable through the recent proliferation of personal digital assistants (PDAs). These possibilities are not currently available for blind users. This paper describes an interface that has been built on a standard PDA and allows its user to browse the information stored on it through a combination of screen touches coupled with auditory feedback. The system also supports the storage and management of personal information so that addresses, music, directions, and other supportive information can be readily created and then accessed anytime and anywhere by the PDA user. The paper describes the system along with the related design choices and design rationale. A user study is also reported.","2006-06","2023-07-06 01:08:42","2023-07-21 05:11:00","2023-07-06 01:08:42","4-22","","1","5","","Univ Access Inf Soc","AudioBrowser","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7TDSSBIY","journalArticle","2022","Sekhavat, Yoones A.; Azadehfar, Mohammad Reza; Zarei, Hossein; Roohi, Samad","Sonification and interaction design in computer games for visually impaired individuals","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-022-11984-3","https://link.springer.com/10.1007/s11042-022-11984-3","Video games are changing how we interact and communicate with each other. They can provide an authentic and collaborative platform for building new communities and connecting people. Since video games generally rely on visual elements that must be recognized by the players, most of them are not accessible by visually impaired people. In this research, we study the sonification and interaction issues in the design of computer games for visually impaired individuals. We have proposed an audio game called GrandEscape with the focus on the special needs of visually impaired people while playing. A comprehensive set of user studies has been performed to evaluate different interaction and sonification techniques in terms of providing a sense of presence and gaming experience.","2022-03","2023-07-06 01:08:42","2023-07-21 04:33:18","2023-07-06 01:08:42","7847-7871","","6","81","","Multimed Tools Appl","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K8KSNRRA","bookSection","2014","Parkinson, Adam; Tanaka, Atau","Making Data Sing: Embodied Approaches to Sonification","Sound, Music, and Motion","978-3-319-12975-4 978-3-319-12976-1","","","https://link.springer.com/10.1007/978-3-319-12976-1_9","","2014","2023-07-06 01:08:42","2023-07-06 01:08:42","2023-07-06 01:08:42","151-160","","","8905","","","Making Data Sing","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-12976-1_9","","","","","","Aramaki, Mitsuko; Derrien, Olivier; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VU8NV66R","bookSection","2014","Cordeiro, João; Barbosa, Álvaro","Privacy in Sound-Based Social Networks","Multidisciplinary Social Networks Research","978-3-662-45070-3 978-3-662-45071-0","","","http://link.springer.com/10.1007/978-3-662-45071-0_29","","2014","2023-07-06 01:08:42","2023-07-06 01:08:42","2023-07-06 01:08:42","355-367","","","473","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Communications in Computer and Information Science DOI: 10.1007/978-3-662-45071-0_29","","","","","","Wang, Leon Shyue-Liang; June, Jason J.; Lee, Chung-Hong; Okuhara, Koji; Yang, Hsin-Chang","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"79EEIHF5","journalArticle","2019","Rönnberg, Niklas","Sonification supports perception of brightness contrast","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-019-00311-0","http://link.springer.com/10.1007/s12193-019-00311-0","","2019-12","2023-07-06 01:08:42","2023-07-06 01:08:42","2023-07-06 01:08:42","373-381","","4","13","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/KJB69436/Rönnberg - 2019 - Sonification supports perception of brightness con.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z7FSYVAZ","journalArticle","2006","Tong, Kam-pang Maya; Wong, Kam-wah","Schematic interface of sound creation for computer animators","Journal of Zhejiang University-SCIENCE A","","1673-565X, 1862-1775","10.1631/jzus.2006.A1141","http://link.springer.com/10.1631/jzus.2006.A1141","As an audiovisual medium, computer animations require superior image quality and professional soundtrack to lead audiences into their fascinating virtual world. Without sound, the impact of storytelling is reduced or the story is even not understandable. Despite the importance of sound, most animators are unfamiliar with sound editing software. Limited budget projects such as independent or student works have difficulty hiring sound professionals to create tailor-made soundtrack. Therefore, we need a suitable sound tool to express their individual ideas. In this paper, we propose an approach using schematic for both computer animation and sound. Our approach provides (1) physical simulation on sound through animation parameters and (2) a new channel for animators to add aesthetic values in sound through their experience of using schematics in existing animation software. We demonstrate our idea through several examples, such as Doppler shift, obstacle effect, importance, energetic, and sputtering effect.","2006-07","2023-07-06 01:08:42","2023-07-20 06:50:29","2023-07-06 01:08:42","1141-1151","","7","7","","J. Zhejiang Univ. - Sci. A","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7ASRKWHF","bookSection","2012","McGookin, David; Brewster, Stephen A.","Understanding Auditory Navigation to Physical Landmarks","Haptic and Audio Interaction Design","978-3-642-32795-7 978-3-642-32796-4","","","http://link.springer.com/10.1007/978-3-642-32796-4_1","We present two studies that seek to better understand the role spatialised (3D) audio can play in supporting effective pedestrian navigation. 24 participants attempted to navigate and locate physical landmarks in a local botanical gardens using a gpsTunes [1] based auditory navigation system coupled with a map. Participants were significantly better at locating prominent than non-prominent physical landmarks. However, no significant quantative difference was found between the use of a map only and map + audio. Qualitative analysis revealed significant issues when physical landmarks are used, and common strategies when combining audio and map navigation. We highlight the implications of these in relation to existing work, and provide guidelines for future designers to employ.","2012","2023-07-06 01:08:42","2023-07-20 00:15:11","2023-07-06 01:08:42","1-10","","","7468","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-32796-4_1","","","","","","Magnusson, Charlotte; Szymczak, Delphine; Brewster, Stephen","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6XJBZ57H","bookSection","2001","Murphy, David; Pitt, Ian","Spatial Sound Enhancing Virtual Story Telling","Virtual Storytelling Using Virtual Reality Technologies for Storytelling","978-3-540-42611-0 978-3-540-45420-5","","","http://link.springer.com/10.1007/3-540-45420-9_3","Spatial sound information/cues may enhance the sense of immersiveness in virtual story telling. However, their role within complex, loosely-structured narratives is little understood. This paper describes a virtual heritage project that aims to convey a factual story using interactive virtual environments. Sound was added to the existing project in an effort to enhance the virtual experience. The use of sound is assessed through a user-study in order to assess its effectiveness and suggest methods of improvement.","2001","2023-07-06 01:10:52","2023-07-21 05:14:54","2023-07-06 01:10:52","20-29","","","2197","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-45420-9_3","","","","","","Balet, Olivier; Subsol, Gérard; Torguet, Patrice","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X6TEBY7U","bookSection","2007","Bergstrom, Tony; Karahalios, Karrie","Seeing More: Visualizing Audio Cues","Human-Computer Interaction – INTERACT 2007","978-3-540-74799-4 978-3-540-74800-7","","","http://link.springer.com/10.1007/978-3-540-74800-7_3","Using audio visualization, we seek to demonstrate how natural interaction is augmented with the addition of interaction history. Our Conversation Clock visualization captures and represents audio in a persistent and meaningful representation to provide social cues not available in an otherwise ephemeral conversation. In this paper we present user study evaluation of the Conversation Clock as utilized by familiar groups and demonstrate how individuals use the salient cues to evaluate their own interaction.","2007","2023-07-06 01:10:52","2023-07-20 05:55:49","2023-07-06 01:10:52","29-42","","","4663","","","Seeing More","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-74800-7_3","","/Users/minsik/Zotero/storage/3NWLNWRY/Bergstrom and Karahalios - 2007 - Seeing More Visualizing Audio Cues.pdf","","","","Baranauskas, Cécilia; Palanque, Philippe; Abascal, Julio; Barbosa, Simone Diniz Junqueira","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2B6L9R9J","journalArticle","2006","Mion, Luca; D’Incà, Gianluca","Analysis of expression in simple musical gestures to enhance audio in interfaces","Virtual Reality","","1359-4338, 1434-9957","10.1007/s10055-006-0029-3","http://link.springer.com/10.1007/s10055-006-0029-3","Expression could play a key role in the audio rendering of virtual reality applications. Its understanding is an ambitious issue in the scientific environment, and several studies have investigated the analysis techniques to detect expression in music performances. The knowledge coming from these analyses is widely applicable: embedding expression on audio interfaces can drive to attractive solutions to emphasize interfaces in mixed-reality environments. Synthesized expressive sounds can be combined with real stimuli to experience augmented reality, and they can be used in multi-sensory stimulations to provide the sensation of first-person experience in virtual expressive environments. In this work we focus on the expression of violin and flute performances, with reference to sensorial and affective domains. By means of selected audio features, we draw a set of parameters describing performers’ strategies which are suitable both for tuning expressive synthesis instruments and enhancing audio in human–computer interfaces.","2006-05","2023-07-06 01:10:52","2023-07-21 05:14:28","2023-07-06 01:10:52","62-70","","1","10","","Virtual Reality","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L6T6GYXE","journalArticle","2013","Usher, Raymond; Robertson, Paul; Sloan, Robin","Physical responses (arousal) to audio in games","The Computer Games Journal","","2052-773X","10.1007/BF03392340","http://link.springer.com/10.1007/BF03392340","This study investigates the role that audio plays in the video gaming experience. Two groups of participants played three games (of different styles). One group played the games with the audio switched on; the other group played the games with the audio switched off. A bioharness was used to measure the heartbeat and respiration rates of the participants as they played the game. The results showed that for all three games, the heartbeat and respiration rates of the participants in the group (playing the games with the audio switched on) were higher than the heartbeat and respiration rates of participants in the other group. This suggests that audio effects in video games provide a measurable enhancement of the game-playing experience.","2013-08","2023-07-06 01:10:52","2023-07-21 05:05:03","2023-07-06 01:10:52","5-13","","2","2","","Comput Game J","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K3799PS7","journalArticle","1988","Miyazaki, Ken’ichi","Musical pitch identification by absolute pitch possessors","Perception & Psychophysics","","0031-5117, 1532-5962","10.3758/BF03207484","http://link.springer.com/10.3758/BF03207484","Musical pitch identification was investigated in two experiments in which absolute pitch (AP) possessors and nonpossessors categorized tones presented in isolation into predetermined pitch classes. Stimuli consisted of 60 different tones per octave (at intervals of 20 cents). The experiments were designed to minimize the possibility that subjects could use strategies other than AP in performing the task. The results clearly differentiated AP possessors from nonpossessors in accuracy and speed of responding. Those subjects who had AP could categorize the tones quite consistently by using musical qualities of the tones (tone chroma). However, they did not respond uniformly to all stimuli; they responded more accurately and quickly to some musically important tones in a C-major mode (C, E, or G). On the other hand, those who had no AP showed almost random response patterns. In the absence of a tonal context, they could not use tone chroma, but only tone height. It isargued that tone chroma should be defined as the musical characteristics of tones in a tonal context, and that AP possessors are unique in that they can perceive it absolutely in the absence of any musical context. Although AP was believed to be very rare, it was proved here that aphenomenally large proportion of the subjects tested had AP. The correlation was observed between AP possession and early musical training that started at the age of 3 to 5.","1988-11","2023-07-06 01:10:52","2023-07-21 04:44:09","2023-07-06 01:10:52","501-512","","6","44","","Perception & Psychophysics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/WR5RV7TS/Miyazaki - 1988 - Musical pitch identification by absolute pitch pos.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VQ8LJRBP","bookSection","2013","Rajan, Rahul; Hsiao, Joey; Lahoti, Deven; Selker, Ted","“Roger that!” — The Value of Adding Social Feedback in Audio-Mediated Communications","Human-Computer Interaction – INTERACT 2013","978-3-642-40497-9 978-3-642-40498-6","","","http://link.springer.com/10.1007/978-3-642-40498-6_37","","2013","2023-07-06 01:10:52","2023-07-06 01:10:52","2023-07-06 01:10:52","471-488","","","8120","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-40498-6_37","","/Users/minsik/Zotero/storage/DJRYY4QE/Rajan et al. - 2013 - “Roger that!” — The Value of Adding Social Feedbac.pdf","","","","Kotzé, Paula; Marsden, Gary; Lindgaard, Gitte; Wesson, Janet; Winckler, Marco","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UA5ZYT8Q","bookSection","2002","McElligott, Lisa; Dillon, Michelle; Leydon, Krispin; Richardson, Bruce; Fernström, Mikael; Paradiso, Joseph A.","‘ForSe FIElds’ - Force Sensors for Interactive Environments","UbiComp 2002: Ubiquitous Computing","978-3-540-44267-7 978-3-540-45809-8","","","http://link.springer.com/10.1007/3-540-45809-3_13","In this paper we discuss the development of ‘Z-Tiles’ in conjunction with a sister project, ‘Self-Organising Sensors’ (SOS). Combined, these projects will result in a pressure sensitive, self-organising, interactive sensor design that can be embedded into appropriate environments. The shared objective of these projects is to further our understanding of movement and gesture. In this paper, we discuss the design and behaviour of a force sensing material, the physical design of the sensor encasement and the software that allows the sensors to communicate and self-organise. The issues of modularity and portability are also discussed in this paper, while consideration has also been given to the conceptualisation and development of a variety of prototypes; ranging from entertainment to potential therapeutic applications. Essentially, the Z-tiles sensor can be used in control surfaces where force, weight distribution or motion is used as control parameters.","2002","2023-07-06 01:10:52","2023-07-21 05:07:51","2023-07-06 01:10:52","168-175","","","2498","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-45809-3_13","","","","","","Borriello, Gaetano; Holmquist, Lars Erik","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LGM8NXDA","journalArticle","2016","Tordini, Francesco; Bregman, Albert S.; Cooperstock, Jeremy R.","Prioritizing foreground selection of natural chirp sounds by tempo and spectral centroid","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-016-0223-x","http://link.springer.com/10.1007/s12193-016-0223-x","","2016-09","2023-07-06 01:10:52","2023-07-06 01:10:52","2023-07-06 01:10:52","221-234","","3","10","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8XNKF6AF","bookSection","2010","Ortega-González, Vladimir; Garbaya, Samir; Merienne, Frédéric","Reducing Reversal Errors in Localizing the Source of Sound in Virtual Environment without Head Tracking","Haptic and Audio Interaction Design","978-3-642-15840-7 978-3-642-15841-4","","","http://link.springer.com/10.1007/978-3-642-15841-4_10","","2010","2023-07-06 01:10:52","2023-07-06 01:10:52","2023-07-06 01:10:52","85-96","","","6306","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-15841-4_10","","/Users/minsik/Zotero/storage/YZN5GVAT/Ortega-González et al. - 2010 - Reducing Reversal Errors in Localizing the Source .pdf","","","","Nordahl, Rolf; Serafin, Stefania; Fontana, Federico; Brewster, Stephen","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PW9UJI8I","bookSection","2013","Carroll, Dustin; Chakraborty, Suranjan; Lazar, Jonathan","Designing Accessible Visualizations: The Case of Designing a Weather Map for Blind Users","Universal Access in Human-Computer Interaction. Design Methods, Tools, and Interaction Techniques for eInclusion","978-3-642-39187-3 978-3-642-39188-0","","","http://link.springer.com/10.1007/978-3-642-39188-0_47","","2013","2023-07-06 01:10:52","2023-07-06 01:10:52","2023-07-06 01:10:52","436-445","","","8009","","","Designing Accessible Visualizations","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-39188-0_47","","/Users/minsik/Zotero/storage/MAPHYHKB/Carroll et al. - 2013 - Designing Accessible Visualizations The Case of D.pdf","","","","Stephanidis, Constantine; Antona, Margherita","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BTW9XBWE","bookSection","2019","Ty, Jayzon; Inoue, Naoki; Plopski, Alexander; Okahashi, Sayaka; Sandor, Christian; Hsu, Hsiu-Yun; Kuo, Li-Chieh; Su, Fong-Chin; Kato, Hirokazu","Integration of Augmented Reality with Pressing Evaluation and Training System for Finger Force Training","Human Aspects of IT for the Aged Population. Social Media, Games and Assistive Environments","978-3-030-22014-3 978-3-030-22015-0","","","http://link.springer.com/10.1007/978-3-030-22015-0_45","One major concern for the elderly is the decline in their ability to control their hands, which can significantly affect their ability to perform activities of daily living. One of the important hand functions that deteriorate over time is the ability to control finger force exertion, due to the gradual decrease in finger muscle strength as people age. Previous studies have shown that with proper training, it is possible to regain finger strength. However, when designing training systems for finger force control, visualization of the finger forces plays an important role in its effectiveness. In this paper, we describe the development of the augmented reality pressing and evaluation system (AR-PETS), an augmented reality based prototype system for finger force control training. We discuss the development of the system, as well as the design considerations during the development of the system.","2019","2023-07-06 01:10:52","2023-07-20 05:50:35","2023-07-06 01:10:52","575-587","","","11593","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-22015-0_45","","","","","","Zhou, Jia; Salvendy, Gavriel","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5IYE46RN","journalArticle","2017","Dyer, J. F.; Stapleton, P.; Rodger, M. W. M.","Advantages of melodic over rhythmic movement sonification in bimanual motor skill learning","Experimental Brain Research","","0014-4819, 1432-1106","10.1007/s00221-017-5047-8","http://link.springer.com/10.1007/s00221-017-5047-8","","2017-10","2023-07-06 01:10:52","2023-07-06 01:10:52","2023-07-06 01:10:52","3129-3140","","10","235","","Exp Brain Res","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/ADANNAX4/Dyer et al. - 2017 - Advantages of melodic over rhythmic movement sonif.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VFD28Z9V","journalArticle","1997","Kaiser, Mary K.; Montegut, Michael J.","Of red planets and indigo computers: Mars database visualization as an example of platform downsizing","Behavior Research Methods, Instruments, &amp Computers","","0743-3808, 1532-5970","10.3758/BF03200566","http://link.springer.com/10.3758/BF03200566","The last decade has witnessed tremendous advancements in the computer hardware and software used to perform scientific visualization. In this paper, we consider how the visualization of a particular data set, the digital terrain model derived from the Viking orbiter imagery,has been realized in four distinct projects over this period. These examples serve to demonstrate how the vast improvements in computational performance both decrease the cost of such visualization efforts and permit an increasing level of interactivity. Wethen consider how even today's graphical systems require the visualization designer to make intelligent choices and tradeoffs in database rendering. Finally,we discuss how insights gleaned from an understanding of human visual perception can guide these design decisions, and suggest new options for visualization hardware and software.","1997-03","2023-07-06 01:10:52","2023-07-19 23:34:44","2023-07-06 01:10:52","48-53","","1","29","","Behavior Research Methods, Instruments, & Computers","Of red planets and indigo computers","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/SG9SCWQ4/Kaiser and Montegut - 1997 - Of red planets and indigo computers Mars database.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E4IDCSPK","bookSection","2021","Kailas, Ganesh; Tiwari, Nachiketa","Design for Immersive Experience: Role of Spatial Audio in Extended Reality Applications","Design for Tomorrow—Volume 2","9789811601187 9789811601194","","","https://link.springer.com/10.1007/978-981-16-0119-4_69","The incredible growth of extended reality (XR) applications will be leading us to a world beyond our imaginations in the coming decades. Extended reality is an umbrella term that encompasses different categories of immersive technologies like virtual reality (VR), augmented reality (AR), and mixed reality (MR). From the traditional applications like entertainment and training, XR has been spreading its wings into a large number of applications in health care, aerospace, product design and prototyping, e-commerce, workspace productivity, architecture, and building industries. Immersibility of the virtual reality scene into the physical world will be crucial for its acceptance by mainstream industries and future development. In addition to the virtual scene's visual perception, spatial audio is a key feature in designing truly immersive XR. Hearing is the fastest sense of humans, which makes virtual auditory display (VAD) an ineluctable part of any XR application. In this work, the importance of three-dimensional spatial audio in XR applications is explored in the user perspective approach. User experience (UX) is improved to a large extent when the applications make use of spatial audio compared to direction-less hearing experience. Spatial sound has a crucial role in giving information regarding actions in the background and beyond the field of view (FOV), and thus in making proper three-dimensional realism. However, designing user-dependent virtual audio reality is challenging because of its parametric dependence on human anthropometric features. This work also suggests the possibilities of utilizing computer-aided designing (CAD) and computer-aided engineering (CAE) tools in producing personalized virtual audio reality. While the immersive extended reality experience evolves as the next frontier in user experience designing, a sophisticated 3D audio experience will be there at the heart of it.","2021","2023-07-06 01:10:52","2023-07-19 23:54:56","2023-07-06 01:10:52","853-863","","","222","","","Design for Immersive Experience","","","","","Springer Singapore","Singapore","en","","","","","DOI.org (Crossref)","","Series Title: Smart Innovation, Systems and Technologies DOI: 10.1007/978-981-16-0119-4_69","","","","","","Chakrabarti, Amaresh; Poovaiah, Ravi; Bokil, Prasad; Kant, Vivek","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M4RWBWLE","journalArticle","1998","Loomis, Jack M.; Klatzky, Roberta L.; Philbeck, John W.; Golledge, Reginald G.","Assessing auditory distance perception using perceptually directed action","Perception & Psychophysics","","0031-5117, 1532-5962","10.3758/BF03211932","http://link.springer.com/10.3758/BF03211932","","1998-09","2023-07-06 01:10:52","2023-07-06 01:10:52","2023-07-06 01:10:52","966-980","","6","60","","Perception & Psychophysics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/EJCJITJF/Loomis et al. - 1998 - Assessing auditory distance perception using perce.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SZGUPRHM","journalArticle","2003","Pittarello, Fabio","Accessing information through multimodal 3D environments: towards universal access","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-003-0044-z","http://link.springer.com/10.1007/s10209-003-0044-z","3D environments represent a great opportunity for universal access to information, as they offer an intuitive interaction paradigm, similar to what is experienced by humans in their everyday lives. In spite of that, several 3D interfaces are characterized by poor structures and are hard to navigate. This paper presents the multimodal concept of the Interaction Locus (IL) as a means to give structure to 3D scenes, helping the user to interact with and access information inside them. The concept was initially developed with particular reference to desktop virtual reality (2.5 D virtual reality), but it is general enough to be extended to other contexts, such as real 3D scenes. The final part of this work shows how the IL concept addresses the need for a unified authoring methodology, capable of allowing access to different target user groups from a variety of different devices.","2003-06-01","2023-07-06 01:10:52","2023-07-21 05:12:31","2023-07-06 01:10:52","189-204","","2","2","","Universal Access in the Information Society","Accessing information through multimodal 3D environments","","","","","","","","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TYEFVHBL","journalArticle","1984","O’Leary, Ann; Rhodes, Gillian","Cross-modal effects on visual and auditory object perception","Perception & Psychophysics","","0031-5117, 1532-5962","10.3758/BF03205954","http://link.springer.com/10.3758/BF03205954","Cross-modal influences on perceptual organization were demonstrated using adisplay that combined astimulus for auditory stream segregation with its visual apparent movement analogue. Both phenomena give rise to the perception of either one or two objects, depending on the rate of presentation of the stimuli. At slower rates, one object is perceived, while two are perceived at faster rates. Subjects indicated the stimulus onset asynchrony (SOA) between successive stimuli at which the perceptual shift occurred in each modality. Then visual and auditory stimuli were presented concurrently and subjects responded to the ""target"" modality sequence. Two intergroup separations for the nontarget stimuli were used. Distances were chosen, based on the subject's calibration data, which represented one and two objects, respectively, at the stream segregation point for the target sequence. Segregation occurred at larger SOAs when the nontarget stimulation indicated two objects than when it represented one. This was true for both visual and auditory target sequences.","1984-11","2023-07-06 01:10:52","2023-07-21 04:44:21","2023-07-06 01:10:52","565-569","","6","35","","Perception & Psychophysics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/AF5P4DC9/O’Leary and Rhodes - 1984 - Cross-modal effects on visual and auditory object .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J993ZY8H","bookSection","2010","Bakker, Saskia; Van Den Hoven, Elise; Eggen, Berry","Exploring Interactive Systems Using Peripheral Sounds","Haptic and Audio Interaction Design","978-3-642-15840-7 978-3-642-15841-4","","","http://link.springer.com/10.1007/978-3-642-15841-4_7","Our everyday interaction in and with the physical world, has facilitated the development of auditory perception skills that enable us to selectively place one auditory channel in the center of our attention and simultaneously monitor others in the periphery. We search for ways to leverage these auditory perception skills in interactive systems. In this paper, we present three working demonstrators that use sound to subtly convey information to users in an open office. To qualitatively evaluate these demonstrators, each of them has been implemented in an office for three weeks. We have seen that such a period of time, sounds can start shifting from the center to the periphery of the attention. Furthermore, we found several issues to be addressed when designing such systems, which can inform future work in this area.","2010","2023-07-06 01:10:52","2023-07-20 00:14:17","2023-07-06 01:10:52","55-64","","","6306","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-15841-4_7","","/Users/minsik/Zotero/storage/ZMRRTALN/Bakker et al. - 2010 - Exploring Interactive Systems Using Peripheral Sou.pdf","","","","Nordahl, Rolf; Serafin, Stefania; Fontana, Federico; Brewster, Stephen","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DIMAQICE","bookSection","2007","Bologna, Guido; Deville, Benoît; Pun, Thierry; Vinckenbosch, Michel","Identifying Major Components of Pictures by Audio Encoding of Colours","Nature Inspired Problem-Solving Methods in Knowledge Engineering","978-3-540-73054-5 978-3-540-73055-2","","","http://link.springer.com/10.1007/978-3-540-73055-2_10","","2007","2023-07-06 01:10:52","2023-07-06 01:10:52","2023-07-06 01:10:52","81-89","","","4528","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73055-2_10","","","","","","Mira, José; Álvarez, José R.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JT36GZKR","bookSection","2007","Mahmud, Murni; Sporka, Adam J.; Kurniawan, Sri H.; Slavík, Pavel","A Comparative Longitudinal Study of Non-verbal Mouse Pointer","Human-Computer Interaction – INTERACT 2007","978-3-540-74799-4 978-3-540-74800-7","","","http://link.springer.com/10.1007/978-3-540-74800-7_44","","2007","2023-07-06 01:10:52","2023-07-06 01:10:52","2023-07-06 01:10:52","489-502","","","4663","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-74800-7_44","","/Users/minsik/Zotero/storage/XKB3ZMX3/Mahmud et al. - 2007 - A Comparative Longitudinal Study of Non-verbal Mou.pdf","","","","Baranauskas, Cécilia; Palanque, Philippe; Abascal, Julio; Barbosa, Simone Diniz Junqueira","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4XSCJTIW","bookSection","2012","Kim, Hee-Cheol","An Experimental Study to Explore Usability Problems of Interactive Voice Response Systems","Intelligent Information and Database Systems","978-3-642-28492-2 978-3-642-28493-9","","","http://link.springer.com/10.1007/978-3-642-28493-9_19","While interactive voice response (IVR) systems employing touch tone interface (TTI) are popularly used these days, they are generally known for their inconvenience. This is not only because of the characteristics that TTI inherently has, but also because of lack of understanding of IVR system users. This study is aimed at contributing to capture an understanding of the users, which eventually leads to better system design. In particular, we have developed an IVR system simulator to enable efficient, flexible, and rich usability tests for IVR systems. This paper presents an experimental study on usability of IVR systems utilizing the simulator. In the experiment, 41 subjects performed three different tasks concerning phone charges using the simulator to identify various usability problems. The analytic results are hopefully a basis for user-centered IVR systems design.","2012","2023-07-06 01:12:37","2023-07-20 06:35:11","2023-07-06 01:12:37","169-177","","","7198","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-28493-9_19","","","","","","Pan, Jeng-Shyang; Chen, Shyi-Ming; Nguyen, Ngoc Thanh","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I2DXKVE8","bookSection","1998","Brewster, Stephen","Using Earcons to Improve the Usability of a Graphics Package","People and Computers XIII","978-3-540-76261-4 978-1-4471-3605-7","","","http://link.springer.com/10.1007/978-1-4471-3605-7_18","This paper describes how non–speech sounds can be used to improve the usability of a graphics package. Sound was specifically used to aid problems with tool palettes and finding the current mouse coordinates when drawing. Tool palettes have usability problems because users need to see the information they present but they are often outside the area of visual focus. An experiment was conducted to investigate the effectiveness of adding sound to tool palettes. Earcons were used to indicate the current tool and when tool changes occurred. Results showed a significant reduction in the number of tasks performed with the wrong tool. Therefore users knew what the current tool was and did not try to perform tasks with the wrong tool. All of this was not at the expense of making the interface any more annoying to use.","1998","2023-07-06 01:12:37","2023-07-21 04:41:52","2023-07-06 01:12:37","287-302","","","","","","","","","","","Springer London","London","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-1-4471-3605-7_18","","/Users/minsik/Zotero/storage/PZG5K4EX/Brewster - 1998 - Using Earcons to Improve the Usability of a Graphi.pdf","","","","Johnson, Hilary; Nigay, Lawrence; Roast, Christopher","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I99E9DEG","bookSection","2008","Mendels, Philip; Frens, Joep","The Audio Adventurer: Design of a Portable Audio Adventure Game","Fun and Games","978-3-540-88321-0 978-3-540-88322-7","","","http://link.springer.com/10.1007/978-3-540-88322-7_5","In this paper we describe the design of a portable device for playing audio adventure games. This device enables the player to explore an audio world, interact with it, and solve challenges while a narrative evolves. To avoid the difficulties that can arise when freely navigating open spaces in audio-only worlds, we structured our audio world as a network of paths. Different ways of panning the audio to fit this model are proposed. Two initial rotational devices for navigating the audio world were created and evaluated: a relative and an absolute one. The relative one was worked out to a final prototype. Inventory functionality was added to increase the interactive possibilities and to make the device more expressive. Initial reactions were positive, but additional content and experiments are needed to investigate whether the Audio Adventurer can offer a long-lasting immersive and engaging experience.","2008","2023-07-06 01:12:37","2023-07-20 00:07:08","2023-07-06 01:12:37","46-58","","","5294","","","The Audio Adventurer","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-88322-7_5","","","","","","Markopoulos, Panos; De Ruyter, Boris; IJsselsteijn, Wijnand; Rowland, Duncan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FRRQSGEX","journalArticle","1975","Fulgosi, Ante; Zaja, Bozo","Information transmission of 3.1 bits in absolute identification of auditory pitch","Bulletin of the Psychonomic Society","","0090-5054","10.3758/BF03333207","http://link.springer.com/10.3758/BF03333207","Eleven tones (from 50 to 11,000 Hz), differing only in pitch, were presented to five subjects 108 times each. The subjects were told that in the ""training"" phase of the experiment, they should reach a definite, but unspecified, ""level"" of performance before they could advance to the final or ""test"" phase of the experiment. In the final quarter of the experiment, the median value of the identification performance was 3.11 bits.","1975-10","2023-07-06 01:12:37","2023-07-19 23:37:37","2023-07-06 01:12:37","379-380","","4","6","","Bull. Psychon. Soc.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/D6EFCRUA/Fulgosi and Zaja - 1975 - Information transmission of 3.1 bits in absolute i.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5ATY4GIU","journalArticle","2021","Salselas, Inês; Penha, Rui; Bernardes, Gilberto","Sound design inducing attention in the context of audiovisual immersive environments","Personal and Ubiquitous Computing","","1617-4909, 1617-4917","10.1007/s00779-020-01386-3","https://link.springer.com/10.1007/s00779-020-01386-3","Sound design has been a fundamental component of audiovisual storytelling in linear media. However, with recent technological developments and the shift towards non-linear and immersive media, things are rapidly changing. More sensory information is available and, at the same time, the user is gaining agency upon the narrative, being offered the possibility of navigating or making other decisions. These new characteristics of immersive environments bring new challenges to storytelling in interactive narratives and require new strategies and techniques for audiovisual narrative progression. Can technology offer an immersive environment where the user has the sensation of agency, of choice, where her actions are not mediated by evident controls but subliminally induced in a way that it is ensured that a narrative is being followed? Can sound be a subliminal element that induces attentional focus on the most relevant elements for the narrative, inducing storytelling and biasing search in an immersive non-linear audiovisual environment? Herein, we present a literature review that has been guided by this prospect. With these questions in view, we present our exploration process in finding possible answers and potential solution paths. We point out that consistency, in terms of coherency across sensory modalities and emotional matching may be a critical aspect. Finally, we consider that this review may open up new paths for experimental studies that could, in the future, provide new strategies in the practice of sound design in the context of non-linear media.","2021-08","2023-07-06 01:12:37","2023-07-21 04:54:19","2023-07-06 01:12:37","737-748","","4","25","","Pers Ubiquit Comput","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"72LSQMME","journalArticle","2020","Groß-Vogt, Katharina; Frank, Matthias; Höldrich, Robert","Focused Audification and the optimization of its parameters","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-019-00317-8","http://link.springer.com/10.1007/s12193-019-00317-8","Abstract             We present a sonification method which we call Focused Audification (FA; previously: Augmented Audification) that allows to expand pure audification in a flexible way. It is based on a combination of single-side-band modulation and a pitch modulation of the original data stream. Based on two free parameters, the sonification’s frequency range is adjustable to the human hearing range and allows to interactively zoom into the data set at any scale. The parameters have been adjusted in a multimodal experiment on cardiac data by laypeople. Following from these results we suggest a procedure for parameter optimization to achieve an optimal listening range for any data set, adjusted to human speech.","2020-06","2023-07-06 01:12:37","2023-07-06 01:12:37","2023-07-06 01:12:37","187-198","","2","14","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/2NA6KK5Q/Groß-Vogt et al. - 2020 - Focused Audification and the optimization of its p.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KGIBLVYM","bookSection","2014","Taibbi, Marzia; Bernareggi, Cristian; Gerino, Andrea; Ahmetovic, Dragan; Mascetti, Sergio","AudioFunctions: Eyes-Free Exploration of Mathematical Functions on Tablets","Computers Helping People with Special Needs","978-3-319-08595-1 978-3-319-08596-8","","","http://link.springer.com/10.1007/978-3-319-08596-8_84","","2014","2023-07-06 01:12:37","2023-07-06 01:12:37","2023-07-06 01:12:37","537-544","","","8547","","","AudioFunctions","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-08596-8_84","","","","","","Miesenberger, Klaus; Fels, Deborah; Archambault, Dominique; Peňáz, Petr; Zagler, Wolfgang","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FNF4V7W4","journalArticle","2022","Mitre-Ortiz, Andres; Muñoz-Arteaga, Jaime; Cardona-Reyes, Héctor","Developing a model to evaluate and improve user experience with hand motions in virtual reality environments","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-022-00882-y","https://link.springer.com/10.1007/s10209-022-00882-y","In video games, the evaluation of the user experience (UX) mainly refers to two main groups of aspects, those that refer to the player that is mainly oriented to make the player feel good while playing and those that refer to the video game that is oriented to make the video game easy to understand and play. The aspects considered that are related to the player are engagement, enjoyment, and flow; the aspects related to video game, usability, and dependability. Virtual reality environments today have changed the paradigm in various fields of application, such as health, education, entertainment, among others. Therefore, it is important to observe the effects of handedness with hand movements in virtual reality environments. This work proposes a model to evaluate and improve the user experience considering player and video game aspects, taking into account handedness with hand movements in virtual reality environments. Player and video game aspects can be added to evaluations of the effect of handedness, especially in virtual reality environments, in order to know the user’s behavior in terms of skill, performance, and accuracy, among other features by using a particular hand to perform specific tasks. Next, a case study is presented with two groups of users using a virtual reality environment to perform several user tasks considering the dominant and non-dominant hand. By evaluating the user tasks it is possible to know the levels of engagement, enjoyment, motivation, and usability in a virtual reality environment. Finally, an analysis of results is presented in which several improvements of UX are presented.","2022-05-26","2023-07-06 01:12:37","2023-07-21 05:12:09","2023-07-06 01:12:37","","","","","","Univ Access Inf Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/AZ72SMY5/Mitre-Ortiz et al. - 2022 - Developing a model to evaluate and improve user ex.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3YB7BJ6K","bookSection","2005","Adachi, Kazuya; Iwai, Ken’ichiro; Yamada, Eiji; Cohen, Michael","Multimodal Wayfinding in a Driving Simulator for the $^{\rm {\bf S}}_{\rm {\bf c}}{\rm {\bf ha}}_{\rm {\bf i}}{\rm {\bf r}}^{\rm {\bf e}}$ Internet Chair, a Networked Rotary Motion Platform","Entertainment Computing - ICEC 2005","978-3-540-29034-6 978-3-540-32054-8","","","http://link.springer.com/10.1007/11558651_50","We are exploring idss (intelligent driver support systems), especially including way-finding presented via spatial audio. (“Way-finding” refers to giving a driver directions, as via car navigation [“Car-Nabi”] gps/gis systems.) We have developed a networked driving simulator as a virtual-reality based interface (control/display system) featuring integration with the Schaire rotary motion platform for azimuth-display, stereographic display for 3d graphics, and spatial audio (sound spatialization) way-finding cues.","2005","2023-07-06 01:12:37","2023-07-19 23:59:26","2023-07-06 01:12:37","511-514","","","3711","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11558651_50","","","","","","Kishino, Fumio; Kitamura, Yoshifumi; Kato, Hirokazu; Nagata, Noriko","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YQ2IX2CC","journalArticle","2022","Norwood, Michael Francis; Lakhani, Ali; Watling, David Phillip; Marsh, Chelsea Hannah; Zeeman, Heidi","Efficacy of Multimodal Sensory Therapy in Adult Acquired Brain Injury: A Systematic Review","Neuropsychology Review","","1040-7308, 1573-6660","10.1007/s11065-022-09560-5","https://link.springer.com/10.1007/s11065-022-09560-5","Abstract                            Adults who experience an acquired brain injury often experience disorders of consciousness, physical difficulties, and maladaptive behaviours. Multimodal sensory therapy may benefit brain injured patients, however the extent this therapy can facilitate rehabilitation is not well understood. This systematic review aimed to synthesize multimodal sensory therapy research for adults affected by acquired brain injury. PRISMA guidelines were followed and searches for work published up until July 2021 were undertaken in 5 databases, finding 1054 articles. 43 articles were included in the study. Results describe 29 studies related to coma following an acquired brain injury and 14 to no coma studies (mostly stroke). Multimodal sensory therapy was mostly used as a coma arousal technique following traumatic brain injury, finding positive effects. Multimodal sensory therapy was less applied in stroke, no coma rehabilitation, where most studies found improvement in somatosensory sensation and motor control in an affected limb. In several no coma studies, effects were maintained after several months. The most common senses stimulated in coma studies were audio (               N                = 30), tactile (               N                = 28), visual (               N                = 26), olfactory (               N                = 22), and gustatory (               N                = 17), while the most common senses stimulated in stroke, no coma studies were proprioception (               N                = 7), tactile (               N                = 8), and stereognosis (               N                = 4). Multimodal sensory therapy can be beneficial for patients, especially those in a minimally conscious state or attempting physical rehabilitation following stroke. Negative findings are infrequent in the current literature base. Multimodal sensory therapy appears to be a low-risk intervention with positive outcomes.","2022-09-02","2023-07-06 01:12:37","2023-07-06 01:12:37","2023-07-06 01:12:37","","","","","","Neuropsychol Rev","Efficacy of Multimodal Sensory Therapy in Adult Acquired Brain Injury","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/38IS75QI/Norwood et al. - 2022 - Efficacy of Multimodal Sensory Therapy in Adult Ac.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G6QCB5PK","bookSection","2009","Eslambochilar, Parisa; Buchanan, George; Loizides, Fernando","Hear It Is: Enhancing Rapid Document Browsing with Sound Cues","Research and Advanced Technology for Digital Libraries","978-3-642-04345-1 978-3-642-04346-8","","","http://link.springer.com/10.1007/978-3-642-04346-8_9","","2009","2023-07-06 01:12:37","2023-07-06 01:12:37","2023-07-06 01:12:37","75-86","","","5714","","","Hear It Is","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-04346-8_9","","/Users/minsik/Zotero/storage/QE77ZCAR/Eslambochilar et al. - 2009 - Hear It Is Enhancing Rapid Document Browsing with.pdf","","","","Agosti, Maristella; Borbinha, José; Kapidakis, Sarantos; Papatheodorou, Christos; Tsakonas, Giannis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LKHCV6Q7","journalArticle","2017","Yuan, Yougen; Xie, Lei; Fu, Zhong-Hua; Xu, Ming; Cong, Qi","Sound image externalization for headphone based real-time 3D audio","Frontiers of Computer Science","","2095-2228, 2095-2236","10.1007/s11704-016-6182-2","http://link.springer.com/10.1007/s11704-016-6182-2","3D audio effects can provide immersive auditory experience, but we often face the so-called in-head localization (IHL) problem in headphone sound reproduction. To address this problem, we propose an effective sound image externalization approach. Specifically, we consider several important factors related to sound propagation, which include image-source model based early reflections with distance decay, wall absorption and air absorption, late reverberation and other dynamic factors like head movement. We apply our sound image externalization approach to a headphone based real-time 3D audio system. Subjective listening tests show that the sound image externalization performance is significantly improved and the sound source direction is preserved as well. A/B preference test further shows that, as compared with a recent popular approach, the proposed approach is mostly preferred by the listeners.","2017-06","2023-07-06 01:12:37","2023-07-20 00:06:59","2023-07-06 01:12:37","419-428","","3","11","","Front. Comput. Sci.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZE9LK3WP","bookSection","2011","Rhee Oh, Yoo; Kim, Hong Kook; Choi, Seung Ho","An Efficient Approach to Machine Control Sound Generation for Interfacing Real and Virtual Environments","Informatics Engineering and Information Science","978-3-642-25326-3 978-3-642-25327-0","","","http://link.springer.com/10.1007/978-3-642-25327-0_43","This paper presents the motivations and methods to generate the machine control sound for the virtual environments. The factors of machine control sound are investigated to handle the sound efficiently in the virtual systems and some of the controllable factors are implemented. First, we propose a new sound file format to find or generate the proper sound with the sound factors in the virtual systems that cause various kind of events and sounds. Then, we apply the proposed sound generating technique to the virtual system for a reality model, especially focused on the MP player.","2011","2023-07-06 01:12:37","2023-07-20 06:33:56","2023-07-06 01:12:37","507-516","","","251","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Communications in Computer and Information Science DOI: 10.1007/978-3-642-25327-0_43","","","","","","Abd Manaf, Azizah; Zeki, Akram; Zamani, Mazdak; Chuprat, Suriayati; El-Qawasmeh, Eyas","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CXYF83H5","bookSection","1999","Fenton-Kerr, Tom","GAIA: An Experimental Pedagogical Agent for Exploring Multimodal Interaction","Computation for Metaphors, Analogy, and Agents","978-3-540-65959-4 978-3-540-48834-7","","","http://link.springer.com/10.1007/3-540-48834-0_9","This paper discusses GAIA (Graphic-Audio Interface Agent), an experimental interface agent used in a pedagogical simulation program, REM(the Re-mapping Europa Mission), where the learning task is the discrimination of specific locations on a series of unlabelled maps. The agent’s task is to enhance the learning experience by providing timely, contextual clues mediated through a graphic/audio interface. Factors that influence such an agent’s ability to provide effective help, such as modes of agent representation, are discussed in the context of differing uses requiring alternative mode choices. The experimental context is explored with an in-depth look at the REM program. The paper concludes with comments on audio interfaces, suggestions for multimodal agent design and likely future directions for multimodal agent interfaces.","1999","2023-07-06 01:12:37","2023-07-19 23:47:17","2023-07-06 01:12:37","154-164","","","1562","","","GAIA","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-48834-0_9","","","","","","Nehaniv, Chrystopher L.","Goos, G.; Hartmanis, J.; Van Leeuwen, J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JHW4YTWV","bookSection","2013","Singh, Surya P. N.; Pounds, Paul E. I.; Kurniawati, Hanna","I-Ball: A Programmable Sporting Aid for Children with a Visual Impairment to Play Soccer","Universal Access in Human-Computer Interaction. Design Methods, Tools, and Interaction Techniques for eInclusion","978-3-642-39187-3 978-3-642-39188-0","","","http://link.springer.com/10.1007/978-3-642-39188-0_63","","2013","2023-07-06 01:12:37","2023-07-06 01:12:37","2023-07-06 01:12:37","584-591","","","8009","","","I-Ball","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-39188-0_63","","","","","","Stephanidis, Constantine; Antona, Margherita","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5NZKJ5CJ","bookSection","2010","Nordahl, Rolf; Berrezag, Amir; Dimitrov, Smilen; Turchet, Luca; Hayward, Vincent; Serafin, Stefania","Preliminary Experiment Combining Virtual Reality Haptic Shoes and Audio Synthesis","Haptics: Generating and Perceiving Tangible Sensations","978-3-642-14074-7 978-3-642-14075-4","","","http://link.springer.com/10.1007/978-3-642-14075-4_18","We describe a system that can provide combined auditory and haptic sensations that arise while walking on different grounds. The simulation is based on a physical model that drives both haptic transducers embedded in sandals and headphones. The model is able to represent walking interactions with solid surfaces that can creak, be covered with crumpling material. The simulation responds to pressure on the floor by a vibrotactile signal felt by the feet. In a preliminary discrimination experiment, 15 participants were asked to recognize four different surfaces in a list of sixteen possibilities and under three different conditions, haptics only, audition only and combined haptic-audition. The results indicate that subjects are able to recognize most of the stimuli in the audition only condition, and some of the material properties such as hardness in the haptics only condition. The combination of auditory and haptic cues does not significantly improve recognition.","2010","2023-07-06 01:12:37","2023-07-20 00:17:04","2023-07-06 01:12:37","123-129","","","6192","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-14075-4_18","","/Users/minsik/Zotero/storage/GT94Z256/Nordahl et al. - 2010 - Preliminary Experiment Combining Virtual Reality H.pdf","","","","Kappers, Astrid M. L.; Van Erp, Jan B. F.; Bergmann Tiest, Wouter M.; Van Der Helm, Frans C. T.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XAXYS2C7","bookSection","2005","Bologna, Guido; Vinckenbosch, Michel","Eye Tracking in Coloured Image Scenes Represented by Ambisonic Fields of Musical Instrument Sounds","Mechanisms, Symbols, and Models Underlying Cognition","978-3-540-26298-5 978-3-540-31672-5","","","http://link.springer.com/10.1007/11499220_34","We present our recent project on visual substitution by Ambisonic 3D-sound fields. Ideally, our system should be used by blind or visually impaired subjects having already seen. The original idea behind our targeted prototype is the use of an eye tracker and musical instrument sounds encoding coloured pixels. The role of the eye tracker is to activate the process of attention inherent in the vision and to restore by simulation the mechanisms of central and peripheral vision. Moreover, we advocate the view that cerebral areas devoted to the integration of information will play a role by rebuilding a global image of the environment. Finally, the role of colour itself is to help subjects distinguishing coloured objects or perceiving textures, such as sky, walls, grass and trees, etc ...","2005","2023-07-06 01:12:37","2023-07-21 04:28:55","2023-07-06 01:12:37","327-337","","","3561","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11499220_34","","","","","","Mira, José; Álvarez, José R.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RFI6NRC7","bookSection","2007","Kimura, Atsunobu; Ihara, Masayuki; Kobayashi, Minoru; Manabe, Yoshitsugu; Chihara, Kunihiro","Visual Feedback: Its Effect on Teleconferencing","Human-Computer Interaction. HCI Applications and Services","978-3-540-73109-2","","","http://link.springer.com/10.1007/978-3-540-73111-5_67","We present shared visual feedback for supporting conversations in contingent auditory environments like teleconferences. To facilitate the initiation of conversations in such environments, it is critical that the caller be able to grasp the auditory channel between the caller’s mouth and the receiver’s ear, and to vocalize at the voice level proper for the receiver. To achieve this goal, feedback of the voice level as measured at the receiver’s ear is needed. Our starting points were a first generation prototype that displays visual feedback on the caller’s screen and a second generation prototype that projects visual feedback onto the floor in the receiver’s room. To enhance the speaker’s assurance and to make installation easier, a third prototype is implemented as a LED device and a microphone. An experiment is conducted on the third prototype and its effectiveness in supporting natural conversations in telecommunication sessions in daily use environments is confirmed.","2007","2023-07-06 01:14:13","2023-07-20 06:31:17","2023-07-06 01:14:13","591-600","","","4553","","","Visual Feedback","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73111-5_67","","","","","","Jacko, Julie A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FDCNTN8U","journalArticle","2022","Kwon, Nahyun; Lee, Yunjung; Oh, Uran","Supporting a crowd-powered accessible online art gallery for people with visual impairments: a feasibility study","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-021-00814-2","https://link.springer.com/10.1007/s10209-021-00814-2","While people with visual impairments are interested in artwork as much as their sighted peers, their experience is limited to few selective artworks that are exhibited at certain museums. To enable people with visual impairments to access and appreciate as many artworks as possible at ease, we propose an online art gallery that allows users to explore different parts of a painting displayed on their touchscreen-based devices while listening to corresponding verbal descriptions of the touched part on the screen. To investigate the scalability of our approach, we first explored if anonymous crowd who may not have expertise in art are capable of providing visual descriptions of artwork as a preliminary study. Then we conducted a user study with 9 participants with visual impairments to explore the potential of our system for independent artwork appreciation by assessing if and how well the system supports 4 steps of Feldman Model of Criticism. The findings suggest that visual descriptions of artworks produced by an anonymous crowd are sufficient for people with visual impairments to interpret and appreciate paintings with their own judgments which is different from existing approaches that focused on delivering descriptions and opinions written by art experts. Based on the lessons learned from the study, we plan to collect visual descriptions of a greater number of artwork and distribute our online art gallery publicly to make more paintings accessible for people with visual impairments.","2022-11","2023-07-06 01:14:13","2023-07-21 05:11:41","2023-07-06 01:14:13","967-982","","4","21","","Univ Access Inf Soc","Supporting a crowd-powered accessible online art gallery for people with visual impairments","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/SWX8Z9AG/Kwon et al. - 2022 - Supporting a crowd-powered accessible online art g.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J5Y8HEUJ","journalArticle","2022","Chai, Yujin; Weng, Yanlin; Wang, Lvdi; Zhou, Kun","Speech-driven facial animation with spectral gathering and temporal attention","Frontiers of Computer Science","","2095-2228, 2095-2236","10.1007/s11704-020-0133-7","https://link.springer.com/10.1007/s11704-020-0133-7","In this paper, we present an efficient algorithm that generates lip-synchronized facial animation from a given vocal audio clip. By combining spectral-dimensional bidirectional long short-term memory and temporal attention mechanism, we design a light-weight speech encoder that learns useful and robust vocal features from the input audio without resorting to pre-trained speech recognition modules or large training data. To learn subject-independent facial motion, we use deformation gradients as the internal representation, which allows nuanced local motions to be better synthesized than using vertex offsets. Compared with state-of-the-art automatic-speech-recognition-based methods, our model is much smaller but achieves similar robustness and quality most of the time, and noticeably better results in certain challenging cases.","2022-06","2023-07-06 01:14:13","2023-07-20 00:06:50","2023-07-06 01:14:13","163703","","3","16","","Front. Comput. Sci.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9X826PHN","journalArticle","2017","Dawid, Herbert; Decker, Reinhold; Hermann, Thomas; Jahnke, Hermann; Klat, Wilhelm; König, Rolf; Stummer, Christian","Management science in the era of smart consumer products: challenges and research perspectives","Central European Journal of Operations Research","","1435-246X, 1613-9178","10.1007/s10100-016-0436-9","http://link.springer.com/10.1007/s10100-016-0436-9","","2017-03","2023-07-06 01:14:13","2023-07-06 01:14:13","2023-07-06 01:14:13","203-230","","1","25","","Cent Eur J Oper Res","Management science in the era of smart consumer products","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/8BW59UPB/Dawid et al. - 2017 - Management science in the era of smart consumer pr.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LINLKGUS","bookSection","2010","Graf, Christian","Verbally Annotated Tactile Maps – Challenges and Approaches","Spatial Cognition VII","978-3-642-14748-7 978-3-642-14749-4","","","http://link.springer.com/10.1007/978-3-642-14749-4_26","Survey knowledge of spatial environments can be successfully conveyed by visual maps. For visually impaired people, tactile maps have been proposed as a substitute. The latter are hard to read and to understand. This paper proposes how the cognitive disadvantages can be compensated for by Verbally Annotated Tactile (VAT) maps. VAT maps combine two representational components: a verbal annotation system as a propositional component and a tactile map as a spatial component. It is argued that users will benefit from the cross-modal interaction of both. In a pilot study it is shown that using tactile You-Are-Here maps that only implement the spatial component is not optimal. I argue that some of the problems observed can be compensated for by incorporating verbal annotations. Research questions on cross-modal interaction in VAT maps are formulated that address the challenges that have to be overcome in order to benefit from propositional and spatial representations induced by VAT maps.","2010","2023-07-06 01:14:13","2023-07-21 05:02:28","2023-07-06 01:14:13","303-318","","","6222","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-14749-4_26","","","","","","Hölscher, Christoph; Shipley, Thomas F.; Olivetti Belardinelli, Marta; Bateman, John A.; Newcombe, Nora S.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5QBZZG2T","journalArticle","2017","Wang, Qi; Markopoulos, Panos; Yu, Bin; Chen, Wei; Timmermans, Annick","Interactive wearable systems for upper body rehabilitation: a systematic review","Journal of NeuroEngineering and Rehabilitation","","1743-0003","10.1186/s12984-017-0229-y","http://jneuroengrehab.biomedcentral.com/articles/10.1186/s12984-017-0229-y","","2017-12","2023-07-06 01:14:13","2023-07-06 01:14:13","2023-07-06 01:14:13","20","","1","14","","J NeuroEngineering Rehabil","Interactive wearable systems for upper body rehabilitation","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/EEIRBVFR/Wang et al. - 2017 - Interactive wearable systems for upper body rehabi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JLXLCGIH","bookSection","2015","Yonezawa, Tomoko","Auditory Browsing Interface of Ambient and Parallel Sound Expression for Supporting One-to-many Communication","Distributed, Ambient, and Pervasive Interactions","978-3-319-20803-9 978-3-319-20804-6","","","http://link.springer.com/10.1007/978-3-319-20804-6_21","In this paper, we introduce an auditory browsing system for supporting one-to-many communication in parallel with an ongoing discourse, lecture, or presentation. The live reactions of audiences should reflect the main speech from the viewpoint of active participation. In order to browse numerous live comments from audiences, the speaker stretches her/his neck toward a particular section of the virtual audience group. We adopt the metaphor of “looking inside” toward the direction of the seating position with repositioned and overlaid audiences’ voices corresponding to the length of the voice regardless of the seating of real audiences. As a result, the speaker could browse the comments of the audience and show the communicative behaviors when she/he was interested in a particular group of the audience’s utterances.","2015","2023-07-06 01:14:13","2023-07-19 23:58:12","2023-07-06 01:14:13","224-233","","","9189","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-20804-6_21","","/Users/minsik/Zotero/storage/39WJEZH4/Yonezawa - 2015 - Auditory Browsing Interface of Ambient and Paralle.pdf","","","","Streitz, Norbert; Markopoulos, Panos","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UDMM9HHF","bookSection","2001","Walker, Ashley; Brewster, Stephen; McGookin, David; Ng, Adrian","Diary in the Sky: A Spatial Audio Display for a Mobile Calendar","People and Computers XV—Interaction without Frontiers","978-1-85233-515-1 978-1-4471-0353-0","","","http://link.springer.com/10.1007/978-1-4471-0353-0_33","","2001","2023-07-06 01:14:13","2023-07-06 01:14:13","2023-07-06 01:14:13","531-539","","","","","","Diary in the Sky","","","","","Springer London","London","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-1-4471-0353-0_33","","/Users/minsik/Zotero/storage/PB6HN3RT/Walker et al. - 2001 - Diary in the Sky A Spatial Audio Display for a Mo.pdf","","","","Blandford, Ann; Vanderdonckt, Jean; Gray, Phil","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I8KEESWQ","bookSection","2018","Ustek, Dilan; Chow, Kevin; Zhang, Haihua; MacLean, Karon","A Multimodal Illusion of Force Improves Control Perception in Above-Surface Gesture: Elastic Zed-Zoom","Haptics: Science, Technology, and Applications","978-3-319-93444-0 978-3-319-93445-7","","","http://link.springer.com/10.1007/978-3-319-93445-7_26","Emerging above-surface technology is an opportunity to exploit interaction spaces above a device’s surface; however, the resulting loss of the proprioceptive feedback available from on-surface interactions degrades the user’s sense of control and precision. We asked whether a pseudohaptic illusion (PHI) could help: a sense of force in the absence of actual contact, induced by manipulating the relation of body motion to graphical and auditory cues. To examine the value of above-surface PHIs, we used a zooming microtask, because finger occlusion impedes current implementations on small displays such as smartwatches. In a qualitative study (N = 12), we were able to trigger a physical illusion most often described as elasticity in 92% of participants through physical control/graphical display (C/D) manipulation, and that audio cues significantly strengthened the illusion. Participants experiencing this PHI reported improved sense of control when zooming, and found the interaction’s physicality natural.","2018","2023-07-06 01:14:13","2023-07-20 00:17:17","2023-07-06 01:14:13","295-308","","","10893","","","A Multimodal Illusion of Force Improves Control Perception in Above-Surface Gesture","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-93445-7_26","","","","","","Prattichizzo, Domenico; Shinoda, Hiroyuki; Tan, Hong Z.; Ruffaldi, Emanuele; Frisoli, Antonio","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VUYPTH23","bookSection","2007","Brodersen, Christina; Iversen, Ole Sejer","Dressing up for school work: Supporting a collaborative environment with heterogeneous technologies","ECSCW 2007","978-1-84800-030-8","","","http://link.springer.com/10.1007/978-1-84800-031-5_14","This paper approaches heterogeneity and heterogeneous technology as assets, rather than limitations, in the development of computer supported cooperative work. We demonstrate how heterogeneous technologies sustain teachers’ and students’ school work by presenting four different prototypes (the HyConExplorer, the eCell, the iGame- Floor and the eBag) that complement one another because they offer different functionalities and are, at the same time, designed with the wholeness of school activities, particularly group-based ones, in mind. Thus, they provide teachers and students with a broad range of IT support to aid them in and outside of the classroom. We take the school domain as our point of departure, but argue that the focus on heterogeneous technologies is applicable for the general area of CSCW.","2007","2023-07-06 01:14:13","2023-07-19 23:58:23","2023-07-06 01:14:13","251-270","","","","","","Dressing up for school work","","","","","Springer London","London","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-1-84800-031-5_14","","","","","","Bannon, Liam J.; Wagner, Ina; Gutwin, Carl; Harper, Richard H. R.; Schmidt, Kjeld","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3V9ZX7T5","bookSection","2017","Song, Yucheng; Wang, Xiaochen; Yang, Cheng; Gao, Ge; Chen, Wei; Tu, Weiping","Frame-Independent and Parallel Method for 3D Audio Real-Time Rendering on Mobile Devices","MultiMedia Modeling","978-3-319-51813-8 978-3-319-51814-5","","","http://link.springer.com/10.1007/978-3-319-51814-5_19","As 3D audio is a fundamental medium of virtual reality (VR), 3D audio real-time rendering technique is essential for the implementation of VR, especially on the mobile devices. While constrained by the limited computational power, the computation load is too high to implement 3D audio real-time rendering on the mobile devices. To solve this problem, we propose a frame-independent and parallel method of framing convolution, to parallelize process of 3D audio rendering using head-related transfer function (HRTF). In order to refrain from the dependency of overlap-add convolution over the adjacent frames, the data of convolution result is added on the final results of the two adjacent frames. We found our method could reduce the calculation time of 3D audio rendering significantly. The results were 0.74 times, 0.5 times and 0.36 times the play duration of si03.wav (length of 27 s), with Snapdragon 801, Kirin 935 and Helio X10 Turbo, respectively.","2017","2023-07-06 01:14:13","2023-07-21 04:30:58","2023-07-06 01:14:13","221-232","","","10133","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-51814-5_19","","","","","","Amsaleg, Laurent; Guðmundsson, Gylfi Þór; Gurrin, Cathal; Jónsson, Björn Þór; Satoh, Shin’ichi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"55GZQ9K2","bookSection","2007","Taylor, Robyn; Kazakevich, Maryia; Boulanger, Pierre; Garcia, Manuel; Bischof, Walter F.","Multi-modal Interface for Fluid Dynamics Simulations Using 3–D Localized Sound","Smart Graphics","978-3-540-73213-6 978-3-540-73214-3","","","http://link.springer.com/10.1007/978-3-540-73214-3_17","","2007","2023-07-06 01:14:13","2023-07-06 01:14:13","2023-07-06 01:14:13","182-187","","","4569","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73214-3_17","","/Users/minsik/Zotero/storage/FI5Q79XD/Taylor et al. - 2007 - Multi-modal Interface for Fluid Dynamics Simulatio.pdf","","","","Butz, Andreas; Fisher, Brian; Krüger, Antonio; Olivier, Patrick; Owada, Shigeru","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XBCEA5AL","bookSection","2007","Vilimek, Roman; Zimmer, Alf","Development and Evaluation of a Multimodal Touchpad for Advanced In-Vehicle Systems","Engineering Psychology and Cognitive Ergonomics","978-3-540-73330-0 978-3-540-73331-7","","","http://link.springer.com/10.1007/978-3-540-73331-7_92","","2007","2023-07-06 01:14:13","2023-07-06 01:14:13","2023-07-06 01:14:13","842-851","","","4562","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73331-7_92","","/Users/minsik/Zotero/storage/MF2BDLAF/Vilimek and Zimmer - 2007 - Development and Evaluation of a Multimodal Touchpa.pdf","","","","Harris, Don","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YPQHDGGX","bookSection","2005","Bentley, Peter J.; Novakovic, Gordana; Ruto, Anthony","Fugue: An Interactive Immersive Audiovisualisation and Artwork Using an Artificial Immune System","Artificial Immune Systems","978-3-540-28175-7 978-3-540-31875-0","","","http://link.springer.com/10.1007/11536444_1","The role of T-cells within the immune system is to confirm and assess anomalous situations and then either respond to or tolerate the source of the effect. To illustrate how these mechanisms can be harnessed to solve real-world problems, we present the blueprint of a T-cell inspired algorithm for computer security worm detection. We show how the three central T-cell processes, namely T-cell maturation, differentiation and proliferation, naturally map into this domain and further illustrate how such an algorithm fits into a complete immune inspired computer security system and framework.","2005","2023-07-06 01:14:13","2023-07-19 11:32:50","2023-07-06 01:14:13","1-12","","","3627","","","Fugue","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11536444_1","","/Users/minsik/Zotero/storage/A6MQ9TEN/Bentley et al. - 2005 - Fugue An Interactive Immersive Audiovisualisation.pdf","","","","Jacob, Christian; Pilat, Marcin L.; Bentley, Peter J.; Timmis, Jonathan I.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2IJH7HJ2","journalArticle","2020","Patrick, Rafael N. C.; Letowski, Tomasz R.; McBride, Maranda E.","A multimodal auditory equal-loudness comparison of air and bone conducted sounds","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-020-00320-4","http://link.springer.com/10.1007/s12193-020-00320-4","The term ‘multimodal’ typically refers to the combination of two or more sensory modalities; however, through the advancement of technology, modality variations within specific sensory systems are being discovered and compared in regards to physiological perception and response. The ongoing evaluation of air vs bone conduction auditory perception modalities is one such comparison. Despite an increased awareness of the potential benefits of utilizing bone conduction pathways, a complete understanding of the human auditory system, more specifically, the relationship between air conducted and bone conducted sound remains a critical deficiency hindering the development of advanced multimodal auditory displays. Conduction equivalency ratios (CERs), which were defined as the difference in sound intensity levels (in dB) between equally loud signals transmitted in air conduction (AC) (sound field) and bone conduction (BC) modes provided a link between these two modes of hearing by determining the relationship between spectral content of AC and BC sound. The current report aims to describe, in depth, the establishment of such CERs at three BC transducer contact locations on a listener’s head over a range of audible frequencies presented over three signal intensities within a controlled free-field listening environment. Results indicated the AC–BC relationship is not unique and depends on sound intensity, frequency, and BC transducer location. In addition, in terms of head sensitivity, results support similar findings which indicate that the Mastoid and Condyle locations can be considered interchangeable in terms of their frequency-related sensitivity while the Forehead was found to be considerably less sensitive compared to the other locations.","2020-06","2023-07-06 01:14:13","2023-07-20 07:04:01","2023-07-06 01:14:13","199-206","","2","14","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PFT2GLIP","bookSection","2020","Lyu, Yanjun; Mechtley, Brandon; Hayes, Lauren; Sha, Xin Wei","Tableware: Social Coordination Through Computationally Augmented Everyday Objects Using Auditory Feedback","HCI International 2020 – Late Breaking Papers: Digital Human Modeling and Ergonomics, Mobility and Intelligent Environments","978-3-030-59986-7 978-3-030-59987-4","","","http://link.springer.com/10.1007/978-3-030-59987-4_24","This research develops a novel way of rethinking cultural and social behavior using computationally augmented artifacts. These ‘instruments’ provide various types of auditory feedback when manipulated by certain actions within social contexts, such as a bar or dining space. They foster affective social engagement through the habitual and explorative actions that they afford in everyday contexts, and their resulting auditory feedback. The goal is not only to observe how social interactions are affected by the manipulation of augmented artifacts, but also to observe how the sounds and manipulations affect psycho-sociological [1] changes towards more collaborative social relations during the processes of participatory sense-making [2]. In this paper, we present: a) a study of dynamic social interaction and how we instrumented tangible artifacts to reflect and induce engagement, b) a literature review that provides background for our design methodology, c) ‘vocal prototyping’–a responsive media technique for developing action-sonic mappings, d) our experimental prototype based on this design methodology.","2020","2023-07-06 01:14:13","2023-07-20 05:50:04","2023-07-06 01:14:13","332-349","","","12429","","","Tableware","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-59987-4_24","","","","","","Stephanidis, Constantine; Duffy, Vincent G.; Streitz, Norbert; Konomi, Shin'ichi; Krömker, Heidi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QRCF4SB7","journalArticle","2007","Porta, Marco","Human–Computer input and output techniques: an analysis of current research and promising applications","Artificial Intelligence Review","","0269-2821, 1573-7462","10.1007/s10462-009-9098-5","http://link.springer.com/10.1007/s10462-009-9098-5","","2007-10","2023-07-06 01:14:13","2023-07-06 01:14:13","2023-07-06 01:14:13","197-226","","3","28","","Artif Intell Rev","Human–Computer input and output techniques","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GJLZXH79","bookSection","1999","Mikhak, Bakhtiar; Martin, Fred; Resnick, Mitchel; Berg, Robbie; Silverman, Brian","The Children’s Machines: Handheld and Wearable Computers Too","Handheld and Ubiquitous Computing","978-3-540-66550-2 978-3-540-48157-7","","","http://link.springer.com/10.1007/3-540-48157-5_5","In this paper we describe the material of a construction kit designed to allow children to build their own handheld and wearable devices to meet their interests and passions. Children don’t work with these machines, they learn, play and grow with them. Informed by the types of projects that children have done with this material in the context of educational research in science and engineering, we present a few scenarios for why children would build their own handheld or wearable computational devices. We believe that these application scenarios and their appeal to children are strong evidence for why we should rethink the design of computational devices, particularly for children.","1999","2023-07-06 01:14:13","2023-07-20 00:09:31","2023-07-06 01:14:13","31-43","","","1707","","","The Children’s Machines","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-48157-5_5","","/Users/minsik/Zotero/storage/PYER3B6A/Mikhak et al. - 1999 - The Children’s Machines Handheld and Wearable Com.pdf","","","","Gellersen, Hans-W.","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5SLLETHL","journalArticle","2010","Norman, Neil L.","Feasts in Motion: Archaeological Views of Parades, Ancestral Pageants, and Socio-Political Process in the Hueda Kingdom, 1650–1727 AD","Journal of World Prehistory","","0892-7537, 1573-7802","10.1007/s10963-010-9037-z","http://link.springer.com/10.1007/s10963-010-9037-z","This paper examines the socio-political processes surrounding spectacular religious parades and more private acts of veneration and supplication within the Hueda Kingdom (c. 1650–1727 AD) in the Republic of Bénin, West Africa. The first goal of this paper is to posit the role that such public and private ceremonies played in framing negotiations of political and moral authority. It argues that ceremonial hosts and assembled audience members worked to channel toward their own interests the social, political, and economic outpouring that resulted from ritually sanctioned performances of wealth. The second goal of the paper is to illustrate material and spatial dimensions of such ceremonial spaces. These themes, drawn from a historically well-known polity, work together to aid in comparative theory building on feasting and the politics of spectacle in the deeper archaeological past.","2010-12","2023-07-06 01:14:13","2023-07-20 06:50:12","2023-07-06 01:14:13","239-254","","4","23","","J World Prehist","Feasts in Motion","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5E5BJ5E3","bookSection","2015","Jeon, Myounghoon; Croschere, Jayde","Sorry, I’m Late; I’m Not in the Mood: Negative Emotions Lengthen Driving Time","Engineering Psychology and Cognitive Ergonomics","978-3-319-20372-0 978-3-319-20373-7","","","http://link.springer.com/10.1007/978-3-319-20373-7_22","A considerable amount of research has shown that anger degenerates driving performance [e.g., 1, 2, 3], but little research has empirically shown other affective effects on driving. To investigate angry and sad effects on driving, we conducted a driving simulation study with induced affective states. In cognitive psychology, there is the “sadder but wiser” phenomenon, but given that driving is a complex, dynamic task that engages not only basic cognitive processes, but also other critical elements such as decision making, action selection, and motor control, it might result in different outcomes. Thirty-two participants were induced into sad, angry, or neutral affective states and asked to complete a driving task using a medium fidelity driving simulator. Measures included driving performance, subjective mood ratings, and a NASA-TLX workload index. Results showed that participants in the angry and sad conditions took significantly more time to complete the driving task compared to the neutral condition.","2015","2023-07-06 01:14:13","2023-07-19 23:58:44","2023-07-06 01:14:13","237-244","","","9174","","","Sorry, I’m Late; I’m Not in the Mood","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-20373-7_22","","","","","","Harris, Don","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EZ7Z6S9J","journalArticle","2011","Zhou, Feng; Xu, Qianli; Jiao, Roger Jianxin","Fundamentals of product ecosystem design for user experience","Research in Engineering Design","","0934-9839, 1435-6066","10.1007/s00163-010-0096-z","http://link.springer.com/10.1007/s00163-010-0096-z","Recognizing the importance of user experience (UX) in product ecosystems, this paper examines the fundamental issues underlying product ecosystem design, including implications of a product ecosystem, the notion of the ambience, as well as UX in terms of affect and cognition. A conceptual model is outlined to elucidate the critical factors and the operational mechanism of product ecosystem design for user satisfaction. A technical framework of product ecosystem design for UX is presented, consisting of three consecutive and iterative stages, namely, affective-cognitive need acquisition, affective-cognitive analysis, and affective-cognitive fulfillment. An application to subway station design is reported to illustrate the key techniques for product ecosystem design, including ambient intelligence, data mining, Petri-net modeling, and simulation.","2011-01","2023-07-06 01:15:52","2023-07-21 04:57:14","2023-07-06 01:15:52","43-61","","1","22","","Res Eng Design","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MCHLYJ67","journalArticle","2016","Lech, Michal; Kostek, Bozena; Czyzewski, Andrzej","Multimedia polysensory integration training system dedicated to children with educational difficulties","Journal of Intelligent Information Systems","","0925-9902, 1573-7675","10.1007/s10844-015-0390-3","http://link.springer.com/10.1007/s10844-015-0390-3","","2016-12","2023-07-06 01:15:52","2023-07-06 01:15:52","2023-07-06 01:15:52","531-552","","3","47","","J Intell Inf Syst","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/HV7A8M9F/Lech et al. - 2016 - Multimedia polysensory integration training system.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y92X6F6G","bookSection","2015","Tanuma, Motoki; Oka, Makoto; Mori, Hirohiko","Human Characteristics of Figure Recognition in Tactile Feedback","Human Interface and the Management of Information. Information and Knowledge Design","978-3-319-20611-0 978-3-319-20612-7","","","http://link.springer.com/10.1007/978-3-319-20612-7_44","In car, information presented to drivers is increasing and most of information is done using the visual and auditory displays. Presenting the information only to visual and auditory modes must cause drivers’ cognitive overloads in the near future and it is necessary to find the way using other modes to reduce them. In this study, we especially focus on human tactile figure recognition of the train of sticking stimuli and examine the human characteristics of what kinds of figures people can recognize as the tactile feedback. We developed tactile device that expresses four figures. We found there are interactions between the interval time of each sticking and the figures and human has quite different mechanisms between the cases of the simultaneous sticking and the consecutive sticking in recognizing the figures.","2015","2023-07-06 01:15:52","2023-07-20 05:53:03","2023-07-06 01:15:52","458-465","","","9172","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-20612-7_44","","","","","","Yamamoto, Sakae","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HRU4M56U","bookSection","2004","Lyon, Kirstin; Nürnberg, Peter J.","Interface Design – Use of Audio as an Output","Metainformatics","978-3-540-22010-7 978-3-540-24647-3","","","http://link.springer.com/10.1007/978-3-540-24647-3_7","This paper analyses a number of audio interface models that are currently in use or being developed. It describes a space for describing various models of interfaces that could be used by both visually impaired (VI) and mobile computer users. This paper is concerned only with the use of audio as an output cue. Visualisation is an increasingly important method for people to understand complex information and to navigate around structured information. Computer-based visualisation techniques often depend almost entirely on high-resolution graphics. There are several situations in which this is insufficient.","2004","2023-07-06 01:15:52","2023-07-21 04:29:42","2023-07-06 01:15:52","79-88","","","3002","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-24647-3_7","","","","","","Hicks, David L.","Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3Y7U6IA9","bookSection","2021","Marcondes, Francisco S.; Durães, Dalila; Gonçalves, Filipe; Fonseca, Joaquim; Machado, José; Novais, Paulo","In-Vehicle Violence Detection in Carpooling: A Brief Survey Towards a General Surveillance System","Distributed Computing and Artificial Intelligence, 17th International Conference","978-3-030-53035-8 978-3-030-53036-5","","","http://link.springer.com/10.1007/978-3-030-53036-5_23","Violence is a word that encompasses several meanings ranging from an actual fight to theft and several types of harassment. Therefore, violence detection through surveillance systems can be a quite difficult yet important task. The increasing use of carpooling services and vehicle sharing brought the need to implement a sufficient general surveillance system for monitoring these vehicles for assuring the passengers’ safety during the ride. This paper raised the literature for this matter, finding fewer research papers than it was expected for the in-vehicle perspective, noticeably to sexual harassment. Most of the research papers focused on out-vehicle issues such as runs over and vehicle theft. In-vehicle electronic components security and cockpit user experience were perceived as major concern areas. This paper discusses these findings and presents some insights about in-vehicle surveillance.","2021","2023-07-06 01:15:52","2023-07-19 23:57:39","2023-07-06 01:15:52","211-220","","","1237","","","In-Vehicle Violence Detection in Carpooling","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Advances in Intelligent Systems and Computing DOI: 10.1007/978-3-030-53036-5_23","","","","","","Dong, Yucheng; Herrera-Viedma, Enrique; Matsui, Kenji; Omatsu, Shigeru; González Briones, Alfonso; Rodríguez González, Sara","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IAJE6CYI","bookSection","2016","Karam, M.; Langdon, P. M.","Designing Human Somatosensory System Interactions: Not Just for Haptics Any More!","Designing Around People","978-3-319-29496-4 978-3-319-29498-8","","","http://link.springer.com/10.1007/978-3-319-29498-8_19","We present an elaboration and application of a proposed framework highlighting the somatosensory system in our understanding of the design and development of computer interactions for the human body. The Somatosensory system encompasses the entire range of sensations, organisms, and mechanisms relating to the human sense of touch, and this framework is intended to serve as a tool for broadening our understanding of the multidisciplinary aspects that influence all interactions designed for the body. The framework illustrates a preliminary approach to organizing all touch-based systems into four categories of critical parameters that can enable the effective comparison of different technologies and systems applications, and approaches to human somatosensory system interactions (HSI). In this paper, the framework is applied to evaluate and compare four speech-to-touch (ST) systems towards informing the design of a novel system that uses tactile-acoustic devices to improve speech comprehension.","2016","2023-07-06 01:15:52","2023-07-19 23:56:00","2023-07-06 01:15:52","187-196","","","","","","Designing Human Somatosensory System Interactions","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-3-319-29498-8_19","","","","","","Langdon, Pat; Lazar, Jonathan; Heylighen, Ann; Dong, Hua","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MD3B9AK3","bookSection","2020","Regimbal, Juliette; Radi, Nusaiba; Weill–Duflos, Antoine; Cooperstock, Jeremy R.","Single-Actuator Simultaneous Haptic Rendering for Multiple Vital Signs","HCI International 2020 - Late Breaking Papers: Multimodality and Intelligence","978-3-030-60116-4 978-3-030-60117-1","","","http://link.springer.com/10.1007/978-3-030-60117-1_19","Haptic displays have been investigated as a possible way to reduce the effects of alarm fatigue in clinical environments. Previous displays have employed multiple vibrotactile actuators, using the spatial dimension to aid in conveying information of a number of vital signs. However, inspired by prior work investigating multidimensional tactons, we wished to examine the effectiveness of a single actuator to communicate information regarding multiple vital signs simultaneously. The results of our evaluation suggest that this is not only feasible, but that with a carefully designed encoding strategy, we may be able obtain perception performance comparable to that achievable with multi-actuator displays.","2020","2023-07-06 01:15:52","2023-07-20 05:49:46","2023-07-06 01:15:52","261-270","","","12424","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-60117-1_19","","","","","","Stephanidis, Constantine; Kurosu, Masaaki; Degen, Helmut; Reinerman-Jones, Lauren","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ETGVZRRX","bookSection","2004","Zhang, Qiong; Chen, Taiyi","A Progressive Sounding Object Model in Virtual Environment","Entertainment Computing – ICEC 2004","978-3-540-22947-6 978-3-540-28643-1","","","http://link.springer.com/10.1007/978-3-540-28643-1_75","Realistic audio is a requisite part of an immersive VR system. Previous research primarily focused on sound transmission modeling, e.g. room acoustics modeling. A progressive sounding object model based on modal synthesis is proposed in this article to integrate sound source modeling with sound transmission simulation. It is characterized by direct construction from geometry data plus a handful of material properties of the virtual object, progressive representation in multiple levels and natural binaural modeling.","2004","2023-07-06 01:15:52","2023-07-19 23:59:56","2023-07-06 01:15:52","571-576","","","3166","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-28643-1_75","","","","","","Rauterberg, Matthias","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2Y5YBFLQ","journalArticle","1972","Aiken, Leona S.; Griffin, Lawrence R.","Visual and auditory processing of common pattern class structure","Perception & Psychophysics","","0031-5117, 1532-5962","10.3758/BF03210942","http://link.springer.com/10.3758/BF03210942","Visual and auditory classification of equivalent class-structured patterns were examined. Underlying patterns from two classes were translated into auditory tone sequences and visual polygons. All Ss classified 50 visual patterns and their direct auditory analogs. Visual classification accuracy exceeded auditory accuracy (p < .01); however, auditory accuracy improved when auditory classification was preceded by the visual task (p < .01). Based on group data, classification strategies appeared similar across modalities, with accuracy of classification of individual patterns predicted to the same degree by common measures of physical class structure across modalities. Ss' drawings of the prototypes also suggested a common strategy across modalities. While group data suggest some consistency of classification strategy across modalities, individual Ss were not at all consistent in their visual and auditory classifications.","1972-11","2023-07-06 01:15:52","2023-07-21 04:43:04","2023-07-06 01:15:52","492-496","","6","12","","Perception & Psychophysics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/DI7LVED5/Aiken and Griffin - 1972 - Visual and auditory processing of common pattern c.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"75JBB4P2","bookSection","2003","Ilmonen, Tommi; Kontkanen, Janne","Software Architecture for Multimodal User Input - FLUID","Universal Access Theoretical Perspectives, Practice, and Experience","978-3-540-00855-2 978-3-540-36572-3","","","http://link.springer.com/10.1007/3-540-36572-9_25","Traditional ways to handle user input in software are uncomfortable when an application wishes to use novel input devices. This is especially the case in gesture based user interfaces. In this paper we describe these problems and as a solution we present an architecture and an implementation of a user input toolkit. We show that the higher level processing of user input such as gesture recognition requires a whole newkind of paradigm. The system we designed and implemented — FLexible User Input Design (FLUID) — is a lightweight library that can be used in different kinds of software. The potential application areas include all system where novel input devices are in use: virtual reality, entertainment systems and embedded systems.","2003","2023-07-06 01:15:52","2023-07-21 05:13:31","2023-07-06 01:15:52","319-338","","","2615","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-36572-9_25","","","","","","Carbonell, Noëlle; Stephanidis, Constantine","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BLTNJDC3","bookSection","2001","Ramloll, Rameshsharma; Brewster, Stephen; Yu, Wai; Riedel, Beate","Using Non-speech Sounds to Improve Access to 2D Tabular Numerical Information for Visually Impaired Users","People and Computers XV—Interaction without Frontiers","978-1-85233-515-1 978-1-4471-0353-0","","","http://link.springer.com/10.1007/978-1-4471-0353-0_32","Weinvestiga ted twosolutions for numerical (2D)tab ular data discovery and overview for visually impaired and blind users. One involved accessing information in tables (26 rows x10 columns containing integers between and including 0and 100)by this target user group using both speech and non-speech sounds. The other involved accessing similar information in tables of the same size through speech only by the same user group. Wefound that opportunities to access data through non-speech sounds result in ahighly significant decrease in the overall subjectiveworkloa d, morespecifically in the mental, temporal, performance and frustration workload categories. This subjectiveworkloa dassessment was supported by our quantitativeresults which showed ahighly significant decrease in the averagetime taken to complete agiven data comprehension task and a significant increase in the number of successfully completed tasks.","2001","2023-07-06 01:15:52","2023-07-21 04:42:32","2023-07-06 01:15:52","515-529","","","","","","","","","","","Springer London","London","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-1-4471-0353-0_32","","/Users/minsik/Zotero/storage/SYBKNHN7/Ramloll et al. - 2001 - Using Non-speech Sounds to Improve Access to 2D Ta.pdf","","","","Blandford, Ann; Vanderdonckt, Jean; Gray, Phil","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WXWJJ9PZ","journalArticle","1974","Leshowltz, Barry; Hanzi, Raquel","Serial position effects for tonal stimuli","Memory & Cognition","","0090-502X, 1532-5946","10.3758/BF03197500","http://link.springer.com/10.3758/BF03197500","Serial position effects for tones were studied in a recognition memory experiment. The S was given a stimulus list consisting 01' several tone bursts followed by a number 01' test tones. Accuracy 01' recognition 01' stimulus items as a function 01' input position followed the c1assical bowed serial position curve. Memory strength was a monotonically decreasing function 01' position in the test list. The da ta were fitted with a strength theory model 01' memory. The fit yielded decay parameters corresponding to stimulus- and response-induced interference, wh ich were comparable to the parameters reported for meaningful verbal material.","1974-01","2023-07-06 01:15:52","2023-07-21 04:29:32","2023-07-06 01:15:52","112-116","","1","2","","Memory & Cognition","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/RI3HHGRN/Leshowltz and Hanzi - 1974 - Serial position effects for tonal stimuli.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MBFV9W6Y","journalArticle","1978","Syrdal-Lasky, Ann","Effects of intensity on the categorical perception of stop consonants and isolated second formant transitions","Perception & Psychophysics","","0031-5117, 1532-5962","10.3758/BF03204146","http://link.springer.com/10.3758/BF03204146","Identification and discrimination of two-formant [bae-dae-gae] and [pae-tae-kae] synthetic speech stimuli and discrimination of corresponding isolated second formant transitions (chirps) were performed by six subjects. Stimuli were presented at several intensity levels such that the intensity of the F2 transition was equated between speech and nonspeech stimuli, or the overall intensity of the stimulus was equated. At higher intensity (92 dB), b-d-g and p-t-k identification and between-category discrimination performance declined and bilabial-alveolar phonetic boundaries shifted in location on the continuum towards the F2 steady-state frequency. Between-category discrimination improved from performance at 92 dB when 92-dB speech stimuli were simultaneously masked by 60-dB speech noise; alveolar-velar boundaries shifted to a higher frequency location in the 92-dB-plus-noise condition. Chirps were discriminated categorically when presented at 58 dB, but discrimination peaks declined at higher intensities. Perceptual performance for chirps and p-t-k stimuli was very similar, and slightly inferior to performance for b-d-g stimuli, where simultaneous masking by Fl resulted in a lower effective intensity of F2. The results were related to a suggested model involving pitch comparison and transitional quality perceptual strategies.","1978-09","2023-07-06 01:15:52","2023-07-21 04:44:45","2023-07-06 01:15:52","420-432","","5","23","","Perception & Psychophysics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/SHM82UPG/Syrdal-Lasky - 1978 - Effects of intensity on the categorical perception.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4YB2XSEL","bookSection","2018","Morgan, Phillip L.; Voinescu, Alexandra; Williams, Craig; Caleb-Solly, Praminda; Alford, Chris; Shergold, Ian; Parkhurst, Graham; Pipe, Anthony","An Emerging Framework to Inform Effective Design of Human-Machine Interfaces for Older Adults Using Connected Autonomous Vehicles","Advances in Human Aspects of Transportation","978-3-319-60440-4 978-3-319-60441-1","","","http://link.springer.com/10.1007/978-3-319-60441-1_33","Connected autonomous vehicles (CAVs) represent an exciting opportunity for wider access to mobility; especially for individuals unable to drive manual vehicles. Interaction with CAVs will be through human-machine interfaces (HMIs) providing journey-related and other information with some interactivity. These should be designed with potential users as part of a co-design process to maximize acceptance, engagement, and trust. This paper presents an emerging framework to inform the design of in-vehicle CAV HMIs with a focus on older adults (70-years+). These could be amongst early adopters of CAVs and tend to have the highest level of cognitive, sensory, and physical impairments. Whilst there are numerous principles on HMI design for older adults there are fewer on HMIs for AVs, and a need for research on CAV HMI design principles for older adults. Our emerging framework is novel and important for designers of CAV HMIs for older adults and other potential users.","2018","2023-07-06 01:15:52","2023-07-19 11:11:50","2023-07-06 01:15:52","325-334","","","597","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Advances in Intelligent Systems and Computing DOI: 10.1007/978-3-319-60441-1_33","","/Users/minsik/Zotero/storage/99BN8XFZ/Morgan et al. - 2018 - An Emerging Framework to Inform Effective Design o.pdf","","","","Stanton, Neville A","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DF5KRXKD","bookSection","2000","Pai, Dinesh K.","Robotics in Reality-Based Modeling","Robotics Research","978-1-4471-1254-9 978-1-4471-0765-1","","","http://link.springer.com/10.1007/978-1-4471-0765-1_43","Building realistic models of physical objects presents both a significant challenge for robotics and a potentially important area of applications. These models must support interactive simulation in rich, multimodal virtual environments with not only 3D graphics, but also haptic force feedback and auditory displays. In this paper we discuss the problems and opportunities of acquiring such comprehensive models using robotic measurement facilities. We also describe ACME, the UBC Active Measurement Facility, a telerobotic system for building reality-based models.","2000","2023-07-06 01:15:52","2023-07-21 04:59:09","2023-07-06 01:15:52","353-358","","","","","","","","","","","Springer London","London","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-1-4471-0765-1_43","","","","","","Hollerbach, John M.; Koditschek, Daniel E.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J4KHCDRG","journalArticle","1977","McGovern, Katharine; Strange, Winifred","The perception of /r/ and /l/ in syllable-initial and syllable-final position","Perception & Psychophysics","","0031-5117, 1532-5962","10.3758/BF03198720","http://link.springer.com/10.3758/BF03198720","American English liquids Irl and III have been considered intermediate between stop consonants and vowels acoustically, articulatorily, phonologically, and perceptually. Cutting (l947a) found position-dependent ear advantages for liquids in a dichotic listening task: syllable-initial liquids produced significant right ear advantages, while syllable-final liquids produced no reliable ear advantages. The present study employed identification and discrimination tasks to determine whether Irl and III are perceived differently depending on syllable position when perception is tested by a different method. Fifteen subjects listened to two synthetically produced speech series-/lil to Iril and lilI to lir/-in which stepwise variations of the third formant cued the difference in consonant identity. The results indicated that: (1) perception did not differ between syllable positions (in contrast to the dichotic listening results), (2) liquids in both syllable positions were perceived categorically, and (3) discrimination of a nonspeech control series did not account for the perception of the speech sounds.","1977-03","2023-07-06 01:15:52","2023-07-21 04:44:01","2023-07-06 01:15:52","162-170","","2","21","","Perception & Psychophysics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/8ID89L6D/McGovern and Strange - 1977 - The perception of r and l in syllable-initial .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DVKX7PW9","journalArticle","2007","Persad, Umesh; Langdon, Patrick; Clarkson, John","Characterising user capabilities to support inclusive design evaluation","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-007-0083-y","http://link.springer.com/10.1007/s10209-007-0083-y","Designers require knowledge and data about users to effectively evaluate product accessibility during the early stages of design. This paper addresses this problem by setting out the sensory, cognitive and motor dimensions of user capability that are important for product interaction. The relationship between user capability and product demand is used as the underlying conceptual model for product design evaluations and for estimating the number of people potentially excluded from using a given product.","2007-08-17","2023-07-06 01:15:52","2023-07-21 05:12:17","2023-07-06 01:15:52","119-135","","2","6","","Univ Access Inf Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BGM6CUSL","bookSection","2017","Xu, Jiawang; Wang, Xiaochen; Zhang, Maosheng; Yang, Cheng; Gao, Ge","Binaural Sound Source Distance Reproduction Based on Distance Variation Function and Artificial Reverberation","MultiMedia Modeling","978-3-319-51813-8 978-3-319-51814-5","","","http://link.springer.com/10.1007/978-3-319-51814-5_9","In this paper, a method combining the distance variation function (DVF) and image source method (ISM) is presented to generate binaural 3D audio with accurate feeling of distance. The DVF is introduced to indicate the change in intensity and inter-aural difference when the distance between listener and source changes. Then an artificial reverberation simulated by ISM is added. The reverberation introduces the energy ratio of direct-to-reverberant, which provides an absolute cue to distance perception. The distance perception test results indicate improvement for distance perception when sound sources located within 50 cm. In addition, the variance of perceptual distance was much smaller than that using DVF only. The reduction of variance is a proof that the method proposed in this paper can generate 3D audio with more accurate and steadier feeling of distance.","2017","2023-07-06 01:15:52","2023-07-21 04:31:05","2023-07-06 01:15:52","101-111","","","10133","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-51814-5_9","","","","","","Amsaleg, Laurent; Guðmundsson, Gylfi Þór; Gurrin, Cathal; Jónsson, Björn Þór; Satoh, Shin’ichi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3K59YW7X","bookSection","2015","Erkut, Cumhur; Rajala-Erkut, Anu; Dahl, Sofia","Exploring Felt Qualities of Embodied Interaction with Movement and Sound","Arts and Technology","978-3-319-18835-5 978-3-319-18836-2","","","http://link.springer.com/10.1007/978-3-319-18836-2_10","We present approaches for teaching and designing embodied interaction in collaboration with a contemporary dance choreographer. Our approaches are based on the felt qualities of movement, providing a shared experience, vocabulary for self-expression, and appreciation for movement as a design material for interaction design practitioners. In parallel, such activities provide art professionals competencies for new contexts. We present two workshops conducted at different times. The first workshop, back in 2009, brought about novel sonic interaction paradigms, technologies, and artifacts. The second workshop was carried out in March 2014, and we are in the process of developing interactive sketches by pairing our observations with motion tracking. In this paper, the activities in these workshops are presented, and reflected upon. In particular, we are investigating whether or not these activities guided the participants from the prevailing notion of command/control in embodied interaction towards experiences related to the felt qualities of movement.","2015","2023-07-06 01:15:52","2023-07-19 11:35:03","2023-07-06 01:15:52","77-85","","","145","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering DOI: 10.1007/978-3-319-18836-2_10","","","","","","Brooks, Anthony Lewis; Ayiter, Elif; Yazicigil, Onur","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8J2WALDF","journalArticle","2006","Tahboub, Karim A.","Intelligent Human-Machine Interaction Based on Dynamic Bayesian Networks Probabilistic Intention Recognition","Journal of Intelligent and Robotic Systems","","0921-0296, 1573-0409","10.1007/s10846-005-9018-0","http://link.springer.com/10.1007/s10846-005-9018-0","","2006-01","2023-07-06 01:16:15","2023-07-06 01:16:15","2023-07-06 01:16:15","31-52","","1","45","","J Intell Robot Syst","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4FXGQGXN","journalArticle","2020","Mathew, Justin D.; Huot, Stéphane; Katz, Brian F. G.","Comparison of spatial and temporal interaction techniques for 3D audio trajectory authoring","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-019-00314-x","http://link.springer.com/10.1007/s12193-019-00314-x","With the popularity of immersive media, developing usable tools for content development is important for the production process. In the context of 3D audio production, user interfaces for authoring and editing 3D audio trajectories enable content developers, composers, practitioners, and recording and mixing engineers to define how audio sources travel in time. However, common interaction techniques in 3D audio production tools can make the workflow of this task tedious and difficult to accomplish. This study investigates this problem by classifying the atomic tasks (spatially and temporally) of a general composite task of authoring 3D audio trajectories and then evaluating different interaction techniques across these tasks. Common graphical user interfaces were compared with input devices having varying degrees-of-freedom for spatial atomic tasks in order to investigate the effect of direct manipulation and integrality of interaction techniques. Continuous and discrete interaction techniques were compared for temporal tasks in order to investigate the effect of direct manipulation. Results suggest that interaction techniques with high degrees of integrality and direct manipulation reduce task completion time compared to standard GUI techniques. The design of temporal tasks can create a visual bias, and discrete-time controls can be a suitable method for traversing a small number of control points. These results and further observations provide directions on the study of interaction technique design for 3D audio tools, which in turn should improve workflows of 3D audio content creation.","2020-03","2023-07-06 01:16:15","2023-07-20 07:01:58","2023-07-06 01:16:15","83-100","","1","14","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/648YFS96/Mathew et al. - 2020 - Comparison of spatial and temporal interaction tec.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W8PIUDMD","journalArticle","2012","Katz, Brian F. G.; Kammoun, Slim; Parseihian, Gaëtan; Gutierrez, Olivier; Brilhault, Adrien; Auvray, Malika; Truillet, Philippe; Denis, Michel; Thorpe, Simon; Jouffrais, Christophe","NAVIG: augmented reality guidance system for the visually impaired: Combining object localization, GNSS, and spatial audio","Virtual Reality","","1359-4338, 1434-9957","10.1007/s10055-012-0213-6","http://link.springer.com/10.1007/s10055-012-0213-6","","2012-11","2023-07-06 01:16:15","2023-07-06 01:16:15","2023-07-06 01:16:15","253-269","","4","16","","Virtual Reality","NAVIG","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FZT5WNQ7","journalArticle","2017","Taffou, Marine; Ondřej, Jan; O’Sullivan, Carol; Warusfel, Olivier; Viaud-Delmon, Isabelle","Judging crowds’ size by ear and by eye in virtual reality","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-016-0228-5","http://link.springer.com/10.1007/s12193-016-0228-5","Judging the size of a group of people is an everyday task, on which many decisions are based. In the present study, we investigated whether judgment of size of different groups of people depended on whether they were presented through the auditory channel, through the visual channel, or through both auditory and visual channels. Groups of humanoids of different sizes (from 8 to 128) were presented within a virtual environment to healthy participants. They had to judge whether there was a lot of people in each group and rate their discomfort in relation to the stimuli with Subjective Units of Distress. Our groups of 96 and 128 virtual humans were judged as crowds regardless of their sensory presentation. The sensory presentation influenced participants’ judgment of virtual human group size ranging from 8 to 48. Moreover, while the quantity judgments in the auditory condition increased linearly with the group size, participants judged the quantity of people in a logarithmic manner in the two other sensory conditions. These results suggest that quantity judgment based on auditory information in a realistic context may often involve implicit arithmetic. Even though our participants were not phobic of crowds, our findings are of interest for the field of virtual reality-based therapy for diverse disorders because they indicate that quantity judgment can potentially be altered in a sensory-specific manner in patients with fear of crowds.","2017-03","2023-07-06 01:16:15","2023-07-20 07:04:56","2023-07-06 01:16:15","57-65","","1","11","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/672LHLEU/Taffou et al. - 2017 - Judging crowds’ size by ear and by eye in virtual .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""