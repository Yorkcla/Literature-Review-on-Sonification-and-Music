"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"MTMX7Z4C","journalArticle","2018","Axon, Louise; Goldsmith, Michael; Creese, Sadie","Sonification Mappings: Estimating Effectiveness, Polarities and Scaling in an Online Experiment","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19875","Sonification is a technique to present data arrays as sound, thereby taking advantage of the human ability to hear patterns that might otherwise not be apparent. Mappings from parameters of data to parameters of sound form the basis of parameter-mapping sonification. The choice of mappings and their design can influence both the utility of the sonification system and the ability of users to interpret the sounds. In this article the authors demonstrate the use of a time-efficient methodology with an experimental online platform for assessing mappings. Experiments explored the effectiveness of various mappings, and the discussions explore the implications of each approach. Based on the responses of 100 participants in an online Magnitude Estimation experiment, the effectiveness of 16 data-sound mappings was explored. Results showed that mappings involving certain sound parameters were generally effective, while those using other sound parameters varied in their effectiveness. In some cases the ability to interpret mappings and the polarities with which they were perceived varied among individuals using them. The mappings that used the tempo parameter were generally perceived effectively, while those using other sound parameters varied. Exploratory observations suggest that differences among participants might be related to different levels of musical experience.","2018","2023-07-12 05:56:55","2023-07-19 03:37:30","","1016–1032","","12","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H97AVHSQ","journalArticle","2012","Bujacz, Michal; Skulimowski, Piotr; Strumillo, Pawel","Naviton—A Prototype Mobility Aid for Auditory Presentation of Three-Dimensional Scenes to the Visually Impaired","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16374","To augment the task of navigation and orientation of blind individuals, a new travel aid uses 3D scene sonification to present information about the environment using nonverbal audio. The model is composed of two classes of objects: obstacles and planes. The algorithm uses scene image segmentation, personalized spatial audio, musical tones, and sonar-like sound patterns. Individually measured head-related transfer functions were used to provide users with the illusion of sounds originating from the locations of sonified scene elements. Using a segmented and parametric description overcomes the sensory mismatch between visual and auditory perception. In a pilot study using both blind and sighted volunteers, subjects were able to utilize the prototype for spatial orientation and obstacle avoidance after a few minutes of training, attaining 90% accuracy in estimating the direction and depth of obstacles.","2012","2023-07-12 05:57:01","2023-07-19 03:45:35","","696–708","","9","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"52FJ88PV","journalArticle","2018","Roma, Gerard; Xambó, Anna; Freeman, Jason","User-independent Accelerometer Gesture Recognition for Participatory Mobile Music","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19582","With the widespread use of smartphones that have multiple sensors and sound processing capabilities, there is a great potential for increased audience participation in music performances. This paper proposes a framework for participatory mobile music based on mapping arbitrary accelerometer gestures to sound synthesizers. The authors describe Handwaving, a system based on neural networks for real-time gesture recognition and sonification on mobile browsers. Based on a multiuser dataset, results show that training with data from multiple users improves classification accuracy, supporting the use of the proposed algorithm for user-independent gesture recognition. This illustrates the relevance of user-independent training for multiuser settings, especially in participatory music. The system is implemented using web standards, which makes it simple and quick to deploy software on audience devices in live performance settings.","2018","2023-07-12 05:57:04","2023-07-19 04:41:39","","430–438","","6","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7747JFUY","journalArticle","2012","Vogt, Katharina; Höldrich, Robert","Translating Sonifications","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16636","Over the last few decades there have been numerous explorations of sonification, a concept that may be loosely defined as communicating nonaudio information as sound. As with any developing field, there comes a time when a formal structure is needed to provide a framework for understanding the collection of ad hoc experiments. To make the mapping between data and sound more explicit and less prone to misunderstandings, a sonification operator has been suggested. The authors created “notation modules” to formulate this mapping for various fields. An example of a specific sonification operator in the field of physics is given. Nine subjects from research were used in a study to evaluate the experience of this formalism.","2012","2023-07-12 05:57:09","2023-07-19 10:53:28","","926–935","","11","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B5J42F5S","journalArticle","2018","Yang, Jiajun; Hermann, Thomas","Interactive Mode Explorer Sonification Enhances Exploratory Cluster Analysis","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19712","Exploratory Data Analysis (EDA) refers to the process of detecting patterns of data when explicit knowledge of such patterns within the data is missing. Because EDA predominantly employs data visualization, it remains challenging to visualize high-dimensional data. To minimize the challenge, some information can be shifted into the auditory channel using humans’ highly developed listening skills. This paper introduces Mode Explorer, a new sonification model that enables continuous interactive exploration of datasets with regards to their clustering. The method was shown to be effective in supporting users in the more accurate assessment of cluster mass and number of clusters. While the Mode Explorer sonification aimed to support cluster analysis, the ongoing research has the goal of establishing a more general toolbox of sonification models, tailored to uncover different structural aspects of high-dimensional data. The principle of extending the data display to the auditory domain is applied by augmenting interactions with 2D scatter plots of high-dimensional data with information about the probability density function.","2018","2023-07-12 05:57:12","2023-07-19 10:58:24","","703–711","","9","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DLTF7BGB","journalArticle","2012","Grosshauser, T.; Bläsing, B.; Spieth, C.; Hermann, T.","Wearable Sensor-Based Real-Time Sonification of Motion and Foot Pressure in Dance Teaching and Training","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16369","As with tasks involved with motion and gesture, teaching dance can take advantage of auditory displays that map specific dance steps into their acoustic counterparts. Wearable sensors based on acoustic “fingerprints” accompany the dance movements in real-time. This kind of audio feedback has a positive influence on motor movement and perception. For example, joint angles, weight distribution, and energy of jumps are easily recognized through sound. With practice, a student can hear if a complex movement was correctly executed. The auditory system can hear complex patterns of rapid motion, especially aspects of a dance that are not easily seen.","2012","2023-07-12 05:57:18","2023-07-19 04:03:00","","580–589","","7/8","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MX8LVIVI","journalArticle","2020","Hansen, Brian; Burchett, Joseph N.; Forbes, Angus G.","Quasar Spectroscopy Sound: Analyzing Intergalactic and Circumgalactic Media via Data Sonification","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21000","In this paper, we present sonification approaches to support research in astrophysics, using sound to enhance the exploration of the intergalactic medium and the circumgalactic medium. Astrophysicists often analyze matter in these media using a technique called absorption line spectroscopy. Our sonification approaches convey key spectral features identified via this technique, including the presence and width of spectral absorption lines within a region of the Universe, the relationship of a particular redshift location with respect to the absorption peak of a spectral absorption line, and the density of gas at various regions of the Universe. In addition, we introduce Quasar Spectroscopy Sound, a novel software tool that enables researchers to perform these sonification techniques on cosmological data sets, potentially accelerating the discovery and classification of matter in the intergalactic medium and circumgalactic medium.","2020","2023-07-12 05:57:23","2023-07-19 04:03:28","","865–875","","11","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CMAKZTKN","journalArticle","2018","Burloiu, Grigore; Mihai, Valentin; Damian, Stefan","Layered Motion and Gesture Sonification in an Interactive Installation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19860","SoundThimble is an interactive sound installation based on the relationship between human motion and virtual objects in 3D space. A Vicon infrared motion-capture system and custom software are used to track, interpret, and sonify the movement and gestures of a performer relative to a virtual object. The authors explore the resulting possibilities for layered sonification dynamics and extended perception and expression in internal tests as well as in a public demo. Experimental evaluation reveals an average object search time of around 60 s, as well as thresholding ranges for effective gesture spotting. The underlying software platform is open source and portable to similar hardware systems, leaving room for extension and variation. This paper presents the pilot application of the proposed framework. Audience members entering the tracking area shift among the roles of game player, sonic performer, and composer/arranger, according to an iterative interaction schema. The central vehicle in all three layers is the “sound-thimble” itself, a virtual object with particular spatial, sonic, and interaction attributes.","2018","2023-07-12 05:57:34","2023-07-19 03:45:55","","770–778","","10","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AVGQN2AI","journalArticle","2012","Parseihian, Gaëtan; Katz, Brian F. G.","Morphocons: A New Sonification Concept Based on Morphological Earcons","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16355","Sonification, a form of auditory display, is a means of mapping arbitrary information such as the distance to an obstacle or changes in temperature into sound. For the visually impaired, sonification can make an important contribution to increasing autonomy. In contrast to earcons (the audio analog of icons), which map a unique sound to a particular meaning, morphocons are short audio units that are used to construct a sonic grammar based on temporal-frequency patterns, rather than fixed sound samples. For example, a rhythmic repetition can be used to modify or add meaning to any base sound sample. Results indicate that both blind and sighted subjects were able to perceive temporal variations of acoustic parameters as an abstract form, independent of the base sound sample, allowing the extraction of consistent category information from a range of different customizable sounds.","2012","2023-07-12 05:57:39","2023-07-19 04:37:12","","409–418","","6","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N9HGRJVK","journalArticle","2012","Schaffert, Nina; Gehret, Reiner; Mattes, Klaus","Modeling the Rowing Stroke Cycle Acoustically","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16366","Because elite athletes require an unconscious and automated sense of time, and because sound is especially appropriate for conveying timing information, acoustic feedback can be especially useful in training of rowers. In the context of human movement, rhythm is a time accurate sequence of motor actions. Rhythm and synchronization are inseparable within a moving context. An auditory feedback signal based on boat acceleration helps rowers control their activities, and this sonified data can be stored in an audio file for later training and analysis. The improved sensitivity to the time-critical nature of the rowing cycle yielded an improved synchronization among the crew, as well as an improvement of individual athlete’s rowing technique.","2012","2023-07-12 05:57:41","2023-07-19 04:47:05","","551–560","","7/8","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G5XEQ86V","journalArticle","2012","Stewart, Rebecca; Sandler, Mark","Spatial Auditory Display in Music Search and Browsing Applications","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16637","User interfaces for searching and browsing collections of music often use nonaudio for presenting information about the contents of the collection. This study reviews the literature to unify the various ways in which auditory spatialization can be used to augment the presentation of data. The authors examined 22 user interfaces that use such concepts as auditory icons, perceived location, amplitude panning, and a usability evaluation. Commonalities among the designs are discussed including the chosen spatialization approaches and evaluation methods.","2012","2023-07-12 05:57:46","2023-07-19 04:49:19","","936–946","","11","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TT5VK6U7","journalArticle","2012","Terasawa, Hiroko; Berger, Jonathan; Makino, Shoji","In Search of a Perceptual Metric for Timbre: Dissimilarity Judgments among Synthetic Sounds with MFCC-Derived Spectral Envelopes","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16372","Because the spectral envelope of a sound is a crucial aspect of timbre perception, the authors propose a quantitative model of spectral envelope perception using a set of orthogonal basis functions, analogous to the three primary colors in vision. The goal is find a quantitative mapping between the physical description of the spectral envelope and its perception. This allows for a meaningful and reliable way of controlling timbre in sonification. This paper presents a quantitative metric to describe the multidimensionality of spectral envelope perception, i.e., the perception that is specifically related to the spectral element of timbre. Mel-frequency cepstral coefficients (MFCC) were chosen as a metric for spectral envelope perception because of their linearity, orthogonality, and multidimensionality. Quantitative data from two experiments illustrate the linear relationship between the subjective perception of spectrally-varied synthetic sounds and the MFCC.","2012","2023-07-12 05:57:49","2023-07-19 04:52:08","","674–685","","9","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V4ZRWKSC","journalArticle","2023","Dupré, Théophile; Denjean, Sébastien; Aramaki, Mitsuko; Kronland-Martinet, Richard","Spatial Integration of Dynamic Auditory Feedback in Electric Vehicle Interior","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22142","With the development of electric motor vehicles, the domain of automotive sound design addresses new issues and is now concerned with creating suitable and pleasant soundscapes inside the vehicle. For instance, the absence of predominant engine sound changes the driver perception of the dynamic of the car. Previous studies proposed relevant sonification strategies to augment the interior sound environment by bringing back vehicle dynamics with synthetic auditory cues. Yet, users report a lack of blending with the existing soundscape. In this study, the authors analyze acoustical and perceptual spatial characteristics of the car soundscape and show that the spatial attributes of sound sources are fundamental to improve the perceptual coherency of the global environment.","2023","2023-07-12 05:57:52","2023-07-19 03:54:01","","349–362","","6","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N62DXSB5","journalArticle","2012","Metatla, Oussama; Bryan-Kinns, Nick; Stockman, Tony","The Effects of Using Headphones and Speakers on Collaboration in an Audio-Only Workspace","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16365","The means by which audio was delivered (headphones or loudspeakers) in a shared workplace environment influenced the dynamics of collaborations. In an experiment designed to show the influence of audio delivery, pairs of sighted individuals used audio as the sole means for communicating with one another while editing a shared diagram. The choice of working style affects how collaborators attend to the sounds present in a collaborative space, which in turn influences how they structure and organize their interactions. That in turn determines which information is relevant, dynamically changing according to how collaborators choose to work with sounds. Another conclusion was that the mere physical presence of audio in a shared space does not necessary imply that it is being attended to by those hearing it.","2012","2023-07-12 05:57:59","2023-07-19 04:31:26","","540–550","","7/8","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AN972NCJ","journalArticle","2021","Bown, Oliver; Ferguson, Sam; Dos Santos, Augusto Dias Pereira; Mikolajczyk, Kurt","Supporting Creative Practice in Wireless Distributed Sound Installations Given Technical Constraints","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21470","In this paper we present creative practice-led research into building large, scalable “multiplicitous media” artworks in which many networked devices control lights and speakers and are coordinated over Wi-Fi to create holistic artistic and environmental experiences. We discuss competing constraints, in particular the creative constraints associated with the challenge of coding complex multi-device behaviors, maximizing creative freedom and simplifying complex engineering and design decisions. Based on recent experience building multi-device digital installation works, we propose an approach, the “broadcast-first recipe,” that aims to simplify the space of creative possibilities, with a trade-off between expressive power and creative efficiency that we argue is worth adopting. We examine this approach in light of hard technical constraints such as central processing unit (CPU)and Wi-Fi bandwidth budgets, which we discuss in a concrete example. We consider how the effectiveness of the proposed approach could be further leveraged in the provision of support tools.","2021","2023-07-12 05:58:01","2023-07-19 03:44:24","","757–767","","10","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CBM3DHYM","journalArticle","2013","Lech, Michal; Kostek, Bozena","Testing A Novel Gesture-Based Mixing Interface","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16822","With a digital audio workstation, in contrast to the traditional mouse-keyboard computer interface, hand gestures can be used to mix audio with eyes closed. Mixing with a visual representation of audio parameters during experiments led to broadening the panorama and a more intensive use of shelving equalizers. Listening tests proved that the use of hand gestures produces mixes that are aesthetically as good as those obtained using a mouse, keyboard, and MIDI controller. The human and artistic factor is an essential part of the art, which includes the way in which sound tools are controlled. Alternative means of control are part of sound art.","2013","2023-07-12 05:58:04","2023-07-19 04:18:07","","301–313","","5","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SPCP6MJC","journalArticle","2022","Baratè, Adriano; Ludovico, Luca A.","Web MIDI API: State of the Art and Future Perspectives","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22016","The Web MIDI API is intended to connect a browser app with Musical Instrument Digital Interface (MIDI) devices and make them interact. Such an interface deals with exchanging MIDI messages between a browser app and an external MIDI system, either physical or virtual. The standardization by the World Wide Web (W3C) Consortium started about 10 years ago, with a first public draft published on October 2012, and the process is not over yet. Because this technology can pave the way for innovative applications in musical and extra-musical fields, the present paper aims to unveil the main features of the API, remarking its advantages and drawbacks and discussing several applications that could take benefit from its adoption.","2022","2023-07-12 05:58:06","2023-07-19 03:38:49","","918–925","","11","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VRVQR38Y","journalArticle","2018","Liang, Beici; Fazekas, György; Sandler, Mark","Measurement, Recognition, and Visualization of Piano Pedaling Gestures and Techniques","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19584","When playing the piano, pedaling is one of the important techniques that lead to expressive performance, comprising not only the onset and offset information that composers often indicate in the score, but also gestures related to the musical interpretation by performers. This research examines pedaling gestures and techniques on the sustain pedal from the perspective of measurement, recognition, and visualization. Pedaling gestures can be captured by a dedicated measurement system where the sensor data is simultaneously recorded alongside the piano sound under normal playing conditions. Recognition is comprised of two separate tasks on the sensor data: pedal onset/offset detection and classification by technique. The onset and offset times of each pedaling technique were computed using signal processing algorithms. Based on features extracted from every segment when the pedal is pressed, the task of classifying the segments by pedaling technique was undertaken using machine-learning methods. High accuracy was obtained by cross validation. The recognition results can be represented using novel pedaling notations and visualized in an audio-based score-following application.","2018","2023-07-12 05:58:09","2023-07-19 04:20:07","","448–456","","6","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WYLKJ84E","journalArticle","2007","Short, Kevin M.; Garcia, Ricardo A.; Daniels, Michelle L.","Multichannel Audio Processing Using a Unified-Domain Representation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14155","[Engineering Report] The unified-domain representation for synchronized multichannel audio streams is introduced. This lossless and invertible transformation describes multiple streams of audio as a single-frequency-domain magnitude component multiplied by a complex matrix encoding the spatial and phase relationship information for each channel. Unified-domain analysis and signal-processing techniques for applications such as high-resolution frequency analysis, sound source separation, spatial psychoacoustic models, and low-bit-rate audio coding are presented.","2007","2023-07-12 05:58:11","2023-07-19 04:48:15","","156–165","","3","55","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QVFYILHA","journalArticle","2018","Xambó, Anna; Roma, Gerard; Shah, Pratik; Tsuchiya, Takahiko; Freeman, Jason; Magerko, Brian","Turn-taking and Online Chatting in Remote and Co-located Collaborative Music Live Coding","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19391","This paper looks into co-located and remote turn-taking and online chatting in collaborative music live coding (CMLC) using the web-based computer science education platform EarSketch. Duo and trio live coding are considered from an autoethnographic stance. An online survey with six practitioners in live coding and collaboration complements the autoethnographic findings. It was identified that turn-taking in duo and trio live coding was more promising in an education context than in performance. It is expected that turn-taking and online chatting in CMLC, among small groups of two, three, or four people can be useful in the classroom for pedagogical purposes. The role of a chat window is important as a tool for supporting communication in CMLC, but the proposal of semantic hashtags should be reconsidered as a tailorable vocabulary adapted to the needs of each group and perhaps linked to a notification system that facilitates the collaboration. From the four use cases based on trio/duo versus co-located/remote situations, it was discovered that a co-located trio live coding mediated by a turn-taking mechanism can be more interesting for group dynamics because the roles of a driver and two navigators can specialize and adapt easily during the musical improvisation act, while combining both verbal and nonverbal communication.","2018","2023-07-12 05:58:16","2023-07-19 10:57:58","","253–266","","4","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6LRBWTCF","journalArticle","2020","Xambó, Anna; Støckert, Robin; Jensenius, Alexander Refsum; Saue, Sigurd","Learning to Code Through Web Audio: A Team-Based Learning Approach","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20989","In this article, we discuss the challenges and opportunities provided by teaching programming using web audio technologies and adopting a team-based learning (TBL) approach among a mix of colocated and remote students, mostly novices in programming. The course has been designed for cross-campus teaching and teamwork, in alignment with the two-city master's program in which it has been delivered. We present the results and findings from (1) students' feedback; (2) software complexity metrics; (3) students' blog posts; and (4) teacher's reflections. We found that the nature of web audio as a browser-based environment, coupled with the collaborative nature of the course, was suitable for improving the students' level of confidence about their abilities in programming. This approach promoted the creation of group course projects of a certain level of complexity, based on the students' interests and programming levels. We discuss the challenges of this approach, such as supporting smooth cross-campus interactions and assuring students' preknowledge in web technologies (HTML, CSS, and JavaScript) for an optimal experience. We conclude by envisioning the scalability of this course to other distributed and remote learning scenarios in academic and professional settings. This is in line with the foreseen future scenario of cross-site interaction mediated through code.","2020","2023-07-12 05:58:19","2023-07-19 10:58:07","","727–737","","10","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WHD5HJKS","journalArticle","2023","Meyer-Kahlen, Nils; Kastemaa, Miranda; Schlecht, Sebastian J.; Lokki, Tapio","Measuring Motion-to-Sound Latency in Virtual Acoustic Rendering Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22145","Few studies that employ virtual acoustic rendering systems accurately specify motion-to-sound latency. To make such assessments more common, we present two methods for latency measurements using either impulsive or periodic movements. The methods only require hardware available in every acoustics lab: a small microphone and a loudspeaker. We provide open-source tools that implement analysis according to the methods. The methods are evaluated on a high-quality optical tracking system. In addition, three small trackers based on inertial measurement units were tested. The results show the reliability of the method for the optical system and the difficulties in defining the latency of inertial measurement unit-based trackers.","2023","2023-07-12 05:58:23","2023-07-19 04:31:34","","390–398","","6","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VGL4E6SQ","journalArticle","2012","Schönstein, David; Katz, Brian F.G.","Variability in Perceptual Evaluation of HRTFs","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16552","Because appropriate head-related transfer functions (HRTFs) are key to binaural rendering, an evaluation is required to assess processing steps when individual HRTFs are not available. This study involving six subjects showed significant response variability in perceptual evaluations of HRTFs when subjects were asked to judge six sets of HRTFs, including individual HRTFs, with three different attributes. Insufficient reproducibility is problematic when trying to select nonindividual HRTFs. In order to minimize the effect of learning, adequate training should be provided. By using attribute evaluations and assessor selection, this study offers a methodology that might be used to produce consistent evaluations in commercial binaural syntheses.","2012","2023-07-12 05:58:29","2023-07-19 04:47:13","","783–793","","10","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T3SD48RF","journalArticle","2020","Vindrola, Lucas; Melon, Manuel; Chamard, Jean-Christophe; Gazengel, Bruno","Pressure Matching With Forced Filters for Personal Sound Zones Application","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20997","This paper presents a rethinking of the Pressure Matching Method (PM) used in the generation of Personal Sound Zones when the responses of some filters are already known. They are then imposed in the calculation, resulting in a Forced Pressure Matching method. This new formulation is implemented to control two zones—a reproduction zone and dark zone—in a two-seat configuration aimed toward the Transportation industry. Due to variations in transportation acoustic environments, the computational time is added to the metrics typically used in the Personal Sound Zones literature (such as acoustic contrast, effort, error, etc.), foreseeing the need of an adaptive system. Perfect Dirac delta functions were forced as filters of the loudspeakers closest to the reproduction zone. The new formulation achieved the same acoustic contrast, effort, and reproduction error very similar to that of the conventional PMbut calculated the filters 24% faster.","2020","2023-07-12 05:58:31","2023-07-19 10:53:18","","832–842","","11","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SJDFIAWM","journalArticle","2020","Salmon, François; Hendrickx, Étienne; Épain, Nicolas; Paquier, Mathieu","The Influence of Vision on Perceived Differences Between Sound Spaces","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20888","Few studies have investigated the influence of visual cues on sound space perception beyond the influence of visual cues on sound source position. Previous studies suggest that the perception of late reflections is not affected by the visual impression of a room; however, only a limited number of spatial sound attributes were investigated. In the present paper, audiovisual interactions were examined without making assumptions on the number and nature of perceptual dimensions involved in the perception of sound space. In a virtual environment that employed a Head Mounted Display and dynamic binaural playback, subjects were asked to judge the perceived dissimilarity between sound spaces while watching the same visual stimulus. Pairwise comparisons were repeated using multiple visual conditions, including an audio-only condition. One sound source, a male voice reciting a poem, was considered in the listening test. It appeared that the visual modality did not impact the perceived differences between sound spaces.","2020","2023-07-12 05:58:33","2023-07-19 04:46:07","","522–531","","7/8","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PUAEAMK6","journalArticle","2023","Ois Salmon, Franç; Changenet, Frédéric; Colas, Tom; Verron, Charles; Paquier, Mathieu","A Comparative Study of Multichannel Microphone Arrays Used in Classical Music Recording","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22149","With the growing advent of object-based audio productions, a major challenge for sound recordists is to determine multichannel microphone arrays that are suitable on several sound reproduction systems. Various multichannel 3D microphone arrays have been designed for the production of immersive content and it seems necessary to assess their qualities on several playback systems. This study concerns the subjective evaluation of six multichannel microphone arrays used for the recording of classical music: Decca Tree, ESMA-3D, MMAD, 2L-Cube, and first-order and second-order ambisonic microphone arrays. Subjects evaluated the sound recordings according to four perceptual attributes (precision of localization, envelopment, spectral quality, and preference) as well as on two reproduction systems (a 5.1.4 multichannel loudspeaker setup and a dynamic binaural playback). As observed previously with stereophonic reproduction, results showed that coincident systems can provide a good localization accuracy but can lack in the sensation of envelopment by reverberation. Moreover, they are more likely to be perceived differently under different rendering conditions. The greatest sense of envelopment was produced by ESMA-3D for the two rendering conditions. No particular system was preferred by the subjects for creating a mix with spot microphones.","2023","2023-07-12 05:58:36","2023-07-19 04:35:33","","441–454","","7/8","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F88FPUWY","journalArticle","2022","Engel, Isaac; Alon, David L.; Scheumann, Kevin; Crukley, Jeff; Mehra, Ravish","On the Differences in Preferred Headphone Response for Spatial and Stereo Content","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21564","When reproducing spatial audio over headphones, ensuring that these have a flat frequency response is important to produce an accurate rendering. However, previous studies suggest that, when reproducing nonspatial content such as stereo music, the headphone response should resemble that of a loudspeaker system in a listening room (e.g., the so-called Harman target). It is not yet clear whether a pair of headphones calibrated in such way would be preferred by listeners for spatial audio reproduction too. This study investigates how listeners' preference regarding headphone frequency response differs in the cases of stereo and spatial audio content reproduction, rendered using individual binaural room impulse responses. Three listening tests that evaluate seven different target headphone responses, two headphones, and two reproduction bandwidths are presented with over 20 listeners per test. Results suggest that a flat headphone response is preferred when listening to spatial audio content, whereas the Harman target was preferred for stereo content. This effect was found to be stronger when user-specific equalization was used and was not significantly affected by the choice of headphone or reproduction bandwidth.","2022","2023-07-12 05:58:49","2023-07-19 03:54:35","","271–283","","4","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FDIATTI7","journalArticle","2012","Wersényi, György","Virtual Localization by Blind Persons","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16368","In order for blind people to better use personal computers, an auditory virtual environment can be used to present information that might otherwise be available only with vision. Auditory objects can be spatial placed in the virtual environment if the user can successfully identify their location. In contrast to sighted subjects, blind subjects were better at detecting movements in the horizontal plane around the head, localizing static frontal audio sources, and orientation in a 2-D virtual audio display. On the other hand, sighted subjects performed better identifying ascending sound sources in the vertical plane and detecting static sources in the back.","2012","2023-07-12 05:58:55","2023-07-19 10:55:26","","568–579","","7/8","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QKKVD953","journalArticle","2012","Altinsoy, M. Ercan","The Quality of Auditory-Tactile Virtual Environments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16164","In our daily lives, we usually perceive an event via more than one sensory modality (e.g., vision, hearing, touch). Therefore, multimodal integration and interactions play an important role when we use objects and for event recognition in our environment. A virtual environment (VE) is a computer simulation of a realistic-looking and interactive world. VEs should take into account the multisensory nature of humans and communicate with the user not only through vision but also through other modalities. In addition to vision, hearing and touch are the most commonly used communication channels. Recently, a variety of products with additional tactile input and output capabilities have been developed (e.g., Apple iPhone and other touch-screen devices, NintendoWii, etc.). Some of these devices provide new possibilities for interacting with a computer, including the auditory modality. Binaural synthesis and rendering are becoming key technologies for multimedia products. Virtual environments are no longer limited to academic research; they have commercial applications, particularly in medicine, game, and entertainment industries. Thus, the quality of VEs is becoming increasingly important. User interaction with a VE is a key issue in the perception of its quality. Several studies have discussed the quality of displays, input and output devices (for different modalities) as well as software and hardware issues; however, multimodal user interaction should also be examined. This paper focuses on the parameters that influence the quality of audio-tactile VEs.","2012","2023-07-12 06:29:43","2023-07-19 03:36:11","","38–46","","1/2","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U2M8WD2K","journalArticle","2014","Begault, Durand R.; Bittner, Rachel M.; Anderson, Mark R.","Multimodal Information Management: Evaluation of Auditory and Haptic Cues for NextGen Communication Displays","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17333","Auditory communication displays within the Next Generation Air Transport System (currently under development in the United States) will likely require an improvements in the user interface for selecting amongst multiple incoming messages. Interface design can impact both user performance and preference. Two design factors were evaluated: physical pressure-sensitive switches versus flatpanel ""virtual switches,"" and auditory feedback from switch contact. Performance with stimuli using physical switches was 1.2 s faster than virtual switches (2.0 s vs. 3.2 s); auditory feedback provided a 0.6 s performance advantage (2.3 s vs. 2.9 s). The subjective results show a significant preference and superior performance for physical pressure-sensitive switches having audio feedback, compared to touch-panel virtual switches. The correlation between objective measures of performance and subjective ratings of preference and performance was shown to be high. Overall, the results indicate that any replacement of physical controls by virtual touch screens must be considered carefully, and should include audio feedback.","2014","2023-07-12 06:29:57","2023-07-19 03:40:54","","375–385","","6","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NXJQ75NP","journalArticle","2012","Jensen, Kristoffer; Hjortkjær, Jens","An Improved Dissonance Measure Based on Auditory Memory","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16325","Because dissonance is an important part of music analysis and psychoacoustic research, improving the model for predicting dissonance is useful. In this new model, musical events are assumed to reside in short-term memory, lasting on the range of 3 to 5 seconds. The total dissonance is the sum of the local dissonance from the new event and the interaction with elements in memory. In subjective tests with different kinds of music, the model with memory consistently better predicts listeners’ reports of dissonance. Dissonant sounds in music give rise to a physiological arousal response in listeners.","2012","2023-07-12 06:30:01","2023-07-19 04:08:30","","350–354","","5","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FRNBMGKT","journalArticle","2018","Cuadrado, Francisco","Touch the Sound: Design and Development of a Tangible System for Sound Experimentation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19587","“Touch the Sound” is a technological tool specifically designed for children training in experimentation of sound. It is based on the interaction with physical and tangible elements (passive objects that children can organize and move inside a two-axis space). A tablet based app uses computer-vision procedures to recognize each object and its position and movement to playback audio files and modify different sound parameters in real time. The technological approach and design adopted according to the philosophy and objectives of the Touch the Sound project has proved to be effective, despite the drawbacks and technical problems encountered during the development process. Programming based on native Android OS tools has enabled the design of a system that achieves optimal performance results on devices with limited hardware resources. The system contributes to the media literacy of children, making them aware of the narrative possibilities of sound, and teaching them to control and modify its parameters.","2018","2023-07-12 06:30:18","2023-07-19 03:50:46","","478–485","","6","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6D7EUGA9","journalArticle","2012","Merchel, Sebastian; Altinsoy, M. Ercan; Stamm, Maik","Touch the Sound: Audio-Driven Tactile Feedback for Audio Mixing Applications","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16165","In this study experiments were conducted to determine if a person could distinguish percussive audio loops by their fingertips using audio-driven tactile feedback. The audio signal was adapted to generate a vibration signal (tactile feedback) taking into account the limited capabilities of the tactile modality. A systematic approach to find the different adaptation parameters is discussed. The vibrations were created by an electrodynamic shaker mounted behind a touch-sensitive screen. Results indicate percussive loops are best distinguished if the source features (e.g., frequency spectrum) and sequence features (e.g., rhythm) are maintained.","2012","2023-07-12 06:30:31","2023-07-19 04:31:17","","47–53","","1/2","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P9EQJTQP","journalArticle","2019","Wühle, Tom; Merchel, Sebastian; Altinsoy, M. Ercan","The Precedence Effect in Scenarios with Projected Sound","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19890","One solution to realize spatial sound reproduction without a large number of distributed loudspeakers is to create virtual sources in the desired directions by using highly focusing real sources that project sound on reflective boundaries. Auditory events are then ideally located in the direction of the virtual sources created by sound projection rather than the direction of the real sources. However, due to physically limited focusing capabilities of real sources, the perception of the listener is also influenced by sound that is directly radiated from the real source and, therefore, arrives earlier at the position of the listener. This study showed how emerging precedence caused by the leading direct sound affected auditory perception in scenarios with lagging projected sound. The level range from which the direct sound caused first noticeable changes in auditory events until it finally dominated the localization was found to be approximately 20 dB. It was shown that localization dominance of the projected sound did not immediately occur after localization dominance of the direct sound vanished. However, localization dominance of the projected sound occurred even though the presence of the direct sound was still perceptible. The results indicate that the temporal structure of the direct sound plays an important perceptual role in scenarios with projected sound. Real playback signals with complex temporal structures causing impulsive loudness fluctuations were shown to be more favorable for the perceptual dominance of the leading direct sound. Such structures are therefore critical in terms of sound projection.","2019","2023-07-12 06:30:35","2023-07-19 10:57:25","","92–100","","3","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MR4DAAHT","journalArticle","2016","Lafay, Grégoire; Misdariis, Nicolas; Lagrange, Mathieu; Rossignol, Mathias","Semantic Browsing of Sound Databases without Keywords","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18371","With the growing capability of recording and storage devices, the problem of indexing large audio databases has been the object of much attention. Most of this effort is dedicated to automatic inferences from indexed metadata. In contrast, browsing audio databases in an effective manner has been less considered. This report studies the relevance of a semantic organization of sounds to ease the browsing of a sound database. For such a task, semantic access to data is traditionally implemented by a keyword selection process. However, various limitations of written language, such as word polysemy, ambiguities, or translation issues, may bias the browsing process. Two sound presentation strategies organized sounds spatially to reflect an underlying semantic hierarchy. For the sake of comparison, the authors also considered a display whose spatial organization was only based on acoustic cues. Those three displays were evaluated in terms of search speed in a crowdsourcing experiment using two different corpora: environmental sounds from urban environments and sounds produced by musical instruments. Coherent results demonstrate the usefulness of an implicit semantic organization for representing sounds in terms of both search speed and of learning efficiency.","2016","2023-07-12 06:30:46","2023-07-19 04:16:53","","628–635","","9","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SFEMZ42S","journalArticle","2006","Väljamäe, Aleksander; Larsson, Pontus; Västfjäll, Daniel; Kleiner, Mendel","Vibrotactile Enhancement of Auditory-Induced Self-Motion and Spatial Presence","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13883","The entertainment industry frequently uses vibroacoustic stimulation, where chairs with embedded loudspeakers and shakers enhance the experience. Scientific investigations of the effect of such enhancers on illusory self-motion (vection) and spatial presence are largely missing. The current study examined whether auditory-induced vection (AIV) may be further augmented by the simultaneous presentation of additional vibrotactile cues delivered via mechanical shakers and low-frequency sound. It was found that mechanically induced vibrations increase AIV and spatial presence responses significantly. This cross-modal enhancement was stronger for stimuli containing an auditory–tactile simulation of a vehicle engine, demonstrating the benefits of the multisensory representation of virtual environments.","2006","2023-07-12 06:30:50","2023-07-19 04:54:47","","954–963","","10","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5LCI59SG","journalArticle","1994","Begault, Durand R.; Erbe, Tom","Multichannel Spatial Auditory Display for Speech Communications","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=6924","A spatial auditory display for multiple speech communications was developed at NASA/Ames Research Center. Input is spatialized by the use of simplified head-related transfer functions, adapted for FIR filtering on Motorola 56001 digital signal processors. Hardware and firmware design implementations are overviewed for the initial prototype developed for NASA-Kennedy Space Center. An adaptive staircase method was used to determine intelligibility levels of four-letter call signs used by launch personnel at NASA against diotic speech babble. Spatial positions at 30° azimuth increments were evaluated. The results from eight subjects showed a maximum intelligibility improvement of about 6-7 dB when the signal was spatialized to 60 or 90° azimuth positions.","1994","2023-07-12 06:31:26","2023-07-19 03:41:03","","819–826","","10","42","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PQQEFLWP","journalArticle","2012","Côté, Nicolas; Koehl, Vincent; Möller, Sebastian; Raake, Alexander; Wältermann, Marcel; Gautier-Turbin, Valérie","Diagnostic Instrumental Speech Quality Assessment in a Super-Wideband Context","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16213","A new speech quality model, DIAL (Diagnostic Instrumental Assessment of Listening), provides diagnostic information in both narrow-band and super wide-band contexts. It is “intrusive,” assuming that the original source audio is available. Because many quality-measuring techniques collapse all degradations into a single score, they do not help developers to diagnose the basis of that score. In contrast, the proposed DIAL model uses four quality dimensions: coloration, continuity, noisiness, and loudness.","2012","2023-07-12 06:31:35","2023-07-19 03:50:11","","156–164","","3","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2QCVM727","journalArticle","1998","Begault, Durand R.","Virtual Acoustics, Aeronautics, and Communications","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12140","An optimal approach to auditory display design for commercial aircraft would utilize both spatialized (3-D) audio techniques and active noise cancellation for safer operations. Results from several aircraft simulator studies conducted at NASA Ames Research Center are reviewed, including Traffic alert and Collision Avoidance System (TCAS) warnings, spoken orientation 'beacons' for gate identification and collision avoidance on the ground, and hardware for improved speech intelligibility. The implications of hearing loss among pilots is also considered.","1998","2023-07-12 06:31:38","2023-07-19 03:40:45","","520–530","","6","46","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X4GF795Z","journalArticle","2022","Elmosnino, Stephane","A Review of Literature in Critical Listening Education","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21737","This paper reviews the literature on critical listening education. Broadly speaking, academic research in this field is often limited to qualitative descriptions of curriculum and studies on the effectiveness of technical ear training. Furthermore audio engineering textbooks often view critical listening as secondary to technical concepts. To provide a basis for the development of curriculum and training, this paper investigates both academic and non-academic work in the field. Consequently a range of common curriculum topics is advanced as the focus areas in current practice. Moreover this paper uncovers pedagogical best practice for training sequence and the use of sounds/sight within instruction. A range of specific instructional activities, such as technical ear training, is also explored, thus providing insights into training in this field. Beyond a direct benefit to pedagogues, it is hoped that this review of the literature can provide a starting point for research in critical listening education.","2022","2023-07-12 06:31:41","2023-07-19 03:54:27","","328–339","","5","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IK2DL2KC","journalArticle","2016","Mo, Ronald; Wu, Bin; Horner, Andrew","The Effects of Reverberation on the Emotional Characteristics of Musical Instruments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18055","Previous research has shown that reverberation influences the perception of clarity, spaciousness, and other aspects of music. But the degree to which reverberation influences the emotional experience of musical instrument sounds is still not known. The authors conducted a listening test to compare the effect of reverberation on the emotional characteristics of eight instrument sounds representing the wind and bowed string families. The subjects compared paired stimuli for eight emotional categories: Happy, Sad, Heroic, Scary, Comic, Shy, Romantic, and Mysterious. For simple parametric reverberation, the results showed the following: a significant effect on Mysterious and Romantic for the back of a large hall; a medium effect on Sad, Scary, and Heroic for the back of a large hall; a mild effect on Happy for the front of a small hall; relatively little effect on Shy; and the opposite effect on Comic, with listeners judging anechoic sounds most Comic. These results give audio engineers and musicians an interesting perspective on simple parametric artificial reverberation. The motivation for this research was to understand how emotional characteristics vary with reverberation length and amount in simple parametric reverberation, which are equivalent to the hall size and the listeners distance to the front.","2016","2023-07-12 06:31:44","2023-07-19 04:32:15","","966–979","","12","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"83IYBR56","journalArticle","2014","Hjortkjær, Jens; Walther-Hansen, Mads","Perceptual Effects of Dynamic Range Compression in Popular Music Recordings","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17084","The belief that the use of dynamic range compression in music mastering deteriorates sound quality needs to be formally tested. In this study normal hearing listeners were asked to evaluate popular music recordings in original versions and in remastered versions with higher levels of dynamic range compression. Surprisingly, the results failed to reveal any evidence of the effects of dynamic range compression on subjective preference or perceived depth cues. Perceptual data suggest that listeners are less sensitive than commonly believed to even high levels of compression. As measured in terms of differences in the peak-to-average ratio, compression has little perceptual effect other than increased loudness or clipping effects that only occur at high levels of compression. One explanation for the inconsistency between data and belief might result from the fact that compression is frequently accompanied by additional processing such as equalization and stereo enhancement.","2014","2023-07-12 06:31:47","2023-07-19 04:06:18","","37–41","","1/2","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3EEP4QC9","journalArticle","2019","Pulkki, Ville; Pöntynen, Henri; Santala, Olli","Spatial Perception of Sound Source Distribution in the Median Plane","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20700","Modern spatial audio reproduction techniques with headphones or loudspeakers seek to control the perceived spatial image as accurately as possible in three dimensions. The mechanisms of spatial perception have been studied mainly in the horizontal plane, and this article attempts to shed some light on the corresponding phenomena in the median plane. Spatial perception of concurrently active sound sources was investigated in an exploratory listening experiment. Incoherent noise source distributions of varying spatial characteristics were presented from loudspeaker arrays in anechoic conditions. The arrays were coinciding with the ±45° angular sectors in the frontal median and horizontal planes. The task for immobile subjects was to report the directions of loudspeakers they perceived emitting sound. The results from median plane distributions suggest that two concurrent sources located along the vertical midline can be perceived individually without resorting to head movements when they are separated in elevation by 60° or more. With source pairs separated by less than 60°, and with more complex physical distributions, the distributions were perceived inaccurately, biased, and spatially compressed but nevertheless not as point-like auditory images.","2019","2023-07-12 06:31:51","2023-07-19 04:39:35","","855–870","","11","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LLBRRDXY","journalArticle","2006","Karjalainen, Matti; Mäki-patola, Teemu; Kanerva, Aki; Huovilainen, Antti","Virtual Air Guitar","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13884","[Engineering Report] A combination of handheld controllers and a guitar synthesizer is called “virtual air guitar” (VAG). The name refers to playing an “air” guitar, that is, just acting the playing with music playback, and the term virtual refers to making a playable synthetic instrument. Sensing of the left-to-right-hand distance is used for pitch control, the right-hand movements are used for plucking, and in advanced versions of the VAG the finger positions of both hands can be used for other features of sound production. Three different hand gesture controllers are discussed. The sound synthesis algorithm simulates the electric guitar, augmented with sound effects such as tube amplifier distortion, as well as intelligent mapping from playing gestures to synthesis parameters. The realization of the virtual instrument is described, and sound demonstrations are available on a Web site.","2006","2023-07-12 06:31:55","2023-07-19 04:10:14","","964–980","","10","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N8AIKQYG","journalArticle","2021","Wühle, Tom; Merchel, Sebastian; Altinsoy, M. Ercan","Localization Masking—Reducing the Influence of the Direct Sound on Localization in Sound Projection by the Additional Generation of One or More Leading Sounds","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21464","In spatial audio reproduction with sound projection, instead of placing loudspeakers in specific directions, virtual sources are created by projecting sound on reflective boundaries. The projected sound should dominate the localization. One limiting factor is the leading direct sound occurring because of physical limitations of the focusing capabilities of sound projectors. In this paper, localization masking, a method to reduce the influence of this direct sound on localization, is introduced. Localization masking was investigated in an anechoic chamber with cascaded lead-lag pairs representing the sounds involved. The sounds were reproduced via individual loudspeakers. Natural percussion signals with transient temporal structures were used. The lag localization dominance threshold, defined as the maximum lead level at which the direction of the auditory events is in the direction of the lag, was measured using a method of adjustment. Localization masking caused this threshold to shift to up to a 7-dB higher lead level. Therefore localization masking reduced the influence of the initial lead, representing the direct sound, on localization. In practical sound projection scenarios, localization masking may improve the projection of signals with transient structures or reduce the requirements on the focusing capabilities of sound projectors that are used to project such signals.","2021","2023-07-12 06:32:19","2023-07-19 10:57:33","","683–693","","9","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZXBVZW65","journalArticle","2014","Merchel, Sebastian; Altinsoy, M. Ercan","The Influence of Vibrations on Musical Experience","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17134","The coupled perception of sound and vibration is a well-known phenomenon during live rock and pop concerts. Measurements in concert halls and churches have confirmed that sound can excite perceivable vibrations on the surface of the body even during classical performances. This research explores if vibrations have an influence on the quality of the listening experiences. Therefore, sound and seat vibrations were controlled separately in an audio reproduction scenario. Vibrations were generated from audio recordings using various approaches. Different parameters during this process were examined in relation to their perceptual consequences. It can be concluded that vibrations play a significant role in the perception of music. Real concert halls might benefit from amplifying the vibrations (passively or actively) in the auditorium.","2014","2023-07-12 06:32:27","2023-07-19 04:31:08","","220–234","","4","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J2IIK4RJ","journalArticle","2015","Manor, Ella; Martens, William; Marui, Atsushi; Cabrera, Densil","Nearfield Crosstalk Increases Listener Preferences for Headphone-Reproduced Stereophonic Imagery","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17638","Although final mixing and mastering is monitored over loudspeakers, the majority of music listeners use headphones on mobile devices. Preferences for spatial process depend on the method of reproduction. For a variety of program material using headphones, listeners often prefer a stereophonic image that is created by simulating nearfield crosstalk compared to the biphonic spatial image. This novel approach, called Nearfield Crosstalk Simulation, describes crosstalk that simulates closely located loudspeakers. Previous work used farfield crosstalk simulation in an effort to produce an enhanced stereophonic effect, but such results were less preferred. The primary difference between the more conventional farfield crosstalk and the novel nearfield crosstalk developed for this study was the introduction of a level and a time difference at low frequency, consistent with what actually occurs for sound sources very close to a listener’s head.","2015","2023-07-12 06:32:33","2023-07-19 04:26:51","","324–335","","5","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MYD9BU5I","journalArticle","2009","Korhola, Henri; Karjalainen, Matti","Perceptual Study and Auditory Analysis on Digital Crossover Filters","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14825","The extensive research on the perceptual attributes of analog filters used for loudspeaker crossover networks does not necessarily apply to digital filters. In this study finite-impulse response (FIR) and Linkwitz–Riley (LR) digital crossover filters were examined for their perceptual artifacts. Subjective tests with headphones and loudspeakers showed that for LR filters the audibility of phase distortion can be predicted by group delay errors. But FIR filters of high order produce audible artifacts because of time smear created by extensive ringing. LR filters of order 8 or less and FIR filters of about 600 were without problems. These safety limits should be respected.","2009","2023-07-12 06:32:37","2023-07-19 04:16:09","","413–429","","6","57","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7NP8A589","journalArticle","1994","Stuart, J. Robert","Noise: Methods for Estimating Detectability and Threshold","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=6959","It is often necessary to estimate or compare the detectability or significance of noises. A review and development of the underlying psychoacoustics of noise detection and threshold phenomena are presented. Particular attention is paid to noises of arbitrary spectral shape. Various measures of human auditory frequency selectivity are contrasted, including critical band, critical ratio, auditory filter, and active process considerations. Estimation techniques based on weighting are compared with those given by detection criteria applied to more sophisticated auditory modeling. Some definitive recommendations are made.","1994","2023-07-12 06:32:46","2023-07-19 04:50:12","","124–140","","3","42","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GFC9ZW4X","journalArticle","2017","Shirley, Ben Guy; Meadows, Melissa; Malak, Fadi; Woodcock, James Stephen; Tidball, Ash","Personalized Object-Based Audio for Hearing Impaired TV Viewers","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18562","Age demographics have led to an increase in the proportion of the population suffering from some form of hearing loss. The introduction of object-based audio to television broadcasting has the potential to improve the viewing experience for millions of hearing impaired people. Personalization of object-based audio can assist in overcoming difficulties in understanding speech and the narrative audio. This research presented describes a Multi-Dimensional Audio (MDA) implementation of object-based clean audio that presents independent object streams based on object-category elicitation. Evaluations were carried out with hearing impaired people, and participants were able to personalize audio levels independently for four object-categories using an on-screen menu: speech, music, background effects, and foreground effects related to on-screen events. Results show considerable preference variation across subjects but nevertheless the expanding object-category personalization beyond a binary speech/nonspeech categorization can substantially improve the viewing experience for some hearing impaired people.","2017","2023-07-12 06:32:49","2023-07-19 04:47:49","","293–303","","4","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LR7PKK6A","journalArticle","2016","Mo, Ronald; Choi, Ga Lam; Lee, Chung; Horner, Andrew","The Effects of MP3 Compression on Perceived Emotional Characteristics in Musical Instruments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18523","Musical instrument sounds have distinct timbral and emotional characteristics that can change when audio processing is applied. This paper investigates the effects of MP3 compression on the emotional characteristics of eight sustained instrument sounds using listening tests. The experimental paradigm involved a pairwise comparison of compressed and uncompressed samples at several bit rates over ten emotional categories. The results showed that MP3 compression strengthened neutral and negative emotional characteristics such as Mysterious, Shy, Scary, and Sad, and weakened positive emotional characteristics such as Happy, Heroic, Romantic, Comic, and Calm. Angry was relatively unaffected by MP3 compression, probably because the background “growl” artifacts added by MP3 compression decreased positive emotional characteristics and increased negative characteristics such as Mysterious and Scary. Compression effected some instruments more and others less; trumpet was the most effected and the horn the least.","2016","2023-07-12 06:33:03","2023-07-19 04:31:51","","858–867","","11","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UATCP3PU","journalArticle","2020","Kim, Chungeun; Lim, Veranika; Picinali, Lorenzo","Investigation Into Consistency of Subjective and Objective Perceptual Selection of Non-individual Head-Related Transfer Functions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20996","The binaural technique uses a set of direction-dependent filters known as Head-Related Transfer Functions (HRTFs) in order to create 3D soundscapes through a pair of headphones. Although each HRTF is unique to the person it ismeasured from, due to the cost and complexity of the measurement process pre-measured non-individual HRTFs are generally used. This study investigates whether it is possible for a listener to perceptually select the best-fitting non-individual HRTFs in a consistent manner, using both subjective and objective methods. 16 subjects participated in 3 repeated sessions of binaural listening tests. During each session, participants firstly listened tomoving sound sources spatialized using 7 different non-individual HRTFs and ranked them according to perceived plausibility and externalization (subjective selection). They then performed a localization task with sources spatialized using the same HRTFs (objective selection). In the subjective selection, 3 to 9 participants showed test-retest reliability levels that could be regarded as good or excellent depending on the attribute under question, the source type, and the trajectory. The reliability was better for participants with musical training and critical audio listening experience. In the objective selection, it was not possible to find significant differences between the tested HRTFs based on localization-related performances.","2020","2023-07-12 06:33:12","2023-07-19 04:11:25","","819–831","","11","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F97CEIJW","journalArticle","2021","Turchet, Luca; Rinaldo, Edoardo","Technical Performance Assessment of the Ableton Link Protocol Over Wi-Fi","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21469","To date, Ableton Link is the most widely adopted synchronization protocol for musical applications based on Wi-Fi networks. However the limitations of Link over Wi-Fi in terms of scalability are not known, an understanding that may be useful to designers of musical ecosystems involving many nodes to be synchronized. In this paper we present four experiments aiming to investigate how the protocol performance is affected by the number of connected devices, kind of Wi-Fi access point utilized, and connection or disconnection of nodes. Results showed the reliability of the protocol only for a limited number of nodes, which was 22 for a consumer-grade portable router and 41 for a mesh network created by two high-end access points. The protocol performances were found to decrease with the number of devices and when nodes connected or disconnected. Furthermore, the performances of Link are tightly bounded to that of Wi-Fi, which can vary significantly from day to day depending on network load and interferences. Taken together, our findings indicate that Link over Wi-Fi is not suitable for ensuring synchronization in ecosystems with a high number of nodes, and we call for new wireless technologies suitable for large scale synchronizations in co-located settings.","2021","2023-07-12 06:33:15","2023-07-19 04:53:57","","748–756","","10","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FD8MEDIZ","journalArticle","2006","Faller, Christof","Multiple-Loudspeaker Playback of Stereo Signals","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13886","A perceptually motivated spatial decomposition for two-channel stereo audio signals, capturing the information about the virtual sound stage, is proposed. The spatial decomposition allows resynthesizing audio signals for playback over sound systems other than twochannel stereo. With the use of more front loudspeakers the width of the virtual sound stage can be increased beyond ±30° and the sweet-spot region is extended. Optionally, lateral independent sound components can be played back separately over loudspeakers on the sides of a listener to increase listener envelopment. It is also explained how the spatial decomposition can be used with surround sound and wavefield synthesis–based audio systems.","2006","2023-07-12 06:33:19","2023-07-19 03:55:28","","1051–1064","","11","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8XKCN68J","journalArticle","2016","Rummukainen, Olli; Romblom, David; Guastavino, Catherine","Diffuse Field Modeling Using Physically-Inspired Decorrelation Filters and B-Format Microphones: Part II Evaluation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18128","The Diffuse Field Model (DFM) described in Part 1 is perceptually evaluated in this article. Two experiments were conducted. In first experiment, sound recording professionals rated different treatments of DFM presented on a 20-channel array. This evaluation included the geometric modeling of reflections, strategies involving the early portion of the B-Format Room Impulse Response (RIR), and a comparison between 0th- and 1st-order RIR. Results indicate that it is necessary to model the earliest reflections and to use all four channels of the B-Format room impulse response. In the second experiment, musicians and sound recording professionals were asked to rate DFM and common microphone techniques presented on 3/2 stereophonic setup. DFM was found to be perceptually comparable to the Hamasaki Square technique. DFM approach used in this study is part of a physically-plausible virtual acoustic model for sources that were captured with close microphone placement. This model replaces the panning, delay, and reverberation that would typically be used. DFM is a perceptually viable method to create room impression that allows free placement of anechoic point sources in arbitrary multichannel loudspeaker setups.","2016","2023-07-12 06:33:22","2023-07-19 04:42:36","","194–207","","4","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3EHJAYK6","journalArticle","2016","Mulder, Johannes","Amplified Music and Sound Level Management: A Discussion of Opportunities and Challenges","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18123","Because music-induced hearing disorders and noise pollution from concerts emerge at the crossroads of technology, culture, and society, this paper argues that multidisciplinary approaches are required to address these two issues. Even though noise regulations and hearing risk-mitigation policies may be different, best practices originate from procedures and policies that are developed from an understanding of the multiple stakeholder perspectives. Each stakeholder has a unique perspective but these conflicting differences must be reconciled one way or another. Technology may offer options.","2016","2023-07-12 06:33:25","2023-07-19 04:34:06","","124–131","","3","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IW326I3K","journalArticle","2022","Lladó, Pedro; Mckenzie, Thomas; Meyer-Kahlen, Nils; Schlecht, Sebastian J.","Predicting Perceptual Transparency of Head-Worn Devices","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21825","Acoustically transparent head-worn devices are a key component of auditory augmented reality systems, in which both real and virtual sound sources are presented to a listener simultaneously. Head-worn devices can exhibit high transparency simply through their physical design but in practice will always obstruct the sound field to some extent. In this study, a method for predicting the perceptual transparency of head-worn devices is presented using numerical analysis of device measurements, testing both coloration and localization in the horizontal and median plane. Firstly, listening experiments are conducted to assess perceived coloration and localization impairments. Secondly, head-related transfer functions of a dummy head wearing the head-worn devices are measured, and auditory models are used to numerically quantify the introduced perceptual effects. The results show that the tested auditory models are capable of predicting perceptual transparency and are therefore robust in applications that they were not initially designed for.","2022","2023-07-12 06:33:30","2023-07-19 04:21:16","","585–600","","7/8","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2X6F9HGH","journalArticle","2021","Shier, Jordie; McNally, Kirk; Tzanetakis, George; Brooks, Ky Grace","Manifold Learning Methods for Visualization and Browsing of Drum Machine Samples","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21015","The use of electronic drum samples is widespread in contemporary music productions, with music producers having an unprecedented number of samples available to them. The task of organizing and selecting from these large collections can be challenging and time consuming, which points to the need for improved methods for user interaction. This paper presents a system that computationally characterizes and organizes drum machine samples in two dimensions based on sound similarity. The goal of the work is to support the development of intuitive drum sample browsing systems. The methodology presented explores time segmentation, which isolates temporal subsets from the input signal prior to audio feature extraction, as a technique for improving similarity calculations. Manifold learning techniques are compared and evaluated for dimensionality reduction tasks, and used to organize and visualize audio collections in two dimensions. This methodology is evaluated using a combination of objective and subjective methods including audio classification tasks and a user listening study. Finally, we present an open-source audio plug-in developed using the JUCE software framework that incorporates the findings from this study into an application that can be used in the context of a music production environment.","2021","2023-07-12 06:33:34","2023-07-19 04:47:40","","40–53","","1/2","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DBUXRZD8","journalArticle","2014","Pihlajamäki, Tapani; Santala, Olli; Pulkki, Ville","Synthesis of Spatially Extended Virtual Source with Time-Frequency Decomposition of Mono Signals","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17339","Auditory displays, driven by nonauditory data, are often used to present a sound scene to a listener. Typically, the sound field places sound objects at different locations, but the scene becomes aurally richer if the perceived sonic objects have a spatial extent (size), called volumetric virtual coding. Previous research in virtual-world Directional Audio Coding has shown that spatial extent can be synthesized from monophonic sources by applying a time-frequency-space decomposition, i.e., randomly distributing time-frequency bins of the source signal. This technique does not guarantee a stable size and the timbre can degrade. This study explores how to optimize volumetric coding in terms of timbral and spatial perception. The suggested approach for most types of audio uses an STFT window size of 1024 samples and then distributes the frequency bands from lowest to highest using the Halton sequence. The results from two formal listening experiments are presented.","2014","2023-07-12 06:33:39","2023-07-19 04:38:17","","467–484","","7/8","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PLJSW8DJ","journalArticle","1993","Persterer, Alexander; Opitz, Martin; Koppensteiner, Christian; Nefjodova, M.; Müller, Christian; Berger, Meinhard","AUDIMIR: Directional Hearing at Microgravity","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=7005","In October 1991 the first Austrian cosmonaut spent one week on board the Soviet space station MIR. One of the 14 experiments carried out there was AKG's AUDIMIR, a psychoacoustics technological experiment. AUDIMIR was designed to investigate the accuracy of directional hearing and its role as part of the human orientation system at microgravity.","1993","2023-07-12 06:33:53","2023-07-19 04:37:52","","239–247","","4","41","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZU2RNVBQ","journalArticle","2012","Herre, Jürgen; Purnhagen, Heiko; Koppens, Jeroen; Hellmuth, Oliver; Engdegård, Jonas; Hilper, Johannes; Villemoes, Lars; Terentiv, Leon; Falch, Cornelia; Hölzer, Andreas; Valero, María Luis; Resch, Barbara; Mundt, Harald; Oh, Hyen-O","MPEG Spatial Audio Object Coding—The ISO/MPEG Standard for Efficient Coding of Interactive Audio Scenes","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16371","In 2010 the ISO/MPEG Audio standardization group issued the Spatial Audio Object Coding (SAOC) specification to define technology for parametric low bit-rate coding of audio object signals with a mono or stereo downmix. This paper provides an overview of MPEG SAOC technology, discussing recent verification tests. The authors examine operation modes for typical application scenarios by taking advantage of object-based processing. Most important, SAOC enables transmission of multi-object signals at data rates of the same order of magnitude as those used to represent two-channel audio. The important application scenarios are envisaged to be high-quality spatial teleconferencing, personal audio, interactive gaming, and rich media. Because the SAOC representation is independent of any particular loudspeaker setup, SAOC signals can be rendered efficiently on either a target loudspeaker configuration or portable device.","2012","2023-07-12 06:33:56","2023-07-19 04:05:32","","655–673","","9","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IFCRQYDI","journalArticle","2015","Rottondi, Cristina; Buccoli, Michele; Zanoni, Massimiliano; Garao, Dario; Verticale, Giacomo; Sarti, Augusto","Feature-Based Analysis of the Effects of Packet Delay on Networked Musical Interactions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18047","When musicians at multiple physical locations attempt to play together, the limiting factor is unavoidable packet delays and jitter introduced by the IP network that connects them together. This research investigates the musical tolerance of adverse network conditions as a function of rhythmic complexity and tempo. Results show that with higher network latency: (a) musicians exhibit a more pronounced tendency to decelerate with more rhythmically complex pieces; (b) rhythmical complexity does not significantly worsen musician perception of the delay and interaction quality; (c) among the timbral features, instruments with a higher spectral entropy and spectral flatness (such as guitars and drums) lead to larger tempo slowdown. Low-latency networks promise to revolutionize interactive music such as remote rehearsals and music teaching.","2015","2023-07-12 06:33:59","2023-07-19 04:42:12","","864–875","","11","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M24IF9QK","journalArticle","2019","Woodcock, James; Davies, William J.; Cox, Trevor J.","Influence of Visual Stimuli on Perceptual Attributes of Spatial Audio","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20494","Although audio is often reproduced with a visual counterpart, the audio technology for these systems is often researched and evaluated in isolation from the visual component. Previous research indicates that the auditory and visual modalities are not processed separately by the brain. For example, visual stimuli can influence ratings of audio quality and vice versa. This paper presents an experiment to investigate the influence of visual stimuli on a set of attributes relevant to the perception of spatial audio. Eighteen participants took part in a paired comparison listening test where they were asked to judge pairs of stimuli rendered with fourteen-, five-, and two-channel systems using ten perceptual attributes. The stimuli were presented in both audio only and audiovisual conditions. The results show a significant and large effect of the loudspeaker configuration for all the tested attributes other than overall spectral balance and depth of field. The effect of visual stimuli was found to be small and significant for the attributes realism, sense of space, and spatial clarity. These results suggest that evaluations of audiovisual technologies that are aimed to evoke a sense of realism or presence should consider the influence of both the audio and visual modalities.","2019","2023-07-12 06:34:10","2023-07-19 10:56:47","","557–567","","7/8","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F8RA5M76","journalArticle","2011","Jansen, Reinier J.; Özcan, Elif; van Egmond, René","PSST! Product Sound Sketching Tool","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15936","Designing and evaluating a product’s sound during the conceptual phase is both more effective and efficient. Many products produce sounds that are intentional (such as the ring of a phone) and as artifacts (such as the motor noise of a vacuum cleaner). In most cases, the sonic aspects of a product are considered after the design is mostly complete where the choices are limited. A tool that allows sounds to be “sketched” is suggested as a means for industrial designers to begin the sound design process at the earliest stages. A preliminary analysis suggests that an inexperienced sound designer can successfully sketch a sound with the appropriate adjectives.","2011","2023-07-12 06:34:14","2023-07-19 04:08:14","","396–403","","6","59","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"92V2TZIC","journalArticle","2020","Çakmak, Cem; Hamilton, Rob","od: Composing Spatial Multimedia for the Web","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20991","Composers have been exploring multi-channel sound field spatialization since the early days of electronic music. However, reproduction of such works outside of specialized concert spaces and research facilities or even their accurate reproduction within those spaces remain difficult and unpredictable at best. Combining the reach and simplicity of web browsers with ambisonic to binaural rendering, Web Audio-based tools can ensure greater accessibility for existing spatial works as well as acting as a platform upon which new ones can be implemented. At times with such practices the developing technologies become deprecated or obsolete during the period of making the work. This paper describes the technical design and artistic conception of od, a spatial multimedia production for binaural listening on the web. The project has led us to develop a workflow without relying on specific tools that can be of use as a framework for documenting existing spatial works or novel browser-based creative applications.","2020","2023-07-12 06:34:17","2023-07-19 03:47:04","","747–755","","10","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3R2PLLMI","journalArticle","1975","Richmond, Charles","A Practical Theatrical Sound Console","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=2715","The requirements for a sound control console for the legitimate theater are unique and in many ways the reverse of those of a recording board. A stock unit is described which solves most of these requirements simply and economically, allowing a single operator to effectively adjust over 100 controls simultaneously without the need for computer automation interfacing.","1975","2023-07-12 06:34:27","2023-07-19 04:40:35","","36–40","","1","23","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G2DS54H6","journalArticle","2022","Allan, Jon; Leijonhufvud, Susanna","Listener Preferences in Streamed Music","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21557","A cross-disciplinary study between the two research areas of Audio Technology and Music Education was performed to assess how different aspects of education and experience may influence the experience of music listening given a typical streaming service---Spotify.1 The point of departure is that streamed media facilitates a plenitude of versions of the same song. The paper focuses on the differences that these different songs yield from various mastering processes and production choices motivated by the end distribution media and user settings in the playback system that aim to alter the sound. These variations may all lead to differences in musical dynamics and timbre. A listening test was conducted to examine listeners' preferences, the assessed audio quality, and subjects' reports on how the music content affected them when given the possibility to compare versions in a controlled environment. The test subjects (n = 76) represented populations with various educational backgrounds and experience within music and audio technology. Among the results, it was found that education and experience in some cases do affect preferences.","2022","2023-07-12 06:34:30","2023-07-19 03:36:02","","156–176","","3","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8W9DVZPI","journalArticle","2017","Ronan, Malachy; Ward, Nicholas; Sazdov, Robert; Lee, Hyunkook","The Perception of Hyper-Compression by Mastering Engineers","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19176","Hyper-compressed popular music is associated with the overuse of dynamic range processing in an effort to gain a competitive advantage in music production. This behavior should be unnecessary given the availability of loudness normalization algorithms across the industry; the practice has been denounced by mastering engineers as generating audible artefacts. However, the audibility of these artefacts to mastering engineers has not been examined. This study probes this question using an ABX listening experiment with 20 mastering engineers. On average, mastering engineers correctly discriminated 17 out of 24 conditions, suggesting that the sound quality artefacts generated by hyper-compression are difficult to perceive. The findings in the study suggest that audibility depends on the crest factor (CF) of the music rather than the amount of CF reduction, thus proposing the existence of a threshold of audibility.","2017","2023-07-12 06:34:33","2023-07-19 04:41:56","","613–621","","7/8","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HHNAT55Z","journalArticle","2019","Melchior, Vicki R.","High-Resolution Audio: A History and Perspective","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20455","Exploration of audio at digital resolutions higher than CD began in the late 1980s, arising from a wealth of interdependent sources including listening experiences, rapid technical advances, an appreciation of psychoacoustics, acoustic measurement, and the ethos that music recording should capture the full range of audible sound. High-resolution formats were developed and incorporated into successive generations of distribution media from DVD, SACD, and Blu-ray, to internet downloads and now to streaming. A continuing debate throughout has been whether, and especially why, higher resolutions should be audibly superior. This review covers the history of the period, focusing on the main drivers of experimentation and development up to the present, and then seeks to explain the current view that, beyond dynamic range, the most likely technical sources differentiating the sound of digital formats are the filtering chains that are ubiquitous in traditional digital sampling and reconstruction of analog music sources.","2019","2023-07-12 06:34:39","2023-07-19 04:30:18","","246–257","","5","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7H9FYTNP","journalArticle","2011","Sabin, Andrew Todd; Rafii, Zafar; Pardo, Bryan","Weighted-Function-Based Rapid Mapping of Descriptors to Audio Processing Parameters","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15938","User interfaces of audio tools and processors can be difficult for novices to use because technical parameters provide little guidance about their sonic manifestation. Borrowing a technique from psychoacoustics, the authors explore an efficient means for mapping users’ descriptors (target adjectives) to technical parameters. Weighting functions are created based on the relative influence of a parameter in influencing the adjective descriptor, such as warm, bright, and full. This approach was tested on two common types of processing: equalization and reverberation. Despite the relative simplicity of the approach, the results are promising.","2011","2023-07-12 06:34:42","2023-07-19 04:45:58","","419–430","","6","59","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3LKJKQ3F","journalArticle","2022","Moffat, David; De Man, Brecht; Reiss, Joshua D.","Semantic Music Production: A Meta-Study","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21823","This paper presents a systematic review of semantic music production, including a meta-analysis of three studies into how individuals use words to describe audio effects within music production. Each study followed different methodologies and stimuli. The SAFE project created audio effect plug-ins that allowed users to report suitable words to describe the perceived result. SocialFX crowdsourced a large data set of how non-professionals described the change that resulted from an effect applied to an audio sample. The Mix Evaluation Data Set performed a series of controlled studies in which students used natural language to comment extensively on the content of different mixes of the same groups of songs. The data sets provided 40,411 audio examples and 7,221 unique word descriptors from 1,646 participants. Analysis showed strong correlations between various audio features, effect parameter settings, and semantic descriptors. Meta-analysis not only revealed consistent use of descriptors among the data sets but also showed key differences that likely resulted from the different participant groups and tasks. To the authors' knowledge, this represents the first meta-study and the largest-ever analysis of music production semantics.","2022","2023-07-12 06:34:45","2023-07-19 04:32:23","","548–564","","7/8","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YGI6QI9G","journalArticle","2013","Xie, Bosun; Zhong, Xiaoli; Yu, Guangzheng; Guan, Shanquin; Rao, Dan; Liang, Zhiqiang; Zhang, Chengyun","Report on Research Projects on Head-Related Transfer Functions and Virtual Auditory Displays in China","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16826","In recent years, audio researchers in China have been devoting significant resources to the study of head-related transfer functions (HRTFs) and virtual auditory displays (VADs) in such applications as acoustics, computer games, signal processing, hearing aids, spatial sound reproductions, and many others. With financial support from the National Nature Science Fund of China, South China University of Technology and the Beijing University of Posts and Telecommunications have launched a long-term research project on HRTFs and VADs. This engineering report presents highlights of these research projects, including studies on HRTF measurements and database construction, statistical analysis on measured HRTFs, a dynamic VAD system, and an algorithm for reducing timbre coloration on virtual surround sound reproduction.","2013","2023-07-12 06:34:49","2023-07-19 10:58:15","","314–326","","5","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V9RHFEHU","journalArticle","2015","Karandreas, Alex; Christensen, Flemming","Influence of Visual Appearance on Loudspeaker Sound Quality Evaluation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17872","This study investigates the importance of both the auditory and visual modalities when evaluating subjective quality. Bimodal experiments comprising audiovisual and unimodal presentations were used to explore the interaction between modalities. Audio stimuli of varied degradation were coupled with both actual loudspeakers of different visual appearance and scaled photographs of the same loudspeakers. As would be expected, the factor audio had the strongest influence on quality in the audiovisual session. However, in visual only presentations, the factor visual was statistically significant. This indicates that when presented in isolation, the differences between the visual stimuli are perceived clearly and are judged to be substantial but become obscure in the presence of audio stimuli. From a product design perspective, the results suggest that the modalities are independent and that a change in subjective quality in either modality would combine linearly.","2015","2023-07-12 06:34:52","2023-07-19 04:09:54","","684–697","","9","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TAA374LI","journalArticle","2021","Brezina, Pavol","Perspectives of Advanced Ear Training Using Audio Plug-Ins","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21039","Technical ear training is currently gaining more and more attention during the training of professional sound engineers. In recent years, a number of web applications and standalone programs have been created to provide a basic training interface for ear training. However, with the greater accessibility of audio plug-ins and the rising demand to use them in production, the question arises as to how these tools could be integrated into the ear training process. The aim of this article is to point out the specifics and possibilities of using audio plug-ins as training tools through the unique prototype of a proprietary, standalone host.","2021","2023-07-12 06:34:55","2023-07-19 03:45:00","","351–358","","5","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HJZV3Y5L","journalArticle","2014","Kim, Sungyoung; Ikeda, Masahiro; Martens, William L.","Reproducing Virtually Elevated Sound via a Conventional Home-Theater Audio System","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17244","While adding a height dimension to a surround sound system implies the addition of loudspeakers on the ceiling, this study explores the means for synthesizing the 3rd dimension by signal processing. Creating a virtual elevated auditory image by using the conventional lateral 5.1 loudspeaker configuration greatly simplifies the burden on the consumer. The proposed system has been implemented using a crosstalk-cancellation method optimized for three of the five channels in a home-theater system: center, left-surround, and right-surround. This hybrid method, based on earlier work by Klepko, required the calculation of two inverse filters. Preliminary listening tests showed that loudspeakers at ear level could render sound sources perceived to be at higher elevations, and that the perceived elevation angles increased monotonically with the target elevation angles.","2014","2023-07-12 06:35:01","2023-07-19 04:11:53","","337–344","","5","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DFYPTSNU","journalArticle","2004","Andersen, Tue Haste; Jensen, Kristoffer","Importance and Representation of Phase in the Sinusoidal Model","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13029","Work is presented on the representation and perceptual importance of phase. Based on a standard sinusoidal analysis/synthesis system, the phase alignment of the sound components is analyzed. A novel phase representation, partial-period phase, is introduced, which characterizes phase evolution over time with an almost stationary parameter for many musical sounds. The proposed partial-period phase representation is used to control the phase when synthesizing sounds. Sounds synthesized with varying amounts of phase information are compared in a listening experiment with 11 subjects. It is shown that phase is of great importance to the perception of the sound quality of common harmonic musical sounds, but indications are found that phase is not of importance to the slightly inharmonic piano sounds. In particular, the sound degradation is large for low-pitched sounds, approaching ""slightly annoying"" when no phase information is used. In addition, a model based on the partialperiod phase representation has a significantly better perceived sound quality than sounds with random phase shifts.","2004","2023-07-12 06:35:14","2023-07-19 03:36:21","","1157–1169","","11","52","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WIVWR8LM","journalArticle","2019","Lund, Thomas; Mäkivirta, Aki; Naghian, Siamäk","Time for Slow Listening","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20547","Conscious perception is influenced by long-term experience and learning, to an extent that it might be more accurately understood and studied as primarily a reach-out phenomenon, at least in adults. Considering human hearing, time is a deciding factor on several scales, and the sensory information flow rate, otherwise termed the perceptual bandwidth, is modest. We introduce the term “slow listening” and discuss how new findings from other fields of science should be taken into account in pro audio, for instance when conducting subjective tests, and when preserving content for future generations to enjoy.","2019","2023-07-12 06:35:17","2023-07-19 04:25:34","","636–640","","9","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V79VZT7N","journalArticle","2014","Terrell, Michael; Simpson, Andrew; Sandler, Mark","The Mathematics of Mixing","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17081","Although audio mixing has always been viewed as the artistic task of either a conductor balancing the musicians in a live performance or a mixing engineer combining multiple tracks in a sound studio, this research considers mixing as a mathematical optimization problem. Using an auditory model, the authors demonstrated how numerical optimization can be used to pose and solve a mix problem. There is interplay between artistic objectives, perceptual constraints, and engineering methods. Taking loudness as an example, it is shown that the nonlinearity in the perceptual model leads to complex behavior, which can be overcome by careful choice of optimization strategies and parameters.","2014","2023-07-12 06:35:20","2023-07-19 04:52:17","","4–13","","1/2","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VVR8JD2J","journalArticle","2021","Gori, Matteo; Ceccarelli, Andrea","Benchmarking Networked Music Performance Tools With the NMP-Bench Model and Architecture","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21466","Networked Music Performance (NMP) aims at establishing a live interaction between musicians remotely connected that performs as if they were in the same room. While several NMP tools have been proposed through the last 20 years, a benchmark that aims at measuring and comparing their quality is currently not present. In this paper we propose the Networked Music Performance Benchmark (NMP-Bench), the first approach to systematically analyze and compare the performance of NMP tools. Focused on server-based NMP and its auditory component, NMP-Bench provides a comprehensive approach to measure and quantitatively compare NMP tools, encompassing network, music, and effectiveness metrics, with the goal of understanding the technical gaps that may reduce the experience of the performers. The paper presents the NMP-Bench model and architecture, which is then applied to benchmark two NMP tools (in simulated settings with no actual musicians) over three music pieces of different music styles. Results show differences between the two tools; this supports our statement that NMP-Bench can be used to select the most suitable tool and highlight strengths and weaknesses of NMP tools.","2021","2023-07-12 06:35:23","2023-07-19 04:02:40","","708–719","","10","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8QA8VZWX","journalArticle","2023","Pedersen, Rasmus Lundby; Picinali, Lorenzo; Kajs, Nynne; Patou, François","Virtual-Reality-Based Research in Hearing Science: A Platforming Approach","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22144","The lack of ecological validity in clinical assessment, as well as the challenge of investigating multimodal sensory processing, remain key challenges in hearing science. Virtual Reality (VR) can support hearing research in these domains by combining experimental control with situational realism. However, the development of VR-based experiments is traditionally highly resource demanding, which places a significant entry barrier for basic and clinical researchers looking to embrace VR as the research tool of choice. The Oticon Medical Virtual Reality (OMVR) experiment platform fast-tracks the creation or adaptation of hearing research experiment templates to be used to explore areas such as binaural spatial hearing, multimodal sensory integration, cognitive hearing behavioral strategies, auditory-visual training, etc. In this paper, the OMVR's functionalities, architecture, and key elements of implementation are presented, important performance indicators are characterized, and a use-case perceptual evaluation is presented.","2023","2023-07-12 06:35:28","2023-07-19 04:37:28","","374–389","","6","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G2Z2PAZT","journalArticle","2016","Wilson, Alex; Fazenda, Bruno M.","Perception of Audio Quality in Productions of Popular Music","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18102","In the context of recorded sound there is great debate over which parameters influence the perception of quality. To gain insight into the dimensions of quality perception, subjective and objective evaluation of musical program material, extracted from commercial CDs, was undertaken. It was observed that perception of audio quality and liking of the music can be affected by separate factors. Familiarity with stimuli affected like ratings, while quality ratings were most associated with signal features related to perceived loudness and dynamic range compression. Additionally, the sonic attributes describing quality ratings indicate a diverse lexicon relating to timbre, space, defects, and other concepts. The results also suggest that, while the perceived quality of popular music may have decreased over recent years, like ratings were unaffected. Like ratings were strongly influenced by song familiarity, implying that aspects of preference and liking are distinct from the interpretation of quality and might not be the best descriptors for studies where technical quality is the percept being sought. Quality in music production is revealed as a perceptual construct distinct from hedonic, musical preference. Audio quality can be predicted from objective features in the signal, and can be adequately and consensually described using verbal attributes.","2016","2023-07-12 06:35:31","2023-07-19 10:56:09","","23–34","","1/2","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UKCSI35G","journalArticle","2015","Hafezi, Sina; Reiss, Joshua D.","Autonomous Multitrack Equalization Based on Masking Reduction","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17637","In multitrack music production, some sounds get masked by other sounds and the listener has less ability to fully hear and distinguish the sound sources in the mix. The authors designed a simplified measure of masking based on best practices, and then implemented both an off-line and real-time, autonomous multitrack equalization system that reduces masking in multitrack audio. The system used objective measures of spectral masking in the resultant mixes. Listening tests provided a subjective comparison between the mix results of different implementations of the system, a raw mix, and manual mixes made by an amateur and a professional mix engineer. The results showed that autonomous systems reduce both the perceived and objective masking. The offline semi-autonomous system is capable of improving the raw mix better than an amateur and close to a professional mix by simply controlling one user parameter. The results also suggest that existing objective measures of masking are ill-suited for quantifying perceived masking in multitrack musical audio.","2015","2023-07-12 06:35:34","2023-07-19 04:03:11","","312–323","","5","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YF6DWJTG","journalArticle","2021","Hupke, Robert; Nophut, Marcel; Preihs, Stephan; Peissig, Jürgen","Toward Professional Distributed Performances: Effects of a Global Metronome on Networked Musical Ensemble Interactions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21467","Modern communication networks enable audiovisual interaction between geographically distant locations in near real time, leading to an increasing interest in networked music performance (NMP) and growing availability of related tools and applications. An important point of such distributed performances is the nature of interaction between the performers, which poses challenges toward, e.g., the network latency in the communication chain. Extensive research in the field of NMPs has shown that it is possible to achieve stabilization of synchrony and tempo deviation by providing a global time reference signal at each location of an NMP. In this study, for the first time, both an auditory and visual global metronome were integrated into the ecosystem of a physical NMP to evaluate the objective musical outcome and perceived benefit of the metronome with a professional music ensemble of five musicians between Munich and Hanover. The objective analysis shows that the metronome has a positive effect in terms of tempo stability at high latency levels, whereas synchrony strongly depends on the individual coping strategy of each musician. The subjective analysis suggests that a perceivable positive effect of the metronome is discernible for the musicians at all latency levels.","2021","2023-07-12 06:35:39","2023-07-19 04:07:29","","720–736","","10","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CMYVWN86","journalArticle","2016","Aguilera, Emanuel; Lopez, Jose J.; Cooperstock, Jeremy R.","Spatial Audio for Audioconferencing in Mobile Devices: Investigating the Importance of Virtual Mobility and Private Communication and Optimizations","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18138","Audioconferencing systems are becoming increasingly sophisticated, seeking to improve immersion, intelligibility, and sense of presence. Various opportunities exist to make mobile multiparty conference calls more realistic, interactive, and immersive. These include spatial audio, interactive manipulation of avatar positions, and whisper mode. The authors analyze the utility of avatar movement and sidebar whisper-mode functionality within which a subset of participants can engage in an ad-hoc sidebar conversation privately from the remaining participants. Participants found that avatar movement was useful, but approximately half the participants only used this feature for initial positioning of the avatars in a desired configuration in the first task. The ability to maintain sidebar conversations was used frequently and rated highly in negotiation activity. However, participants made almost no use of this feature in the conversation activity for which negotiation was not involved.","2016","2023-07-12 06:35:47","2023-07-19 03:34:59","","332–341","","5","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IR372ZDT","journalArticle","2016","Zacharakis, Asterios; Pastiadis, Konstantinos","Revisiting the Luminance-Texture-Mass Model for Musical Timbre Semantics: A Confirmatory Approach and Perspectives of Extension","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18372","This study describes a listening experiment designed to further examine the previously proposed luminance-texture-mass (LTM) model for timbral semantics. Thirty two musically trained listeners rated twenty four instrument tones on six predefined semantic scales: brilliance, depth, roundness, warmth, fullness, and richness. These six scales were analyzed with Principal Component Analysis (PCA) and Multidimensional Scaling (MDS) to produce two different timbre spaces. These timbre spaces were subsequently compared for their configurational and dimensional similarity with the LTM semantic space and the direct MDS perceptual space obtained from the same stimuli. The results showed that the selected semantic scales are adequately representing the LTM model and are fair at predicting the configurations of the sounds that result from pairwise dissimilarity ratings.","2016","2023-07-12 06:35:50","2023-07-19 10:58:59","","636–645","","9","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZMP23PZA","journalArticle","2016","Fan, Jianyu; Thorogood, Miles; Pasquier, Philippe","Automatic Soundscape Affect Recognition Using A Dimensional Approach","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18373","Soundscape studies have demonstrated a variety of approaches for investigating how soundscapes affect is part of immersive experiences. This research tries to develop an automatic affect recognition system that soundscape composers can use to create emotional compositions to evoke a target mood. In addition, this system can offer sound designers a more streamlined workflow for creating suitable sound effects for films and can offer engineers a way to design mood-enabled recommendation systems for retrieval of soundscape recordings. This research uses ground truth data collected from an online survey, and an analysis of the corpus shows that participants have a high level of agreement on the valence and arousal of soundscapes. The authors then generated a gold standard by averaging user responses. The propose system obtained better results than an expert-user model.","2016","2023-07-12 06:35:53","2023-07-19 03:55:47","","646–653","","9","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RTI3IZ6Q","journalArticle","2021","Yeoward, Christopher; Shukla, Rishi; Stewart, Rebecca; Sandler, Mark; Reiss, Joshua D.","Real-Time Binaural Room Modelling for Augmented Reality Applications","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21532","This paper proposes and evaluates an integrated method for real-time, head-tracked, 3D binaural audio with synthetic reverberation. Virtual vector base amplitude panning is used to position the sound source and spatialize outputs from a scattering delay network reverb algorithm running in parallel. A unique feature of this approach is its realization of interactive auralization using vector base amplitude panning and a scattering delay network, within acceptable levels of latency, at low computational cost. The rendering model also allows direct parameterization of room geometry and absorption characteristics. Varying levels of reverb complexity can be implemented, and these were evaluated against two distinct aspects of perceived sonic immersion. Outcomes from the evaluation provide benchmarks for how the approach could be deployed adaptively, to balance three real-time spatial audio objectives of envelopment, naturalness, and efficiency, within contrasting physical spaces.","2021","2023-07-12 06:35:57","2023-07-19 10:58:42","","818–833","","11","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RIKQA6NB","journalArticle","2003","Chen, Fang","Localization of 3-D Sound Presented through Headphone - Duration of Sound Presentation and Localization Accuracy","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12188","The relationship between the duration of a sound presentation and the accuracy of human localization is investigated. The three-dimensional sound is presented via headphones. The head-tracking system was integrated together with the sound presentation. Generalized headrelated transfer functions (HRTFs) are used in the experiment. Six different types of sounds with durations of 0.5, 2, 4, and 6 seconds were presented in random order on any azimuth in the horizontal plane. Thirty subjects participated in the study. A special location indication system called DINC (directional indication compass) was developed. With DINC the judged location of every test can be recorded accurately. The results showed that the localization accuracy is significantly related to the duration of the sound presentation. As long as the sound has a broad frequency bandwidth, the sound type has little effect on the localization accuracy. A presentation of at least 4-second duration is recommended. There is no significant difference between male and female subjects in the accuracy of detection.","2003","2023-07-12 06:36:00","2023-07-19 03:47:52","","1163–1171","","12","51","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8KIMMPXY","journalArticle","2019","Wilson, Alex; Fazenda, Bruno M.","User-guided Rendering of Audio Objects Using an Interactive Genetic Algorithm","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20490","One of the advantages of object-based audio/broadcast over traditional channel-based delivery is that it allows for the rendering of personalized content when delivered to the listeners. The methods by which personalization are achieved often require an in-depth understanding of the problem domain. This paper describes the design and evaluation of an interactive audio renderer, which is used to optimize an audio mix based on the feedback of the listener. A panel of 14 trained participants was recruited to try the system. When using the proposed system in a simple music mixing task, participants were able to create a range of mixes of audio objects comparable to those made using the conventional fader-based system. This suggests that the system is not an obstacle to the creation of desired content, and does not impose noticeable limits on what content can be created. Evaluation using the System Usability Scale showed a low level of physical and mental burden and so is predicted that the system would be suitable for a variety of applications where physical interaction is to be kept low, such as an interface for users with vision and/or mobility impairments.","2019","2023-07-12 06:36:03","2023-07-19 10:56:20","","522–530","","7/8","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TS3EEV5R","journalArticle","2019","Fenton, Steven; Lee, Hyunkook","A Perceptual Model of “Punch” Based on Weighted Transient Loudness","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20484","This paper proposes and evaluates a perceptual model for the measurement of “punch” in musical signals based on a novel algorithm. Punch is an attribute that is often used to characterize music or sound sources that convey a sense of dynamic power or weight to the listener. A methodology is explored that combines signal separation, onset detection, and low-level feature measurement to produce a perceptually weighted punch score. The model weightings are derived through a series of listening tests using noise bursts, which reveal the perceptual relevance of the onset time and frequency components of the signal across octave bands. The punch score is determined by a weighted sum of these parameters using coefficients derived through regression analysis. The model outputs are evaluated against subjective scores obtained through a pairwise comparison listening test using a wide variety of musical stimuli and against other computational models. The model output PM95 outperformed the other models showing a “very strong” correlation with punch perception.","2019","2023-07-12 06:36:07","2023-07-19 03:57:19","","429–439","","6","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5PDVBNLP","journalArticle","2020","Malecki, Pawel; Piotrowska, Magdalena; Sochaczewska, Katarzyna; Piotrowski, Szymon","Electronic Music Production in Ambisonics-Case Study","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20723","This paper presents a case study of an original electronic music production in stereo and means for then creating an Ambisonic remix. The main goal of this work was to explore the potential for extending all dimensions into extended space. The stereo and Ambisonic mixes, as well as the stereo and binaural renders, were subjectively evaluated by experts performers. When compared, the stereo and Ambisonic mixes differ not only in terms of space but also with regard to the timbre and dynamics. In general, the listeners preferred the spaciousness and selectivity of the Ambisonic version over the stereo. The obtained results are consistent with the outcomes from other studies that focused on the differences between stereo and multichannel reproduction. The most interesting conclusion can be formulated from a comparison between the stereo and binaural renders of the Ambisonic mix. The results clearly show the preference of spaciousness of the binaural version, and the general preference also indicated binaural as preferred. Due to the popularity of headphone playback, the obtained results show the potential of Ambisonic productions with targeted binaural playback.","2020","2023-07-12 06:36:10","2023-07-19 04:26:33","","87–94","","1/2","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CHR5ZRM5","journalArticle","2021","Riionheimo, Janne; Lokki, Tapio","Movie Sound, Part 2: Preference and Attribute Ratings of Six Listening Environments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21017","In this study, the assessors evaluated the alterations in the sound field of six movie listening environments. The sound fields of the listening environments were auralized to an anechoic listening room with 45 loudspeakers so that assessors could compare the rooms with each other directly. 31 experienced listeners evaluated five descriptive attributes on a continuous scale for each room with two program material items, dialogue and music. The preference ratings for the rooms were also collected. The perceptual evaluations were compared to the objective electroacoustic data of the rooms. The sense of space, clarity, and distance match the measured clarity C50 at the middle frequencies, while the brightness matches the level of the high frequencies in the electroacoustic response above 4 kHz. No psychoacoustical support was found for the current standards, according to which the high frequencies should be attenuated more in large cinemas with longer reverberation than in small cinemas. It turned out that the movie sound professionals do not prefer either too dead or too live listening environments.","2021","2023-07-12 06:36:17","2023-07-19 04:40:55","","68–79","","1/2","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AIXF8I55","journalArticle","2017","Volk, Christer P.; Bech, Søren; Pedersen, Torben H.; Christensen, Flemming","Modeling Perceptual Characteristics of Loudspeaker Reproduction in a Stereo Setup","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18729","Loudspeaker specifications traditionally describe their physical characteristics rather than the perceptual properties of the sound reproduction. This study explores three metrics for predicting the perceived characteristics of loudspeakers’ sound in a stereo setup evaluated in a standardized listening room. Perceptual evaluations of eleven loudspeakers were conducted on the basis of six selected sensory descriptors chosen by trained listeners during consensus meetings. Four of these descriptors were found suitable for modeling metrics that predicted Bass depth, Punch, Brilliance, and Dark-Bright respectively; Bass depth and Punch were however combined because of a high correlation between them. The input for the metrics was recordings made using a head-and-torso simulator and processed using a loudness model. The prediction models were trained on a subset of seven sets of loudspeakers and validated on four others. The range of correlation coefficients between perceptual evaluations and outputs of the metrics were r = 0.85-0.96.","2017","2023-07-12 06:36:24","2023-07-19 10:53:38","","356–366","","5","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HBWH5E4E","journalArticle","2017","Kaplanis, Neofytos; Bech, Søren; Tervo, Sakari; Pätynen, Jukka; Lokki, Tapio; Waterschoot, Toon; Jensen, Søren Holdt","A Rapid Sensory Analysis Method for Perceptual Assessment of Automotive Audio","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18550","As today’s automotive audio systems rapidly evolve, it is unclear if the current perceptual assessment protocols fully capture the human sensations evoked by such new systems. The highly complex and acoustically hostile environment of the automobile cabin hinders the effectiveness of standard objective metrics, while lacking robustness, repeatability, and perceptual relevance. This report examines the current assessment protocols and their identified limitations. A new design of an assessment protocol is proposed. It uses the Spatial Decomposition Method for acquiring, analyzing, and reproducing the sound field in a laboratory over loudspeakers, thereby allowing instant comparisons of automotive audio systems. A rapid sensory analysis protocol, the Flash Profile, is employed for evaluating the perceptual experience using individually elicited attributes, in a time-efficient manner. A pilot experiment is described, where experts, experienced, and naive assessors followed the procedure and evaluated three sound fields. Current findings suggest that this method allows for the assessment of both spatial and timbral properties of automotive sound.","2017","2023-07-12 06:36:27","2023-07-19 04:09:44","","130–146","","1/2","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2KQIK3RZ","journalArticle","2021","Cairns, Patrick; Daffern, Helena; Kearney, Gavin","Parametric Evaluation of Ensemble Vocal Performance Using an Immersive Network Music Performance Audio System","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21541","This paper describes an immersive audio Network Music Performance (NMP) system designed for group singing. A prototype of this design (audio only) was deployed to ten singers across Europe, who participated in a duet vocal performance study, operating the system from their home networks. Parametric evaluation of these vocal performances was conducted in order to provide characterization of musical interactivity between performers and explore the challenges and opportunities presented for immersive audio NMP systems in practical usecase settings. Results demonstrate that it is possible to achieve performance that conforms to expectations of live interactivity and estimate the conditions under which this may be achieved. Significant effect of latency, and in one case virtual room ""type,"" is observed across performances. Informal questionnaire responses present discussion of the potential for virtual acoustics and latency to impact the perceptual experience of networked performers.","2021","2023-07-12 06:36:31","2023-07-19 03:46:47","","924–933","","12","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J9C5I74R","journalArticle","2016","Williams, Duncan","Toward Emotionally-Congruent Dynamic Soundtrack Generation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18374","Real-time control of the emotional content of sound has utility in video game soundtracking where the player controls the narrative trajectory, and the affective attributes of the sound should ideally match this trajectory. Perceived emotions can be represented in a 2-dimensional space composed of valence (positivity, e.g. happy, sad, fearful) and arousal (intensity, e.g. mild vs strong). This report is a speculative exploration of measuring and manipulating sound effects to achieve emotional congruence. An initial study suggests that timbral features can exert an influence on the perceived emotional response of a listener. A panel of listeners responded to stimuli in a set with varying timbres, while maintaining pitch, loudness, and other musical and acoustic features such as key, melodic contour, rhythm and meter, reverberant environment etc. The long term goal is to create an automated system that utilizes timbre morphing in real time to manipulate perceived affect in soundtrack generation.","2016","2023-07-12 06:36:34","2023-07-19 10:56:01","","654–663","","9","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8ZLWJRML","journalArticle","2017","Zhou, Tingting; Zeng, Yumin; Wang, Rongrong","Single-Channel Speech Enhancement Based on Psychoacoustic Masking","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18560","Speech enhancement processing can improve the performance of speech communication systems in noisy environments, such as in mobile communication systems, speech recognition, or hearing aids. Single-channel speech enhancement is more difficult than it is with multiple channels since there is no independent source of information that can help separate the speech and noise signals. This paper addresses single-channel speech enhancement based on the masking properties of the human auditory system. A complete implementation of speech enhancement using psychoacoustic masking is presented. The incorporation of temporal masking along with simultaneous masking (as compared to using only simultaneous masking) produces results that are more consistent with human auditory characteristics. The combined masking is then used to adapt the subtraction parameters to obtain the best trade-off among noise reduction, speech distortion, and the level of residual perceptual noise. The application of objective measures and subjective listening tests demonstrate that the proposed algorithm outperforms comparable speech enhancement algorithms.","2017","2023-07-12 06:36:37","2023-07-19 10:59:30","","272–284","","4","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TBCRZ3JV","journalArticle","2012","Bahne, Adrian","Perceived Sound Quality of Small Original and Optimized Loudspeaker Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16163","The perceived sound quality of small loudspeaker systems with and without digital optimization was empirically evaluated in a listening experiment. Further, it was investigated how the presentation order in the performed paired comparisons influenced the results, as well as whether a self-evaluation was of potential use for variance reduction. The systems were optimized by means of FIR filters. The two versions of each loudspeaker system were rated in a paired comparison test for music stimuli. For the purpose of analysis a linear Gaussian model was applied, resulting in an interval scale revealing interesting information about certainty and discrimination ability of the listeners. The test investigated whether linear pre-compensation of small and inexpensive loudspeaker systems results in a significant improvement of the perceived audio quality in a typical listening situation. The results indicated a significant preference for the optimized version and a significant dependency on the presentation order was detected. The self-evaluation was found to be uncorrelated to the test results.","2012","2023-07-12 06:36:40","2023-07-19 03:37:41","","29–37","","1/2","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L4BL2LTJ","journalArticle","2022","Hill, Adam J.; Mulder, Johannes; Burton, Jon; Kok, Marcel; Lawrence, Michael","Sound Level Monitoring at Live Events, Part 3–Improved Tools and Procedures","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21552","This is the final installment in a series of three papers looking into the subject of sound level monitoring at live events. The first two papers revealed how practical shortcomings and audience and neighbor considerations (in the form of sound level limits) can impact the overall live experience. This paper focuses on an improved set of tools for sound engineers to ensure a high-quality and safe live event experience while maintaining compliance with local sound level limits. This includes data processing tools to predict future limit violations and guidelines for improved user interface design. Practical procedures, including effective sound level monitoring practice, alongside resourceful mixing techniques are presented to provide a robust toolset that can allow sound engineers to perform their best without compromising the listening experience in response to local sound level limits.","2022","2023-07-12 06:36:47","2023-07-19 04:06:07","","73–82","","1/2","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8DDH2325","journalArticle","2016","Drossos, Konstantinos; Kaliakatsos-Papakostas, Maximos; Floros, Andreas; Virtanen, Tuomas","On the Impact of The Semantic Content of Sound Events in Emotion Elicitation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18338","Sound events are known to have an influence on the listener’s emotions, but the reason for this influence is less clear. Take for example the sound produced by a gun firing. Does the emotional impact arise from the fact that the listener recognizes that a gun produced the sound (semantic content) or does it arise from the attributes of the sound created by the firing gun? This research explores the relation between the semantic similarity of the sound events and the elicited emotions. Results indicate that the semantic content seems to have a limited role in the conformation of the listener’s affective states. However, when the semantic content is matched to specific areas in the Arousal-Valence space or when the source’s spatial position is considered, the effect of the semantic content is higher, especially for the cases of medium to low valence and medium to high arousal or when the sound source is at the lateral positions of the listener’s head.","2016","2023-07-12 06:36:53","2023-07-19 03:53:53","","525–532","","7/8","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8CVB35AH","journalArticle","2023","Corcuera, Andrea; Chatziioannou, Vasileios; Ahrens, Jens","Perceptual Significance of Tone-Dependent Directivity Patterns of Musical Instruments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22132","Musical instruments are complex sound sources that exhibit directivity patterns that not only vary depending on the frequency, but can also change as a function of the played tone. It is yet unclear whether the directivity variation as a function of the played tone leads to a perceptible difference compared to an auralization that uses an averaged directivity pattern. This paper examines the directivity of 38 musical instruments from a publicly available database and then selects three representative instruments among those with similar radiation characteristics (oboe, violin, and trumpet). To evaluate the listeners' ability to perceive a difference between auralizations of virtual environments using tone-dependent and averaged directivities, a listening test was conducted using the directivity patterns of the three selected instruments in both anechoic and reverberant conditions. The results show that, in anechoic conditions, listeners can reliably detect differences between the tone-dependent and averaged directivities for the oboe but not for the violin or the trumpet. Nevertheless, in reverberant conditions, listeners can distinguish tone-dependent directivity from averaged directivity for all instruments under study.","2023","2023-07-12 06:37:01","2023-07-19 03:49:54","","293–302","","5","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XE4FPZQJ","journalArticle","2021","Battello, Riccardo; Comanducci, Luca; Antonacci, Fabio; Cospito, Giovanni; Sarti, Augusto","Experimenting With Adaptive Metronomes in Networked Music Performances","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21468","The increasing attention toward interactions at a distance and the improvement of digital communications networks have steadily increased the interest toward Networked Music Performance both regarding entertainment and education. Unfortunately the unavoidable network latency remains one of the main issues that prevents a satisfiable remote performance. In this work we propose three different techniques that try to contrast this issue by relying on adaptive metronomes, i.e., metronomes that are able to track the tempo of the musicians through a beat tracking technique. We present a series of preliminary experiments with both professional and amateur musicians that demonstrate that these techniques could be a promising approach as an additional tool for contrasting the impact of latency.","2021","2023-07-12 06:37:05","2023-07-19 03:39:07","","737–747","","10","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BMC4BJ59","journalArticle","2016","Thorogood, Miles; Fan, Jianyu; Pasquier, Philippe","Soundscape Audio Signal Classification and Segmentation Using Listeners Perception of Background and Foreground Sound","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18334","A soundscape recording captures the sonic environment at a given location at a given time using one or more fixed or moving microphones. In most cases, the soundscape is uncontrolled and unscripted. Human listeners experience sonic components as being either background or foreground depending on their salient perceptual characteristics, such as proximity, repetition, and spectral attributes. Analyzing soundscapes in research tasks requires the classification and segmentation of the important sonic components, but that process is time consuming when done manually. This research establishes the background and foreground classification task within a musicological and soundscape context and then presents a method for the automatic segmentation of soundscape recordings. Using a soundscape corpus with ground truth data obtained from a human perception study, the analysis shows that participants have a high level of agreement on the category assigned to background samples (92.5%), foreground samples (80.8%), and background with foreground samples (75.3%). Experiments demonstrate how smaller window sizes affect the performance of the classifier.","2016","2023-07-12 06:37:25","2023-07-19 04:52:43","","484–492","","7/8","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KAWK342P","journalArticle","2017","Stitt, Peter; Bertet, Stéphanie; van Walstijn, Maarten","Off-Center Listening with Third-Order Ambisonics: Dependence of Perceived Source Direction on Signal Type","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18554","In multichannel reproduction, listening at off-center positions involves differences in the arrival times of the signals from different loudspeakers, which influences localization. The difference between transient or nontransient signals was found to have an influence on the localization of an auditory source image with third-order Ambisonics when the listener is located off-center. The transients were found to have larger errors on the placement of the Ambisonic pointer because of stronger localization dominance of the earlier arriving loudspeakers. Subjects performed consistently over a number of repetitions, but there was a larger difference among different subjects. Results were compared to those obtained with several prediction models, including an extended version of the energy vector model that incorporates the precedence effect. Compared to two binaural models, the extended vector model is shown to provide the best predictions over all conditions. The results confirm that the type of signal must be taken into account in predictive modeling. Furthermore, the extended energy vector exhibits about 50% less error than the standard energy vector.","2017","2023-07-12 06:37:28","2023-07-19 04:49:36","","188–197","","3","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2EGSHFU3","journalArticle","2017","Paasonen, Juhani; Karapetyan, Aleksandr; Plogsties, Jan; Pulkki, Ville","Proximity of Surfaces — Acoustic and Perceptual Effects","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19365","When simulating a virtual reality, surfaces near the subject location can play a perceptual role. The authors studied the acoustic effect of nearby objects with both binaural response measurements and subjective listening tests. With a large, smooth, and flat surface, significant acoustical effects exist when the distance is less than about 50 cm. The largest effects can be attributed to comb filtering caused by the interaction between direct sound and reflected sound. At very short distances, some frequency-dependent resonances and shadowing effects that were evoked by the sound field between the subject and the surface could also be observed. Measurements showed some effects that could not be attributed to either simple comb filtering or resonance effects. In a listening test the subjects were asked to sort three binaurally-rendered samples in growing order of perceived distance. With relatively small distance triples, i.e., 1-11-21 cm, 3-13-23 cm, or 7-17-27 cm, the subjects performed better than guessing. However, even with shortest tested distances the proportion of correct answers was of the order of 50%. Two of 12 subjects consistently reported the distances in reverse order, which shows that the acoustic effect was significant, but it did not lead to the correct perception of distance of the surface. The results suggest that the acoustic effect of objects being close to an avatar’s ear may in some cases improve realism and sound quality in an acoustic virtual reality both in anechoic and in reverberant conditions.","2017","2023-07-12 06:37:37","2023-07-19 04:36:35","","997–1004","","12","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PB4MQ5A7","journalArticle","2016","Chau, Chuck-jee; Mo, Ronald; Horner, Andrew","The Emotional Characteristics of Piano Sounds with Different Pitch and Dynamics","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18528","Previous research has shown that both sustained and nonsustained musical instrument sounds have strong emotional characteristics. This report explores how the effects of pitch and dynamics influence the emotional characteristics of isolated one-second piano sounds. Listeners compared the sounds pairwise over ten emotion categories. The results showed that all ten emotional categories were significantly affected by pitch and nine of them by dynamics. In particular, the emotional characteristics Happy, Romantic, Comic, Calm, Mysterious, and Shy generally increased with pitch, but sometimes decreased at the highest pitches. The characteristics Heroic, Angry, and Sad generally decreased with pitch. Scary was strong in the extreme low and high registers. With regard to dynamics, the results showed that the characteristics Heroic, Comic, Angry, and Scary were stronger for loud notes, while Romantic, Calm, Mysterious, Shy, and Sad were stronger for soft notes. Surprisingly, Happy was not affected by dynamics. These results help quantify the emotional characteristics of piano sounds.","2016","2023-07-12 06:37:52","2023-07-19 03:47:43","","918–932","","11","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YGQJQMXE","journalArticle","2003","Laroche, Jean","Efficient Tempo and Beat Tracking in Audio Recordings","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12235","Automatic beat tracking consists of estimating the number of beats per minutes at which a music track is played and identifying exactly when these beats occur. Applications range from music analysis, sound-effect synchronization, and audio editing to automatic playlist generation and deejaying. An off-line beat-tracking technique for estimating a time-varying tempo in an audio track is presented. The algorithm uses an MMSE estimation of local tempo and beat location candidates, followed by a dynamic programming stage used to determine the optimum choice of candidate in each analysis frame. The algorithm is efficient in its use of computation resource, yet provides very good results on a wide range of audio tracks. The algorithm details are presented, followed by a discussion of the performance and suggestions for further improvements.","2003","2023-07-12 06:37:58","2023-07-19 04:17:21","","226–233","","4","51","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KCGPE7J8","journalArticle","2012","Möller, Sebastian; Kettler, Frank; Gierlich, Hans-Wilhelm; Poschen, Silvia; Côté, Nicolas; Raake, Alexander; Wältermann, Marcel","Extending the E-Model for Capturing Noise Reduction and Echo Canceller Impairments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16214","The E-model for predicting speech quality mouth-to-ear can be extended with additional parameters to describe the effect of imperfect noise reduction and echo cancellation. As shown by subjective tests, quality-prediction accuracy of noise reduction and echo cancelling improves. Future work is planned to better refine the proposed approach.","2012","2023-07-12 06:38:04","2023-07-19 04:32:58","","165–175","","3","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"54TDHKKH","journalArticle","2018","Francombe, Jon; Woodcock, James; Hughes, Richard J.; Mason, Russell; Franck, Andreas; Pike, Chris; Brookes, Tim; Davies, William J.; Jackson, Philip J. B.; Cox, Trevor J.; Fazi, Filippo M.; Hilton, Adrian","Qualitative Evaluation of Media Device Orchestration for Immersive Spatial Audio Reproduction","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19581","The challenge of installing and setting up dedicated spatial audio systems can make it difficult to deliver immersive listening experiences to the general public. However, the proliferation of smart mobile devices and the rise of the Internet of Things mean that there are increasing numbers of connected devices capable of producing audio in the home. “Media device orchestration” (MDO) is the concept of utilizing an ad hoc set of devices to deliver or augment a media experience. In this paper, the concept is evaluated by implementing MDO for augmented spatial audio reproduction using object-based audio with semantic metadata. A system that augmented a stereo pair of loudspeakers with an ad hoc array of connected devices is described. The MDO approach aims to optimize aspects of the listening experience that are closely related to listener preference rather than attempting to recreate sound fields as devised during production. A thematic analysis of positive and negative listener comments about the system revealed three main categories of responses: perceptual, technical, and content-dependent aspects. MDO performed particularly well in terms of immersion/envelopment, but the quality of listening experience was partly dependent on loudspeaker quality and listener position.","2018","2023-07-12 06:38:09","2023-07-19 04:00:44","","414–429","","6","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W3LMXGMP","journalArticle","2006","Pulkki, Ville; Merimaa, Juha","Spatial Impulse Response Rendering II: Reproduction of Diffuse Sound and Listening Tests","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13664","Spatial impulse response rendering (SIRR) is a method for reproducing room impulse responses over multichannel loudspeaker setups. The applied analysis and synthesis methods were introduced in a companion paper. Time–frequency analysis is used to obtain directional and diffuseness information from the recorded sound field. Nondiffuse sound is then reproduced as pointlike virtual sources, and diffuse sound is synthesized with a decorrelation technique. The proposed synthesis methods for diffuse sound are examined in more detail and a hybrid method is derived. The relationship between diffuseness and interaural coherence is also studied. In addition, results of two listening tests are presented. It is shown that with a large loudspeaker setup under anechoic conditions, SIRR reproduction is at best indistinguishable from the original sample. Furthermore, in a listening test conducted in a standard listening room with real measured responses, SIRR reproduction is evaluated as the most natural one of the systems studied.","2006","2023-07-12 06:38:12","2023-07-19 04:39:26","","3–20","","1/2","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KB3PD9FD","journalArticle","2020","Lee, Jake Ryan Rajjayabun; Reiss, Joshua D.","Real-Time Sound Synthesis of Audience Applause","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20732","We investigate a procedural model for synthesizing applause sounds that contains novel aspects to ensure high quality and usability. Synthesis of a single clap is generated as a result of filtering a noise source and applying an envelope with exponential decay, based on prior art and existing experimental data. An ensemble approach is introduced to simulate many clappers in a spatially distributed environment. This renders how applause interacts with the space in which it is hosted, including the room impulse response, and where each clap is situated relative to the listener’s position. The applause features realistic build-up and fadeout based on natural audience response. The implementation contains meaningful parameters that allow a user to configure and change the sound to achieve a multitude of different types of applause, such as an “enthusiasm parameter” to simulate the greater perceived intensity from an enthusiastic audience. Subjective evaluation was performed to compare our method against recorded samples and four other popular sound synthesis techniques. It showed that the pro- posed implementation produced significantly more realistic results than other forms of applause synthesis, and it was almost indistinguishable from real-life recordings.","2020","2023-07-12 06:38:17","2023-07-19 04:19:49","","261–272","","4","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A5XT3Z36","journalArticle","2019","McGinnity, Siobhan; Mulder, Johannes; Beach, Elizabeth Francis; Cowan, Robert","Management of Sound Levels in Live Music Venues","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20710","With the increased recognition of potential damage to listeners when subjected to excessively loud sound, software-based sound level management systems can be viewed as a component of a strategy for reducing sound exposure to patrons and staff in live music venues. However, the use of level management tools in small indoor music venues, which represent a unique environment, has not been systematically explored. In an experimental approach for sound level management, a software system was tried in six indoor live-music venues in Melbourne. Comparing a control (without sound level management software) and the experimental condition (using the software), there was no reduction in mean LAeq,T, although there was a reduction in the number of events with extreme volume levels. Subjective questionnaires indicated that one-fifth of the patrons preferred lower sound levels than they experienced. The findings suggest that modifications to the software system may be necessary if the aim of the system is to reduce patron and staff sound exposure rather than simply to avoid exceeding legislative sound level limits. Recommended alterations could include greater flexibility in choice of target, matching with context of the performance, or changes to the system's visual display so that staying below, not at target, is positively reinforced.","2019","2023-07-12 06:38:21","2023-07-19 04:29:45","","972–985","","12","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y2GZFTZD","journalArticle","2017","Chau, Chuck-jee; Gilburt, Samuel J. M.; Mo, Ronald; Horner, Andrew","The Emotional Characteristics of Bowed String Instruments with Different Pitch and Dynamics","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19173","This paper investigates how emotional characteristics vary with pitch and dynamics within the bowed string instrument family. Listening tests compared the effects of pitch and dynamics on emotional characteristics of the violin, viola, cello, and double bass. Listeners compared the sounds pairwise over ten emotional categories. Results showed that the emotional characteristics Happy, Heroic, Romantic, Comic, and Calm generally increased with pitch, but decreased at the highest pitches. Angry and Sad generally decreased with pitch. Scary was strong in the extreme low and high registers, while Shy and Mysterious were unaffected by pitch. For dynamics, the results showed that Heroic, Comic, and Angry were stronger for loud notes, while Romantic, Calm, Shy, Sad, and the high register for Happy were stronger for soft notes. Scary and Mysterious were unaffected by dynamics. The results also showed significant differences between different bowed string instruments on notes of the same pitch and dynamic level. The results provide audio engineers and musicians with suggestions for emphasizing emotional characteristics of bowed strings in sound recordings and performances.","2017","2023-07-12 06:38:27","2023-07-19 03:47:34","","573–588","","7/8","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6TTAK2FL","journalArticle","2019","Thuillier, Etienne; Lähdeoja, Otso; Välimäki, Vesa","Feedback Control in an Actuated Acoustic Guitar using Frequency Shifting","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20480","Recent research demonstrated that the classical guitar can be advantageously augmented using a pickup to drive an actuator mounted on the guitar’s back plate. This allows enrichment of the instrument’s timbral palette with audio effect processors in the loop. The feedback problem that results from such a setup is similar to what occurs in live music performance setups where the sound of a guitar is amplified using a loudspeaker. In the present case, measurements of the augmented guitar’s open-loop response demonstrate that instabilities are susceptible to occurring from the string’s modes and not from the guitar’s sound-box. In particular, the shape of the magnitude response suggests frequency shifting as a viable solution to string instability. Introduction of an upward frequency shift in the forward path is proposed as a means for stabilizing the closed-loop system. Experimental results demonstrate that the proposed solution leads to improved stability even for a modest frequency shift of 3 Hz. The achieved gain margin improvement, which is shown to be of at least 3 dB, then comes at the cost of a clearly perceptible amplitude modulation, which may be acceptable in conjunction with other audio effects chosen by the performer.","2019","2023-07-12 06:38:36","2023-07-19 04:52:51","","373–381","","6","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HQ2WA4E2","journalArticle","2014","Le Bagousse, Sarah; Paquier, Mathieu; Colomes, Catherine","Categorization of Sound Attributes for Audio Quality Assessment—A Lexical Study","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17549","For the evaluation of perceived quality in audio coding, two well-known subjective test methods, both of which are based on Basic Audio Quality (BAQ), are recommended by the International Telecommunication Union. Although a predictor of quality, BAQ is likely to be multidimensional. Listening tests can be used to evaluate other attributes that contribute to impairments created by coding. The goal of this study is to define categories of additional attributes, thereby providing a complement to the single BAQ metric. When quality attributes are sorted, there appears to be three groups: one related to space, a second related to defects, and a third split into timbre and quality.","2014","2023-07-12 06:38:39","2023-07-19 04:17:49","","736–747","","11","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3QXDZUVP","journalArticle","2006","Phatak, Sandeep A.; Ratnam, Rama; Wheeler, Bruce C.; O’brien, William D., Jr.; Feng, Albert","Effect of Reflectors on Sound-Source Localization with Two Microphones","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13689","[Engineering Report] The localization performance of the source localization algorithms degrades in reverberant conditions. The performance of one such localization algorithm, the localization–extraction (LE) algorithm, was measured systematically as a function of the number of reflecting surfaces in a cubical enclosure. Localization was qualitatively measured using a localization plot and quantized using two objective parameters. A broad-band noise burst and a speech signal were used as stimuli. The degradation of the localization performance was monotonic but not uniform with an increase in the number of reflectors. The performance was found to be proportional to the bandwidth of the stimulus. The performance of the LE algorithm was benchmarked against that of a commonly used signal-subspace technique—multiple signal classification (MUSIC). The LE algorithm was less affected by reflections than the MUSIC algorithm. Degradation of the source localization under high reverberation was found to be more severe at low frequencies, which resulted in the detection of a “phantom” source at 0° for the speech signal.","2006","2023-07-12 06:38:42","2023-07-19 04:38:01","","512–524","","6","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"47ZAI3H4","journalArticle","2017","Howie, Will; King, Richard; Martin, Denis","Listener Discrimination Between Common Speaker-Based 3D Audio Reproduction Formats","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19353","Over the last decade, numerous three-dimensional audio playback formats have been introduced and standardized for cinema, broadcast, and home theater environments. They differ in terms of number of speakers, speaker positions in the horizontal and vertical planes, and workflow strategies: channel-based, object-based, or some hybrid of the two. Each system possesses inherent pros and cons. This research attempts to determine whether listeners could discriminate among four currently standardized three-dimensional audio formats for reproduction of acoustic music. Double-blind listening tests showed that listeners could discriminate between NHK 22.2 Multichannel Sound (22.2) and several lower-channel-count 3D reproduction formats with a high degree of success, regardless of the musical stimulus. Listeners were also able to discriminate between three relatively similar 3D audio formats: ATSC 11.1, KBS 10.2, and Auro 9.1, although with significantly less success than with the 22.2. This suggests each of these formats deliver a perceptually different listening experience, with 22.2 being particularly different from the other formats under investigation.","2017","2023-07-12 06:38:52","2023-07-19 04:06:53","","796–805","","10","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y26SXWW3","journalArticle","2007","Bai, Mingsian R.; Chen, Meng-chun","Intelligent Preprocessing and Classification of Audio Signals","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14164","An audio processor that integrates intelligent classification and preprocessing algorithms is presented. Audio features in the time and frequency domains are extracted and processed prior to classification. Classification algorithms, including the nearest neighbor rule (NNR), artificial neural networks (ANN), fuzzy neural networks (FNN), and hidden Markov models (HMM), are used to classify and identify singers and musical instruments. A training phase is required to establish a feature space template, followed by a test phase in which the audio features of the test data are calculated and matched to the feature space template. In addition to audio classification, the proposed system provides several independent component analysis (ICA)-based preprocessing functions for blind source separation, voice removal, and noise reduction. The proposed techniques were applied to process various kinds of audio program materials. The test results reveal that the performance of the methods is satisfactory, but varies slightly with the algorithm and program materials used in the tests.","2007","2023-07-12 06:38:55","2023-07-19 03:37:50","","372–384","","5","55","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6E7YQ6KZ","journalArticle","2016","Bolaños, Javier Gómez; Mäkivirta, Aki; Pulkki, Ville","Automatic Regularization Parameter for Headphone Transfer Function Inversion","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18517","Binaural synthesis enables headphone presentation of the same auditory impression that a listener would perceive if present in the original sound field. While the presentation of a binaural signal requires a head-related impulse response and/or a binaural room response, it also requires compensation for the headphone response. A method is proposed for automatically regularizing the inversion of a headphone transfer function for headphone equalization. The problem arises from the fact that the inversion cannot treat peaks and notches as being perceptually equivalent. Notch inversion can create high-Q resonances that can be very unpleasant. Evaluation of the proposed method indicates that it provides an inversion filter that can maintain the accuracy of the conventional regularized inverse method while limiting the inversion of notches in a perceptually acceptable manner. The results show that the proposed method can produce perceptually better equalization than the regularized inverse method used with a fixed regularization factor or the complex smoothing method used with a half-octave smoothing window.","2016","2023-07-12 06:38:58","2023-07-19 03:43:36","","752–761","","10","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XUA3HN9T","journalArticle","2022","Lazaro, May Jorella; Kim, Sungho; Choi, Minsik; Kim, Kichang; Park, Dongchul; Moon, Soyoun; Yun, Myung Hwan","Design and Evaluation of Electric Vehicle Sound Using Granular Synthesis","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21566","Electric vehicles (EVs) generally do not have the natural engine sound; thus, sound design for such vehicles is more crucial. This study focuses on proposing an EV interior sound design through utilizing the granular synthesis method. Moreover, it aims to investigate the effect of each granular synthesis parameter on the drivers' affective experience. In the model proposed, four granular synthesis parameters (sample source, grain duration, grain envelope, and revolution per minute [RPM] range) with three levels each were selected for synthesis. By combining different values of each sound parameter in an orthogonal array, 27 EV sound samples were generated. A jury test was conducted with 32 participants (20 male, 12 female), which evaluated each sound sample based on three EV-related affective adjectives (""refined,"" ""sporty,"" and ""futuristic"") and ratings of overall satisfaction. The results showed that each granular synthesis parameter has a different impact on the perception of EV-related affect and satisfaction. Moreover, it also found that combining different values of each parameter may result in inducing a specific emotion or experience. The EV sound design methodology proposed in this study can contribute to the development of future EV sound to increase its effectiveness in improving the drivers' auditory experience.","2022","2023-07-12 06:39:01","2023-07-19 04:17:40","","294–304","","4","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6WFMWNJS","journalArticle","2022","Kim, Taeho; Pöntynen, Henri; Pulkki, Ville","Vertical Direction Control Using Difference-Spectrum Filters in Stereophonic Loudspeaker Reproduction","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21555","This paper introduces difference-spectrum filters that can be used to control the perceived vertical direction of a sound source presented from ear-level loudspeakers. The difference-spectrum filter was designed to mimic the macroscopic changes in the spectral envelope of head-related transfer functions (HRTFs) between a target elevation angle and the ear-level elevation (0?), where the HRTF envelopes were obtained from averaging an extensive collection of individual HRTFs in a database. Localization tests were conducted to evaluate the effectiveness of difference-spectrum filters on elevation perception, which showed a promising result in the two-channel stereophonic condition for the virtual sound source. The perceived elevation correlated well with the target elevation angle of difference-spectrum filters in the stereophonic condition, although a weak correlation was observed in the monophonic condition. Thus, the test results show that difference-spectrum filters can create robust illusory elevation perception and enable vertical direction control over a wide range of elevation angles in stereophonic loudspeaker reproduction.","2022","2023-07-12 06:39:08","2023-07-19 04:12:20","","128–139","","3","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GKYDMRGE","journalArticle","2016","Rocchesso, Davide; Mauro, Davide Andrea; Drioli, Carlo","Organizing a sonic space through vocal imitations","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18333","This research investigates how the vocal mimicking capabilities of humans may be exploited to access and explore a given sonic space. Experiments showed that prototype vocal sounds can be represented in a two-dimensional space and still remain perceptually distinct from each other. Experiments provide a measure of how meaningful the machine distribution and grouping of vocal sounds are to humans, and confirms that humans are able to effectively use the acoustic and articulatory cues at their disposal to associate sounds to given prototypes. When used in an automatic clustering process, these cues are sufficiently consistent with those used by humans when categorizing acoustic phenomena. The procedure of dimensionality reduction and clustering is demonstrated in the case of imitations of engine sounds, which then represent the sonic space of a motor sound model. A two-dimensional space is particularly attractive for sound design because it can be used as a sonic map where the landmarks contain both a synthetic sound and its vocal imitation.","2016","2023-07-12 06:39:21","2023-07-19 04:41:11","","474–483","","7/8","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6PPDREHK","journalArticle","2010","Tzanetakis, George; Martins, Luis Gustavo; McNally, Kirk; Jones, Randy","Stereo Panning Information for Music Information Retrieval Tasks","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15454","A music information retrieval system can extract information that arises from how various sound sources are panned between channels during the mixing and recording process. The authors propose augmenting standard audio features, which are based on the source music, with one of two methods for extracting panning and contrast features. These additional features provide statistically important information for nontrivial audio classifications tasks. Traditional classifications focus on information about pitch, rhythm, and timbre. Other types of mixing parameters are proposed for future work.","2010","2023-07-12 06:39:27","2023-07-19 04:54:15","","409–417","","5","58","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"34CY6QQF","journalArticle","2017","Geravanchizadeh, Masoud; Avanaki, Hadi Jamshidi; Dadvar, Paria","Binaural Speech Intelligibility Prediction in the Presence of Multiple Babble Interferers Based on Mutual Information","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18561","This paper describes a predictor for binaural speech intelligibility that computes speech reception thresholds (SRT) without the need to perform subjective listening tests. Although listening tests are considered to be the most reliable indicators of performance, such tests are time consuming and costly. The proposed model computes SRTs in two stages. First, it calculates the binaural advantage. Then, it derives the SRTs based on the computed mutual information of the speech and mixture envelopes. Listening tests were conducted with 13 normal-hearing listeners in 15 spatial configurations, covering one, two, and three babble interferers. The proposed predictor performs as well as the baseline model in predicting the intelligibility of binaural vowel-consonant-vowel signals contaminated by multiple nonstationary babble noise sources. The model is evaluated in anechoic conditions and compared with subjective data as well as with the predictions obtained from a baseline binaural speech intelligibility model.","2017","2023-07-12 06:39:31","2023-07-19 04:02:15","","285–292","","4","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7IMSQMNF","journalArticle","1988","Fielder, Louis D.; Benjamin, Eric M.","Subwoofer Performance for Accurate Reproduction of Music","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=5147","The spectra and the maximum output levels for accurately reproducing low-frequency musical signals are determined from published research and new measurements. Analysis of commercial recordings shows substantial musical information in the octave from 32 to 16 Hz and some down to 12 Hz. Psychoacoustic data are used to establish to what degree errors (such as total harmonic distortion, FM distortion, modulation noise, and bandwidth limits) are perceptible. Criteria are set for proper subwoofer performance at peak sound pressure levels of 110 dB.","1988","2023-07-12 06:39:34","2023-07-19 03:58:02","","443–456","","6","36","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GI3YL3E3","journalArticle","2004","Burred, Juan José; Lerch, Alexander","Hierarchical Automatic Audio Signal Classification","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13015","The design, implementation, and evaluation of a system for automatic audio signal classification is presented. The signals are classified according to audio type, differentiating between three speech classes, 13 musical genres, and background noise. A large number of audio features are evaluated for their suitability in such a classification task, including MPEG-7 descriptors and several new features. The selection of the features is carried out systematically with regard to their robustness to noise and bandwidth changes, as well as to their ability to distinguish a given set of audio types. Direct and hierarchical approaches for the feature selection and for the classification are evaluated and compared.","2004","2023-07-12 06:39:37","2023-07-19 03:46:04","","724–739","","7/8","52","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C25EGXP6","journalArticle","2016","Stitt, Peter; Bertet, Stéphanie; van Walstijn, Maarten","Extended Energy Vector Prediction of Ambisonically Reproduced Image Direction at Off-Center Listening Positions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18135","Spatial audio techniques, such as Ambisonics and Wave Field Synthesis, aim to reproduce the sound field in the limited area of the sweet spot, which is approximately equidistant from all loudspeakers. This research extends the energy vector technique to improved localization at off-centered positions. In determining the source direction, a perceptual weight is assigned to each loudspeaker gain that takes into account the relative arrival times, levels, and directions of the loudspeaker signals. Conversely, uncompensated differences in arrival time can trigger the precedence effect. The proposed model was evaluated and compared to the original energy vector model and two binaural models. The extended energy vector version was at least 50% more accurate than the second best predictor with an average error of about 4°.","2016","2023-07-12 06:39:39","2023-07-19 04:49:27","","299–310","","5","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DXYQDQU6","journalArticle","2009","Hiekkanen, Timo; Mäkivirta, Aki; Karjalainen, Matti","Virtualized Listening Tests for Loudspeakers","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14816","When comparing loudspeakers and trying to eliminate the influence of their location, problems arise due to listeners’ short auditory memory. But if a virtual loudspeaker in a virtual room using headphones was equivalent to the real environment, comparison testing would avoid the problem of memory. Switching could be done instantaneously. Subjective tests showed that the quality of virtual loudspeakers depended highly on the test signal and upon the difficulty of creating accurate room- and head-related transfer functions at high frequencies. Nevertheless, virtualized loudspeakers can be imperceptible from reality in many cases.","2009","2023-07-12 06:39:42","2023-07-19 04:05:49","","237–251","","4","57","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2DV8WZAR","journalArticle","2006","Heusdens, Richard; Jensen, Jesper; Kleijn, W. Bastiaan; Kot, Valery; Niamut, Omar A.; Van De Par, Steven; Van Schijndel, Micholle H.","Bit-Rate Scalable Intraframe Sinusoidal Audio Coding Based on Rate-Distortion Optimization","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13673","A coding methodology that aims at rate-distortion optimal sinusoid + noise coding of audio and speech signals is presented. The coder divides the input signal into variable-length time segments and distributes sinusoidal components over the segments such that the resulting distortion (as measured by a perceptual distortion measure) is minimized subject to a prespecified rate constraint. The coder is bit-rate scalable. For a given target bit budget it automatically adapts the segmentation and distribution of sinusoids in a rate-distortion optimal manner. The coder uses frequency-differential coding techniques in order to exploit intrasegment correlations for efficient quantization and encoding of the sinusoidal model parameters. This technique makes the coder more robust toward packet losses when used in a lossy-packet channel environment as compared to time-differential coding techniques, which are commonly used in audio or speech coders. In a subjective listening experiment the present coder showed similar or better performance than a set of four MPEG-4 coders operating at bit rates of 16, 24, 32, and 48 kbit/s, each of which was state of the art for the given target bit rate.","2006","2023-07-12 06:39:48","2023-07-19 04:05:40","","167–188","","3","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VYKT92G9","journalArticle","2016","Bitzer, Joerg; Kissner, Sven; Holube, Inga","Privacy-Aware Acoustic Assessments of Everyday Life","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18298","In order to enhance people’s ability to interact with their acoustic environments, hearing devices are common tools. However, it is difficult to evaluate the benefit of those tools or to measure acoustically challenging situations in natural environments. This paper proposes a way to measure the most important features of everyday acoustics environments by extracting a limited set of features while not compromising the privacy of partners and bystanders. The respective national laws on how to deal with audio privacy are very different among countries. The authors proposed using a smartphone as the source recorder but splitting the feature extraction into two phases: an initial feature processing in the smartphone and a later processing on a more powerful computer. For a given feature set, a statistical analysis shows comparable results from the extracted data when using either the original audio or the new privacy-aware extraction methods. A comparison shows that different scenarios result in separable features using the new extraction method.","2016","2023-07-12 06:40:00","2023-07-19 03:42:40","","395–404","","6","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BRCZN7AI","journalArticle","1992","Begault, Durand R.","Perceptual Effects of Synthetic Reverberation on Three-Dimensional Audio Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=7027","A psychoacoustic investigation was conducted in which five subjects gave localization judgments for headphone-delivered speech stimuli processed by nonindividual head-related transfer functions, with and without synthetic ""spatial"" reverberation added to the stimuli. Spatial reverberation minimized intracranially heard stimuli, but increased the magnitude of azimuth and elevation localization errors. The results are applicable to three-dimensional sound systems and spatial sound field processors designed to increase the sensation of auditory ""spaciousness.""","1992","2023-07-12 06:40:04","2023-07-19 03:40:34","","895–904","","11","40","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LD6HAV7R","journalArticle","2019","Su, Hengwei; Marui, Atsushi; Kamekawa, Toru","The Auditory Source Widening Effect in Binaural Synthesis with Spatial Distribution of Frequency Bands","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20545","A binaural technique (involving direct control of signals transferred into both ears of listeners), not only can solve the problem of spatial impression of headphone reproduction but also has the ability to provide realistic auditory experiences, especially in 3D spatial acoustic reproduction. In this study, monophonic source signals were processed by frequency-band decomposition and distribution to achieve spatially widened perceived source widths in binaural synthesis. Stimuli with different widths were synthesized, and the perceived widths were evaluated by conducting a listening experiment to investigate the relationship of the perceived width and the synthesized width. Three different bandwidths of frequency bands and two center positions of synthesized widths were used in the processing, and the relevant effects on perception of source width were investigated. The results of the listening experiment suggested that under proper processing conditions the perceived width could increase with increasing synthesized widths. However, dependencies of source signal characteristics and variations between participants were observed. Degradations of timbre and spatial quality were also evaluated. The results suggested that this method suffered less degradation than a conventional decorrelation method while it achieved comparable widening effects for binaural reproduction. For example, for a cello source signal with 1/12-octave bandwidth, the perceived width increased with increasing synthesis width. This suggests that under appropriate conditions this method could control the perceived width of a monophonic source in binaural synthesis.","2019","2023-07-12 06:40:07","2023-07-19 04:50:31","","691–704","","9","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WEN6YKXX","journalArticle","2005","Sanchez-bote, Jose-luis; Gonzalez-rodriguez, Joaquin; Ortega-garcia, Javier","Audible Noise Suppression with a Real-Time Broad-Band Superdirective Microphone Array","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13419","A novel audible noise suppression (ANS) multichannel processor is proposed and implemented for real-time processing of data from a 15-microphone nested linear array. The ANS processor, based on the masking properties of the human auditory system, has been used successfully in single-channel systems. An enhanced multichannel version has now been developed, taking advantage of the extra information available in the acoustic spatial samples from the microphone array. This is used to improve the clean speech signal estimates used to calculate dynamically the noise hearing thresholds for ANS filtering, which will benefit the perceived and objective quality of the processed signal. While off-line experiments with a multichannel recorded database under different noise and reverberation conditions have previously assessed the performance of the system, several on-line experiments assessing the real-time prototype are described.","2005","2023-07-12 06:40:10","2023-07-19 04:46:15","","403–418","","5","53","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CBM6EKK4","journalArticle","2012","Raake, Alexander; Wältermann, Marcel; Wüstenhagen, Ulf; Feiten, Bernhard","How to Talk about Speech and Audio Quality with Speech and Audio People","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16212","Typically, audio quality assessment uses MUSHRA (Multi Stimulus with Hidden Reference and Anchors), while speech quality assessment uses ACR (Absolute Category Rating). Since many applications are transporting both speech and music, such as mobile devices, a conversion technique between the two types of ratings would be useful. Two speech and two audio quality listening tests are compared with different content types. The results illustrate when and how the two types of measurements are consistent, complementary, and inconsistent.","2012","2023-07-12 06:40:13","2023-07-19 04:39:44","","147–155","","3","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6CUMZA74","journalArticle","2009","Vilkamo, Juha; Lokki, Tapio; Pulkki, Ville","Directional Audio Coding: Virtual Microphone-Based Synthesis and Subjective Evaluation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14838","Directional Audio Coding (DirAC) is a perceptual method for reproducing spatial audio using existing microphones and surround sound configurations. In the analysis phase, the direction and diffuseness are continuously estimated in frequency bands. This information, together with input signals remapped into a set of virtual microphones corresponding to the reproduction configuration, is used to recreate the spatial audio experience in the listening setup. Subjective listening tests in both an anechoic chamber and in a standard listening room showed that the mean opinion scores were almost always good to excellent, which is higher quality than traditional techniques.","2009","2023-07-12 06:40:20","2023-07-19 04:55:31","","709–724","","9","57","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DDB4R7FU","journalArticle","1991","Hansen, Villy; Munch, Gert","Making Recordings for Simulation Tests in the Archimedes Project","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=5961","Archimedes is a psychoacoustic research project with three partners aiming to use experimental results from free-field simulation of room acoustics to improve sound-reproduction systems. The preparatory work of one partner is presented, involving monophonic recording of voice and various solo instruments under anechoic and semi-reverberant conditions. These recording are for comprehensive experiments using simulation of a standard listening room.","1991","2023-07-12 06:40:30","2023-07-19 04:03:37","","768–774","","10","39","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LXY8JRNW","journalArticle","2014","Tervo, Sakari; Laukkanen, Perttu; Pätynen, Jukka; Lokki, Tapio","Preferences of Critical Listening Environments Among Sound Engineers","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17241","A study of preferred listening environments among fifteen sound engineers illustrates the universal principle that “one size does not fit everyone.” By using the measured impulse responses of nine studio control rooms that were then encoded using the Spatial Decomposition Method, each space was simulated in an anechoic chamber with a 30-channel reproduction system. Preferences depended on the occupation of the sound engineer and on the nature of the song. While mixing engineers preferred acoustically dry environments with high clarity, mastering engineers preferred more reverberant environments with less clarity. Reverberation and clarity appear to be the dominant dimensions for preference. Extensive interviews with the subjects provided more nuanced explanations of how the sound engineers experience a listening space.","2014","2023-07-12 06:40:42","2023-07-19 04:52:26","","300–314","","5","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6JBZPU9S","journalArticle","2015","Drossos, Konstantinos; Floros, Andreas; Kermanidis, Katia L.","Evaluating the Impact of Sound Events’ Rhythm Characteristics to Listener’s Valence","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17573","While modern sound researchers generally focus on speech and music, mammalian hearing arose from the need to sense those events in the environment that produced sound waves. Such unorganized sound stimuli, referred to as Sound Events (SEs), can also produce an affective and emotional response. In this research, the investigators explore valence recognition of SEs utilizing rhythm-related acoustics cues. A well-known data set with emotionally annotated SEs was employed; various rhythm-related attributes were then extracted and several machine-learning experiments were conducted. The results portray that the rhythm of a SE can affect the listener’s valence up to an extent and, combined with previous works on SEs, could lead to a comprehensive recognition of the rhythm’s effect on the emotional state of the listener.","2015","2023-07-12 06:40:48","2023-07-19 03:53:44","","139–153","","3","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RSGU6NE6","journalArticle","2022","Pretto, Niccolò; Pozza, Nadir Dalla; Padoan, Alberto; Chmiel, Anthony; Werner, Kurt James; Micalizzi, Alessandra; Schubert, Emery; Roda, Antonio; Milani, Simone; Canazza, Sergio","A Workflow and Digital Filters for Correcting Speed and Equalization Errors on Digitized Audio Open-Reel Magnetic Tapes","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21798","This paper presents a workflow and digital filters for compensating speed and equalization errors that can impact digitized audio open-reel tapes. Thirty cases of mismatch between recording and reproducing speed (3.75, 7.5, 15, and 30 in/s) and equalization standards [National Association of Broadcasters (NAB), Consultative Committee for International Radio (CCIR), and Audio Engineering Society] were considered. For three frequent cases of mismatch (NAB 3.75 in/s---CCIR 7.5 in/s; NAB 3.75 in/s---CCIR 15 in/s; and NAB 7.5 in/s---CCIR 15 in/s), MUltiple Stimuli with Hidden Reference and Anchor--inspired tests with =21 participants assessed the workflow and digital filters, using excerpts of music and voice. Two different correction filters were used, both of which provided promising results. Following this, subsequent analyses examined predictive variables for correct and incorrect MUltiple Stimuli with Hidden Reference and Anchor performance, as well as spectral and numerical differences between filters, which provide key insights and recommendations for further related work.","2022","2023-07-12 06:40:52","2023-07-19 04:39:10","","495–509","","6","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"922XZHYC","journalArticle","2015","Mu, Hao; Gan, Woon-Seng","Perceptual Quality Improvement and Assessment for Virtual Bass Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18050","Because of size limitation and cost constraints, small loudspeakers cannot efficiently reproduce the sound in the low-frequency range, resulting in poor bass perception and a lack of strong rhythms. A psychoacoustic bass enhancement system, known as the virtual bass system (VBS), enhances the perception of bass reproduced by small loudspeakers by tricking the human auditory system into perceiving bass that does not physically exit. A VBS is based on the psychoacoustic phenomenon called the missing fundamental effect, wherein the higher harmonics of the fundamental frequency can produce the sensation of the fundamental frequency. However, these harmonics can result in perceived distortion. This report describes two techniques to improve the audio quality of the VBS, including an improved hybrid VBS and the timbre matching weighting. Objective and subjective tests are presented to compare the performance of the proposed and the conventional VBS techniques.","2015","2023-07-12 06:40:55","2023-07-19 04:33:56","","900–913","","11","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RTXGP74U","journalArticle","2018","Vryzas, Nikolaos; Kotsakis, Rigas; Liatsou, Aikaterini; Dimoulas, Charalampos A.; Kalliris, George","Speech Emotion Recognition for Performance Interaction","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19585","This research explores the relevance of machine-driven Speech Emotion Recognition (SER) as a way to augment theatrical performances and interactions, such as controlling stage color/light, stimulating active audience engagement, actors’ interactive training, etc. It is well known that the meaning of a speech utterance arises from more than the linguistic content. Emotional affect can dramatically change meaning. As the basis for classification experiments, the authors developed the Acted Emotional Speech Dynamic Database (AESDD, which contains spoken utterances from 5 actors with 5 emotions. Several audio features and various classification techniques were implemented and evaluated using this database, as well comparing results with the Surrey Audio-Visual Expressed Emotion (SAVEE) database. The training classified was integrated into a novel application that performed live SER, fitting the needs of actor training.","2018","2023-07-12 06:41:00","2023-07-19 10:53:55","","457–467","","6","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K7MIZKQI","journalArticle","2011","Sarroff, Andy M.; Bello, Juan P.","Toward a Computational Model of Perceived Spaciousness in Recorded Music","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15975","A validated computational model of perceived spaciousness in sound recordings serves to represent the subjective experience of listeners. Three dimensions of spaciousness form the basis of the model: width of source ensemble, extent of reverberation, and extent of immersion. The model is trained and tested to learn the audio parameters that contribute to these three dimensions. The resulting model predicted spaciousness 32% above that of a baseline predictor. The model can be used to show an audio engineer the value of the three dimensions in real time.","2011","2023-07-12 06:41:03","2023-07-19 04:46:40","","498–513","","7/8","59","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TNZT3B77","journalArticle","2003","Soulodre, Gilbert A.; Lavoie, Michel C.; Norcross, Scott G.","Objective Measures of Listener Envelopment in Multichannel Surround Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12205","A common goal in multichannel musical recordings is to create a better approximation of the concert-hall experience than can be achieved with a traditional stereo reproduction system. Listener envelopment (LEV) is known to be an important part of good concert-hall acoustics and is therefore desirable in multichannel reproduction. In the present study a series of subjective tests were conducted to determine which acoustic parameters are important to the creation of LEV. It is shown that LEV can be controlled systematically in a home listening environment by varying the level and angular distribution of the late arriving sound. While the perceptual transition point between early and late energy has traditionally been set to 80 ms when predicting LEV, this matter has not been investigated rigorously. Subjective tests were conducted wherein the temporal and spatial distributions of the late energy were varied. A new frequency-dependent objective measure GSperc was derived, and it was shown to outperform other objective measures significantly.","2003","2023-07-12 06:41:06","2023-07-19 04:48:52","","826–840","","9","51","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YCK49YKX","journalArticle","2019","Ward, Lauren A.; Shirley, Ben G.","Personalization in Object-based Audio for Accessibility: A Review of Advancements for Hearing Impaired Listeners","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20496","Hearing loss is widespread and significantly impacts an individual’s ability to engage with broadcast media. Access for people with impaired hearing can be improved through new object-based audio personalization methods. Utilizing the literature on hearing loss and intelligibility, this paper develops three dimensions that have the potential to improve intelligibility: spatial separation, speech-to-noise ratio, and redundancy. These can be personalized, individually or concurrently, using object-based audio. A systematic review of all work in object-based audio personalization is then undertaken. These dimensions are utilized to evaluate each project’s approach to personalization, identifying successful approaches, commercial challenges, and the next steps required to ensure continuing improvements to broadcast audio for hard-of-hearing individuals. Although no single solution will address all problems faced by individuals with hearing impairments when accessing broadcast audio, several approaches covered in this review show promise.","2019","2023-07-12 06:41:15","2023-07-19 10:54:39","","584–597","","7/8","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W4RIDB95","journalArticle","2017","Francombe, Jon; Brookes, Tim; Mason, Russell; Woodcock, James","Evaluation of Spatial Audio Reproduction Methods (Part 2): Analysis of Listener Preference","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18556","A paired-comparison preference rating experiment was performed in combination with a free-elicitation task for eight reproduction methods (consumer and professional systems with a wide range of expected quality) and seven program items (representative of potential broadcast material). The experiment was performed by groups of experienced and inexperienced listeners. Both groups preferred systems with increased spatial content; nine- and five-channel systems were most preferred. The use of elicited attributes was analyzed alongside the preference ratings, resulting in an approximate hierarchy of attribute importance. Three attributes (amount of distortion, output quality, and bandwidth) were found to be important for differentiating systems where there was a large preference difference; sixteen were always important (most notably enveloping and horizontal width); and seven were used alongside small preference differences. Although the presence of more spatial content increases preference, adding loudspeaker channels does not necessarily give a corresponding increase in preference.","2017","2023-07-12 06:41:21","2023-07-19 03:58:53","","212–225","","3","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HP96QIXQ","journalArticle","2017","Adami, Alexander; Taghipour, Armin; Herre, Jürgen","On Similarity and Density of Applause Sounds","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19359","With applause an audience expresses its enthusiasm and appreciation; it is therefore a vital part of live performances and thus live recordings. Applause sounds range from very sparse sounds with easily distinguishable individual claps in small crowds to extremely dense and almost noise-like sounds in very large crowds. Commonly used perceptual attributes such as loudness, pitch, and timbre seem insufficient to characterize different types of applause, while “density,” which was recently introduced, is an important perceptual attribute for characterizing applause-like sounds. In this paper, the perceptual properties of applause sounds are investigated with a focus on how spectral equalization affects their perception. Two experiments are presented: the extent to which spectral equalization influences the perception of density and the impact of spectral equalization on the perceived similarity of applause sounds with different densities. Additionally, the data of both experiments were jointly evaluated to explore the effect of perceived density and spectral equalization on the perceived similarity of applause sounds. A statistical analysis of the experimental data suggests that spectral equalization has no statistically significant effect on density and only a small but significant effect on similarity. A linear mixed effects model fitted to the experimental data revealed that perceived density differences as well as spectral equalization significantly predict applause similarity. Density differences were found to be the dominating factor. Finally, the results appear applicable to other impulsive sounds with a high rate of transient events.","2017","2023-07-12 06:41:26","2023-07-19 03:33:57","","897–913","","11","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QCYAYR8L","journalArticle","2023","Ackermann, David; Domann, Julian; Brinkmann, Fabian; Arend, Johannes M.; Schneider, Martin; Pörschmann, Christoph; Weinzier, Stefan","Recordings of a Loudspeaker Orchestra With Multichannel Microphone Arrays for the Evaluation of Spatial Audio Methods","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22032","For live broadcasting of speech, music, or other audio content, multichannel microphone array recordings of the sound field can be used to render and stream dynamic binaural signals in real time. For a comparative physical and perceptual evaluation of conceptually different binaural rendering techniques, recordings are needed in which all other factors affecting the sound (such as the sound radiation of the sources, the room acoustic environment, and the recording position) are kept constant. To provide such a recording, the sound field of an 18- channel loudspeaker orchestra fed by anechoic recordings of a chamber orchestra was captured in two rooms with nine different receivers. In addition, impulse responses were recorded for each sound source and receiver. The anechoic audio signals, the full loudspeaker orchestra recordings, and all measured impulse responses are available with open access in the Spatially Oriented Format for Acoustics (SOFA 2.1, AES69-2022) format. The article presents the recording process and processing chain as well as the structure of the generated database.","2023","2023-07-12 06:41:32","2023-07-19 03:33:33","","62–73","","1/2","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G4JTHMAL","journalArticle","2019","Brinkmann, Fabian; Dinakaran, Manoj; Pelzer, Robert; Grosche, Peter; Voss, Daniel; Weinzierl, Stefan","A Cross-Evaluated Database of Measured and Simulated HRTFs Including 3D Head Meshes, Anthropometric Features, and Headphone Impulse Responses","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20546","The individualization of head related transfer functions (HRTFs) can make an important contribution to improving the quality of binaural technology applications. One approach to individualization is to exploit the relationship between the shape of HRTFs and the anthropometric features of the ears, head, and torso of the corresponding listeners. To identify statistically significant relationships between the two sets of variables, a relatively large database is required. For this purpose full-spherical HRTFs of 96 subjects were acoustically measured and numerically simulated. A detailed cross-evaluation showed a good agreement to previous data between repeated measurements and between measured and simulated data. In addition to 96 HRTFs, the database includes high-resolution head-meshes, a list of 25 anthropometric features per subject, and headphone transfer functions for two headphone models.","2019","2023-07-12 06:41:35","2023-07-19 03:45:08","","705–718","","9","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MYNQIG9X","journalArticle","2014","Wu, Bin; Horner, Andrew; Lee, Chung","The Correspondence of Music Emotion and Timbre in Sustained Musical Instrument Sounds","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17544","While melody, rhythm, and harmony are important emotional triggers in music, there has been little consideration of timbre. The authors designed a series of listening tests to compare the emotionality of sounds from eight wind and bowed stringed instruments. The violin, trumpet, and clarinet were best at evoking the emotions of happy, joyful, heroic, and comic. Conversely, the horn and flute evoked the emotions of sad and depressed. The oboe was emotionally neutral. Emotions correlated with average spectral centroid and spectral centroid deviation. The results suggest that the even/odd harmonic ratio is perhaps the most salient timbral feature after attack time and brightness. This research has direct implications for musicians and audio engineers who are doing orchestration for such applications as computer games, film sound, and stage music.","2014","2023-07-12 06:41:39","2023-07-19 10:57:16","","663–675","","10","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YSP47RBH","journalArticle","2015","Wallis, Rory; Lee, Hyunkook","The Effect of Interchannel Time Difference on Localization in Vertical Stereophony","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18040","When listeners localize in the median plane (vertical), binaural cues are absent because the sound in the two ears is the same; median plane localization depends solely on spectral cues. In order to analyze the localization of band-limited stimuli in vertical stereophony, listening tests were conducted using seven octave bands of pink noise centered at frequencies from 125 to 8000 Hz as well as broadband pink noise. Experimental results showed that localization is generally governed by the so-called “pitch-height” effect, with the high-frequency stimuli generally being localized significantly higher than the low-frequency stimuli for all conditions. The relationship between pitch and height was found to be nonlinear. As frequency increased, subjective judgments appeared to become more erratic because of interchannel time differences.","2015","2023-07-12 06:41:43","2023-07-19 10:54:21","","767–776","","10","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MX7JF7B2","journalArticle","1991","Wöhr, Martin; Theile, Günther; Goeres, Hans-Jürgen; Persterer, Alexander","Room-Related Balancing Technique: A Method for Optimizing Recording Quality","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=5968","In contrast to natural sounds, in the case of stereophonic reproduction the ears localize the loudspeakers as the source of the sound. The spatial depth of a recording can therefore not be reproduced adequately with conventional microphone balancing techniques. It is described how to simulate spatial depth artificially. Listening tests demonstrate that stereophonic reflections clearly improve the quality of a sound image, making it more natural. It was shown that the favorable imaging characteristics found for an appropriate main microphone can be transferred consistently to the spot-microphone signals with the aid of modern computer technology.","1991","2023-07-12 06:41:47","2023-07-19 10:56:29","","623–631","","9","39","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BVFKEIYZ","journalArticle","2007","Wun, Simon; Horner, Andrew","Evaluation of Weighted Principal-Component Analysis Matching for Wavetable Synthesis","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14194","[Engineering Report] Wavetable matching of musical instrument tones using principal-component analysis (PCA) takes advantage of spectral correlation information to find the basis spectra. Although PCA matching is efficient, it usually matches the low-amplitude parts of a tone poorly because of its inherent statistical bias. Weighted PCA methods are described, which normalize the tone prior to PCA to fairly weight its different parts. Matching results for a range of instruments show that the PCA of a frame-weighted spectrum with a roughly constant loudness throughout improves on unweighted PCA by an average of about 4% relative spectral error. Listening test results show that frame-weighted PCA gives some perceptual improvements for most of the instruments.","2007","2023-07-12 06:41:53","2023-07-19 10:57:50","","762–774","","9","55","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QDQSTSUF","journalArticle","2014","Goetze, Stefan; Albertin, Eugen; Rennies, Jan; Habets, Emanuël A.P.; Kammeyer, Karl-Dirk","Speech Quality Assessment for Listening-Room Compensation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17334","There is extensive research on listening-room compensation algorithms to remove the unwanted degradations of speech quality. Objective measures for quality assessment are required in order to evaluate such algorithms. They exist in two classes: reverberation-cancellation algorithms that equalize the room impulse response of the acoustic channel, and reverberation-suppression algorithms that remove the reverberant part of the speech signal by calculating the spectral weight for time-frequency coefficients. This paper focuses on evaluating the former. Algorithms are analyzed regarding their capability to assess reverberation, coloration, spectral distortion, perceived distance, and overall quality of the signals. An evaluation of the sound quality of the dereverberated signals is derived from subjective listening tests and then compared to objective measures.","2014","2023-07-12 06:41:56","2023-07-19 04:02:32","","386–399","","6","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GI28658I","journalArticle","2017","Rasumow, Eugen; Blau, Matthias; Doclo, Simon; Par, Steven van de; Hansen, Martin; Püschel, Dirk; Mellert, Volker","Perceptual Evaluation of Individualized Binaural Reproduction Using a Virtual Artificial Head","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18778","In binaural recordings, spatial information can be captured by using an artificial head that emulates a real human head having average anthropometric geometries and with ear microphones. Because artificial heads are generic without the individual characteristics of the actual listener, recordings often produce perceptual deficiencies such as front-back confusion and internalized source images. Alternatively, individually measured head-related transfer functions (HRTFs) can be approximately synthesized using a microphone array in conjunction with a filter-and-sum beamformer, called a virtual artificial head (VAH). This approach allows for the possibility of adapting a recording to an individual’s HRTF in the recording studio by an appropriate modification of the directivity pattern of the VAH. In this study, binaural reproductions using the VAH, two traditional artificial heads, and individual HRTFs were perceptually evaluated in the horizontal plane with respect to the original free-field presentation. The results show that individual HRTFs in conjunction with individually equalized headphone transfer function result in the best subjective appraisals. The ratings obtained for the VAH-setup indicate a high level of acceptance among the subjects. Mean ratings were often good to excellent.","2017","2023-07-12 06:42:00","2023-07-19 04:40:19","","448–459","","6","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M78RJSNM","journalArticle","2009","Martin, Aengus; Jin, Craig; Schaik, André van","Psychoacoustic Evaluation of Systems for Delivering Spatialized Augmented-Reality Audio","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15234","Spatialized augmented-reality audio systems superimpose virtual sound sources onto sounds from a real acoustic environment. Applications include assistance for visually impaired users, guidance systems, teleconferencing, attention-focusing systems, and electronic games. In the proposed system, acoustically transparent earpieces are used to allow environmental sounds to be heard directly while the virtual reality is created electronically. The results show that the listener can localize virtual sound sources as accurately as when using earphones, which are standard for virtual auditory space presentation, while there is only minor degradation in his ability to localize real sound sources.","2009","2023-07-12 06:42:03","2023-07-19 04:28:29","","1016–1027","","12","57","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NS7P272N","journalArticle","2013","Lee, Hyunkook; Rumsey, Francis","Level and Time Panning of Phantom Images for Musical Sources","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17075","The localization behaviors of panning based on interchannel level difference (ICLD) and interchannel time difference (ICTD) at different target image positions were investigated using musical sources with different spectral and temporal characteristics as well as a wideband speech source. The results indicate that a level panning can perform robustly regardless of the spectral and temporal characteristics of source signals, whereas time panning is not suitable for a continuous source with a high fundamental frequency. Statistical differences between the data obtained for different sources were found to be insignificant, and a unified set of ICLD and ICTD values for 10°, 20°, and 30° image positions was derived. Linear level and time panning functions for the two separate panning regions of 0°–20° and 21°–30° are further proposed, and their applicability to arbitrary loudspeaker base angle is also considered. These perceptual panning functions are expected to be more accurate than the theoretical sine or tangent law in terms of matching between predicted and actually perceived image positions.","2013","2023-07-12 06:42:07","2023-07-19 04:19:40","","978–988","","12","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B7N3XC9A","journalArticle","2015","Kendrick, Paul; Li, Francis; Fazenda, Bruno; Jackson, Iain; Cox, Trevor","Perceived Audio Quality of Sounds Degraded by Nonlinear Distortions and Single-Ended Assessment Using HASQI","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17873","For field recordings and user-generated content recorded on phones, tablets, and other mobile devices, poor audio quality arises in part from nonlinear distortions caused by clipping and limiting at pre-amplification stages and by dynamic range control. Based on the Hearing Aid Sound Quality Index (HASQI), a single-ended method to quantify perceived audio quality in the presence of nonlinear distortions has been developed. Validations on music and soundscapes yielded single-ended estimates within ±0.19 of HASQI on a quality range from 0.0 and 1.0. Perceptual tests were carried out to validate the method for music and soundscapes. HASQI has also been shown to predict quality degradations for processes other than nonlinear distortions including additive noise, linear filtering, and spectral changes. By including these other causes of quality degradations, the current model for nonlinear distortion assessment could be expanded.","2015","2023-07-12 06:42:13","2023-07-19 04:10:32","","698–712","","9","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R4ICTTXX","journalArticle","2018","Eichas, Felix; Zölzer, Udo","Gray-Box Modeling of Guitar Amplifiers","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19874","Musical distortion circuits, especially guitar amplifiers, have been the subject of virtual analog modeling for years. There exists two main modeling approaches: white-box modeling, where the internal properties are fully known, and gray-box modeling, where only the input and output are available. This work proposes a gray-box modeling approach for analog guitar amplifiers using iterative optimization to adjust the parameters of a block-based model. The only assumption made about the reference system is its basic structure. The digital model is an extended Wiener–Hammerstein model consisting of a linear time-invariant (LTI) block, a nonlinear block with a nonlinear mapping function, and another LTI block connected in series. The model is adapted in two steps: first the filters are measured, and then the parameters for the nonlinear part of the digital model are optimized with the Levenberg–Marquardt method to minimize a cost-function describing the error between the digital model and the analog reference system. A small number of guitar amplifiers were modeled, the adapted model was evaluated with objective scores, and a listening test was performed to rate its quality.","2018","2023-07-12 06:42:25","2023-07-19 03:54:19","","1006–1015","","12","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SJTTYEA4","journalArticle","1980","Johnson, George Philip","New Audio Transformations in Acoustical Transduction Based on Optical Principles","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=3997","The historical development of mathematical transformations in audio systems analysis is examined as the changes in sound recording practice and the consequent insight into the operation of the ears are described. The progress in the formal modeling of the audio transformations which the ears seem to perform on sound appears to have reached a theoretical limit in information which may be derived from a set of single point pressure intercepts. Principles of optical interferometry are examined broadly as a possible basis for new acoustical intercepts which will provide more information about acoustic phenomena for audio analysis. New audio transformations developed from experience with acoustical transducers operating onoptical principles may lead to important discoveries about the operation of the ear, which in turn may improve the audio illusion well beyond present standards.","1980","2023-07-12 06:42:28","2023-07-19 04:09:00","","140–149","","3","28","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VYRVKACS","journalArticle","2016","Cecchi, Stefania; Virgulti, Marco; Primavera, Andrea; Piazza, Francesco; Bettarelli, Ferruccio; Li, Junfeng","Investigation on Audio Algorithms Architecture for Stereo Portable Devices","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18106","Due to the size constraints of portable stereo listening devices, their small and closely spaced loudspeakers lead to a poor spatial sound image. An audio algorithms architecture is proposed to compensate for the weak sound image. The system is composed of a spatializer based on an improved version of the recursive ambiophonics crosstalk elimination algorithm, integrated with a combined quasi-anechoic equalization approach, and a virtual bass algorithm capable of enhancing the loudspeakers performance. The proposed solution has been successfully implemented on both Android and iOS operating systems. Listening tests show positive results. Although the spatializer is capable of enhancing the spatial impression, the main audio quality suffers, which therefore requires a compensating equalizer and virtual bass algorithm.","2016","2023-07-12 06:42:37","2023-07-19 03:47:25","","75–88","","1/2","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3R6V7AJP","journalArticle","2015","Zhou, Tingting; Zhang, Ming; Li, Chen","A Model for Calculating Psychoacoustical Fluctuation Strength","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17874","When sound is modulated at frequencies up to 20 Hz, the sensation is that of fluctuation strength. At faster amplitude variations, the sensation is that of roughness. This report describes a new model for calculating fluctuation strength based on equivalent rectangular bandwidth. By using 75 filter channels on the ERB number scale, the total fluctuation strength is calculated by weighting, filtering, and adding the generalized modulation depth (GMD) in each channel. The model changes the way that GMD is converted into specific fluctuation strength. Using an ERB number scale instead of Bark scale provides other advantages. The calculated results using the new model are more consistent with subjective ratings with an RMS error that is decreased by 90% and correlation coefficients are increased by 20%. The proposed model can be used for both narrow and wideband noises.","2015","2023-07-12 06:42:51","2023-07-19 10:59:39","","713–724","","9","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HY6498GG","journalArticle","2015","Ntalampiras, Stavros","Audio Pattern Recognition of Baby Crying Sound Events","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17641","Infants can communicate their internal state (such as pain, hunger, fear, fatigue, or stress) by the nature of their crying. Experts in linguistics suggest that the cry comprises the first speech manifestations. This article describes the design methodology for classifying baby crying sound events according to the pathological status of the infant. Such an automated system can be an aid to an attending physician performing a diagnosis. In order to address this challenge, a great variety of audio parameters (Perceptual Linear Prediction, Mel Frequency Cepstral Coefficients, Perceptual Wavelet Packets, Teager Energy Operator, Temporal Modulation) were considered. Classification techniques, including Multilayer Perception, Support Vector Machine, Random Forest, Reservoir Network, Gaussian Mixture model, and Hidden Markov model were customized. The goal is to provide an automatic and noninvasive framework for monitoring infants and helping inexperienced/trainee pediatricians, parents, and baby caregivers to identify the baby’s pathological status.","2015","2023-07-12 06:42:58","2023-07-19 04:35:15","","358–369","","5","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BYLE2SRN","journalArticle","2019","Silzle, Andreas; Schmidt, Rebekka; Bleisteiner, Werner; Epain, Nicolas; Ragot, Martin","Quality of Experience Tests of an Object-based Radio Reproduction App on a Mobile Device","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20495","Object-based audio (OBA) provides many enhancements and new features; yet, many of these require the user to be active in choosing and selecting the functionalities in visual representations and graphical interfaces. Basic investigations of the user experience of OBA within the EU research project OPRHEUS helped to identify the necessary criteria and dimensions. The user experience in object-based media comprises three dimensions: audio, information, and usability experience. During the project, a radio app for mobile devices was designed, developed, and tested. It includes many of the end-user features available with OBA. A first Quality of Experience (QoE) test to evaluate the radio app was carried out at JOSEPHS, an open innovation lab located in Nuremberg, Germany. The second QoE test took place at b<>com’s user experience lab in Rennes, France. For both investigations, the main objective was to find out how users can access, interact, and appreciate the various new features of OBA. For the first test, two typical user and listening scenarios were simulated: mobile listening and at home. The general acceptance of the new features and functions that come along with OBA is very high. The usability is rated high. Further possibilities for improvements were provided by the test users. The very good perceived sound quality with surround sound over loudspeakers or binaural reproduction over headphones impressed the listeners most. The second test focused mainly on the approach of comparing and evaluating the features from acceptability to acceptance, or from expectations to fulfillment. In the second test, the most appreciated feature was to set fore-to-background balance. This feature was number two in the first test. The importance of speech intelligibility for Radio and TV is a known and well discussed issue. Now, with OBA and the Next Generation Audio (NGA) codec MPEG-H, solutions are at hand to address it.","2019","2023-07-12 06:43:04","2023-07-19 04:48:27","","568–583","","7/8","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9NFI6AIA","journalArticle","2015","Sun, Shuyuan; Shen, Yong; Liu, Ziyun; Feng, Xuelei","The Effects of Recording and Playback Methods in Virtual Listening Tests","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17843","To better control confounding variables found in many listening test, researchers often use virtual listening tests, an accepted method in psychoacoustic research. A virtual listening test method can insure that listening conditions are the same for every test subject, but the choices of recording and playback methods now play an important role. In this report, the results of live and virtual listening tests with four different loudspeakers were compared. The analyses on listeners’ reliability, dispersion of data, preference rating, and Least Significant Difference (LSD) comparison results are presented. The experimental evidence showed the significant effects of recording and playback methods in the virtual listening tests. These results provide evidence that there are significant effects of recording and playback methods in virtual listening tests. A reliable and authentic evaluation can be obtained from virtual listening tests more easily and efficiently by the comprehensive consideration and selection of recording and playback methods.","2015","2023-07-12 06:43:10","2023-07-19 04:50:40","","570–582","","7/8","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EA3Z9QGH","journalArticle","2021","Rund, František; Vencovský, Václav; Semanský, Marek","An Evaluation of Click Detection Algorithms Against the Results of Listening Tests","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21123","This paper evaluates the ability of several algorithms to detect impulse distortions (clicks) in audio signals. The systems are evaluated against data from a listening test conducted using real audio signals provided by a vinyl manufacturer. Some of the signals contained clicks due to damage during the manufacturing process. An evaluation of click detection algorithms against listening test results focuses on the ability of the click-detection algorithms to detect perceptible clicks. The results presented in this paper show that an algorithm that employs a hearing model detected audible clicks with a lower false detection rate than the other algorithms in the test and that the wavelet transform–based algorithm with a dynamic threshold outperformed the other algorithms.","2021","2023-07-12 06:43:13","2023-07-19 04:45:50","","586–593","","7/8","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z676A7LX","journalArticle","2017","Adrian, Jens-Alrik; Gerkmann, Timo; van de Par, Steven; Bitzer, Joerg","Synthesis of Perceptually Plausible Multichannel Noise Signals Controlled by Real World Statistical Noise Properties","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19360","Algorithms in speech and audio applications are often evaluated under adverse conditions to evaluate their robustness against additive noise. This research describes a method to generate artificial but perceptually plausible acoustic disturbances, thereby providing a controlled and repeatable context for evaluating algorithms. This allows for control of such noise parameters as coloration, modulation, and amplitude distribution independently of each other, while also providing the means to define the amount of coherence among all the signal channels. Results of a listening test in a monaural setup show no significant difference in naturalness between synthesized and original signal. It is not always obvious how to create natural noise. For example, it was observed that white Gaussian noise is often an inappropriate noise. Frequency-dependent modulations on a short time scale appear to contribute to naturalness. Synthesizing vinyl/shellac, which has a particular type of impulse character, requires a unique approach to synthesis. Rain and applause synthesis proved to be challenging.","2017","2023-07-12 06:43:16","2023-07-19 03:34:05","","914–928","","11","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N3PF63AK","journalArticle","1993","Kleiner, Mendel; Dalenbäck, Bengt-Inge; Svensson, Peter","Auralization-An Overview","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=6976","Auralization is a term introduced to be used in analogy with visualization to describe rendering audible (imaginary) sound fields. Several modeling methods are available in architectural acoustics for this purpose. If auralization is done by computer modeling, it can be thought of as ""true"" acoustical computer-aided design. Together with new hardware implementation of signal processing routines, auralization forms the basis of a powerful new technoogy for room simulation and aural event generation. The history, trends, problems, and possibilities of auralization are described. The discussion primarily deals with auralization of auditorium acoustics and loudspeaker installations. The advantages and disadvantages of various approaches are discussed, as are possible testing and verification techniques. The possibility of using acoustic scale models for auralization is also discussed.: Demonstrations of auralizations have been made, but still the technology's ability to reproduce the subjective impression of the audible characteristics of a hall accurately remains to be verified. This limits the credibility of auralization as a design tool, and verification of auralization the foremost problem to be attacked at this time. The verification problem also applies to the basic room impulse response prediction programs. The combination of auralization with transaural reproduction, room equalization, and active noise control could make it possible to expand the applications of the technology beyond the laboratory and beyond simple headphone reproduction. A large number of interesting applications outside the room and psychoacoustics reearch are conceivable, the most interesting of which are probably its use in information, education, and entertainment.","1993","2023-07-12 06:43:19","2023-07-19 04:15:21","","861–875","","11","41","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6GD7ANHH","journalArticle","2020","Malecki, Pawel; Sochaczewska, Katarzyna; Wiciak, Andjerzy","Settings of Reverb Processors from the Perspective of Room Acoustics","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20735","The issues of reverberation in acoustic architecture and music production share the same theoretical core; nevertheless the first one has been scientifically researched in depth while the second one remains at technical and experimental crossroads. It could be stated that the ISO 3382 parameters were proposed in the “analog” era for room acoustics (actual halls) whereasthe “digital” parameters introduced in software, artificial reverbs, are not standardized in any way but help to create desired reverberation for music or audio effects. The interest herein is to bind these two disciplines together and analyze some of the significant descriptors of room acoustics (RT, C50, C80, BR, ER, CT) applied in plug-in reverberant processors to observe how the virtual space is affected by changing the values of different parameters. Psychoacoustic ranges of JND were applied to conclude their relevance (or rather influence) and whether it is possible to perceive alteration. Five of the selected popular and commercial VST reverbs are juxtaposed with five similar settings and the results of analysis might be useful for sound mixers and automated mixing algorithms.","2020","2023-07-12 06:43:26","2023-07-19 04:26:42","","292–301","","4","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6QFTBBSP","journalArticle","2019","Franck, Andreas; Francombe, Jon; Woodcock, James; Hughes, Richard; Coleman, Philip; Menzies, Dylan; Cox, Trevor J.; Jackson, Philip J.B.; Fazi, Filippo Maria","A System Architecture for Semantically Informed Rendering of Object-Based Audio","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20488","Object-based audio promises format-agnostic reproduction and extensive personalization of spatial audio content. However, in practical listening scenarios, such as in consumer audio, ideal reproduction is typically not possible. To maximize the quality of listening experience, a different approach is required, for example modifications of metadata to adjust for the reproduction layout or personalization choices. This paper proposes a novel system architecture for semantically informed rendering (SIR), that combines object audio rendering with high-level processing of object metadata. In many cases, this processing uses novel, advanced metadata describing the objects to optimally adjust the audio scene to the reproduction system or listener preferences. The proposed system is evaluated with several adaptation strategies, including semantically motivated downmix to layouts with few loudspeakers, manipulation of perceptual attributes, perceptual reverberation compensation, and orchestration of mobile devices for immersive reproduction. These examples demonstrate how SIR can significantly improve the media experience and provide advanced personalization controls, for example by maintaining smooth object trajectories on systems with few loudspeakers, or providing personalized envelopment levels. An example implementation of the proposed system architecture is described and provided as an open, extensible software framework that combines object-based audio rendering and high-level processing of advanced object metadata.","2019","2023-07-12 06:43:29","2023-07-19 03:58:28","","498–509","","7/8","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VE7AB7SI","journalArticle","2021","Lee, Hyunkook; Johnson, Dale","3D Microphone Array Comparison: Objective Measurements","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21536","This paper describes a set of objective measurements carried out to compare various types of 3D microphone arrays, comprising OCT-3D, PCMA-3D, 2L-Cube, Decca Cuboid, Eigenmike EM32 (i.e., spherical microphone system), and Hamasaki Square with 0-m and 1-m vertical spacings of the height layer. Objective parameters that were measured comprised interchannel and spectral differences caused by interchannel crosstalk (ICXT), fluctuations of interaural level and time differences (ILD and ITD), interchannel correlation coefficient (ICC), interaural cross-correlation coefficient (IACC), and direct-to-reverberant energy ratio (DRR). These were chosen as potential predictors for perceived differences among the arrays. The measurements of the properties of ICXT and the time-varying ILD and ITD suggest that the arrays would produce substantial perceived differences in tonal quality as well as locatedness. The analyses of ICCs and IACCs indicate that perceived differences among the arrays in spatial impression would be larger horizontally rather than vertically. It is also predicted that the addition of the height channel signals to the base channel ones in reproduction would produce little effect on both source-image spread and listener envelopment, regardless of the array type. Finally, differences between the ear-input signals in DRR were substantially smaller than those observed among microphone signals.","2021","2023-07-12 06:43:35","2023-07-19 04:19:31","","871–887","","11","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9VMFDBPT","journalArticle","2008","Beracoechea, J. A.; Torres-Guijarro, Soledad; García, L.; Casajús-Quirós, Francisco J.; Ortiz, L.","Subjective Intelligibility Evaluation in Multiple-Talker Situation for Virtual Acoustic Opening-Based Audio Environments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14388","A virtual conferencing system, which couples two separated rooms as if they were a single virtual space connected by an open window, can be implemented in a variety of ways. This study examines the performance of four approaches using the metric of intelligibility with multiple simultaneous talks as the criterion. The problem is constrained by the amount of computation required, limitations on channel capacity, reverberation in the source space, and the difficulty using beamforming to isolate talkers and reduce acoustics contamination. Many improvements were not linear once a threshold level of performance was achieved.","2008","2023-07-12 06:43:41","2023-07-19 03:41:40","","339–356","","5","56","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UC74E6BM","journalArticle","2013","Berg, Jan; Bustad, Christofer; Jonsson, Lars; Mossberg, Lars; Nyberg, Dan","Perceived Audio Quality of Realistic FM and DAB+ Radio Broadcasting Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16969","The perceived audio quality of a digital broadcasting system (such as DAB+) is dependent on the type of coding and bit rates selected. Because of bandwidth constraints, the required number of channels, and conflicting auxiliary services, audio quality is sometimes degraded. In designing a broadcast system, it is necessary to have well-defined criteria for minimally acceptable quality. Two studies explored quality criteria and how quality degrades for various bit rates. For DAB+ the subchannel rate should not be less than the currently available maximum of 192 kbits/s for a stereo signal, which would be comparable to the quality of a modern FM system. Rates below 160 kbit/s can significantly degrade certain types of program material. To be truly perceptually transparent, bits rates of close to 300 kbits/s may be needed when using the current generation of coders.","2013","2023-07-12 06:43:53","2023-07-19 03:41:48","","755–777","","10","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AGGZ2WGC","journalArticle","2012","Herre, Jürgen; Falch, Cornelia; Mahane, Dirk; Del Galdo, Giovanni; Kallinger, Markus; Thiergart, Oliver","Interactive Teleconferencing Combining Spatial Audio Object Coding and DirAC Technology","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16155","In teleconferencing applications with multiple participants, it would be desirable to create the perception of a real meeting with each speaker in a specific perceived location. This paper describes how Directional Audio Coding (DirAC) can be used as a front end to MPEG Spatial Audio Object Coding (SAOC) to achieve that goal. A novel parameter transcoder provides an efficient way of combining the two technologies. Reproduction of the virtual environment, where each talker exists in a virtual location, can be achieved using headphones or loudspeakers. Subjective tests on different simple and optimized versions were performed, many of which were rated as “good” on the MUSHRA scale.","2012","2023-07-12 06:43:56","2023-07-19 04:05:13","","924–935","","12","59","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8FUY35DV","journalArticle","2015","Baykaner, Khan; Coleman, Philip; Mason, Russell; Jackson, Philip J. B.; Francombe, Jon; Olik, Marek; Bech, Søren","The Relationship Between Target Quality and Interference in Sound Zone","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17568","Sound zone systems aim to control sound fields in such a way that multiple listeners can enjoy different audio programs within the same room with minimal acoustic interference. Often, there is a trade-off between the acoustic contrast achieved between the zones and the fidelity of the reproduced audio program in the target zone. A listening test was conducted to obtain subjective measures of distraction, target quality, and overall quality of listening experience for ecologically valid programs within a sound zoning system. Sound zones were reproduced using acoustic contrast control, planarity control, and pressure matching applied to a circular loudspeaker array. The highest mean overall quality was a compromise between distraction and target quality. The results showed that the term “distraction” produced good agreement among listeners, and that listener ratings made using this term were a good measure of the perceived effect of the interferer.","2015","2023-07-12 06:44:05","2023-07-19 03:39:26","","78–89","","1/2","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UB4E7LSI","journalArticle","1999","Beerends, John G.; De Caluwe, Frank E.","The Influence of Video Quality on Perceived Audio Quality and Vice Versa","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12105","Three questions are being studied. Question one: 'To what extent is the perceived audio quality of an audiovisual stream influenced by the video quality?' Question two is the reverse: 'To what extent is the perceived video quality of an audiovisual stream influenced by the audio quality?' Finally: 'How do audio and video quality contribute to the overall perceived audiovisual quality?' The quality ranges from broadcast audio and video quality to standard videophone (telephone) quality. The main conclusion is that when subjects are asked to judge the audio quality of an audiovisual stimulus, the video quality will contribute significantly to the subjectively perceived audio quality. The effect is about 1.2 points on a nine-point quality scale. The reversed effect is much smaller, about 0.2 point. Furthermore a simple mapping from the audio and video quality to the overall audiovisual quality given, and it is shown that the video quality dominates the overall perceived audiovisual quality in nonconversational experiments.","1999","2023-07-12 06:44:09","2023-07-19 03:39:56","","355–362","","5","47","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H6FK7XUR","journalArticle","1969","Luce, David; Clark, Melville, Jr.","A Real-Time Multipartial Waveform Analyzer-Synthesizer","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=1567","A real-time analyzer-synthesizer is described that can process complex signals whose waveforms change with time to the limits of the uncertainty principle. Overlap integrals between the input signal and user-selected basis functions are automatically recorded on (during analysis) or read from (during synthesis) film. Special converters permit using trigonometric amplitudes and phases for analysis and synthesis. Frequency tracking with signal-controlled bandwidth may be used during analysis. Frequency tracking uses a coherence detector to distinguish signals to which a frequency may be ascribed from noise. Various constant fractional or constant absolute bandwidths may be selected. The synthesizer can work with user-provided graphs; any of these curves and those on the film may be used as the modulation function for any selected basis functions.","1969","2023-07-12 06:44:13","2023-07-19 04:25:08","","439–444","","4","17","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CQ8TMGUA","journalArticle","2019","Paulus, Jouni; Torcoli, Matteo; Uhle, Christian; Herre, Jürgen; Disch, Sascha; Fuchs, Harald","Source Separation for Enabling Dialogue Enhancement in Object-based Broadcast with MPEG-H","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20489","Low intelligibility of narration or dialogue resulting from high background level is one of the most common complaints in broadcasting. Even when the intelligibility is not compromised, listeners may have personal preferences that differ from the mix being broadcast. Dialogue Enhancement (DE) enables the delivery of optimal dialogue mixing to each listener, be it in terms of intelligibility or for aesthetic preference. This makes DE one of the most promising applications of user interactivity enabled by object-based audio broadcasting, such as MPEG-H. This paper investigates the use of source separation methods to extract dialogue and background from the complex sound mixture for enabling object-based broadcasting when dialogue is not available from the production process, as for example, with legacy content. The presented source separation technology integrates several separation approaches with known limitations into a more powerful overall architecture. In addition, the paper evaluates the subjective benefit of DE using the Adjustment/Satisfaction Test in which the listeners made extensive use of the dialogue level personalization. The fact that the preferred dialogue level had a high variance among the listeners indicates the need for this functionality. Even when an imperfect separation result was used for enabling DE, the possibility for personalizing the dialogue level lead to increased listener satisfaction.","2019","2023-07-12 06:44:41","2023-07-19 04:37:20","","510–521","","7/8","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"II5KAD63","journalArticle","2012","Huckvale, Mark; Hilkhuysen, Gaston","Performance-Based Measurement of Speech Quality with an Audio Proof-Reading Task","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16358","Speech communication systems need to be evaluated across a wide range of signal qualities. However, when signal quality is high, evaluations focus on listener acceptance rather than on task performance because conventional intelligibility tests reach ceiling values. This research considers whether measures of cognitive load could be used to measure the effect of signal quality on communication performance even when intelligibility is high. It is shown that differences in signal quality can affect the ability of listeners to detect transcription errors in an audio proof-reading task. The research also shows that a noise reduction system, which elsewhere has been said to improve listener acceptance, gives no improvement in terms of cognitive load on this task.","2012","2023-07-12 06:44:49","2023-07-19 04:07:20","","444–451","","6","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"84R4YG5B","journalArticle","2020","Geary, David; Torcoli, Matteo; Paulus, Jouni; Simon, Christian; Straninger, Davide; Travaglini, Alessandro; Shirley, Ben","Loudness Differences for Voice-Over-Voice Audio in TV and Streaming","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20995","Voice-over-Voice (VoV) is a common mixing practice observed in news reports and documentaries, where a foreground voice is mixed on top of a background voice, e.g., to translate an interview. This is achieved by ducking the background voice so that the foreground voice is more intelligible, while still allowing the listener to perceive the presence and tone of the background voice. Currently there is little published research on ducking practices for VoV or on technical details such as the Loudness Difference (LD) between foreground and background speech. This paper investigates the ducking practices of nine expert audio engineers and the preferred LDs of 13 non-expert listeners of ages 57 years and older. Results highlight a clear difference between the LDs used by the experts and those preferred by the non-expert listeners. Experts tended toward LDs of 11.5–17 LU, while non-experts preferred a range of 20–30 LU. Based on these results, a minimum LD of 20 LU is recommended for VoV. High inter-subject variance due to personal preference was observed. This variance makes a substantial case for the introduction of personalization in broadcast and streaming. The audiovisual material used for the tests is provided at https://www.audiolabs-erlangen.de/resources/2020-VoV-DB.","2020","2023-07-12 06:44:53","2023-07-19 04:01:47","","810–818","","11","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"43YTF639","journalArticle","2021","Kamaris, Gavriil; Zachos, Panagiotis; Mourjopoulos, John","Low Filter Order Digital Equalization for Mobile Device Earphones","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21035","This work examines the feasibility and acceptability of digitally correcting the response irregularities of typical earphones used for listening with cell phones and other mobile devices. A novel adaptive low filter order response equalization method is introduced since for these applications digital signal processing resources are limited; hence such filters are restricted in order. The method employs parallel infinite impulse response (IIR) filter sections with 5–9 pole pairs that match well a target frequency response known to be suitable for earphone listening. The tests with earphones of varying specifications and price indicate that these short filters can be effective for response equalization and that such processing improves listener preference.","2021","2023-07-12 06:44:56","2023-07-19 04:09:34","","297–308","","5","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QJ9NHQP3","journalArticle","1997","Pulkki, Ville","Virtual Sound Source Positioning Using Vector Base Amplitude Panning","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=7853","A vector-based reformulation of amplitude panning is derived, which leads to simple and computationally efficient equations for virtual sound source positioning. Using the method, vector base amplitude panning (VBAP), it is possible to create two- or three-dimensional sound fields where any number of loudspeakers can be placed arbitrarily. The method produces virtual sound sources that are as sharp as is possible with current loudspeaker configuration and amplotude panning methods. A digital tool that implements two- and three-dimensional VBAP with eight inputs and outputs has been realized.","1997","2023-07-12 06:45:00","2023-07-19 04:39:18","","456–466","","6","45","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SAYYZBER","journalArticle","2011","Oo, Nay; Gan, Woon-Seng; Hawksford, Malcolm O. J.","Perceptually-Motivated Objective Grading of Nonlinear Processing in Virtual-Bass Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16149","The RCA Victor DYNAGROOVE system is a comprehensive system of improvements in sound recording by means of disc records. All aspects of the process are taken into consideration, starting with the artist's conception of the music and ending with the reproduction of the sound in the listener's home.","2011","2023-07-12 06:45:03","2023-07-19 04:36:17","","804–824","","11","59","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LBIACGKJ","journalArticle","2016","Lee, Hyunkook","Perceptual Band Allocation (PBA) for the Rendering of Vertical Image Spread with a Vertical 2D Loudspeaker Array","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18534","Two subjective experiments were conducted to examine a new vertical image-rendering method called Perceptual Band Allocation (PBA), using octave bands of pink noise presented from main and height loudspeaker pairs. The PBA attempts to control the perceived degree of vertical image spread (VIS) by a flexible mapping between frequency band and loudspeaker layer based on the desired positioning of the band in the vertical plane. The first experiment measured the perceived vertical location of the phantom image of octave-band stimuli for the main and height loudspeaker layers individually. Results showed significant differences among the frequency bands in perceived image location. Based on the localization data from this experiment, six different PBA stimuli were created in such a way that each frequency band was mapped to either the main or height loudspeaker layer depending on the target degree of VIS. The second experiment conducted a listening test to grade the perceived magnitudes of VIS for the six stimuli. The results indicated that PBA could significantly increase the perceived magnitude of VIS compared to that of a sound presented only from the main layer. It was also found that the different PBA schemes produced various degrees of perceived VIS with statistically significant differences.","2016","2023-07-12 06:45:07","2023-07-19 04:18:34","","1003–1013","","12","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"46G5GRL3","journalArticle","1997","Olive, Sean E.; Schuck, Peter L.; Ryan, James G.; Sally, Sharon L.; Bonneville, Marc E.","The Detection Thresholds of Resonances at Low Frequencies","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=7868","New experimental data on the detection thresholds of low-frequency resonances and antiresonances are presented. Using an adaptive procedure known as the up-down transformed response (UDTR) rule, the 70.7% detection thresholds were measured for a single added resonance (peak and notch) for different Q values and center frequencies. The signals included pink noise and pulses auditioned through earphones. The results show that detection thresholds are affected in complicated ways by Q, center frequency, and signal type. This makes their detection difficult to predict using current hearing models and frequency response measurements.","1997","2023-07-12 06:45:11","2023-07-19 04:35:42","","116–128","","3","45","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GHVWNC93","journalArticle","1996","Välimäki, Vesa; Huopaniemi, Jyri; Karjalainen, Matti; Jánosy, Zoltán","Physical Modeling of Plucked String Instruments with Application to Real-Time Sound Synthesis","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=7902","An efficient approach for real-time synthesis of plucked string instruments using physical modeling and DSP techniques is presented. Results of model-based resynthesis are illustrated to demonstrate that high-quality synthetic sounds of several string instruments can be generated using the proposed modeling principles. Real-time implementation using a signal processor is described, and several aspects of controlling physical models of plucked string instruments are studied.","1996","2023-07-12 06:45:16","2023-07-19 04:54:38","","331–353","","5","44","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KL8SASIC","journalArticle","2015","Lee, Hyunkook; Gribben, Christopher","Effect of Vertical Microphone Layer Spacing for a 3D Microphone Array","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17560","Subjective listening tests were conducted to investigate how the spacing between main (lower) and height (upper) microphone layers in a 3D main microphone array affects perceived spatial impression and overall preference. It was generally found that layer spacing of 0.5 m, 1 m, and 1.5 m did not produce significant differences in either perceived spatial impression or preference. The 0-m layer had slightly higher ratings than the spaced layers in both spatial impression and preference, depending on the type of source. The four configurations were compared with trumpet, acoustic guitar, percussion quartet, and string quartet using a 9-channel loudspeaker setup. It is suggested that the perceived results were mainly associated with vertical interchannel crosstalk in the signals of each height layer and the magnitude and pattern of spectral change at the listener’s ear caused by each layer. Informal comments suggested that the main preference attributes were tonal quality, as well as spatial quality.","2015","2023-07-12 06:45:20","2023-07-19 04:19:22","","870–884","","12","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F7WWWY9P","journalArticle","2022","Andreopoulou, Areti; Katz, Brian F. G.","Perceptual Impact on Localization Quality Evaluations of Common Pre-Processing for Non-Individual Head-Related Transfer Functions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21738","This article investigates the impact of two commonly used Head-Related Transfer Function (HRTF) processing/modeling methods on the perceived spatial accuracy of binaural data by monitoring changes in user ratings of non-individualized HRTFs. The evaluated techniques are minimum-phase approximation and Infinite-Impulse Response (IIR) modeling. The study is based on the hypothesis that user-assessments should remain roughly unchanged, as long as the range of signal variations between processed and unprocessed (reference) HRTFs lies within ranges previously reported as perceptually insignificant. Objective assessments of the degree of spectral variations between reference and processed data, computed using the Spectral Distortion metric, showed no evident perceptually relevant variations in the minimum-phase data and spectral differences marginally exceeding the established thresholds for the IIR data, implying perceptual equivalence of spatial impression in the tested corpus. Nevertheless analysis of user responses in the perceptual study strongly indicated that variations introduced in the data by the tested methods of HRTF processing can lead to inversions in quality assessment, resulting in the perceptual rejection of HRTFs that were previously characterized in the ratings as the ""most appropriate"" or alternatively in the preference of datasets that were previously dismissed as ""unfit."" The effect appears more apparent for IIR processing and is equally evident across the evaluated horizontal and median planes.","2022","2023-07-12 06:45:28","2023-07-19 03:36:31","","340–354","","5","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3ZJF9LZ5","journalArticle","2023","Brinkmann, Fabian; Kreuzer, Wolfgang; Thomsen, Jeffrey; Dombrovskis, Sergejs; Pollack, Katharina; Weinzierl, Stefan; Majdak, Piotr","Recent Advances in an Open Software for Numerical HRTF Calculation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22155","Mesh2HRTF 1.x is an open-source and fully scriptable end-to-end pipeline for the numerical calculation of head-related transfer functions (HRTFs). The calculations are based on 3D meshes of listener's body parts such as the head, pinna, and torso. The numerical core of Mesh2HRTF is written in C++ and employs the boundary-element method for solving the Helmholtz equation. It is accelerated by a multilevel fast multipole method and can easily be parallelized to further speed up the computations. The recently refactored framework of Mesh2HRTF 1.x contains tools for preparing the meshes as well as specific post-processing and inspection of the calculatedHRTFs. The resultingHRTFs are saved in the spatially oriented format for acoustics being directly applicable in virtual and augmented reality applications and psychoacoustic research. The Mesh2HRTF 1.x code is automatically tested to assure high quality and reliability. A comprehensive online documentation enables easy access for users without in-depth knowledge of acoustic simulations.","2023","2023-07-12 06:45:36","2023-07-19 03:45:17","","502–514","","7/8","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F3ZGMVTK","journalArticle","2017","Sena, Enzo De; Brookes, Mike; Naylor, Patrick A.; Waterschoot, Toon van","Localization Experiments with Reporting by Head Orientation: Statistical Framework and Case Study","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19364","This research focuses on sound localization experiments in which subjects report the position of an active sound source by turning toward it. A statistical framework for the analysis of the data is presented together with a case study from a large-scale listening experiment. The statistical framework is based on a model that is robust to the presence of front/back confusions and random errors. Closed-form natural estimators are derived, and one-sample and two-sample statistical tests are described. The framework is used to analyze the data of an auralized experiment undertaken by nearly nine hundred subjects. The objective was to explore localization performance in the horizontal plane in an informal setting and with little training, which are conditions that are similar to those typically encountered in consumer applications of binaural audio. Results show that responses had a rightward bias and that speech was harder to localize than percussion sounds, which are results consistent with the literature. Results also show that it was harder to localize sound in a simulated room with a high ceiling despite having a higher direct-to-reverberant ratio than other simulated rooms.","2017","2023-07-12 06:45:40","2023-07-19 04:47:22","","982–996","","12","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NDLL68A2","journalArticle","2020","Ackermann, David; Fiedler, Felicitas; Brinkmann, Fabian; Schneider, Martin; Weinzierl, Stefan","On the Acoustic Qualities of Dynamic Pseudo-Binaural Recordings","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20858","The motion-tracked binaural (MTB) technique allows the dynamic, pseudobinaural rendering of spatial sound scenes recorded by a circular array of microphones on a rigid sphere. The system provides a multichannel live audio transmission from which a head-related signal with approximated interaural time and level differences can be derived and played via headphones, head tracking, and a corresponding rendering software. The latter is mainly calculating imperceptible interpolation between channel pairs during head movements. This contribution evaluates the potential of this format for the creation of virtual acoustic envi- ronments. Based on the technical realization of a 16-channel MTB array with omnidirectional diffuse field-corrected electret condenser microphone capsules, the plausibility of 8 and 16-channel recordings was tested against a physical sound source. Furthermore, the sound quality of the pseudobinaural rendering was assessed based on different items of the Spatial Audio Quality Inventory (SAQI) compared to a true dynamic binaural reference. The results show that the overall plausibility of the MTB signal with optimal interpolation is close to the reference. Even if there are small differences with respect to tone color and spatial sound source attributes, the degree of externalization and even the perceived source elevation were, despite the absence of pinna cues, well comparable to the true binaural reference.","2020","2023-07-12 06:45:43","2023-07-19 03:33:47","","418–427","","6","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8EPB8NI7","journalArticle","2013","Vilkamo, Juha; Pulkki, Ville","Minimization of Decorrelator Artifacts in Directional Audio Coding by Covariance Domain Rendering","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16932","Directional Audio Coding (DirAC) is a perceptually motivated microphone technique that models the sound field as a combination of a plane wave and a surrounding diffuse field with a time–frequency resolution that approximates that of the human spatial hearing. In this paper a recently proposed covariance domain spatial-sound rendering method was applied to optimize the DirAC reproduction by minimizing the amount of the decorrelated sound energy. When several semi-independent microphone signals were available, this procedure was shown to improve the overall perceived sound quality, especially with audio content that has an impulsive fine structure, such as applause and speech. In all tests, the covariance rendering method performed similarly or better than the legacy rendering method, making it the preferred choice for performing DirAC synthesis.","2013","2023-07-12 06:45:47","2023-07-19 10:53:00","","637–646","","9","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DDVEB2QW","journalArticle","2012","Vilkamo, Juha; Neugebauer, Bernhard; Plogsties, Jan","Sparse Frequency-Domain Reverberator","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16156","There are many topologies for implementing the late part of artificial reverberation, each of which is a unique trade-off among perceived quality, computational cost, and parameter flexibility. A new approach, which assumes that the late reverberation can be described as a stochastic process, applies the reverberation in frequency bands using a combination of a feedback loop and an efficient sparse decorrelator. Listening tests with ten experts confirmed that the new reverberator topology has a perceived quality that was mostly equivalent to the idealized reverberation using decaying Gaussian noise response in frequency bands.","2012","2023-07-12 06:45:57","2023-07-19 10:52:53","","936–943","","12","59","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IE4YCV6L","journalArticle","1998","Lipshitz, Stanley P.","Dawn of the Digital Age","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12175","This paper attempts to give the reader a concise survey of the history of the development of digital audio technology in the Audio Engineering Society, as seen through its publications.","1998","2023-07-12 06:46:08","2023-07-19 04:20:44","","37–42","","1/2","46","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VA9Z5WNC","journalArticle","1971","Eargle, John M.","Multichannel Stereo Matrix Systems: An Overview","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=2155","In increasing profusion, -4-2-4- matrix playback systems are being introduced to an already confused market place. All of these systems fall short of the -four-channel- performance claimed for them, but many of them are surprisingly accurate in their recreation of auditory perspective around the compass. This paper will examine the basic properties of 4-2-4 matrices along with some of the embellishments which have been used to improve their performance. Finally, higher order matrix systems will be examined briefly.","1971","2023-07-12 06:46:12","2023-07-19 03:54:10","","552–559","","7","19","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VHUS6GPL","journalArticle","1996","Craven, Peter G.; Gerzon, Michael A.","Lossless Coding for Audio Discs","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=7887","Strategies are presented that are being used to achieve efficient lossless compression or packing of PCM audio waveform data, allowing storage and transmission at greatly reduced data rates without any alteration of the signal. The disadvantages of simpler difference schemes and the benefits of more advanced schemes using IIR prediction with Huffman coding are explained and described, particularly with regard to the unique requirements of the future high-quality audio disc (HQAD) standard using high-density CD media.","1996","2023-07-12 06:46:23","2023-07-19 03:50:28","","706–720","","9","44","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"289GYVU3","journalArticle","2013","Neuendorf, Max; Multrus, Markus; Rettelbach, Nikolaus; Fuchs, Guillaume; Robilliard, Julien; Lecomte, Jérémie; Wilde, Stephan; Bayer, Stefan; Disch, Sascha; Helmrich, Christian; Lefebvre, Roch; Gournay, Philippe; Bessette, Bruno; Lapierre, Jimmy; Kjörling, Kristofer; Purnhagen, Heiko; Villemoes, Lars; Oomen, Werner; Schuijers, Erik; Kikuiri, Kei; Chinen, Toru; Norimatsu, Takeshi; Chong, Kok Seng; Oh, Eunmi; Kim, Miyoung; Quackenbush, Schuyler; Grill, Bernhard","The ISO/MPEG Unified Speech and Audio Coding Standard—Consistent High Quality for All Content Types and at All Bit Rates","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17074","With the advent of devices that unite a multitude of functionalities, the industry has an increased demand for an audio codec that can deal equally well with all types of audio content. In early 2012 the ISO/IEC JTC1/SC29/WG11 (MPEG) finalized the new MPEG-D Unified Speech and Audio Coding standard, bringing together the previously separated worlds of general audio and speech coding. It does so by integrating elements from audio coding and speech coding into a unified system over a wide range of bit rates. The present publication outlines all aspects of this standardization effort, starting with the history and motivation of the MPEG work. Technical features of the final system are described. Listening test results and performance numbers show the advantages of the new system over current state-of-the-art codecs.","2013","2023-07-12 06:46:27","2023-07-19 04:34:49","","956–977","","12","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CQJTN66G","journalArticle","2000","Evans, Michael J.; Tew, Anthony I.; Angus, James A. S.","Perceived Performance of Loudspeaker-Spatialized Speech for Teleconferencing","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12051","Spatial audio reproduction using loudspeakers can enhance teleconferencing applications by increasing the perceptual realism or telepresence, of participants' interactions. However, it is critical that any additional perceptual realism afforded by spatial audio does not compromise the perceived performance of the telecommunications in terms of participants' understanding of the words being conveyed. Formal subjective testing methods are used to verify that the spatial reproduction of speech does not compromise perceived performance relative to nonspatialized reproduction. Indeed, significant improvements in the performance as perceived by listeners have been found. Additional tests identify apparent discrepancies between which method permits the most accurate localization and which has the highest perceived performance.","2000","2023-07-12 06:46:30","2023-07-19 03:55:02","","771–785","","9","48","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KW6P3YJE","journalArticle","2003","Smithers, Michael J.; Crockett, Brett G.; Fielder, Louis D.","Ultra-High Quality Video Frame Synchronous Audio Coding","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12196","Two methods of coding and delivering ultra-high-quality audio are presented. Both methods are video frame synchronous and editable at common video frame rates (23.98, 24, 25, 29.97, and 30 frames per second) without the use of sample-rate converters. The first is an ultra-high-quality audio coder that exceeds 4.8 on the ITU-R five-point audio impairment scale at a bit rate of 256 kbit/s per channel and at up to three generations of encoding/decoding. The second is an enhanced method of video frame synchronous PCM packing. Specifically the problem of transmitting 48-kHz audio in 29.97-Hz frames is examined.","2003","2023-07-12 06:46:35","2023-07-19 04:48:35","","1032–1045","","11","51","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MZFZP4QZ","journalArticle","2019","Stuart, J. Robert; Craven, Peter G.","The Gentle Art of Dithering","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20457","There is still disagreement over the ways in which sound quality might benefit from higher sampling rates or wider bit depths in a digital path. The authors show that if a digital pathway includes any unintended or undithered quantizations, then several types of errors will be created whose nature will change with increased sampling rate and word size. Although dither methods for ameliorating quantization error have been well understood in the literature for some time, these insights are not always applied in practice. It is rare for an audio performance to be captured, produced, and played back with a flawless chain. The paper includes a tutorial overview of digital sampling and quantization with additive, subtractive, and noise-shaped dither. The discussions also include more advanced topics, such as cascaded quantizers, fixed and floating-point arithmetic, and time-domain aspects of quantization errors. Guidelines and recommendations are presented, including for the design of listening tests.","2019","2023-07-12 06:46:43","2023-07-19 04:50:22","","278–299","","5","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EHGIUHTW","journalArticle","2015","Politis, Archontis; Laitinen, Mikko-Ville; Ahonen, Jukka; Pulkki, Ville","Parametric Spatial Audio Processing of Spaced Microphone Array Recordings for Multichannel Reproduction","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17580","There are two major categories of recording approaches for multichannel sound reproduction: coincident and spaced microphone arrays. Coincident techniques assume that the microphones are essentially at the same point in space and they provide stable localization of discrete sound events. In contrast, spaced techniques provide a better enveloping sense of the ambient reverberant sound, at the expense of localization performance. In this research a parametric processing method is presented for spaced microphone array recordings using the principles of Directional Audio Coding (DirAC), knowledge of the array configuration, and the directional response of the microphones. The method achieves equal or better quality relative to the standard high-quality version of DirAC and it improves the common direct playback of such recordings by offering improved and stable localization cues and reduction of coloration issues at all frequencies.","2015","2023-07-12 06:46:47","2023-07-19 04:38:36","","216–227","","4","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7CQCFBCM","journalArticle","2022","Majdak, Piotr; Zotter, Franz; Brinkmann, Fabian; De Muynke, Julien; Mihocic, Michael; Noisternig, Markus","Spatially Oriented Format for Acoustics 2.1: Introduction and Recent Advances","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21824","Spatially oriented acoustic data can range from a simple set of impulse responses, such as head-related transfer functions, to a large set of multiple-input multiple-output spatial room impulse responses obtained in complex measurements with a microphone array excited by a loudspeaker array at various conditions. The spatially oriented format for acoustics (SOFA), which was standardized by AES Standard 69, provides a format to store and share such data. SOFA takes into account geometric representations of many acoustic scenarios, data compression, network transfer, and a link to complex room geometries and aims at simplifying the development of interfaces for many programming languages. With the recent advancement of SOFA, the format offers a new continuous-direction representation of data by means of spherical harmonics and novel conventions representing many measurement scenarios, such as source directivity and multiple-input multiple-output spatial room impulse responses. This article reviews SOFA by first providing an introduction to SOFA and then describing examples that demonstrate the most recent features of SOFA 2.1 (AES Standard 69-2022).","2022","2023-07-12 06:46:52","2023-07-19 04:26:15","","565–584","","7/8","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R3YXRY3A","journalArticle","2022","McCormack, Leo; Politis, Archontis; McKenzie, Thomas; Hold, Christoph; Pulkki, Ville","Object-Based Six-Degrees-of-Freedom Rendering of Sound Scenes Captured with Multiple Ambisonic Receivers","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21739","This article proposes a system for object-based six-degrees-of-freedom (6DoF) rendering of spatial sound scenes that are captured using a distributed arrangement of multiple Ambisonic receivers. The approach is based on first identifying and tracking the positions of sound sources within the scene, followed by the isolation of their signals through the use of beamformers. These sound objects are subsequently spatialized over the target playback setup, with respect to both the head orientation and position of the listener. The diffuse ambience of the scene is rendered separately by first spatially subtracting the source signals from the receivers located nearest to the listener position. The resultant residual Ambisonic signals are then spatialized, decorrelated, and summed together with suitable interpolation weights. The proposed system is evaluated through an in situ listening test conducted in 6DoF virtual reality,whereby real-world sound sources are compared with the auralization achieved through the proposed rendering method. The results of 15 participants suggest that in comparison to a linear interpolation-based alternative, the proposed object-based approach is perceived as being more realistic.","2022","2023-07-12 06:46:56","2023-07-19 04:29:19","","355–372","","5","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"69FH6G69","journalArticle","1994","Olive, Sean E.; Schuck, Peter L.; Sally, Sharon L.; Bonneville, Marc E.","The Effects of Loudspeaker Placement on Listener Preference Ratings","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=6930","Through the use of an acoustically adjustable listening room and a binaural recording and reproduction system, live and binaural subjective evaluations were made of different loudspeakers placed in different room locations. The experimental results from both tests show that listener preference ratings for different loudspeakers are significantly influenced by the loudspeaker location within the room. In fact, the positional effects can be larger than the subjective differences between the loudspeakers themselves. The binaural evaluations indicate that listener preferences are influenced significantly by interactions between the loudspeaker, its location, and the type of program material auditioned. These secondary effects were less significant in the live tests, suggesting that traditional real-time listening tests may be inadequate for measuring or controlling these effects.","1994","2023-07-12 06:47:00","2023-07-19 04:35:50","","651–669","","9","42","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"72PCYJPF","journalArticle","2017","Lee, Hyunkook","Sound Source and Loudspeaker Base Angle Dependency of Phantom Image Elevation Effect","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19203","Previous research showed that when identical noise signals are presented from two loudspeakers equidistant from the listener, the resulting phantom image is perceived as being elevated in the median plane. In this study, listening tests used eleven natural sources and four noise sources with different spectral and temporal characteristics reproduced with seven loudspeaker base angles between 0° and 360°. While the degree of perceived elevation depends on the base angle of the loudspeakers, the spectral and temporal characteristics of the sound source also play a significant role in determining perceived elevation. Results generally suggest that the effect is stronger for sources that have a transient nature and a flat frequency spectrum as compared to continuous and low-frequency sources. It is proposed that the perceived degree of elevation is determined by a relative cue related to the spectral energy distribution at high frequencies and also by an absolute cue associated with the acoustic crosstalk and torso reflections at low frequencies. A novel hypothesis about the role of acoustic crosstalk and torso reflection at low frequencies is explored. At frequencies below 3 kHz, the brain might use the first notch in the ear-input spectrum, which is produced by the combination of acoustic crosstalk and torso reflection, as a cue for localizing a phantom source at an elevated position in the median plane. These results may prove useful for 3D sound panning, recording, and mixing without elevated speakers.","2017","2023-07-12 06:47:03","2023-07-19 04:18:42","","733–748","","9","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SHNLQ9CS","journalArticle","2011","Uhle, Christian","Applause Sound Detection","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15926","Detecting applause in both audio recordings and real-time performances is relevant in such applications as music information retrieval and spatial audio coding. A combination of mel-frequency cepstral coefficients and low-level descriptors yielded the best classification performance in the experiments. Low-pass filtering of the feature time series leads to the concept of sigma features. Binary misclassification occurs more often when applause and nonapplause with similar amplitudes are simultaneously present.","2011","2023-07-12 06:47:12","2023-07-19 04:54:30","","213–224","","4","59","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YBE98R5Y","journalArticle","2016","Delikaris-Manias, Symeon; Bolaños, Javier Gómez; Eskelinen, Joona; Huhtakallio, Ilkka; Hæggström, Edward; Pulkki, Ville","Auralization of Source Radiation Pattern Synthesized with Laser Spark Room Responses","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18514","This research describes a method to auralize the effect of a three-dimensional directional pattern of an acoustic source in a reverberant environment using real acoustic measurements of laser sparks. Laser-induced breakdown (LIB) produces a massless and point-like acoustic source. The authors demonstrate the performance of a volumetric array of LIBs for synthesizing arbitrary radiation patterns, auralize the radiation pattern of a loudspeaker and compare the measured and synthesized impulse responses in a reverberant room, and evaluate the method using listening tests. The synthesized room response matched the target response both in room response reconstruction and in listening tests. The proposed method requires no previous knowledge about the room characteristics; auralization is performed by convolving a sound signal with the synthesized room impulse responses using the cloud of laser sparks. The quality of the synthesized version was rated to be excellent.","2016","2023-07-12 06:47:15","2023-07-19 03:51:54","","720–730","","10","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2SLY6H63","journalArticle","2021","Choi, Jeonghwan; Chang, Joon-Hyuk","Exploiting Deep Neural Networks for Two-to-Five Channel Surround Decoder","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21008","We exploited deep neural networks (DNN) for two-to-five channel surround decoding. Specifically DNNs are used to replace the primary-ambient separation and ambient-signal-rendering modules. For the training, the mean-squared error of the magnitude spectra between the decoded and five-channel target signals and the interchannel level differences between the target signals were used as the loss functions. Through this procedure the DNNs can derive the spectral weights that can be used to produce the decoded signals, similar to that for the target signals. The log spectral distance, signal-to-distortion ratio, and multiple stimuli with hidden reference and anchor tests were used for objective and subjective evaluations. The experimental results show that exploiting the DNNs can generate decoded signals that are more similar to the target signals than those obtained via previous methods.","2021","2023-07-12 06:47:34","2023-07-19 03:48:09","","938–949","","12","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6BAG2VSD","journalArticle","2003","Tao, Yufei; Tew, Anthony I.; Porter, Stuart J.","The Differential Pressure Synthesis Method for Efficient Acoustic Pressure Estimation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12212","A differential pressure synthesis (DPS) method is proposed which estimates the free-field acoustic pressure on the boundary of an object from its geometry by precalculating a database of pressure changes caused by introducing orthogonal shape deformations to a template shape. Pressures are synthesized using DPS for a two-dimensional shape and a three-dimensional KEMAR head model. The accuracy of pressure estimates compares favorably with the boundary-element method computation provided that shape deformations are moderate in relation to acoustic wavelength.","2003","2023-07-12 06:47:38","2023-07-19 04:51:50","","647–656","","7/8","51","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I3BPYNJ3","journalArticle","2021","Georg, Götz; Schlecht, Sebastian J.; Ornelas, Abraham Martinez; Pulkki, Ville","Autonomous Robot Twin System for Room Acoustic Measurements","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21033","While room acoustic measurements can accurately capture the sound field of real rooms, they are usually time consuming and tedious if many positions need to be measured. Therefore this contribution presents the Autonomous Robot Twin System for Room Acoustic Measurements (ARTSRAM) to autonomously capture large sets of room impulse responses with variable sound source and receiver positions. The proposed implementation of the system consists of two robots, one of which is equipped with a loudspeaker, while the other one is equipped with a microphone array. Each robot contains collision sensors, thus enabling it to move autonomously within the room. The robots move according to a random walk procedure to ensure a big variability between measured positions. A tracking system provides position data matching the respective measurements. After outlining the robot system, this paper presents a validation, in which anechoic responses of the robots are presented and the movement paths resulting from the random walk procedure are investigated. Additionally the quality of the obtained room impulse responses is demonstrated with a sound field visualization. In summary, the evaluation of the robot system indicates that large sets of diverse and high-quality room impulse responses can be captured with the system in an automated way. Such large sets of measurements will benefit research in the fields of room acoustics and acoustic virtual reality.","2021","2023-07-12 06:47:41","2023-07-19 04:01:58","","261–272","","4","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7CQ6PHB5","journalArticle","2002","Appel, Ronald; Beerends, John G.","On the Quality of Hearing One's Own Voice","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=11084","The way we perceive our own voice is being studied. Contrary to classical speech listening-quality experiments, where subjects judge the speech quality by listening, remaining silent themselves, in talking-quality experiments subjects judge the quality with which they perceive their own voices while actively speaking. In this way the self-listening comfort is measured. Six experiments were carried out. Five used a standard telephone setup where echo and distortions were introduced and judged by subjects on the disturbance. In one experiment subjects judged the talking quality of six different rooms. The subjective results were used to develop an objective perceptual talking-quality measure. The overall correlation between the subjectively perceived quality and the objectively measured quality was 0.97.","2002","2023-07-12 06:47:44","2023-07-19 03:37:02","","237–248","","4","50","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ET5VSLNX","journalArticle","2009","Kulesza, Maciej; Czyzewski, Andrzej","Tonality Estimation and Frequency Tracking of Modulated Tonal Components","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14815","The separation of tonelike and noiselike components within an audio signal is important for many signal processing applications, such as codecs employing a psychoacoustic criterion. Detection of tonal components allows for the creation of tonal tracks, which have specific masking properties. Conventional tonality estimation procedures are not well suited for modulated components that are typically found in music with vibrato and tremolo. In the proposed method, the analysis simultaneously uses a tonality metric based on both historic frames and spectral bins. The results for various metrics are compared.","2009","2023-07-12 06:47:48","2023-07-19 04:16:26","","221–236","","4","57","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RXI8CXXB","journalArticle","1980","Davis, Don; Davis, Chips","The LEDE- Concept for the Control of Acoustic and Psychoacoustic Parameters in Recording Control Rooms","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=3965","An approach to the standardized control room is found in the ""live end-dead end"" (LEDE-) approach. Desirable features include control of the initial time delay, psychoacoustic removal of the directional clues belonging to the control room, and control of the early reflected sound field's density, spacing in time, and acoustic level. This results in an exceptionally neutral acoustic environment and allows development of a sound field at a mixer's ears which correlates remarkably with the sound field appearing at the microphones in the studio, thereby allowing precision judgments to be made at the mixing console.","1980","2023-07-12 06:47:52","2023-07-19 03:51:45","","585–595","","9","28","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7AXLRUY9","journalArticle","2007","Majdak, Piotr; Balazs, Peter; Laback, Bernhard","Multiple Exponential Sweep Method for Fast Measurement of Head-Related Transfer Functions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14190","Presenting sounds in virtual environments requires filtering free-field signals with head-related transfer functions (HRTF). HRTFs describe the filtering effects of pinna, head, and torso measured in the ear canal of a subject. The measurement of HRTFs for many positions in space is a time-consuming procedure. To speed up the HRTF measurement, the multiple exponential sweep method (MESM) was developed. MESM speeds up the measurement by overlapping sweeps in an optimized way and retrieves the impulse responses of the measured systems. MESM and its parameter optimization are described. As an example of an application of MESM, the measurement duration of an HRTF set with 1550 positions is compared to the unoptimized method. Using MESM, the measurement duration could be reduced by a factor of four without a reduction of the signal-to-noise ratio.","2007","2023-07-12 06:47:56","2023-07-19 04:26:00","","623–637","","7/8","55","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HJYI56C8","journalArticle","2020","Lübeck, Tim; Helmholz, Hannes; Arend, Johannes M.; Pörschmann, Christoph; Ahrens, Jens","Perceptual Evaluation of Mitigation Approaches of Impairments due to Spatial Undersampling in Binaural Rendering of Spherical Microphone Array Data","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20859","Spherical microphone arrays (SMAs) are widely used to capture spatial sound fields that can then be rendered in various ways as a virtual acoustic environment (VAE) including headphone-based binaural synthesis. Several practical limitations have a significant impact on the fidelity of the rendered VAE. The finite number of microphones of SMAs leads to spatial undersampling of the captured sound field, which, on the one hand, induces spatial aliasing artifacts and, on the other hand, limits the order of the spherical harmonics (SH) representation. Several approaches have been presented in the literature that aim to mitigate the perceptual impairments due to these limitations. In this article, we present a listening experiment evaluat- ing the perceptual improvements of binaural rendering of undersampled SMA data that can be achieved using state-of-the-art mitigation approaches. We examined the Magnitude Least-Squares algorithm, the Bandwidth Extraction Algorithm for Mi- crophone Arrays, Spherical Head Filters, SH Tapering, and a newly proposed equalization filter. In the experiment, subjects rated the perceived differences between a dummy head and the corresponding SMA auralization. We found that most mitiga- tion approaches lead to significant perceptual improvements, even though audible differences to the reference remain.","2020","2023-07-12 06:47:59","2023-07-19 04:24:07","","428–440","","6","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TPX2LNS6","journalArticle","2006","Biswas, Arijit; den Brinker, Albertus C.","Perceptually Biased Linear Prediction","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13894","A perceptually biased linear prediction scheme is proposed for audio coding. Using only simple modifications of the coefficients defining the normal equations for a least-squares error, the spectral masking effects are mimicked in the prediction synthesis filter without using an explicit psychoacoustic model. The main advantage of such a scheme is reduced computational complexity. The proposed approach was implemented in a Laguerre-based linear prediction scheme, and its performance was evaluated in comparison with a Laguerrebased linear prediction approach controlled by the ISO MPEG-1 Layer I–II model, as well as with one of the latest spectral integration–based psychoacoustic models. Listening tests clearly demonstrate the viability of the proposed method.","2006","2023-07-12 06:48:09","2023-07-19 03:42:32","","1179–1188","","12","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KMGYXZUM","journalArticle","2017","Coleman, Philip; Franck, Andreas; Jackson, Philip J. B.; Hughes, Richard J.; Remaggi, Luca; Melchior, Frank","Object-Based Reverberation for Spatial Audio","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18544","To enable future audio systems to be more immersive, interactive, and easily accessible, object-based frameworks are currently being explored as a means to that ends. In object-based audio, a scene is composed of a number of objects, each comprising audio content and metadata. The metadata is interpreted by a renderer, which creates the audio to be sent to each loudspeaker with knowledge of the speci?c target reproduction system. While recent standardization activities provide recommendations for object formats, the method for capturing and reproducing reverberation is still open. This research presents a parametric approach for capturing, representing, editing, and rendering reverberation over a 3D spatial audio system. A Reverberant Spatial Audio Object (RSAO) allows for an object to synthesize the required reverberation. An example illustrates a RSAO framework with listening tests that show how the approach correctly retains the room size and source distance. An agnostic rendering can be used to alter listener envelopment. Editing the parameters can also be used to alter the perceived room size and source distance; greater envelopment can be achieved with the appropriate reproduction system.","2017","2023-07-12 06:48:12","2023-07-19 03:49:06","","66–77","","1/2","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8CIXGPB8","journalArticle","2007","Breebaart, Jeroen; Hotho, Gerard; Koppens, Jeroen; Schuijers, Erik; Oomen, Werner; Van De Par, Steven","Background, Concept, and Architecture for the Recent MPEG Surround Standard on Multichannel Audio Compression","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14162","An overview of the recently finalized ISO/MPEG standard for multichannel audio compression MPEG Surround is provided. This audio compression scheme enables backwardcompatible multichannel audio coding and transmission at unsurpassed coding efficiency. This is achieved by generating a mono, stereo, or matrixed-surround compatible down mix, which can be transmitted using any existing mono or stereo service, extended with a small amount of parametric side information that describes the perceptually relevant spatial properties of the original multichannel content. The concepts behind spatial parameterization are outlined, and the architecture of the MPEG Surround system is explained. Results of subjective evaluations are included to demonstrate its efficiency.","2007","2023-07-12 06:48:19","2023-07-19 03:44:51","","331–351","","5","55","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N68GNC7U","journalArticle","2015","Shirley, Ben; Oldfield, Rob","Clean Audio for TV broadcast: An Object-Based Approach for Hearing-Impaired Viewers","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17582","As the percentage of the population with hearing loss increases, broadcasters are receiving more complaints about the difficulty in understanding dialog in the presence of background sound and music. This article explores these issues, reviews previously proposed solutions, and presents an object-based approach that can be implemented within MPEG-H to give listeners control of their audio mix. An object-based approach to clean audio, combined with methods to isolate sounds that are important to the narrative and meaning of a broadcast has the potential to enable users to have complete control of the relative levels of all aspects of audio from TV broadcast. This approach was demonstrated at the University of Salford campus in 2013.","2015","2023-07-12 06:48:23","2023-07-19 04:47:57","","245–256","","4","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QQ83YEQT","journalArticle","2020","Moon, Soyoun; Park, Sunghwan; Park, Donggun; Yun, Myunghwan; Chang, Kyongjin; Park, Dongchul","Active Sound Design Development Based on the Harmonics of Main Order From Engine Sound","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20889","Most previous studies on active sound design (ASD) development proposed regression models based on psychoacoustic parameters for engine sound design. However, order-based parameters are required for a real ASD development, considering that an ASD system is controlled by order levels. In this paper, we propose a regression model utilizing order-level–based parameters that can be efficiently applied to ASD development. A jury test was conducted for 27 engine sound recordings using 36 participants with normal hearing ability to evaluate the level of affective adjectives. Then, acoustic parameters were measured from the engine sound recordings to identify the relationship between the adjectives and parameters. Finally, a regression model was derived through statistical analysis. The properties of the model were compared with those of models proposed in previous studies to verify its superiority. The proposed regression model can reduce the time and effort required for ASD development.","2020","2023-07-12 06:48:29","2023-07-19 04:33:21","","532–544","","7/8","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9QNENAEH","journalArticle","2012","Wankling, Matthew; Fazenda, Bruno; Davies, William J.","The Assessment of Low-Frequency Room Acoustic Parameters Using Descriptive Analysis","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16323","In small rooms, low-frequency modes have a degrading influence on the quality of the bass components of music. Using objective measures to correct these modes often fails because they do not correspond to the subjective experience of listeners. This research begins with a procedure that elicits a compact set of four verbal descriptors from subjects: articulation, resonance, strength, and depth. These are then mapped to three objective parameters: modal decay time, room volume, and source/receiver position. Results show the importance of reducing the decay time, which then provides an increase in articulation. Discussions suggest ways of extending the results.","2012","2023-07-12 06:48:36","2023-07-19 10:54:30","","325–337","","5","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ENMZMTSJ","journalArticle","2007","Møller, Henrik; Minnaar, Pauli; Olesen, S. Krarup; Christensen, Flemming; Plogsties, Jan","On the Audibility of All-Pass Phase in Electroacoustical Transfer Functions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14152","Audible effects of second-order all-pass sections with center frequencies in the range of 1-12 kHz were studied in headphone listening experiments. All-pass sections give rise to two effects. 1) A perception of “ringing” or “pitchiness,” which is related to an exponentially decaying sinusoid in the impulse response of all-pass sections with high Q factors. The ringing is especially audible for impulsive sounds, whereas it is often masked with everyday sounds such as speech and music. With an impulse signal the ringing was found to be audible when the decay time constant for the sinusoid exceeds approximately 0.8 ms (peak group delay of 1.6 ms), independent of the center frequency within the frequency range studied. 2) A lateral shift of the auditory image, which occurs when an all-pass section is inserted in the signal path to only one ear. The shift is related to the low-frequency phase and group delays of the all-pass section, and it was found to be audible whenever these exceed approximately 35 s, independent of the signal.","2007","2023-07-12 06:48:40","2023-07-19 04:32:40","","113–134","","3","55","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9L5B8HQX","journalArticle","2012","Ntalampiras, Stavros; Potamitis, Ilyas; Fakotakis, Nikos","Acoustic Detection of Human Activities in Natural Environments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16373","Automatic recognition of sound events can be valuable for efficient analysis of audio scenes. For example, detecting human activities like trespassing and hunting in natural environments can play an important role in their preservation by alerting authorities to take action. In the proposed system, each sound class is represented by a hidden Markov model created from descriptors in the time, frequency, and wavelet domains. The system has the ability to automatically adapt to acoustic conditions of different scenes via the feedback loop that refines an unsupervised model. A reliable testing process was adopted for assessing the performance of the system under adverse conditions characterized by highly nonstationary environmental noise.","2012","2023-07-12 06:48:44","2023-07-19 04:35:24","","686–695","","9","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FCNVS4RJ","journalArticle","2008","Zwan, Pawel; Kostek, Bozena","System for Automatic Singing Voice Recognition","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14634","A neural network was trained and tested to provide automated classification of singing voices, both recognizing voice quality (amateur, semiprofessional, and professional) and voice type (bass, baritone, tenor, alto, mezzo-soprano, and soprano). Parameters related to singing were defined to form feature vectors. Single vowel samples for each singer were judged by six experts to establish a quality index. In a test based on a database of 2690 samples, 90% of the decisions were correct. These results show that it is possible to use neural networks to create an expert system to evaluate singing.","2008","2023-07-12 06:48:48","2023-07-19 11:00:03","","710–723","","9","56","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AYDK5C89","journalArticle","2014","Hendrickx, Etienne; Paquier, Mathieu; Koehl, Vincent","The Influence of Stereoscopy on the Sound Mixing of Movies: A Study on the Front/Rear Balance of Ambience","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17548","There is a range of opinions in the cinema industry about the appropriate influence of stereoscopy on the sound mixing for movies. The present study focuses on the perception of ambience. Eight sequences—in their stereoscopic and nonstereoscopic versions, with several different sound mixes—were presented to 44 subjects. For each presentation, subjects had to judge to what extent the mix sounded frontal or “surround.” The goal was to verify whether stereoscopy had an influence on the perception of the front/rear balance of ambience. Results showed that this influence was weak, which was consistent with a preliminary experiment conducted in a mixing auditorium where subjects had to mix the front/rear balance of several sequences themselves.","2014","2023-07-12 06:48:51","2023-07-19 04:04:55","","723–735","","11","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TID3H57L","journalArticle","2001","Cox, Trevor J.; Li, Francis; Darlington, Paul","Extracting Room Reverberation Time from Speech Using Artificial Neural Networks","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=10197","A novel method to extract the reverberation time from reverberated speech utterances is presented. In this study, speech utterances are restricted to pronounced digits; uncontrolled discourse is not considered. The reverberation times considered are wide band, within the frequency range of speech utterances. A multilayer feed forward neural network is trained on speech examples with known reverberation times generated by a room simulator. The speech signals are preprocessed by calculating short-term rms values. A second decision-based neural network is added to improve the reliability of the predictions. In the retrieve phase, the trained neural networks extract room reverberation times from speech signals picked up in the rooms to an accuracy of 0.1 s. This provides an alternative to traditional measurement methods and facilitates the occupied measurement of room reverberation times.","2001","2023-07-12 06:48:56","2023-07-19 03:50:20","","219–230","","4","49","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LNTF7HDD","journalArticle","2010","Menzer, Fritz; Faller, Christof","Investigations on an Early-Reflection-Free Model for BRIRs","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15517","Simulating the late reverberation of a room using a synthetically generated reverberation tail is a common practice in the design of artificial reverberators. Binaural reverberators could benefit from better knowledge of the perceptual cues that are relevant for the reverberation tail. In this study the use of filtered white Gaussian noise instead of the original tail was subjectively evaluated. Matching the interaural coherence in each frequency band produced better results than full-band matching. In some cases time-dependent matching improved quality. Results are based on subjective studies.","2010","2023-07-12 06:49:03","2023-07-19 04:30:42","","709–723","","9","58","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PQ2FG57L","journalArticle","2016","Hill, Adam J.; Hawksford, Malcolm O. J.; Newell, Philip","Enhanced Wide-Area Low-Frequency Sound Reproduction in Cinemas: Effective and Practical Alternatives to Current Calibration Strategies","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18134","The current strategies for the low-frequency calibration of cinema sound systems are based on a flawed premise of low-frequency acoustics and psychoacoustics. This research shows that there is virtually no benefit in terms of spatiotemporal variance reduction: pre- and post-calibrated systems will exhibit equally position-dependent listening experience differences. For modern cinemas, the typical focus on room-modes when designing a low frequency calibration system is not necessary because the dimensions of the space coupled with low reverberation time results in Schroeder frequencies around 35 Hz. Above this value, effects of room-modes are not perceptible. Comb-filtering between sources and low-order reflections is the primary cause of high spatial variance. Furthermore, there is no evidence that spatial averaging techniques used for measurement and equalization are subjectively beneficial. A new approach needs to be invented.","2016","2023-07-12 06:49:07","2023-07-19 04:05:58","","280–298","","5","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BX2XSCXZ","journalArticle","1996","Møller, Henrik; Sørensen, Michael Friis; Jensen, Clemen Boje; Hammershøi, Dorte","Binaural Technique: Do We Need Individual Recordings?","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=7897","The localization performance was studied when subjects listened 1) to a real sound field and 2) to binaural recordings of the same sound field, made a) in their own ears and b) in the ears of other subjects. The binaural recordings were made at the blocked ear canal entrance, and the reproduction was carried out with individually equalized headphones. Eight subjects participated in the experiments, which took place in a standard listening room. Each stimulus (female speech) was emitted from one of 19 loudspeakers, and the subjects were to indicate the perceived sound source. When compared to real life, the localization performance was preserved with individual recordings. Nonindividual recordings resulted in an increased number of errors for the sound sources in the median plane, where movements were seen not only to nearby directions but also to directions further away, such as confusion between sound sources in front and behind. The number of distance errors increased only slightly with nonindividual recordings. Earlier suggestions that individuals might localize better with recordings fro other individual's found no support.","1996","2023-07-12 06:49:10","2023-07-19 04:32:48","","451–469","","6","44","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VYQW8WB2","journalArticle","1981","Lipshitz, Stanley P.; Vanderkooy, John","The Great Debate: Subjective Evaluation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=3899","A polarization of people has occurred regarding subjective evaluation, separating those who believe that audible differences are related to measurable differences in controlled tests, from those who believe that such differences have no direct relationship to measurements. Tests are necessary to resolve such differences of opinion, and to further the state of audio and open new areas of understanding. We argue that highly controlled tests are necessary to transform subjective evaluation to an objective plane so that preferences and bias can be eliminated, in the quest for determining the accuracy of an audio component. In order for subjective tests to be meaningful to others, the following should be observed. (1) There must be technical competence to prevent obvious and/or subtle effects from affecting the test. (2) Linear differences must be thoroughly excised before conclusions about nonlinear errors can be reached. (3) The subjective judgment required in the test must be simple, such as the ability to discriminate between two components, using an absolute reference wherever possible. (4) The test must be blind or preferably double-blind. To implement such tests we advocate the use of A/B switchboxes. The box itself can be tested for audibly intrusive effects, and several embellishments are described which allow double-blind procedures to be used in listening tests. We believe that the burden of proof must lie with those who make new hypotheses regarding subjective tests. This alone would wipe out most criticisms of the controlled tests reported in the literature. Speculation is changed to fact only by careful experimentation. Recent references are given which support out point of view. The significance of differences in audio components is discussed, and in conclusion we detail some of our tests, hypotheses and speculations.","1981","2023-07-12 06:49:18","2023-07-19 04:20:59","","482–491","","7/8","29","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TCIZRELB","journalArticle","2016","Galvez, Marcos F. Simón; Menzies, Dylan; Mason, Rusell; Fazi, Filippo Maria","Object-Based Audio Reproduction using a Listener-Position Adaptive Stereo System","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18516","Adapting a stereo reproduction system to the listener’s position allows for reproduction of 2D object-based audio with better localization accuracy when the listener is located outside of the sweet spot. The adaptation is composed of two parts: a compensation system that updates the loudspeakers feeds so that these are delivered to the listener with an intended magnitude and phase independently of the listening position, and an object-based rendering system using conventional panning algorithms. Initial localization simulations using the velocity and energy localization vectors predicted that the frequency-dependent panning can pan virtual audio sources outside of the loudspeaker region at low frequency. A perturbation analysis showed that, in practice, localization of audio objects is robust when these are panned between the stereo region, and that the localization of objects outside of the stereo region is both sensitive to errors and affected by the accuracy of the video tracking system and the homogeneity of the radiation pattern of the different loudspeakers of the system. These same results were corroborated with subjective tests.","2016","2023-07-12 06:49:22","2023-07-19 04:01:10","","740–751","","10","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WPIV8VZV","journalArticle","2019","Heilemann, Michael C.; Anderson, David A.; Bocko, Mark F.","Near-Field Object-Based Audio Rendering on Flat-Panel Displays","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20491","Devices such as smartphones and televisions are beginning to employ screens as both a video display and a loudspeaker. This multimodal device is well suited for object-based encoding of audio, where audio objects may be rendered at the location corresponding to the visual images. The audio object renderer must be configured to account for variations in panel behavior at different excitation frequencies. This research proposes a multiband crossover network for the audio object renderer that separates the signal for each audio object into low, midrange, and high-frequency bands. Each band is then reproduced on the panel using a different vibration rendering technique. The different rendering techniques are realized by employing a combination of actuator array processing and the natural vibration localization characteristics of point-driven panels. The cutoff frequencies for each band are determined by the physical properties of the panel. Experiments on a prototype panel employing the multiband crossover system demonstrate that the vibration response behaves as predicted in each frequency range. This system provides a platform for rendering spatial audio on devices when listeners are close to the screen, and where there are restrictions related to weight, power consumption, and form-factor.","2019","2023-07-12 06:49:29","2023-07-19 04:04:39","","531–539","","7/8","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WJA3YQJH","journalArticle","2011","Laitinen, Mikko-Ville; Kuech, Fabrian; Disch, Sascha; Pulkki, Ville","Reproducing Applause-Type Signals with Directional Audio Coding","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15774","Reproduction of surrounding applause-type signals is known to produce audible artifacts with parametric spatial audio coders. This arises from inadequate temporal resolution. Better quality results if the temporal resolution is high enough to analyze each handclap separately, which can be obtained by using different temporal windows for different frequencies. Formal listening tests confirm the improved subjective quality.","2011","2023-07-12 06:49:33","2023-07-19 04:17:11","","29–43","","1/2","59","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E9ATYGAY","journalArticle","2002","Beerends, John G.; Hekstra, Andries P.; Rix, Antony W.; Hollier, Michael P.","Perceptual Evaluation of Speech Quality (PESQ) The New ITU Standard for End-to-End Speech Quality Assessment Part II: Psychoacoustic Model","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=11062","A new model for the perceptual evaluation of speech quality (PESQ) was recently standardized by the International Telecommunications Union as Recommendation P.862. Unlike previous codec assessment models, such as PSQM and MNB (ITU-T P.861), PESQ is able to predict subjective quality with good correlation in a very wide range of conditions, which may include coding distortions, errors, noise, filtering, delay, and variable delay. The psychoacoustic model used in PESQ is introduced. Part I describes the time-delay identification technique that is used in combination with the PESQ psychoacoustic model to predict the end-to-end perceived speech quality.","2002","2023-07-12 06:49:37","2023-07-19 03:40:05","","765–778","","10","50","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5R5UJQU2","journalArticle","2020","Blanco Galindo, Miguel; Coleman, Philip; Jackson, Philip J. B.","Microphone Array Geometries for Horizontal Spatial Audio Object Capture With Beamforming","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20851","Microphone array beamforming can be used to enhance and separate sound sources, with applications in the capture of object-based audio. Many beamforming methods have been proposed and assessed against each other. However, the effects of compact microphone array design on beamforming performance have not been studied for this kind of application. This study investigates how to maximize the quality of audio objects extracted from a horizontal sound field by filter-and-sum beamforming, through appropriate choice of microphone array design. Eight uniform geometrieswith practical constraints of a limited number of microphones and maximum array size are evaluated over a range of physical metrics. Results show that baffled circular arrays outperform the other geometries in terms of perceptually relevant frequency range, spatial resolution, directivity, and robustness. Moreover, a subjective evaluation of microphone arrays and beamformers is conducted with regards to the quality of the target sound, interference suppression, and overall quality of simulated music performance re- cordings. Baffled circular arrays achieve higher target quality and interference suppression than alternative geometries with wideband signals. Furthermore, subjective scores of beamformers regarding target quality and interference suppression agree well with beamformer on-axis and off-axis responses; with wideband signals, the superdirective beamformer achieves the highest overall quality.","2020","2023-07-12 06:49:41","2023-07-19 03:42:57","","324–337","","5","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A6XNL8EU","journalArticle","2021","Werner, Kurt James; Germain, Francois G.; Goldsmith, Cory S.","Energy-Preserving Time-Varying Schroeder Allpass Filters and Multichannel Extensions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21114","We propose time-varying Schroeder allpass filters and Gerzon allpass reverberators that remain energy preserving irrespective of arbitrary variation of their allpass gains or feedback matrices over time. We propose various ways of realizing the unitary matrix involved in the Schroeder structure, based on classic ladder and lattice filters and their generalizations. We show how to construct more elaborate structures including nestings and cascade, giving various strategies for reducing their implementation cost. Extending these algorithms to the multi-input, multi-output case yields time-varying, energy-preserving generalizations of Gerzon’s reverberator, providing a link between Schroeder allpass filters and Schelcht’s recently proposed “Allpass Feedback Delay Networks.” Stability proofs are given for common uses of Schroeder allpass filters, such as inside of Feedback Delay Network reference structures. Finally we give a substantial review of the properties of time-invariant Schroeder allpass filters.","2021","2023-07-12 06:49:45","2023-07-19 10:55:17","","465–485","","7/8","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"35NQCNPN","journalArticle","2013","Tassart, Stéphan","Graphical Equalization Using Interpolated Filter Banks","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16823","While there are many topologies for implementing a graphical equalizer, this research explores a design and implementation that is based on time-varying infinite impulse response (IIR) filters. Filter blocks are obtained by interpolating between two time-invariant biquadratic filters having the same center frequency but possibly different gains and bandwidths. This corresponds to crossfading, which avoids transients during transitions. This design is suitable for digital implementation because the variable parameters are excluded from the feedback loops thus avoiding problems of stability during transitions. Moreover, the gain and the bandwidth of the band filters are jointly optimized to minimize interband interferences. Group delays are smaller than multirate or FIR implementations.","2013","2023-07-12 06:50:04","2023-07-19 04:51:59","","263–279","","5","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W79A8I2K","journalArticle","2019","Menzies, Dylan; Fazi, Filippo Maria","Multichannel Compensated Amplitude Panning, An Adaptive Object-Based Reproduction Method","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20493","Conventional approaches for surround sound panning require loudspeakers to be distributed over the regions where images are required. However in many listening situations it is not practical or desirable to place loudspeakers at some positions, such as behind or above the listener. Compensated Amplitude Panning (CAP) is an object-based reproduction method that adapts dynamically to the listener’s head orientation to provide stable images in any direction in the frequency range up to approximately 1000 Hz. This is achieved by accurately controlling the Interaural Time Difference cue. CAP can also provide images in the near-field range, by controlling the Interaural Level Difference. Using two loudspeakers and with full 6-degrees-of-freedom head tracking, it was previously shown possible to create low band images in any direction, although excessive gain is required for some listener orientations. But with 3 loudspeakers all images directions can be reproduced with moderate gain. Adding more loudspeakers to a stereo configuration does not worsen performance. For comparison, an Ambisonic approach with position tracking and 3 frontal loudspeakers can reproduce horizontal surround images, and 4 loudspeakers can reproduce full 3D.","2019","2023-07-12 06:50:09","2023-07-19 04:31:00","","549–556","","7/8","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IPYBXRAZ","journalArticle","2014","Wendt, Torben; van de Par, Steven; Ewert, Stephan D.","A Computationally-Efficient and Perceptually-Plausible Algorithm for Binaural Room Impulse Response Simulation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17550","Simulating the reverberation of rooms has numerous applications that range from evaluating acoustic scenarios in the development of signal processing algorithms to the exploration of speech intelligibility in virtual rooms with movable sources. A hybrid approach was created to simulate room acoustics, achieving high computational efficiency and perceptual plausibility: early reflections were calculated using the image source model (ISM) and the reverberant tail used a feedback delay network (FDN). The FDN approach was modified to be adaptable to various room dimensions and wall absorption coefficients. Using head-related impulse responses, the authors extended it to create spatially-distributed reverberation for arbitrary source and receiver positions. Subjective ratings of the perceived room attributes and the assessment of various common parameters showed a good correspondence between simulated and real rooms.","2014","2023-07-12 06:50:12","2023-07-19 10:55:07","","748–766","","11","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"44I27VFJ","journalArticle","2018","Vuegen, Lode; Karsmakers, Peter; Vanrumste, Bart; Hamme, Hugo Van","Acoustic Event Classification using Low-Resolution Multi-label Non-negative Matrix Deconvolution","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19567","With the increased proliferation of interconnected devices that have built-in microphones, acoustic event classification and monitoring becomes possible in a wide variety of applications, such as surveillance, healthcare, military, machine diagnostics, and wildlife tracking. The promise and success of these applications depends on robust sensing of acoustic events in the environment. Typically, sound event classes are defined by annotating training data, which is a laborious process. This work introduces an extended version of non-negative matrix deconvolution (NMD), called low-resolution multi-label non-negative matrix deconvolution (LRM-NMD), where both the observation data and the available labeling information are used during training. The proposed extension of NMD was successfully applied to the classification of acoustic events even in noisy conditions with overlapping events. Low-resolution, multi-labeling information simply indicates that the sound classes of the events take place over a longer period of time in the acoustic data without identifying beginning or endings of the individual events.","2018","2023-07-12 06:50:16","2023-07-19 10:54:04","","369–384","","5","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8785DMZU","journalArticle","2013","Beerends, John G.; Schmidmer, Christian; Berger, Jens; Obermann, Matthias; Ullmann, Raphael; Pomy, Joachim; Keyhl, Michael","Perceptual Objective Listening Quality Assessment (POLQA), The Third Generation ITU-T Standard for End-to-End Speech Quality Measurement Part I—Temporal Alignment","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16829","In this and the companion paper Part II, the authors present the Perceptual Objective Listening Quality Assessment (POLQA), the third-generation speech quality measurement algorithm, standardized by the International Telecommunication Union in 2011 as Recommendation P.863. In contrast to the previous standard (P.862 Perceptual Evaluation of Speech Quality), a more complex temporal alignment was developed allowing for the alignment of a wide variety of complex distortions for which P.862 was known to fail, such as multiple delay variations within utterances as well as temporal stretching and compression of the degraded signal. When this new algorithm is used in combination with the advanced perceptual model described in Part II, it provides a new measurement standard for predicting Mean Opinion Scores that outperforms the older PESQ standard, especially for wideband and super wideband speech signals (7 and 14 kHz audio bandwidth). Part I provides the basics of the POLQA approach and outlines the core elements of the temporal alignment.","2013","2023-07-12 06:50:20","2023-07-19 03:40:15","","366–384","","6","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U82CPUTR","journalArticle","2017","Jukic, Ante; van Waterschoot, Toon; Gerkmann, Timo; Doclo, Simon","A General Framework for Incorporating Time–Frequency Domain Sparsity in Multichannel Speech Dereverberation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18540","Effective speech dereverberation is a prerequisite in such applications as hands-free telephony, voice-based human-machine interfaces, and hearing aids. Blind multichannel speech dereverberation methods based on multichannel linear prediction (MCLP) can estimate the dereverberated speech component without any knowledge of the room acoustics. This can be achieved by estimating and subtracting the undesired reverberant component from the reference microphone signal. This report presents a general framework that exploits sparsity in the time–frequency domain of a MCLP-based speech dereverberation. The framework combines a wideband or a narrowband signal model with either an analysis or a synthesis sparsity prior, and generalizes state-of-the-art MCLP-based speech dereverberation methods.","2017","2023-07-12 06:50:30","2023-07-19 04:09:09","","17–30","","1/2","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L5YCDTK9","journalArticle","2023","Marchan, Mick; Allen, Andrew","Multi-Layered Architecture for Efficient and Accurate HRTF Rendering","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22141","AR/VR applications commonly face difficulties binaurally spatializing many sound sources at once because of computational constraints. Existing techniques for efficient binaural rendering, such as Ambisonics, Vector-Based Amplitude Panning, or Principal Component Analysis, alleviate this issue by approximating Head-Related Transfer Function (HRTF) datasets with a linear combination of basis filters. This paper proposes a novel binaural renderer that convolves each basis filter with a layer of low-order finite impulse response filters applied in time-domain and derives both the spatial functions and filter coefficients through the minimization of a perceptually motivated error function. In a MUSHRA test, expert listeners had more difficulty differentiating the proposed method from the HRTF dataset it approximates than it did with existing methods configured with an equivalent number of Fast Fourier Transforms and identical HRTF preprocessing. This was consistent across both an internal Microsoft HRTF dataset and an individual head from the SADIE database.","2023","2023-07-12 06:50:35","2023-07-19 04:27:00","","338–348","","6","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PBRCWAFZ","journalArticle","2022","Hold, Christoph; Mckenzie, Thomas; Götz, Georg; Schlecht, Sebastian J.; Pulkki, Ville","Resynthesis of Spatial Room Impulse Response Tails With Anisotropic Multi-Slope Decays","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21800","Spatial room impulse responses (SRIRs) capture room acoustics with directional information. SRIRs measured in coupled rooms and spaces with non-uniform absorption distribution may exhibit anisotropic reverberation decays and multiple decay slopes. However, noisy measurements with low signal-to-noise ratios pose issues in analysis and reproduction in practice. This paper presents a method for resynthesis of the late decay of anisotropic SRIRs, effectively removing noise from SRIR measurements. The method accounts for both multi-slope decays and directional reverberation. A spherical filter bank extracts directionally constrained signals from Ambisonic input, which are then analyzed and parameterized in terms of multiple exponential decays and a noise floor. The noisy late reverberation is then resynthesized from the estimated parameters using modal synthesis, and the restored SRIR is reconstructed as Ambisonic signals. The method is evaluated both numerically and perceptually, which shows that SRIRs can be denoised with minimal error as long as parts of the decay slope are above the noise level, with signal-to-noise ratios as low as 40 dB in the presented experiment. The method can be used to increase the perceived spatial audio quality of noise-impaired SRIRs.","2022","2023-07-12 06:50:50","2023-07-19 04:06:27","","526–538","","6","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XMCDC9NZ","journalArticle","2001","Minnaar, Pauli; Olesen, S. Krarup; Christensen, Flemming; Møller, Henrik","Localization with Binaural Recordings from Artificial and Human Heads","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=10193","Previous experiments have shown that localization with binaural recordings made with artificial heads is inferior to localization in real life and also to localization with recordings made in the ears of selected humans. These results suggest that artificial heads may be improved. A new experiment was made, employing recordings from two human heads and seven artificial heads some of which had been developed recently. The listening room setup from previous experiments was used and 20 listeners participated. As in the earlier experiments, more directional errors were seen with binaural recordings than in real life. A clear learning effect was seen over five days, emphasizing the need of a balanced experimental design. The new results show that artificial heads are still not as good for recording as a well-selected human head, although some of the new heads come close. The accumulated results from the present and four earlier studies provide sufficient statistics to conclude that there are significant differences between some currently available artificial heads.","2001","2023-07-12 06:50:58","2023-07-19 04:31:43","","323–336","","5","49","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3Q8792HB","journalArticle","2021","Tan, Yiyu; Imamura, Toshiyuki; Kondo, Masaaki","FPGA-Based Acceleration of FDTD Sound Field Rendering","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21121","Sound field rendering is computation-intensive and memory-intensive. This research investigates an FPGA-based accelerator for sound field rendering with an FDTD scheme, in which wave equations are directly implemented by reconfigurable hardware, and spatial blocking is applied to alleviate the memory bandwidth requirement. Compared to software simulation performed on a desktop machine with 128 GB DDR4 RAMs and an Intel i7-7820X processor running at 3.6 GHz, the proposed FPGA-based accelerator achieves up to 2.98 times more in computing performance in the case of different layer sizes and different numbers of nodes computed in parallel even though the FPGA system runs at about 267 MHz.","2021","2023-07-12 06:51:05","2023-07-19 04:51:42","","542–556","","7/8","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"URXKFLYF","journalArticle","2021","Heilemann, Michael C.; Anderson, David A.; Roessner, Stephen; Bocko, Mark F.","The Evolution and Design of Flat-Panel Loudspeakers for Audio Reproduction","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21014","The underlying physics and the design of loudspeakers that radiate sound through the bending vibrations of elastic panels, here referred to generically as flat-panel loudspeakers, are reviewed in this paper. The form factor, reduced weight, and aesthetic appeal of flat-panel speakers have made them a topic of interest for more than 90 years, but these advantages have been overshadowed by acoustical shortcomings, specifically the uneven frequency response and directivity in comparison to conventional cone-radiator loudspeakers. Fundamentally, the design challenges of flat-panel speakers arise from the intrinsically large number of mechanical degrees of freedom of a panel radiator. A number of methods have been explored to compensate for the acoustical shortcomings of flat-panel speakers, such as employing inverse filters, equalization, canceling mechanical resonances with actuator arrays, and modifying the panel material, shape, structure, and boundary conditions. Such methods have been used in various combinations to achieve significant audio performance improvements, and carefully designed flat-panel loudspeakers have been rated in blind listening tests as competitive with some prosumer-grade conventional loudspeakers. This review presents a brief historical account of the evolution of flat-panel loudspeakers and summarizes the essential physics and design methodologies that have been developed to optimize their fidelity and directional response.","2021","2023-07-12 06:51:09","2023-07-19 04:04:47","","27–39","","1/2","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"96XL5BG6","journalArticle","2012","Peres, S. Camille","A Comparison of Sound Dimensions for Auditory Graphs: Pitch Is Not So Perfect","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16367","Visually-impaired individuals and sighted individuals who are conducting tasks in divided-attention situations, both benefit from using sound to display information typically communicated visually. Auditory displays of statistical graphs are one such tool and can be effective in these situations. However, it is not obvious how these graphs should be designed. In a series of experiments in which information was conveyed by sound, subjects were divided into two groups: those hearing graphs using integral (pitch and loudness) and separable (pitch and timing) dimensions of sound. The results showed that pitch alone produced the worst performance and timing the best. However, designs using pitch and loudness redundantly provided good results as well.","2012","2023-07-12 07:04:59","2023-07-19 04:37:44","","561–567","","7/8","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VUR44Y8A","journalArticle","2012","Grond, Florian; Kramer, Oliver; Hermann, Thomas","Balancing Salience and Unobtrusiveness in Auditory Monitoring of Evolutionary Optimization","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16364","Sonification, which is the conversion of physical data (temperature, network traffic, or an electrocardiogram) into sound, is very suitable for augmenting task monitoring. The challenge to the sound designer is to create a monitoring display that maximizes the conveyed information while at the same time remaining unobtrusive. In this article an auditory augmentation approach was developed for monitoring evolutionary optimization algorithms. In order to augment short interaction sounds effectively with the information about the high-dimensional search space, a sonification method known as data sonograms was applied. Various mappings of data to sound features have been investigated, where the delay of the signal has been shown to be an interesting parameter that plays a role in both unobtrusiveness and information conveyed.","2012","2023-07-12 07:05:02","2023-07-19 04:02:51","","531–539","","7/8","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L7Q6MY8L","journalArticle","2012","Roginska, Agnieszka","Effect of Spatial Location and Presentation Rate on the Reaction to Auditory Displays","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16361","The way in which auditory displays are presented can significantly influence the experience of listeners. Spatio-temporal factors influence and redirect the attention toward such displays. These factors include presentation speed, stimulus externalization, and location in the three dimensional space. Results show that stimuli perceived inside the head resulted in a more accurate and faster response than those that were externalized. For externalized auditory displays, those presented in the frontal hemisphere were attended to faster. Response times did not change linearly with presentation speed; there was an optimal presentation rate at which the response time is fastest without compromising accuracy.","2012","2023-07-12 07:05:05","2023-07-19 04:41:19","","497–504","","7/8","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9REG4A3X","journalArticle","1995","Crispien, Kai; Ehrenberg, Tasso","Evaluation of the -Cocktail-Party Effect- for Multiple Speech Stimuli within a Spatial Auditory Display","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=7922","In order to provide access to graphics-based user interfaces for blind computer users, a spatial auditory computer display is under development which conveys the visual information embedded in the user interface in an auditory form. A spatial representation of the auditory display, based on a binaural real-time processing syst;em, is used to provide orientational information and to permit the use of multiple simultaneous verbal and nonverbal audio information. A psychoacoustic experiment was carried out with 31 participating subjects in order to investigate the ability to discriminate multiple simultaneous speech stimuli within a spatial auditory display. Localization performance and speech reception were investigated.","1995","2023-07-12 07:05:11","2023-07-19 03:50:37","","932–941","","11","43","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E2T2NBML","journalArticle","2012","Jeon, Myounghoon; Gupta, Siddharth; Davison, Benjamin K.; Walker, Bruce N.","Auditory Menus Are Not Just Spoken Visual Menus: A Case Study of “Unavailable” Menu Items","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16362","When a visual menu, such as in a computer, needs to be presented with sound, there is a question about how to best communicate an item that is unavailable. This question was explored in three studies that showed that using a whispered voice for unavailable items was favored over an attenuated voice, saying “unavailable,” or skipping such items. In general, participants preferred a female voice over a male voice. Results are discussed in terms of acoustic theory, cognitive menu selection theory, and user interface accessibility. The authors asserted that designers should go beyond a naïve translation from text into speech when creating auditory systems, thereby creating subjective satisfaction.","2012","2023-07-12 07:05:14","2023-07-19 04:08:40","","505–518","","7/8","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TXR3HFCY","journalArticle","2012","Asutay, Erkin; Västfjäll, Daniel; Tajadura-Jiménez, Ana; Genell, Anders; Bergman, Penny; Kleiner, Mendel","Emoacoustics: A Study of the Psychoacoustical and Psychological Dimensions of Emotional Sound Design","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16162","Even though traditional psychoacoustics has provided indispensable knowledge about auditory perception, it has, in its narrow focus on signal characteristics, neglected listener and contextual characteristics. To demonstrate the influence of the meaning the listener attaches to a sound in the resulting sensations we used a Fourier-time-transform processing to reduce the identifiability of 18 environmental sounds. In a listening experiment, 20 subjects listened to and rated their sensations in response to, first, all the processed stimuli and then, all original stimuli, without being aware of the relationship between the two groups. Another 20 subjects rated only the processed stimuli, which were primed by their original counterparts. This manipulation was used in order to see the difference in resulting sensation when the subject could tell what the sound source is. In both tests subjects rated their emotional experience for each stimulus on the orthogonal dimensions of valence and arousal, as well as perceived annoyance and perceived loudness for each stimulus. They were also asked to identify the sound source. It was found that processing caused correct identification to reduce substantially, while priming recovered most of the identification. While original stimuli induced a wide range of emotional experience, reactions to processed stimuli were emotionally neutral. Priming manipulation reversed the effects of processing to some extent. Moreover, even though the 5th percentile Zwickers-loudness (N5) value of most of the stimuli was reduced after processing, neither perceived loudness nor auditory-induced emotion changed accordingly. Thus indicating the importance of considering other factors apart from the physical sound characteristics in sound design.","2012","2023-07-12 07:05:23","2023-07-19 03:37:20","","21–28","","1/2","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JIXTJ5Y8","journalArticle","2014","Sanz, Pablo Revuelta; Mezcua, Belén Ruiz; Pena, José M. Sánchez; Walker, Bruce N.","Scenes and Images into Sounds: A Taxonomy of Image Sonification Methods for Mobility Applications","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17130","Sonification is the systematic representation of data using sounds, such as text-to-speech, color readers, Geiger counters, acoustic radars, and MIDI synthesizers. This paper surveys existing sonification systems and suggests taxonomy of algorithms and devices. The sonification process requires an artificial mapping between two sensory modalities using a model based on either psychoacoustics or artificial heuristics. In the former, the paradigm exploits the natural discrimination of the source spatial parameters (distance, azimuth, and elevation, for instance). In the latter, the paradigm creates an artificial match between graphical and auditory cues. Artificial sonification uses nonspatial characteristics of the sound, such as frequency, brightness or timbre, formants, saturation, and time intervals, which are not related to the physical characteristics or parameters of objects or surroundings.","2014","2023-07-12 07:05:39","2023-07-19 04:46:32","","161–171","","3","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7GYAPSE7","journalArticle","2009","Väljamäe, Aleksander; Larsson, Pontus; Västfjäll, Daniel; Kleiner, Mendel","Auditory Landmarks Enhance Circular Vection in Multimodal Virtual Reality","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14809","The means by which an individual distinguishes between (a) self-movement relative to a fixed external object and (b) a fixed sense of self relative to a moving object involves both sensory input and cognitive processes. The current study examines the cognitive influences of an auditory presentation on the illusion of motion. The illusion of self-motion was strongest when simulating multiple auditory objects of the type that are expected to be immobile: acoustic landmarks. The effect is strongest without visual cues, which can dominate if present. The addition of vibrotactile stimulation of the whole body was only selectively contributing to the experience of being in motion depending on the simulated auditory objects.","2009","2023-07-12 07:05:54","2023-07-19 04:54:55","","111–120","","3","57","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V8X6TWTC","journalArticle","2005","Ferguson, Sam; Cabrera, Densil","Vertical Localization of Sound from Multiway Loudspeakers","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13406","Practical wide-range loudspeakers are usually implemented with multiple drivers, but the systematic effect of the signal frequency upon the vertical localization of sound is scarcely used for loudspeaker enclosure design. Tendencies in vertical localization for the frequency bands characteristic of woofers and tweeters in loudspeakers are shown. Using vertical arrays of individually controlled loudspeakers, synchronous and asynchronous bands of noise were presented to subjects. The frequency of the source affected the vertical position of the lowand high-frequency auditory image pairs significantly and systematically, in a manner broadly consistent with previous studies concerned with single auditory images. Lower frequency sources are localized below their physical positions whereas high-frequency sources are localized at their true positions. This effect is also shown to occur for musical signals. It is demonstrated that low-frequency sources are not localized well when presented in exact synchrony with high-frequency sources, or when they only include energy below 500 Hz.","2005","2023-07-12 07:06:05","2023-07-19 03:57:29","","163–173","","3","53","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U4N5RWVA","journalArticle","2016","Bulla, Wesley","Detection of High-Frequency Harmonics in a Brief Complex Tone","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18100","Prior investigations have generally failed to confirm or deny the perceptual influence of high-frequency harmonics contained in musical sounds. Because harmonics that are beyond the frequency range of auditory detection influence the resulting waveform, they may alter the perception of a sound’s tonal character. This study found no evidence that capable listeners noticed an effect of high-frequency harmonics within a brief complex tone. With regard to the influence of high-frequency harmonic content on timbre perception, subjects were capable of performing the assigned task, the presentation technology was adequate for delivering reliable stimuli, the stimuli were appropriate for the interests of the study, and yet, there were no indications that the presence of high-frequency harmonics influenced listeners’ perception of the timbre of a complex waveform.","2016","2023-07-12 07:06:09","2023-07-19 03:45:44","","4–12","","1/2","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"42NJRG9W","journalArticle","2015","Kim, Sungyoung; Okumura, Hiraku; Otani, Makoto","Near-Field Sound Control Using a Planar Loudspeaker","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17566","Even with the more advanced sound rendering methods, creating a convincing near-field image has remained a challenge, especially when sound is integrated with high resolution video. In order to render a near-field sound image in a relatively simple yet effective way, the authors proposed a new method using an overhead planar loudspeaker. Subjective evaluation showed that planar waves radiating from overhead position generated a sound image very near to the listener when coupled with an additional filter that removes spectral cues associated with an overhead sound source. The results also showed that the proposed method could continuously control the distance of sound image between the screen and the listener position. The planar loudspeaker generated smaller variance of group delay at the listener's ears than conventional loudspeakers.","2015","2023-07-12 07:06:12","2023-07-19 04:12:01","","54–62","","1/2","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K29GQU82","journalArticle","2014","Evreinova, Tatiana V.; Evreinov, Grigori; Raisamo, Roope","An Exploration of Volumetric Data in Auditory Space","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17131","Sonification of 3-dimensional shapes is challenging because there are fundamental differences between the human auditory and visual systems. Subjects were asked to explore the audio representations of top-projected shapes having different levels of visual complexity using several different strategies. After a 50-minute experience with each sonification technique, all subjects agreed that auditory patterns derived from cross-sectional profiles of virtual objects were robust and extremely easy for mental manipulation, enabling them to mentally rebuild virtual shapes. At the beginning of the test, subjects could not imagine that it would be possible to get a sense of a virtual shape by relying exclusively on ordinary MIDI sounds. Two other techniques were not successful.","2014","2023-07-12 07:06:20","2023-07-19 03:55:11","","172–187","","3","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MBV92KBC","journalArticle","2001","Begault, Durand R.; Wenzel, Elizabeth M.; Anderson, Mark R.","Direct Comparison of the Impact of Head Tracking, Reverberation, and Individualized Head-Related Transfer Functions on the Spatial Perception of a Virtual Speech Source","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=10175","A study of sound localization performance was conducted using headphone-delivered virtual speech stimuli, rendered via HRTF-based acoustic auralization software and hardware, and blocked-meatus HRTF measurements. The independent variables were chosen to evaluate commonly held assumptions in the literature regarding improved localization: inclusion of head tracking, individualized HRTFs, and early and diffuse reflections. Significant effects were found for azimuth and elevation error, reversal rates, and externalization.","2001","2023-07-12 07:06:27","2023-07-19 03:41:13","","904–916","","10","49","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z79DRCGJ","journalArticle","2022","Yang, Jing; Barde, Amit; Billinghurst, Mark","Audio Augmented Reality: A Systematic Review of Technologies, Applications, and Future Research Directions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22008","Audio Augmented Reality (AAR) aims to augment people's auditory perception of the real world by synthesizing virtual spatialized sounds. AAR has begun to attract more research interest in recent years, especially because Augmented Reality (AR) applications are becoming more commonly available on mobile and wearable devices. However, because audio augmentation is relatively under-studied in the wider AR community, AAR needs to be further investigated in order to be widely used in different applications. This paper systematically reports on the technologies used in past studies to realize AAR and provide an overview of AAR applications. A total of 563 publications indexed on Scopus and Google Scholar were reviewed, and from these, 117 of the most impactful papers were identified and summarized in more detail. As one of the first systematic reviews of AAR, this paper presents an overall landscape of AAR, discusses the development trends in techniques and applications, and indicates challenges and opportunities for future research. For researchers and practitioners in related fields, this review aims to provide inspirations and guidance for conducting AAR research in the future.","2022","2023-07-12 07:06:30","2023-07-19 10:58:33","","788–809","","10","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7FQ3C2QM","journalArticle","2004","Tan, Chin-Tuan; Moore, Brian C. J.; Zacharov, Nick; Mattila, Ville-Veikko","Predicting the Perceived Quality of Nonlinearly Distorted Music and Speech Signals","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13013","In a previous study perceptual experiments were reported in which subjects had to rate the perceived quality of speech and music that had been subjected to various forms of nonlinear distortion. The subjective ratings were compared to a physical measure of distortion, DS, based on the output spectrum of each nonlinear system in response to a 10-component multitone test signal with logarithmically spaced components. The values of DS were highly negatively correlated with the subjective ratings for stimuli that had been subjected to ""artificial"" distortions such as peak clipping and zero clipping. However, for stimuli that had been subjected to nonlinear distortion produced by real transducers, the correlation between the DS values and the subjective ratings was only moderately negative. A new method predicts the perceived quality of nonlinearly distorted signals based on the outputs of an array of gammatone filters in response to the original signal and the distorted signal. For each filter, the cross correlation is calculated between the outputs in response to the original and the distorted signals for a series of brief samples (frames). The maximum value of the cross correlation for each filter for each frame is determined, and the maximum values are summed across filters, with a weighting that depends on the magnitude of the output of each filter in response to the distorted signal. The resultant weighted cross correlation gives a perceptually relevant measure of distortion called Rnonlin, which can be used to predict subjective ratings. There were high correlations between the predicted ratings and the subjective ratings obtained previously. The correlations were greater than obtained using the DS measure. A new perceptual experiment, using a mixture of artificial and real distortions, confirmed the validity of the new measure.","2004","2023-07-12 07:06:43","2023-07-19 04:51:24","","699–711","","7/8","52","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ANDJ5X66","journalArticle","2021","Tamer, Yahya Burak","Spectral and Dynamic Analyses of Popular Music Playlists: The Concept of Presence Optimization for Digital Music Streaming Playback","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21036","Digital streaming has become the most popular way to experience music today. Loudness normalization applied by streaming services allows for the restoration of dynamic contrast, yet the hypercompressed production trends sustain. This paper studies current dynamic and spectral tendencies through integrated loudness and long-term average spectrum (LTAS) analyses of popular music playlists offered by digital streaming services. Deviations from the large-scale spectral characterizations provided by earlier studies on popular music LTAS were investigated and an increase in the slope of the presence band was observed. The paper concludes with recommendations for the optimization of the presence band using digital filtering via metadata during streaming playback.","2021","2023-07-12 07:06:53","2023-07-19 04:51:05","","309–322","","5","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X6YU2FTL","journalArticle","2001","Blesser, Barry A.","An Interdisciplinary Synthesis of Reverberation Viewpoints","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=10176","Artificial reverberator algorithms, which are implemented using digital signal processing, can be best understood by considering their relationships to several disciplines: the perceptual metrics of the auditory system, the statistical properties of the acoustic spaces, the artistic needs of the music culture, and the mixing techniques in the recording studio. Both the early reverberations, containing the unique spatial personality, and the late part, containing the statistically random process, play different roles in each of the related disciplines. When the temporal and spectral statistics match the perceptual criteria, the process is transparent. Some of the apparent paradoxes are resolved by considering psychoacoustic and statistical models. Moreover, there is sufficient knowledge to predict the performance of an algorithm without extensive ad hoc listening tests. The unifying theme is the question of how the human auditory system builds a sense of space.","2001","2023-07-12 07:06:56","2023-07-19 03:43:06","","867–903","","10","49","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WDZXIV8X","journalArticle","2001","Mason, Russell; Ford, Natanya; Rumsey, Francis; De Bruyn, Bart","Verbal and Nonverbal Elicitation Techniques in the Subjective Assessment of Spatial Sound Reproduction","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=10190","Current research into spatial audio has shown an increasing interest in the way subjective attributes of reproduced sound are elicited from listeners. The emphasis at present is on verbal semantics, however, studies suggest that nonverbal methods of elicitation could be beneficial. Research into the relative merits of these methods has found that nonverbal responses may result in different elicited attributes compared to verbal techniques. Nonverbal responses may be closer to the perception of the stimuli than the verbal interpretation of this perception. There is evidence that drawing is not as accurate as other nonverbal methods of elicitation when it comes to reporting the localization of auditory images. However, the advantage of drawing is its ability to describe the whole auditory space rather than a single dimension.","2001","2023-07-12 07:07:00","2023-07-19 04:29:02","","366–384","","5","49","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BPC4CDDG","journalArticle","2013","Kunka, Bartosz; Kostek, Bozena","New Aspects of Virtual Sound Source Localization Research–Impact of Visual Angle and 3-D Video Content on Sound Perception","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16824","Human beings create an internal picture of the external work by combining the information in all their sense. For example, an image can influence the localization of a virtual sound source, called the “image proximity effect.” And, similarly, the visual presentation of someone talking can strongly influence the perception of phonemes, called the “ventriloquism effect.” This paper focuses on two other aspects related to the multimodal effect: the influence of the screen size on the observed shift of the virtual sound source, and the relationship between the observed image proximity effect and the stereoscopic depth of a 3-D object. Experimental results showed that the visual angle of the presented object determines the image proximity effect regardless of the screen size.","2013","2023-07-12 07:07:06","2023-07-19 04:16:34","","280–289","","5","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FX6HJ9LE","journalArticle","1986","Wrightson, Jack","Psychoacoustic Considerations in the Design of Studio Control Rooms","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=5243","A great deal of discussion has been held concerning opotimal control room shapes and finishes, and their effect upon monitoring accuracy, localization, and illusory ambience. Design considerations based upon experimental data are described which are concerned with the perception of temporal and directional characteristics of reflected sounds. Application of these data to minitoring situations argues against the discrete, high-amplitude, laterally opposed reflections currently in vogue.","1986","2023-07-12 07:07:13","2023-07-19 10:57:05","","789–795","","10","34","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J6N4Y8AS","journalArticle","1991","Begault, Durand R.","Challenges to the Successful Implementation of 3-D Sound","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=10281","The major challenges for the successful implementation of 3-D audio systems involve minimizing reversals, intracranially heard sound, and localization error for listeners. Designers of 3-D audio systems are faced with additional challenges in data reduction and low-frequency response characteristics. The relationship of the head-related transfer function (HRTF) to these challenges is shown, along with some preliminary psychoacoustic results gathered at NASA-Ames.","1991","2023-07-12 07:07:17","2023-07-19 03:40:25","","864–870","","11","39","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ACH99GC6","journalArticle","2016","Woodcock, James; Davies, William J.; Cox, Trevor J.; Melchior, Frank","Categorization of Broadcast Audio Objects in Complex Auditory Scenes","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18297","Because object-based audio is becoming an important framework for the representation of complex sound scenes, this research describes a series of experiments to determine a categorization framework for broadcast audio objects. Categorization is a fundamental human strategy for reducing cognitive load, and knowledge of these categories should be beneficial for the development of perceptually based representations and rendering strategies for object-based audio. In this study, 21 expert and non-expert listeners took part in a free card sorting task using audio objects from a variety of different types of program material. Hierarchical agglomerative clustering suggests that there are 7 general categories, which relate to sounds indicating actions and movement, continuous background sound, transient background sound, clear speech, non-diegetic music and effects, sounds indicating the presence of people, and prominent attention-grabbing transient sounds. A three-dimensional perceptual space calculated via multidimensional scaling suggests that these categories vary along the dimensions of semantic content, continuous-transient, and presence-absence of people. The position of an audio object along the dimensions of the perceptual space relates to its perceived importance.","2016","2023-07-12 07:07:20","2023-07-19 10:56:38","","380–394","","6","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PUED7ARM","journalArticle","2004","Moore, Brian C. J.; Tan, Chin-Tuan; Zacharov, Nick; Mattila, Ville-Veikko","Measuring and Predicting the Perceived Quality of Music and Speech Subjected to Combined Linear and Nonlinear Distortion","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13034","The results of experiments in which subjects rated the perceived quality of speech and music that had been subjected to various forms of both linear and nonlinear distortion are reported. Experiment 1 made use of artificial distortions (such as ripples in frequency response combined with peak clipping). Experiment 2 included both artificial distortions and real distortions introduced by transducers. The results were compared with the predictions of a new model based on a weighted sum of predictions for linear distortion alone and for nonlinear distortion alone. There was a very good correspondence between the obtained and predicted ratings. Correlations were greater than 0.85 for speech stimuli and 0.90 for music stimuli. It is concluded that the new model can predict accurately the perceived quality of speech and music subjected to combined linear and nonlinear distortion.","2004","2023-07-12 07:07:36","2023-07-19 04:33:46","","1228–1244","","12","52","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AR6ILMSJ","journalArticle","2006","Choisel, Sylvain; Wickelmaier, Florian","Extraction of Auditory Features and Elicitation of Attributes for the Assessment of Multichannel Reproduced Sound","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13903","The identification of relevant auditory attributes is pivotal in sound quality evaluation. Two fundamentally different psychometric methods were employed to uncover perceptually relevant auditory features of multichannel reproduced sound. In the first method, called repertory grid technique (RGT), subjects were asked to assign verbal labels directly to the features when encountering them, and to subsequently rate the sounds on the scales thus obtained. The second method, perceptual structure analysis (PSA), required the subjects to consistently use the perceptually relevant features in triadic comparisons, without having to assign them a verbal label; given sufficient consistency, a lattice representation—as frequently used in formal concept analysis (FCA)—can be derived to depict the structure of auditory features.","2006","2023-07-12 07:07:39","2023-07-19 03:48:18","","815–826","","9","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UVISRXNP","journalArticle","1985","Benade, Arthur H.","From Instrument to Ear in a Room: Direct or via Recording","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=4457","The fluctuation statistics of the path between source and detector in room acoustics is reviewed along with some of the perceptual mechanisms used by the musical listener at a concert. Auditory parallel processing is important in both time and frequency domains. It is shown that the listener exploits the room statistics as a means for gaining information unavailable in a reflection-free environment. Properties of a good concert hall are examined as preparation for a discussion of the acoustical requirements of a recording studio. The record/playback process is examined, where two successive room-transmission paths and two types of sources must be dealt with by the listener. Finally, microphone placement is considered, as influenced by musical-instrument radiation patterns.","1985","2023-07-12 07:07:47","2023-07-19 03:41:31","","218–233","","4","33","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UUHNZLYW","journalArticle","2021","Kim, Sungyoung; Howie, Will","Influence of the Listening Environment on Recognition of Immersive Reproduction of Orchestral Music Sound Scenes","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21533","This study investigates how a listening environment (the combination of a room's acoustics and reproduction loudspeaker) influences a listener's perception of reproduced sound fields. Three distinct listening environmentswith different reverberation times and clarity indices were compared for their perceptual characteristics. Binaural recordings were made of orchestral music, mixed for 22.2 and 2-channel audio reproduction, within each of the three listening rooms. In a subjective listening test, 48 listeners evaluate these binaural recordings in terms of overall preference and five auditory attributes: perceived width, perceived depth, spatial clarity, impression of being enveloped, and spectral fidelity. Factor analyses of these five attribute ratings show that listener perception of the reproduced sound fields focused on two salient factors, spatial and spectral fidelity, yet the attributes' weightings in those two factors differs depending on a listener's previous experience with audio production and 3D immersive audio listening. For the experienced group, the impression of being enveloped was the most salient attribute, with spectral fidelity being the most important for the non-experienced group.","2021","2023-07-12 07:07:53","2023-07-19 04:11:44","","834–848","","11","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TCPLDPTJ","journalArticle","2004","Härmä, Aki; Jakka, Julia; Tikander, Miikka; Karjalainen, Matti; Lokki, Tapio; Hiipakka, Jarmo; Lorho, Gaëtan","Augmented Reality Audio for Mobile and Wearable Appliances","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13010","The concept of augmented reality audio characterizes techniques where a real sound environment is extended with virtual auditory environments and communications scenarios. A framework is introduced for mobile augmented reality audio (MARA) based on a specific headset configuration where binaural microphone elements are integrated into stereo earphones. When microphone signals are routed directly to the earphones, a user is exposed to a pseudoacoustic representation of the real environment. Virtual sound events are then mixed with microphone signals to produce a hybrid, an augmented reality audio representation, for the user. An overview of related technology, literature, and application scenarios is provided. Listening test results with a prototype system show that the proposed system has interesting properties. For example, in some cases listeners found it very difficult to determine which sound sources in an augmented reality audio representation are real and which are virtual.","2004","2023-07-12 07:07:56","2023-07-19 04:03:47","","618–639","","6","52","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"URX8AR4Y","journalArticle","2001","Wun, Cheuk-Wai; Horner, Andrew","Perceptual Wavetable Matching for Synthesis of Musical Instrument Tones","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=10195","Recent parameter matching methods for multiple wavetable synthesis have used a simple relative spectral error formula to measure how accurately the synthetic spectrum matches an original spectrum. It is supposed that the smaller the spectral error, the better the match, but this is not always true. A modified error formula is described, which takes into account the masking characteristics of our auditory system, as an improved measure of the perceived quality of the matched spectrum. Selected instrument tones have been matched using both error formulas and resynthesized. Listening test results show that wavetable matching using the perceptual error formula slightly outperforms ordinary matching, especially for instrument tones that have several masked partials.","2001","2023-07-12 07:08:07","2023-07-19 10:57:41","","250–262","","4","49","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UR9K342V","journalArticle","1964","Clark, Melville, Jr.; Milner, Paul","Dependence of Timbre on the Tonal Loudness Produced by Musical Instruments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=764","To determine if the timbre of nonpercussive musical instruments are dependent upon the intensities with which they are played, musically competent subjects were asked to identify the various original dynamic markings with which the instruments were played from tones equalized in loudness by adjustment of the gain upon reproduction. Confusion matrices display the results. The timbre of nonpercussive musical instruments is, at most, a weak function of the intensity with which they are played, with a few exceptions.","1964","2023-07-12 07:08:10","2023-07-19 03:48:45","","28–31","","1","12","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4Z888DJ4","journalArticle","2012","Barrass, Stephen","Digital Fabrication of Acoustic Sonifications","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16375","Because the human brain is often optimal for detecting subtle patterns, this paper explores a novel transformation that maps numerical data into sound. In this research, a set of data taken from head-related transfer functions was used to create physical objects (bells made from stainless steel) whose acoustics were then presented to listeners. The technique is called acoustic sonification. Listeners were able to hear differences in pitch and timbre of bells that were constructed from different datasets, while bells constructed from similar datasets sounded similar. Modulating the shape of a bell with a dataset can influence the acoustic spectrum in a way that results in audible differences |even though there was no apparent visual difference. Acoustic sonification can take advantage of auditory pattern recognition.","2012","2023-07-12 07:08:13","2023-07-19 03:38:58","","709–715","","9","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DKY79PHV","journalArticle","2004","Rao, Preeti; Shandilya, Saurabh","On the Detection of Melodic Pitch in a Percussive Background","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12999","The extraction of pitch (or fundamental frequency) information from polyphonic audio signals remains a challenging problem. The specific case of detecting the pitch of a melodic instrument playing in a percussive background is presented. Time-domain pitch detection algorithms based on a temporal autocorrelation model, including the Meddis-Hewitt algorithm, are considered. The temporal and spectral characteristics of percussive interference degrade the performance of the pitch detection algorithms to various extents. From an experimental study of the pitch estimation errors obtained on a set of synthetic musical signals, the effectiveness of the auditory-perception-based modules of the Meddis-Hewitt pitch detection algorithm in improving the robustness of fundamental frequency tracking in the presence of percussive interference is discussed.","2004","2023-07-12 07:08:18","2023-07-19 04:40:10","","378–390","","4","52","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MRUMB8RI","journalArticle","2019","Geronazzo, Michele; Peruch, Enrico; Prandoni, Fabio; Avanzini, Federico","Applying a Single-Notch Metric to Image-Guided Head-Related Transfer Function Selection for Improved Vertical Localization","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20483","This paper describes an image-guided HRTF selection procedure that exploits the relation between features of the pinna shape and HRTF notches. Using a 2D image of a user’s pinna, the procedure selects from a database the HRTF set that best fits the anthropometry of that user. The proposed procedure is designed to be quickly applied and easy to use for a user without previous knowledge of binaural audio technologies. The entire process is evaluated by means of an auditory model for sound localization in the mid-sagittal plane available from previous literature and a short localization test in virtual reality. Using both virtual and real subjects from a HRTF database, predictions and the experimental evaluation aimed to assess the vertical localization performance with HRTF sets are selected by the proposed procedure. The results report a statistically significant improvement in predictions of the auditory model for localization performance with selected HRTFs compared to KEMAR HRTFs.","2019","2023-07-12 07:08:25","2023-07-19 04:02:23","","414–428","","6","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4HWPJG7S","journalArticle","2001","Papaodysseus, Constantin; Roussopoulos, George; Fragoulis, Dimitrios; Panagopoulos, Athanasios; Alexiou, Constantin","A New Approach to the Automatic Recognition of Musical Recordings","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=10203","A new methodology for the automatic recognition of musical recordings is presented. A system has been developed that performs recognition among a set of specific musical recordings. The system claims a high rate of success (greater than 86%), even when the unknown compositions have suffered from up to a medium degree of distortion. It comprises a database of musical characteristics that correspond to a set of model musical recordings. These characteristics are derived by applying novel feature extraction algorithms to every model musical recording selected. In order to determine whether an unknown musical recording corresponds to a piece represented in the database, the same feature extraction algorithm is applied to it, and the characteristics thus derived are compared to the database contents by means of a set of criteria. The system can operate in parallel, essentially in real time, even for a considerable number of model musical recordings, as long as the hardware necessary is available.","2001","2023-07-12 07:09:25","2023-07-19 04:36:51","","23–35","","1/2","49","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6J38C3H7","journalArticle","2015","McGregor, Iain Peter; Cunningham, Stuart","Comparative Evaluation of Radio and Audio Logo Sound Designs","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18048","This study aims to explore the suitability of capturing designers’ and listeners’ experiences of sound design for a radio drama and audio logos using the repertory grid technique, which is a proven method of information elicitation based on Personal Construct Theory. Sound designs that incorporate sound effects, music, or dialogue can be broken down into discrete sound events that can then be rated using attributes that are meaningful to both designers and listeners. A method for evaluating sound without training casual listeners and without depending on expert listeners is presented. A number of the constructs show strong matches between the sound’s designers and listeners, indicating that these constructs have value as a common vocabulary and can be used to mediate and articulate audio features between the two.","2015","2023-07-12 07:09:28","2023-07-19 04:29:53","","876–888","","11","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J8BI7U9W","journalArticle","2018","Stolfi, Ariane; Sokolovskis, Janis; Goródscy, Fábio; Iazzetta, Fernando; Barthet, Mathieu","Audio Semantics: Online Chat Communication in Open Band Participatory Music Performances","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19868","Technology-mediated audience participation is an emergent topic in creative music technology with a blurred distinction between audience and performers. This paper analyzes communication patterns occurring in the online chat of the Open Band system for participatory live music performance. In addition to acting as a multi-user messaging tool, the chat system also serves as a control interface for the sonification of textual messages from the audience. Open Band performances have been presented at various festivals and conferences since 2016. Its web-based platform enables collective “sound dialogues” that are opened to everyone regardless of musical skills. Drawing on interactive participatory art and networked music performance, the system aims to provide engaging social experiences in colocated music-making situations. The authors collected data from four public performances including over 3,000 anonymous messages sent by audiences. After presenting the design of the system, the authors analyzed the semantic content of messages using thematic and statistical methods. Findings show how different sonification mechanisms alter the nature of the communication between participants who articulate both linguistic and musical self-expression. One of the design goals was to provide a platform for free audience expression as a web “agora.” The various themes that emerged from the analyses endorse this idea, as participants felt free to discuss subjects ranging from love to political opinions.","2018","2023-07-12 07:09:35","2023-07-19 04:49:44","","910–921","","11","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CX22LM96","journalArticle","2020","Kritsis, Kosmas; Garoufis, Christos; Zlatintsi, Athanasia; Bouillon, Manuel; Acosta, Carlos; Martín-Albo, Daniel; Piechaud, Robert; Maragos, Petros; Katsouros, Vassilis","iMuSciCA Workbench: Web-based Music Activities For Science Education","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20990","This paper presents the iMuSciCA Workbench, developed to address secondary school students with the aim to support mastery of core academic content on Science, Technology, Engineering, Arts, and Mathematics (STEAM) subjects, along with the development of creativity and deeper learning skills through the students' participation in music activities. Herein, we focus on the technical implementation of the innovative music-related web environments hosted by the iMuSciCA Workbench.","2020","2023-07-12 07:09:43","2023-07-19 04:16:17","","738–746","","10","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RS3ZP9MY","journalArticle","2023","Cairns, Patrick; Hunt, Anthony; Johnston, Daniel; Cooper, Jacob; Lee, Ben; Daffern, Helena; Kearney, Gavin","Evaluation of Metaverse Music Performance With BBC Maida Vale Recording Studios","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22139","This paper details a case study evaluation of a recording experience in a networked XR simulation of the renowned BBC Maida Vale Recording Studios. The system allows multiple remote musicians to connect over a network, providing a shared virtual acoustic space, with interactive immersive audio, XR display, and low-latency throughput. A four-piece rock band used this system in a live recording session, performing under different latency and audio conditions. Technical setup and case study protocol is detailed. Evaluation is provided in the form of Quality of Experience rating, tempo analysis, and a semi-structured exit interview.","2023","2023-07-12 07:09:46","2023-07-19 03:46:55","","313–325","","6","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EAUY3GBA","journalArticle","2017","Yu, Shiwei; Zhang, Hongjuan; Duan, Zhiyao","Singing Voice Separation by Low-Rank and Sparse Spectrogram Decomposition with Prelearned Dictionaries","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18731","Although the human auditory system can easily distinguish the singing voice from the background music in a music recording, it is extremely difficult for computer systems to replicate this ability, especially when the music mixture is a single channel. The challenge arises from the variety of simultaneous sound sources as well from the rich pitch and timbre variations of a singing voice. Unsupervised spectrogram decomposition involves separating the mixture spectrogram into a sparse spectrogram for the singing voice and a low-rank spectrogram for the background music. This approach has two limitations: the unsupervised nature prevents the prelearning of voice and background in music dictionaries; some components of the singing voice and background music may not show the preferred sparse and low-rank properties. In contrast, the authors propose to decompose the mixture spectrogram into three parts: a sparse spectrogram representing the singing voice, a low-rank spectrogram representing the background music, and a residual spectrogram for the components that are not identified by either the sparse or the low-rank spectrogram. Universal dictionaries for the singing voice and background music are prelearned from isolated singing voice and background music training data, through which prior knowledge of the voice and background music is introduced to the separation process. Evaluations on two datasets show that the proposed method is effective and efficient for both the separated singing voice and music accompaniment at various voice-to-music ratios.","2017","2023-07-12 07:10:00","2023-07-19 10:58:50","","377–388","","5","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DZNS6HLD","journalArticle","2000","Shlien, Seymour","Auditory Models for Gifted Listeners","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12038","Some listeners are especially sensitive to minute codec quantization noise. Various psychoacoustic tests were performed in order to measure the characteristics of these listeners. Though the auditory filter bandwidths of these listeners appeared to be normal, some had unusual abilities to detect weak signals buried in noise.","2000","2023-07-12 07:10:03","2023-07-19 04:48:07","","1032–1044","","11","48","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JTKRITAV","journalArticle","1961","McCoy, Donald S.","Distortion of Auditory Perspective Produced by Interchannel Mixing at High and Low Audio Frequencies","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=488","This paper describes an attempt to determine by subjective test, the nature and degree of distortium of auditory perspective produced when L-R or difference information of the two stereo channels is attenuated at either high or low audio frequencies.","1961","2023-07-12 07:10:13","2023-07-19 04:29:37","","13–18","","1","9","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NK9V4IT8","journalArticle","2017","Zhang, Mingfeng; Lu, Hengwei; Ren, Gang; Smith, Sarah; Beauchamp, James; Bocko, Mark F","A Matlab-Based Signal Processing Toolbox for the Characterization and Analysis of Musical Vibrato","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18734","To assess and manipulate the vibrato in musical sounds, audio engineers either informally listen to the audio or visually inspect waveform envelopes or spectrographic representations. Unfortunately, detailed descriptions of the amplitude and frequency trajectories of harmonic partials are difficult to infer from audio spectrograms, which means quantitative information is limited. This paper describes a collection of signal processing methods and a toolbox for extracting and analyzing vibrato-related parameters from solo audio recordings. The Vibrato Analysis Toolbox (VAT) uses a method based on the Hilbert transform to extract the amplitude and frequency variations as feature tracks. A parameterization algorithm then extracts various descriptive parameters including vibrato depth, frequency, spectral centroid, relative amplitude-frequency modulation phase and time delay, and other relationships based on the vibrato tracks. Together, these parameters provide a quantitative characterization of vibrato. The VAT also provides visualization and resynthesis functions that enable users to interactively explore many musical features. Algorithms are written in the Matlab programming language for easy adaptation, enabling further development by researchers and developers. Applications include music performance pedagogy, musicological studies, music production, and voice analysis.","2017","2023-07-12 07:10:16","2023-07-19 10:59:14","","408–422","","5","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WDCI7QV9","journalArticle","2023","Anemüller, Carlotta; Adami, Alexander; Herre, Jürgen","Efficient Binaural Rendering of Spatially Extended Sound Sources","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22131","In virtual/augmented reality or 3D applications with binaural audio, it is often desired to render sound sources with a certain spatial extent in a realistic way. A common approach is to distribute multiple correlated or decorrelated point sources over the desired spatial extent range, possibly derived from the original source signal by applying suitable decorrelation filters. Based on this basic model, a novel method for efficient and realistic binaural rendering of spatially extended sound sources is proposed. Instead of rendering each point source individually, the target auditory cues are synthesized directly from just two decorrelated input signals. This procedure comes with the advantage of low computational complexity and relaxed requirements for decorrelation filters. An objective evaluation shows that the proposed method matches the basic rendering model well in terms of perceptually relevant objective metrics. A subjective listening test shows, furthermore, that the output of the proposed method is perceptually almost identical to the output of the basic rendering model. The technique is part of the Reference Model architecture of the upcoming MPEG-I Immersive Audio standard.","2023","2023-07-12 07:10:19","2023-07-19 03:36:52","","281–292","","5","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6SBN9CZX","journalArticle","2001","Czerwinski, Eugene; Voishvillo, Alexander; Alexandrov, Sergei; Terekhov, Alexander","Multitone Testing of Sound System Components'Some Results and Conclusions, Part 1: History and Theory","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=10174","An historical retrospective analysis of the measurement of nonlinearities in audio is carried out. A quantitative analysis of the responses of various nonlinear systems (theoretical and experimental) to a multitone signal is made, and multitone testing is compared to conventional harmonic and intermodulation measurements. The multitone test provides more accurate information about the behavior of nonlinear systems when compared to standard harmonic, two-tone intermodulation, and total harmonic distortion measurements. Modeling of the nonlinear reaction of various sound system components to a multitone signal is described.","2001","2023-07-12 07:10:23","2023-07-19 03:50:54","","1011–1048","","11","49","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5NSHNF4Z","journalArticle","2012","Mendonça, Catarina; Campos, Guilherme; Dias, Paulo; Vieira, José; Ferreira, João P.; Santos, Jorge A.","On the Improvement of Localization Accuracy with Non-Individualized HRTF-Based Sounds","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16555","Even though individual head-related transfer function (HRTF) filters produce better performance in virtual-reality environments, measuring individuals is labor intensive and expensive. Can training be used to enhance the performance of generic filters? This research shows that short training sessions with feedback allows for perceptual adaptation where simple exposure to generic HRTF filters did not. The benefits of training were observed not only for the trained sounds but also for other stimulus positions that were not part of the training. Apparently, subjects were actually adapting and generalizing to the generic HRTF filters, which is a manifestation of sensory neural plasticity. Learning profiles are unique to individuals. Any testing of localization performance should recognize the influence of training.","2012","2023-07-12 07:10:26","2023-07-19 04:30:34","","821–830","","10","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VATRESTS","journalArticle","2020","Vowels, M.J.; Mason, R.","Comparison of Pairwise Dissimilarity and Projective Mapping Tasks With Auditory Stimuli","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20895","Two methods for undertaking subjective evaluation were compared: a pairwise dissimilarity task (PDT) and a projective mapping task (PMT). For a set of unambiguous, synthetic, auditory stimuli, the aim was to determine the following: whether the PMT limits the recovered dimensionality to two dimensions; how subjects respond using PMT’s two-dimensional response format; the relative time required for PDT and PMT; and hence, whether PMT is an appropriate alternative to PDT for experiments involving auditory stimuli. The results of both Multi-Dimensional Scaling (MDS) analyses and Multiple Factor Analyses (MFA) indicate that, with multiple participants, PMT allows for the recovery of three meaningful dimensions. The results from the MDS and MFA analyses of the PDT data, on the other hand, were ambiguous and did not enable recovery of more than two meaningful dimensions. This result was unexpected given that PDT is generally considered not to limit the dimensionality that can be recovered. Participants took less time to complete the experiment using PMT compared to PDT (a median ratio of approximately 1:4), and employed a range of strategies to express three perceptual dimensions using PMT’s two-dimensional response format. PMT may provide a viable and efficient means to elicit up to 3-dimensional responses from listeners.","2020","2023-07-12 07:10:48","2023-07-19 10:53:47","","638–648","","9","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CXDLSYTM","journalArticle","1983","Shepard, Roger N.","Demonstrations of Circular Components of Pitch","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=4555","Pitch is usually thought of as a one-dimensional psychological attribute corresponding to the one-dimensional physical variable of frequency. However, complex tones can be continuously varied between pitches along at least three different dimensions: (1) a rectilinear dimension of height, (2) a circular dimension of chroma, and (3) a second circular dimension of musical fifths. Accompanying sound demonstrations illustrate each of these three variations-including illusions in which a tone that is cyclically shifted around the chroma circle seems to go ever higher in pitch, and in which a tone that is simultaneously shifted in opposite directions in height and chroma seems always to be rising in pitch but is clearly lower at the end than at the beginning.","1983","2023-07-12 07:10:52","2023-07-19 04:47:31","","641–649","","9","31","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LPPJHR73","journalArticle","2013","Sunder, Kaushik; Tan, Ee-Leng; Gan, Woon-Seng","Individualization of Binaural Synthesis Using Frontal Projection Headphones","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17076","Non-individualized head related transfer functions (HRTF) limit the spatial accuracy of conventional side projection headphones. This research explores the use of a frontal projection headphone, which customizes the HRTF by introducing idiosyncratic pinna cues. In addition, a robust headphone equalization technique is recommended for frontal projection headphone playback to preserve the embedded personal pinna cues. Perceptual experiments validated the effectiveness of frontal headphone playback over the conventional headphones with reduced front-back confusions and improved frontal localization. It was also observed that the individual spectral cues created by the frontal projection are sufficient for front-back discrimination even with the high frequency pinna cues removed from the non-individual HRTF.","2013","2023-07-12 07:10:59","2023-07-19 04:50:57","","989–1000","","12","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9MN4224U","journalArticle","1969","Lebo, Charles P.; Oliphant, Kenward P.","Music as a Source of Acoustic Trauma","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=1556","Earlier investigations reported by the authors generated considerable interest and comment. One comment, though hardly the most significant, offered a unique challenge -The Desire to Know.- The comment simply stated questioned the right to single out rock & roll as the sole musical cause for acoustic TRAUMA-and many further stated -live symphony music is equally severe.- The reader is invited to verify the findings contained in the paper presented herein.","1969","2023-07-12 07:11:02","2023-07-19 04:17:58","","535–538","","5","17","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A8WD2P36","journalArticle","2023","Klein, Florian; Surdu, Tatiana; Treybig, Lukas; Werner, Stephan","The Ability to Memorize Acoustic Features in a Discrimination Task","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22129","How humans perceive, recognize, and remember room acoustics is of particular interest in the domain of spatial audio. For the creation of virtual or augmented acoustic environments, a room acoustic impression matches the expectations of certain room classes or a specific room. These expectations are based on the auditory memory of the acoustic room impression. In this paper, the authors present an exploratory study to evaluate the ability of listeners to recognize room acoustic features. The task of the listeners was to detect the reference room in a modified ABX double-blind stimulus test that featured a pre-defined playback order and a fixed time schedule. Furthermore, the authors explored distraction effects by employing additional nonacoustic interferences. The results show a significant decrease of the auditory memory capacity within 10 s, which is more pronounced when the listeners were distracted. However, the results suggest that auditory memory depends on what auditory cues are available.","2023","2023-07-12 07:11:05","2023-07-19 04:12:40","","254–266","","5","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WIABIGEX","journalArticle","2022","Compton, Stephen","Managing the Live-Sound Audio Engineer's Most Essential Critical Listening Tool","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21567","Critical listening is the live-sound audio engineer's most essential tool for informed sonic assessment. In producing a cohesive mix that fulfills an event's aims, audio engineers affect the experience and well-being of all live-sound participants. This study compares the results from a 2020 international audio engineer survey with published research. The findings demonstrate that although in theory, engineers recognize their hearing as being their most essential critical listening tool, in practice, many have not foundways to manage their hearing and optimize their assessment ability effectively. Many engineers with impeded or impaired hearing continue to mix, believing that any negative impact on participants is minimal or nonexistent. The livesound experience and participant health and well-being are improved by promoting and acting on appropriate hearing management practices.","2022","2023-07-12 07:11:14","2023-07-19 03:49:22","","305–318","","4","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BSPEJYDD","journalArticle","2015","Francombe, Jon; Mason, Russell; Dewhirst, Martin; Bech, Søren","A Model of Distraction in an Audio-on-Audio Interference Situation with Music Program Material","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17567","There are many situations in which multiple audio programs are replayed over loudspeakers in the same acoustic environment, allowing listeners to focus on their desired target program. Where this situation is deliberately created and the different program items are centrally controlled, each listener can be viewed as having a personal sound zone system. In order to evaluate and optimize such situations in a perceptually relevant manner, the authors created a predictive model using the features that contribute to the distraction from unwanted sounds. Feature extraction was motivated by a qualitative analysis of subject responses. Distraction ratings were collected for one hundred randomly created audio-on-audio interference situations with music target and interferer programs. The selected features were related to the overall loudness, loudness ratio, perceptual evaluation of audio source separation, and frequency content of the interferer. The model was found to predict accurately for the training and validation datasets.","2015","2023-07-12 07:11:18","2023-07-19 04:00:36","","63–77","","1/2","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4942REJY","journalArticle","1979","Cabot, Richard C.; Genter II, C. Roy; Lucke, Thomas","Sound Levels and Spectra of Rock Music","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=3211","The results of a survey of the sound levels encountered by attendees of rock-music entertainment are presented. Recordings were made at several discotheques, nightclubs, and band parties. These recordings were later analyzed for overall sound levels, octave-band levels, and peak-to-average level ratios. Perceived noise level, composite damage risk, and A-, B-, C-, and D-weighted levels were calculated from octave-band levels by means of a special FORTRAN program. Comparisons are made of the variations in levels over an evening, from evening to evening, and from location to location.","1979","2023-07-12 07:11:25","2023-07-19 03:46:23","","267–284","","4","27","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YCW4YX6H","journalArticle","2020","Keyes, Christopher J.; Tan, Alfred","Design and Evaluation of a Spectral Phase Rotation Algorithm for Upmixing to 3D Audio","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20999","This paper details the design and evaluation of a novel frequency-domain digital signal processing algorithm intended for upmixing of previously recorded audio to larger 3D loudspeaker arrays either by itself or in combinationwith other upmixing techniques. The algorithm attempts to mimic the dynamic and complex phase variances experienced by listeners in concerts of acoustic music. Critical listening evaluations support the algorithm’s effectiveness in increasing the perceived spaciousness and liveliness and show a significant preference for its use.","2020","2023-07-12 07:11:32","2023-07-19 04:10:48","","856–864","","11","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3AKEKGNX","journalArticle","2022","Fyfe, Lawrence; Bedoya, Daniel; Chew, Elaine","Annotation and Analysis of Recorded Piano Performances on the Web","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22020","Advancing knowledge and understanding about performed music is hampered by a lack of annotation data for music expressivity. To enable large-scale collection of annotations and explorations of performed music, the authors have created a workflow that is enabled by CosmoNote, aWeb-based citizen science tool for annotating musical structures created by the performer and experienced by the listener during expressive piano performances. To enable annotation tasks with CosmoNote, annotators can listen to the recorded performances and view synchronized music visualization layers including the audio waveform, recorded notes, extracted audio features such as loudness and tempo, and score features such as harmonic tension. Annotators have the ability to zoom into specific parts of a performance and see visuals and listen to the audio from just that part. The annotation of performed musical structures is done by using boundaries of varying strengths, regions, comments, and note groups. By analyzing the annotations collected with CosmoNote, performance decisions will be able to be modeled and analyzed in order to aid in the understanding of expressive choices in musical performances and discover the vocabulary of performed musical structures.","2022","2023-07-12 07:11:35","2023-07-19 04:01:02","","962–978","","11","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NSA7VCLL","journalArticle","1987","Fielder, Louis D.","Evaluation of the Audible Distortion and Noise Produced by Digital Audio Converters","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=5193","A quantitative measurement technique to evaluate audible impairments caused by the noise and distortion of digital audio conversion systems is discussed and is shown to correlate well with subjective impression. Performance at low, medium, and high signal levels is examined using the auditory critical-band concept, signal masking, environmental noise making, and hearing acuity. The technique applies sine-wave signals and divides the output spectrum into auditory critical bands, which are used to determine the audibility of the noise and distortion products.","1987","2023-07-12 07:11:41","2023-07-19 03:57:46","","517–535","","7/8","35","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HKCCT62D","journalArticle","2019","Breebaart, Jeroen; Cengarle, Giulio; Lu, Lie; Mateos, Toni; Purnhagen, Heiko; Tsingos, Nicolas","Spatial Coding of Complex Object-Based Program Material","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20487","Object-based audio (OBA) program material is challenging to distribute over low bandwidth channels and costly to render for thin clients. This research proposes a dynamic object-grouping solution that can represent a complex object-based scene as an equivalent reduced set of object groups while maintaining perceptually transparent rendering quality. This solution is a type of spatial coding. This paper introduces a real-time greedy simplification technique that addresses limitations of previous approaches by modeling spatial release from masking and distributing input objects into to multiple output groups. The core algorithm is extended to preserve other types of artistic metadata beyond object position. Results of perceptual tests show that this solution can achieve a 10:1 reduction in object count while maintaining high-quality audio playback and rendering flexibility at the endpoint. Spatial coding does not require perceptual coding of the objects’ audio essence but can be further combined with audio coding tools to deliver OBA content at low bit rates. This makes spatial coding a key component of an OBA production and distribution workflow. Object-based content creation, distribution, and rendering workflows require novel methods to process, combine, encode, and simplify complex auditory scenes to allow end-point rendering flexibility, efficiency, and adaptability as well as the means to cater for personalized experiences.","2019","2023-07-12 07:11:47","2023-07-19 03:44:42","","486–497","","7/8","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S5RA7GKF","journalArticle","2013","Bank, Balázs","Audio Equalization with Fixed-Pole Parallel Filters: An Efficient Alternative to Complex Smoothing","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16666","A common method for displaying, modeling, and equalizing the frequency response of audio systems is to use smoothing to eliminate the raggedness of the response. Fixed-pole parallel filters, which produce modest computational loading for both signal filtering and parameter estimation, possess the beneficial properties of smoothing. This makes them an efficient method for modeling or equalizing audio systems. The resolution of smoothing is controlled by the choice of pole frequencies: for obtaining a smoothing with 1/ß-octave resolution, ß/2 pole pairs are placed in each octave (e.g., sixth-octave resolution is achieved by having three pole pairs per octave). In addition, an analysis shows the theoretical equivalence of parallel filters and Kautz filters; the formulas for converting the parameters of the two types of filter are given.","2013","2023-07-12 07:11:57","2023-07-19 03:38:31","","39–49","","1/2","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KJWU78D5","journalArticle","2022","Lindetorp, Hans; Falkenberg, Kjetil","Evaluating Web Audio for Learning, Accessibility, and Distribution","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22019","Web Audio has a great potential for interactive audio content in which an open standard and easy integration with other web-based tools makes it particularly interesting. From earlier studies, obstacles for students to materialize creative ideas through programming were identified; focus shifted from artistic ambition to solving technical issues. This study builds upon 20 years of experience from teaching sound and music computing and evaluates howWeb Audio contributes to the learning experience. Data was collected from different student projects through analysis of source code, reflective texts, group discussions, and online self-evaluation forms. The result indicates that Web Audio serves well as a learning platform and that an XML abstraction of the API helped the students to stay focused on the artistic output. It is also concluded that an online tool can reduce the time for getting started with Web Audio to less than 1 h. Although many obstacles have been successfully removed, the authors argue that there is still a great potential for new online tools targeting audio application development in which the accessibility and sharing features contribute to an even better learning experience.","2022","2023-07-12 07:12:12","2023-07-19 04:20:34","","951–961","","11","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QSU9ZFU3","journalArticle","2018","Wierstorf, Hagen; Hold, Christoph; Raake, Alexander","Listener Preference for Wave Field Synthesis, Stereophony, and Different Mixes in Popular Music","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19568","The mixing engineer and the reproduction system both influence the perception of a song by a listener. For unfamiliar music, the mix can significantly influence listener preferences. The goal of this research was to investigate the influence of the reproduction systems and mixing parameters by asking listeners for preference ratings in a paired comparison test. The study generated mixes of four different popular music recordings and introduced systematic changes to the wave field synthesis mix for one song. The mixing parameters were EQ, compression, reverb, and spatial positioning. Listeners rated their preference by comparing two channels with five channel stereophony and wave field synthesis using a circular array of 56 loudspeakers. Even when introducing relatively strong changes to the wave field synthesis mix, listeners still preferred that system most of the time. This preference was dependent on the actual content and might vary between different songs, or even song excerpts. The mixing condition disliked the most by listeners was a very wide arrangement of the foreground elements of popular music, such as vocals, snare and bass drum, and guitars. Overall, using a high number of loudspeakers is preferred by most listeners, and the differences between reproduction methods can have a larger influence than strong variations of single mixing parameters.","2018","2023-07-12 07:12:20","2023-07-19 10:55:35","","385–396","","5","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3BCW8XLW","journalArticle","2020","Liew, Kongmeng; Lindborg, PerMagnus","A Sonification of Cross-Cultural Differences in Happiness-Related Tweets","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20715","Sonification can be defined as any technique that translates data into non-speech sound with a systematic, describable, and reproducible method, in order to reveal or facilitate communication, interpretation, or discovery of meaning that is latent in the data. This paper describes an approach for communicating cross-cultural differences in sentiment data through sonification, which is a powerful technique for the translation of patterns into sounds that are understandable, accessible, and musically pleasant. A machine-learning classifier was trained on sentiment information of two samples of Tweets from Singapore and New York with the keyword of ""happiness."" Positive-valence words that relate to the concept of happiness showed stronger influences on the classifier than negative words. For mapping, Tweet frequency differences of the semantic variable ""anticipation"" affected tempo, positive-affected pitch, and joy-affected loudness, while ""trust"" affected rhythmic regularity. The authors evaluated sonification of the original data from the two cities, together with a control condition generated from random mappings in a listening experiment. Results suggest that the original was rated as significantly more pleasant.","2020","2023-07-12 07:12:23","2023-07-19 04:20:16","","25–33","","1/2","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FQBRMZP4","journalArticle","1987","Strawn, John","Editing Time-Varying Spectra","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=10283","Time-varying spectral analysis is a useful tool for working with sound. Unfortunately a very large amount of data is involved. A menu-driven graphics-based editor for time-varying spectra has been implemented at the Center for Computer Research in Music and Acoustics (CCRMA), Stanford University. The features that have been developed for examining and modifying spectra are outlined, and suggestions are offered for the next generation of editors of this type.","1987","2023-07-12 07:12:31","2023-07-19 04:50:01","","337–352","","5","35","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SW94YAR8","journalArticle","1976","Bernstein, Alan D.; Cooper, Ellis D.","The Piecewise-Linear Technique of Electronic Music Synthesis","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=2611","A new piecewise-linear generator permitting direct analog control of acoustic waveforms, the PL module, is shown to offer significant advantages in electronic music. A FORTRAN program, facilitating the computation of amplitude spectra for piecewise-linear waveforms, is used in discussing some of the properties of the PL module. Numerous applications to electronic music are suggested, beginning simply with generation of PL envelopes and waveforms.","1976","2023-07-12 07:12:38","2023-07-19 03:42:14","","446–454","","6","24","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AWSMFCGQ","journalArticle","2000","Härmä, Aki; Karjalainen, Matti; Savioja, Lauri; Välimäki, Vesa; Laine, Unto K.; Huopaniemi, Jyri","Frequency-Warped Signal Processing for Audio Applications","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12039","Modern audio techniques, such as audio coding and sound reproduction, emphasize the modeling of auditory perception as one of the cornerstones for system design. A methodology, frequency-warped digital signal processing, is presented in a tutorial paper as a means to design and implement digital signal-processing algorithms directly in a way that is relevant for auditory perception. Several audio applications are considered in which this approach shows advantages when used as a design or implementation tool or as a conceptual framework of design.","2000","2023-07-12 07:12:41","2023-07-19 04:03:56","","1011–1031","","11","48","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GNWKT4N7","journalArticle","1966","Beauchamp, James W.","Additive Synthesis of Harmonic Musical Tones","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=1129","The imporance of harmonic tones in musical situations makes it highly desirable to be able to synthesize arbitrary harmonic spectra. the most flexible method for accomplishing this is by additive synthesis of the individual harmonic components. It is also necessary to have control over the time behaviors of the parameters which define a harmonic tone. Consequently, a solid-state additive synthesis device, called the Harmonic Tone Generator, has been constructed which generates an audio tone composed of six harmonics. The fundamental frequency (0 to 2400 Hz), the amplitudes of the individual harmonics and the second harmonic phase-shift are controlled independently by external voltages. Circuitry has also been built to provide independent harmonic amplitude control signals and for the actuation of sound events. The theory of separation of harmonic partials using ultrasonic frequencies is discussed.","1966","2023-07-12 07:12:45","2023-07-19 03:39:37","","332–342","","4","14","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JSXUB7F3","journalArticle","2023","McKenzie, Thomas; Meyer-Kahlen, Nils; Hold, Christoph; Schlecht, Sebastian J.; Pulkki, Ville","Auralization of Measured Room Transitions in Virtual Reality","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22140","To auralize a room's acoustics in six degrees-of-freedom virtual reality (VR), a dense set of spatial room impulse response (SRIR) measurements is required, so interpolating between a sparse set is desirable. This paper studies the auralization of room transitions by proposing a baseline interpolation method for higher-order Ambisonic SRIRs and evaluating it in VR. The presented method is simple yet applicable to coupled rooms and room transitions. It is based on linear interpolation with RMS compensation, although direct sound, early reflections, and late reverberation are processed separately, whereby the input direct sounds are first steered to the relative direction-of-arrival before summation and interpolated early reflections are directionally equalized. The proposed method is first evaluated numerically, which demonstrates its improvements over a basic linear interpolation. A listening test is then conducted in six degrees-of-freedom VR, to assess the density of SRIR measurements needed in order to plausibly auralize a room transition using the presented interpolation method. The results suggest that, given the tested scenario, a 50-cm to 1-m inter-measurement distance can be perceptually sufficient.","2023","2023-07-12 07:12:51","2023-07-19 04:30:02","","326–337","","6","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UPINRUEF","journalArticle","1982","Clark, David","High-Resolution Subjective Testing Using a Double-Blind Comparator","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=3839","A system for the practical implementation of double-blind audibility tests is described. The controller is a self-contained unit, designed to provide setup and operational convenience while giving the user maximum sensitivity to detect differences. Standards for response matching and other controls are suggested as well as statistical methods of evaluating data. Test results to date are summarized.","1982","2023-07-12 07:12:55","2023-07-19 03:48:28","","330–338","","5","30","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3TIFENRY","journalArticle","2017","Ziemer, Tim; Bader, Rolf","Psychoacoustic Sound Field Synthesis for Musical Instrument Radiation Characteristics","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18781","This research describes a sound field synthesis method that reconstructs a desired sound field within an extended listening area by taking into account psychoacoustic perceptual constraints. The proposed approach covers the complete system: (a) measuring the radiation characteristics by means of microphone array technique; (b) storing the radiation characteristics in a database, (c) propagating an arbitrary source sound toward an extended listening area by considering sources as complex point sources, and (d) reconstructing this sound field with a loudspeaker array by solving a linear equation system for discrete listening points that sample the listening area. By capturing and reconstructing the sound radiation characteristics of musical instruments, a spatial sound impression can be created. Psychoacoustic considerations are implemented to allow for wave fronts arriving from different angles and at different points in time while maintaining precise source localization and a natural and spatial sound impression. Furthermore, the psychoacoustic considerations reduce the computational costs, as illustrated by solving the linear equations for only 25 selected frequencies. Strengths and weaknesses and benefits and limitations of the psychoacoustic sound field synthesis approach are investigated in a listening test. A simulation demonstrates that the approach is valid up to a critical spatial frequency that is given by the distribution of the listening points.","2017","2023-07-12 07:12:59","2023-07-19 10:59:55","","482–496","","6","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8M5W4DLK","journalArticle","2021","Lefford, M. Nyssim; Bromham, Gary; Fazekas, György; Moffat, David","Context-Aware Intelligent Mixing Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21022","Intelligent Mixing Systems (IMS) are rapidly becoming integrated into music mixing and production workflows. The intelligences of a human mixer and IMS can be distinguished by their abilities to comprehend, assess, and appreciate context. Humans will factor context into decisions, particularly concerning the use and application of technologies. The utility of an IMS depends on both its affordances and the situation in which it is to be used. The appropriate use for conventional purposes, or its utility for misappropriation, is determined by the context. This study considers how context impacts mixing decisions and the use of technology, focusing on how the mixer’s understanding of context can inform the use of IMS, and how the use of IMS can aid in informing a mixer of different contexts.","2021","2023-07-12 07:13:04","2023-07-19 04:19:58","","128–141","","3","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"69VNJCPJ","journalArticle","2013","Colmenares, Juan A.; Peters, Nils; Eads, Gage; Saxton, Ian; Jacquez, Israel; Kubiatowicz, John D.; Wessel, David","A Multicore Operating System with QoS Guarantees for Network Audio Applications","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16703","Efforts to create low-latency protocols to ensure quality of service (QoS) for audio networks often ignore the importance of the operating system (OS) in the computers at the ends of the network chain. An experimental OS called Tesselation, developed in the Parallel Computing Laboratory at UC Berkeley, was tailored to multicore processors with such features as guaranteed resource allocation and customizable user-level runtimes. To demonstrate performance isolation and service guarantees, the authors tested Tessellation under various conditions using a resource-demanding network application. This OS enables network audio applications to meet their time requirements.","2013","2023-07-12 07:13:08","2023-07-19 03:49:14","","174–184","","4","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A4AU64H3","journalArticle","1976","Bonello, Oscar J.","New Improvements in Audio Signal Processing for AM Broadcasting","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=2615","A new way of audio processing (amplitude and phase) is described, which gives an increase of 6 dB in the radiated energy with reference to the conventional system (compressor limiter) and allows coverage of twice the distance with the same transmission set. The energy graphic plots measured in the laboratory are analyzed and the experiences found under actual conditions in Argentina described.","1976","2023-07-12 07:13:12","2023-07-19 03:43:47","","379–383","","5","24","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5XQ98GGV","journalArticle","1992","Heegaard, Fr.","The Reproduction of Sound in Auditory Perspective and a Compatible System of Stereophony","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=10280","A little more than a year ago we invited the late Holger Lauridsen to contribute to the E.B.U. Review an article describing his work on stereophonic sound reproduction and in particular the sys;tem that he had proposed for stereophonic broadcasting without most of the disadvantages of using separate transmitter chains for the left-ear and right-ear components. Many of our readers will recall Mr. Lauridsen's untimely death in December 1957, which put an end to his valuable contribution to the art and science of broadcasting. Recently, however, interest in the possibilities of stereophony, and particularly in stereophonic broadcasting, has become widespread, and Mr. Heegaard very kindly agreed to write the following article, which is based upon the work of Mr. Lauridsen.'Editor","1992","2023-07-12 07:13:24","2023-07-19 04:04:31","","802–808","","10","40","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UIWJ8GDF","journalArticle","2021","Meaux, Eric; Marchand, Sylvain","Synthetic Transaural Audio Rendering (STAR): A Perceptive 3D Audio Spatialization Method","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21117","The synthetic transaural audio rendering (STAR) method aims at canceling the cross-talk signals between two loudspeakers and the ears of the listener (in a transaural way), with acoustic paths not measured but computed by some model (thus synthetic). Our model is based on perceptive cues used by the human auditory system for sound localization. The aim is to give the listener the sense of the position of each source rather than reconstruct the corresponding acoustic wave or field. Although the method currently focuses on the azimuth dimension, extensions to elevation and distance are now considered, for full 3D sound, with a discussion to conduct further works needed to improve overall quality and validate such extensions.","2021","2023-07-12 07:13:28","2023-07-19 04:30:10","","497–505","","7/8","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZEK3Y8B2","journalArticle","2005","Hawksford, Malcolm J.","System Measurement and Identification Using Pseudorandom Filtered Noise and Music Sequences","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13411","System measurement using pseudorandom filtered noise and music sequences is investigated. A single-pass technique is used to evaluate simultaneously the transfer function and the spectral-domain signal-to-distortion ratio that is applicable to amplifiers, signal processors, digital-to-analog converters, loudspeakers, and perceptual coders. The technique is extended to include a simplified Volterra model expressed as a power series and linear filter bank where for compliant systems, nonlinear distortion can be estimated for an arbitrary excitation without a need for remeasurement.","2005","2023-07-12 07:13:29","2023-07-19 04:04:04","","275–296","","4","53","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RQ8FKX9S","journalArticle","2017","Brinkmann, Fabian; Lindau, Alexander; Weinzierl, Stefan; Par, Steven van de; Müller-Trapet, Markus; Opdam, Rob; Vorländer, Michael","A High Resolution and Full-Spherical Head-Related Transfer Function Database for Different Head-Above-Torso Orientations","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19357","Head-related transfer functions (HRTFs) capture the free-field sound transmission from a sound source to the listeners ears, incorporating all the cues for sound localization, such as interaural time and level differences as well as the spectral cues that originate from scattering, diffraction, and reflection on the human pinnae, head, and body. In this study, HRTFs were acoustically measured and numerically simulated for the FABIAN head-and-torso simulator on a full-spherical and high-resolution sampling grid. HRTFs were acquired for 11 horizontal head-above-torso orientations, covering the typical range of motion of +/-50°. This made it possible to account for head movements in dynamic binaural auralizations. Because of a lack of an external reference for the HRTFs, measured and simulated data sets were cross-validated by applying auditory models for localization performance and spectral coloration. The results indicate a high degree of similarity between the two data sets regarding all tested aspects, thus suggesting that they are free of systematic errors.","2017","2023-07-12 07:13:46","2023-07-19 03:45:27","","841–848","","10","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FILLRT7F","journalArticle","2017","Fleßner, Jan-Hendrik; Huber, Rainer; Ewert, Stephan D.","Assessment and Prediction of Binaural Aspects of Audio Quality","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19361","Binaural or spatial presentation of audio signals has become increasingly important not only in consumer sound reproduction, but also for hearing-assistive devices like hearing aids, where signals in both ears might undergo extensive signal processing. Such processing may introduce distortions to the interaural signal properties that affect perception. In this research, an approach for intrusive binaural auditory-model-based quality prediction (BAM-Q) is introduced. BAM-Q uses a binaural auditory model as front-end to extract the three binaural features: interaural level difference, interaural time difference, and a measure of interaural coherence. The current approach focuses on the general applicability (with respect to binaural signal differences) of the binaural quality model to arbitrary binaural audio signals. Two listening experiments were conducted to subjectively measure the influence of these binaural features and their combinations on binaural quality perception. The results were used to train BAM-Q. Two different hearing aid algorithms were used to evaluate the performance of the model. The correlations between subjective mean ratings and model predictions are higher than 0.9.","2017","2023-07-12 07:13:49","2023-07-19 03:58:19","","929–942","","11","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F5BXLD8G","journalArticle","2013","van Waterschoot, Toon; Tirry, Wouter Joos; Moonen, Marc","Acoustic Zooming by Multi-Microphone Sound Scene Manipulation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16866","Camera zooming would be more compelling if the audio was subjected to a corresponding zoom that matched the video. Psychophysical and neuroimaging results suggest that a cross-modal approach to zooming facilitates multisensory integration. Because auditory distance perception is primarily determined by sound intensity, an audiovisual zoom effect can be obtained by matching the levels of different sources in a sound scene with their visually perceived distance. The authors propose a general theory for independent sound source level control that can be used to attain an acoustic zoom effect. The theory does not require sound source separation, which reduces computational load. An efficient implementation using fixed and adaptive spatial and spectral noise-reduction algorithms is proposed and evaluated. Experimental results using an array of a small number of low-cost microphones confirm that the proposed approach is particularly suited for consumer audiovisual capture applications.","2013","2023-07-12 07:13:53","2023-07-19 04:55:12","","489–507","","7/8","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9VW5DVQC","journalArticle","2022","Bank, Balázs","Warped, Kautz, and Fixed-Pole Parallel Filters: A Review","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21793","In audio signal processing, the aim is the best possible sound quality for a given computational complexity. For this, taking into account the logarithmic frequency resolution of hearing is a good starting point. The present paper provides an overview on warped, Kautz, and fixed-pole parallel filters and demonstrates that they are all capable of achieving logarithmiclike frequency resolution, providing much more efficient filtering or equalization compared to straightforward finite impulse response (FIR) or infinite impulse response (IIR) filters. Besides presenting the historical development of the three methods, the paper discusses their relations and provides a comparison in terms of accuracy, computational requirements, and design complexity. The comparison includes loudspeaker--room response modeling and equalization examples.","2022","2023-07-12 07:13:56","2023-07-19 03:38:40","","414–434","","6","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TC96L9LK","journalArticle","1995","Sandell, Gregory J.; Martens, William L.","Perceptual Evaluation of Principal-Component-Based Synthesis of Musical Timbres","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=7919","Harmonic-based analysis and resynthesis of musical instrument tones, for example, using the phase vocoder method, is a valuable technique, but its data representation is very large. However, this data set is usually highly redundant. Principal components analysis (PCA) can be used to encode such data into a smaller set of orthogonal basis vectors with minimal loss of information. Techniques for applying PCA to such data are explored, and the aural impact of the method on three tones (cello, trombone, and clarinet) are studied in two perception experiments. Results show that nearly identical resyntheses can be produced with a 40-70% data reduction. A preprocessing step called variable-duration temporal partitioning (VDTP) is introduced, which also affords a natural-sounding method for time expansion and contraction of tones. An extension of the PCA technique is also introduced that implements a ""timbre space,"" or coordinate system for interpolation among a group of musical instruments.","1995","2023-07-12 07:14:00","2023-07-19 04:46:24","","1013–1028","","12","43","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P6X4F64S","journalArticle","2022","Fela, Randy Frans; Zacharov, Nick; Forchhammer, Søren","Assessor Selection Process for Perceptual Quality Evaluation of 360 Audiovisual Content","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22010","For accurate and detailed perceptual evaluation of compressed omnidirectional multimedia content, it is imperative for assessor panels to be qualified to obtain consistent and high-quality data. This work extends existing procedures for assessor selection in terms of scope (360? videos with high-order ambisonic), time efficiency, and analytical approach, as described in detail. The main selection procedures consisted of a basic audiovisual screening and three successive discrimination experiments for audio (listening), video (viewing), and audiovisual using a triangle test. Additionally, four factors influencing quality of experience, including the simulator sickness questionnaire, were evaluated and are discussed. After the selection process, a confirmatory study was conducted using three experiments (audio, video, and audiovisual) and based on a rating scale methodology to compare performance between rejected and selected assessors. The studies showed that (i) perceptual discriminations are influenced by the samples, the encoding parameters, and some quality of experience factors; (ii) the probability of symptom occurrence is considerably low, indicating that the proposed procedure is feasible; and (iii) the selected assessors performed better in discrimination than the rejected assessors, indicating the effectiveness of the proposed procedure.","2022","2023-07-12 07:14:11","2023-07-19 03:56:34","","824–842","","10","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M3LW284N","journalArticle","1989","Olive, Sean E.; Toole, Floyd E.","The Detection of Reflections in Typical Rooms","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=6079","Reflected sounds influence the timbre and spatial character of live and reproduced sounds. Most investigations of reflections have focused on the performance of live sounds in large halls. Current interest in the acoustical interactions of rooms, loudspeakers, and listeners requires further and possibly more relevant data than have been available. In this study, the effects of reflected sounds were examined as they would occur in stereophonic reproduction in rooms of domestic or control-room size. Thresholds were determined as a function of level relative to the direct sound, the angle of incidence, spectrum, the temporal form of the signal, and reverberation. The relationships between audible effects and measurements, such as energy-time curves and frequency response, are discussed.","1989","2023-07-12 07:14:33","2023-07-19 04:35:58","","539–553","","7/8","37","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"55V9EFH7","journalArticle","2004","Algazi, V. Ralph; Duda, Richard O.; Thompson, Dennis M.","Motion-Tracked Binaural Sound","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13028","A new method is presented for capturing, recording, and reproducing spatial sound that provides a vivid sense of realism. The method generalizes binaural recording, preserving the information needed for dynamic head-motion cues. These dynamic cues greatly reduce the need for customization to the listener. During either capture or recording, the sound field in the vicinity of the head is sampled with a microphone array. During reproduction, a head tracker is used to determine the microphones that are closest to the positions of the listener's ears. Interpolation procedures are used to produce the headphone signals. The properties of different methods for interpolating the microphone signals are presented and analyzed.","2004","2023-07-12 07:14:36","2023-07-19 03:35:28","","1142–1156","","11","52","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F2BENMUW","journalArticle","2000","Baileyi, Nicholas J.; Cooper, David","Sculptor: Exploring Timbral Spaces in Real Time","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=10265","Characterization of the timbre of a sound from its spectrum or time series is of limited use in the context of electroacoustics because of the huge data set that is usually involved. The concept of a timbral space offers the potential to characterize sounds using a smaller set of variables, but it is not clear how the signal-derived parameters map onto their psychoacoustic correlates. Sculptor is a package that enables the analysis, parameterization, and editing of sounds in real time, requiring only inexpensive hardware. State-space techniques are used to decompose models of a selected sound into parallel second-order sections. The second-order resonator parameters can then be edited and the results auditioned in real time. The sound code of the package is available, so that researchers may provide additional functionality as required.","2000","2023-07-12 07:14:41","2023-07-19 03:38:10","","174–180","","3","48","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3IUDVGJW","journalArticle","2003","Tan, Chin-Tuan; Moore, Brian C. J.; Zacharov, Nick","The Effect of Nonlinear Distortion on the Perceived Quality of Music and Speech Signals","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12197","The effect of various types of nonlinear distortion on the perceived quality of speech and music signals was examined. In experiments 1 and 2, ""artificial"" distortions were used, including hard and soft symmetrical and asymmetrical peak clipping of various amounts, center clipping, and full-range waveform distortion produced by raising the instantaneous absolute value of the waveform to a power (≠1) while preserving the sign. Subjects were asked to rate the perceived amount of distortion on a ten-point scale (where 1 was most distorted and 10 least distorted). In experiment 1 the distortions were applied to the broadband signals. In experiment 2 the distortions were applied to subbands of the signal. Results were highly consistent across subjects and test sessions. Center clipping and soft clipping had only small effects on the ratings, whereas hard clipping and the full-range distortions had large effects. The subjective ratings were compared to physical measures of distortion based on multitone test signals. A distortion measure, DS, derived from the output spectrum of each nonlinear system in response to a 10-component multitone signal gave high negative correlations with the subjective ratings (correlations were negative as large values of DS were associated with low ratings). A further experiment was conducted using stimuli for which nonlinear distortion was introduced by recording the outputs of real transducers. The output signals were digitally filtered to reduce irregularities in the amplitude/frequency response as far as possible. The results showed moderately strong negative correlations between the subjective ratings and the objective measure DS. It was concluded, that an objective measure of nonlinear distortion based on the use of a multitone signal can predict the perceptual effects of nonlinear distortion reasonably well.","2003","2023-07-12 07:14:46","2023-07-19 04:51:15","","1012–1031","","11","51","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TU8MXUTI","journalArticle","2001","Cheng, Corey I.; Wakefield, Gregory H.","Introduction to Head-Related Transfer Functions (HRTFs): Representations of HRTFs in Time, Frequency, and Space","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=10196","In this tutorial, head-related transfer functions (HRTFs) are introduced and treated with respect to their role in the synthesis of spatial sound over headphones. HRTFs are formally defined, and are shown to be important in reducing the ambiguity with which the classical duplex theory decodes a free-field sound's spatial location. Typical HRTF measurement strategies are described, and simple applications of HRTFs to headphone-based spatialized sound synthesis are given. By comparing and contrasting representations of HRTFs in the time, frequency, and spatial domains, different analytic and signal processing techniques used to investigate the structure of HRTFs are highlighted.","2001","2023-07-12 07:14:49","2023-07-19 03:48:01","","231–249","","4","49","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FA95XYJC","journalArticle","2007","Rossiter, David; Tsang, Raymond; So, Richard H. Y.","Beat Deviation for Tempo Estimation Algorithms","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14180","The series of beats in an arbitrary piece of music typically do not have a precisely consistent interval of time between them. A study was carried out to analyze beat deviation. Five musicians tapped beat sequences for 50 recordings taken from 16 different categories of music. A power curve was proposed to represent the beat deviation. The curve is shown to capture the majority (96.9%) of the beat data entered by the subjects in this study, with a minimum chance of nonbeat events being accepted as valid beats. The proposed power curve equation is compared to others in published research. Measures of BPM deviation used in previous studies are also considered.","2007","2023-07-12 07:14:53","2023-07-19 04:42:04","","967–980","","11","55","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TKS2VF9M","journalArticle","1989","Davis, Don; Davis, Carolyn","Application of Speech Intelligibility to Sound Reinforcement","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=6057","Sound reinforcement and reproduction systems have grown to a scale where church committees and auditorium owners spend up to $1,000,000 for them. In the past, such systems were purchased without any assurance that acceptable speech intelligibility would be achieved. Today speech intelligibility can be specified in advance, designed for, and objectively measured with an accuracy as good as that achieved using a panel of ""live"" listeners. The competing techniques are described and evaluated, and some of the remaining problem areas encountered in analyzing nonstandard systems are outlined. the Peutz percent articulation loss of consonants technique (%ALcons), the speech transmission index (STI), and the rapid speech transmission index (RASTI) are all defined, compared to live listener tests made with a large listener sample, and illustrated using currently available analyzers.","1989","2023-07-12 07:14:56","2023-07-19 03:51:36","","1002–1019","","12","37","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PBDBH3LH","journalArticle","1984","Ingebretsen, Robert B.; Stockham, Thomas G., Jr.","Random-Access Editing of Digital Audio","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=4522","The most striking characteristic of digital audio, and possibly the most controversial, is its intrinsically high sonic quality. However, this may not be the most significant benefit in terms of commercial applications. characteristics such as archivability, flexible processing techniques, time-base independence, and rapid accessibility offer efficient and powerful capabilities. The implementation of one unique feature is discussed: random-access editing. Through the use of large-capacity rotating magnetic media and a smoothing buffer, it is possible to create and/or modify splices rapidly, audition them, then play the various cuts in one continuous stream. Such a system also enables various forms of processing (including such standard functions as fading, mixing, and equalization) to be imposed on the signal, as well as permitting different forms of interaction including display of audio waveforms.","1984","2023-07-12 07:14:58","2023-07-19 04:07:57","","114–122","","3","32","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BPWMT25J","journalArticle","2017","Andreopoulou, Areti; Roginska, Agnieszka","Database Matching of Sparsely Measured Head-Related Transfer Functions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19171","The effectiveness of binaural reproductions depends on the accuracy of those spatialization cues that are unique to an individual’s personal physiology. These cues are embedded in the Head-Related Transfer Functions (HRTFs), which guide sound from a given source direction to a listener’s ears. This report discusses the design and evaluation of a HRTF database matching system that pairs users to premeasured HRTF sets created from a sparse set of individualized acoustic measurements. The utilized spatial grid, which was derived from an LDA classifier, consisted of 68 filters nonuniformly distributed across 5 elevations from -30° and +30°. A binaural localization study confirmed the original hypothesis that the similarity between subsets of binaural filters can be generalized to represent the relationship of the HRTFs of origin. Analysis of the participant responses provided strong evidence that HRTF database matching is useful. The designed implementation was successful in providing users with alternative HRTF datasets when the similarity of the matched data to the search query was above a certain similarity threshold. It was also shown that low-similarity HRTFs can lead to decreased spatial localization accuracy.","2017","2023-07-12 07:15:05","2023-07-19 03:36:43","","552–561","","7/8","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A37FFJ7W","journalArticle","1988","Martinez, Charles; Gilman, Samuel","Results of the 1986 AES Audiometric Survey","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=5131","Many members of the Audio Engineering Society (AES) are employed in positions that require critical listening to sounds, frequently at high-intensity levels. Previous audiometric surveys of the membership in 1975 and 1976 suggest that some of the members are at risk for hearing loss due to the high-intensity sound exposure. This prompted the Los Angeles Section of the AES to provide for a follow-up audiometric survey to reassess the hearing level of the AES members. The results of the survey, performed at the 1986 November meeting in Los Angeles, are reported.","1988","2023-07-12 07:15:08","2023-07-19 04:28:52","","686–691","","9","36","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T3K2MYK9","journalArticle","1985","Borish, Jeffrey","An Auditorium Simulator for Domestic Use","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=4447","The enjoyment of music reproductions can be significantly increased by electronically simulating the early reflections characteristic of appropriate performance spaces. The simulation is based on a detailed mathematical modeling of the acoustics of various performing spaces. This analysis provides the impulse responses of the auditoriums for giving source and listener positions. The simulator convolves an audio signal obtained from standard stereo recordings with the desired impulse response in real time. Several impulse responses are stored in the simulator to provide listeners with a choice of environments to suit the musical style of their selection and their own personal taste. Only two additional loudspeakers are required to convey the lateral early reflections, and their positioning is uncritical. The medial reflections do not contribute to the desirable spatial impression, so no effort is made to present these sounds. Also, no effort is made to synthesize the late reflections because they can be and typically are recorded. Consideration of the properties of auditory perception led to other important simplifications. The system is simple to set up and operate, and provides music reproduction that experienced listeners have judged very natural.","1985","2023-07-12 07:15:22","2023-07-19 03:44:06","","330–341","","5","33","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QWUEXQSE","journalArticle","2015","Momose, Tomohiro; Otani, Makoto; Hashimoto, Masami; Kayama, Mizue; Itoh, Kazunori","Adaptive Amplitude and Delay Control for Stereophonic Reproduction that Is Robust against Listener Position Variations","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17569","With stereo reproduction, sound images are correctly located when the listener is located in a small area, called the sweet spot. When the listener is laterally away from this ideal location, the observed interaural level differences, interaural time differences, and interaural phase differences do not match the listener’s assumptions because of unequal distances from the listener to the respective loudspeakers. In order to provide better localization beyond the sweet spot, the proposed system detects the listener’s position to adaptively control the amplitude ratio and delay difference of the two loudspeakers. Results of subjective experiments using the proposed method demonstrate that sound images are localized more accurately than without such a system.","2015","2023-07-12 07:15:26","2023-07-19 04:33:07","","90–98","","1/2","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ED6V8A7U","journalArticle","2011","Horner, Andrew B.; Beauchamp, James W.; So, Richard H. Y","Evaluation of Mel-Band and MFCC-Based Error Metrics for Correspondence to Discrimination of Spectrally Altered Musical Instrument Sounds","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15930","An objective measure of the subjective timbral difference between two musical sounds is a difficult problem. Mel-band-based metrics and single mel-frequency ceptral coefficient were considered as a way of improving results obtained from previous methods based on harmonic and critical-band error metrics. Results indicate that timbral discrimination is determined by the first 5 to 10 harmonics rather than the broad spectral envelope. More sophisticated methods do not offer significant advantage.","2011","2023-07-12 07:15:30","2023-07-19 04:06:36","","290–303","","5","59","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PL67YHVB","journalArticle","2019","Howie, Will; Martin, Denis; Kim, Sungyoung; Kamekawa, Toru; King, Richard","Effect of Audio Production Experience, Musical Training, and Age on Listener Performance in 3D Audio Evaluation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20696","Well executed listening tests often require a great deal of time and resources for the creation or acquisition of appropriate stimuli, design of a testing interface, selection of subjects, and implementation of the experiment in an environment that is acoustically and technologically appropriate. The use of experienced, trained, or practiced subjects in listening tests has been shown in numerous previous studies to allow for a reduction in the number of subjects necessary to gather consistent, meaningful data. This study examined the effect of audio production experience, musical training, technical ear training, age, and previous experience listening to 3D music recordings on listener performance within the context of 3D audio evaluation. The results showed: (a) audio production was the most valuable type of previous experience for predicting listener consistency in making preference or ranking judgments; (b) music training was also found to be a good predictor of subject consistency; (c) technical ear training and previous experience hearing 3D music recordings had no influence on listener consistency; and (d) subjects in their early to mid 30s appear to occupy an optimal age range in terms of ability to focus on the types of listening test tasks described in this study. Stimuli used in this study were limited to orchestral music.","2019","2023-07-12 07:15:34","2023-07-19 04:07:03","","782–794","","10","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MUNRS8ML","journalArticle","2018","Lai, Wen-Hsing; Liang, Sen-Fu","Control of Fundamental Frequency Fluctuation in Taiwanese Singing Synthesis","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19565","This paper analyzes the pitch fluctuations of different notes in Taiwanese singing in order to build an F0 note-type based control model that improves the naturalness of Taiwanese synthesized singing voice by producing the more natural F0 contours. The factors that significantly differentiate singing synthesis from speech synthesis must be taken into consideration when designing a singing synthesizer. Among these, the fundamental frequency (F0) contour is an important feature that deeply affects singing voice perception and needs to be controlled precisely. The F0 contour contains fluctuations instead of a predefined stepwise pitch curve derived from musical notes. These fluctuations are important features that should be taken into consideration in singing-related applications such as singing synthesis, singing voice detection, performance analysis, singing/music recognition, singing style identification, and query-by humming. Overshoot percentage and preparation percentage are proposed to solve the problems of determining the fluctuation extent. Statistics for each note category were established from a corpus of Taiwanese nursery rhymes. Different extents of the overshoot and preparation of separate categories of notes for males, females, and children were modeled according to the statistic results. A PID controller that controls a second-order system is proposed to quickly adjust to the correct F0 level of notes and remain sufficiently steady at the correct F0 level to produce a pleasant singing voice.","2018","2023-07-12 07:15:43","2023-07-19 04:17:02","","343–359","","5","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QDEU2MBQ","journalArticle","2010","Moore, Alastair H.; Tew, Anthony I.; Nicol, Rozenn","An Initial Validation of Individualized Crosstalk Cancellation Filters for Binaural Perceptual Experiments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15240","Delivering binaural stimuli with loudspeakers through crosstalk filters avoids the intrinsic artifacts of using headphones in localization experiments. However, one must first demonstrate that such a system is equivalent to that of a real soundfield. This study demonstrates that listeners did not perceive any meaningful difference between a real sound source at 0 degrees and a virtual rendering using crosstalk cancellation from a pair of loudspeakers at ±90 degrees. Three different stimuli were used: single bursts of wideband noise, click trains, and repeated harmonic pulses. Listeners could not discriminate between the two cases using a forced-choice paradigm.","2010","2023-07-12 07:15:47","2023-07-19 04:33:29","","36–45","","1/2","58","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J68RWW9C","journalArticle","2018","Allan, Jon; Berg, Jan","Evaluating Live Loudness Meters from Engineers’ Actions and Resulting Output Levels","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19706","Loudness discrepancies in television and radio frequently produce listener annoyance. Variations in loudness can be traced to the use of quasi-PPM based audio level meters in conjunction with different amounts of compression dynamics. A satisfactory live loudness meter should assist the engineer to: (a) achieve the recommended target level for a program, (b) compensate for content-dependent delimited offsets in loudness, and (c) compensate for fast changes in loudness. This paper investigates how the ballistic properties of live loudness meters affect the engineers’ actions with fader position and the resulting output levels. In order to explore the quality of loudness meters, the researchers simulated a live broadcast show with mixing engineers who had different degrees of experience. The resulting output levels were analyzed and interpreted using a linear mixing model. The results showed that the meters with the slower integration times produced less dispersion of output levels for parts of the program. Varying integration times of the meters did not cause a significant difference in the reaction time.","2018","2023-07-12 07:16:04","2023-07-19 03:35:38","","556–577","","7/8","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y3FD22IU","journalArticle","2006","Berg, Jan; Rumsey, Francis","Identification of Quality Attributes of Spatial Audio by Repertory Grid Technique","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13681","The evaluation of the perceived spatial quality of an audio system has become more important as the technical possibilities to render spatial information increase. In recent years the field of spatial quality evaluation has been the subject of more thorough investigations than previously, one of the problems being the development of attribute scales appropriate for this purpose. The generation of attributes of spatial audio by means of elements from the repertory grid technique is investigated. In an experiment personal constructs in the form of verbal descriptors were elicited. The constructs were classified by verbal protocol analysis and reduced to a limited number of attributes by cluster analysis. The results show that the repertory grid technique enables a number of attributes of spatial sound quality to be extracted from a group of subjects and that these attributes correspond well with attributes found in other studies. The results also indicate the importance of a definition of the part—the whole or a subset—of the auditory scene to which a specific attribute is referring.","2006","2023-07-12 07:16:08","2023-07-19 03:41:57","","365–379","","5","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VC48JRR8","journalArticle","2015","Satongar, Darius; Pike, Chris; Lam, Yiu W.; Tew, Anthony I.","The Influence of Headphones on the Localization of External Loudspeaker Sources","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18043","When validating systems that use headphones to synthesize virtual sound sources, a direct comparison between virtual and real sources is sometimes needed but the method can be difficult to implement. Often, the listener must wear the headphones throughout the experiment, which will affect the sound transmission from the external loudspeakers to the ears. An analysis of the physical measurements highlighted the that headphones cause a measurable spectral error in HRTF. A maximum spectral ILD distortion of 26.52 dB was found for the close-back headphones. In a localization study, head movement data was used to obtain judgement profiles that showed participants took 0.2 s longer to reach their final judgements and used 0.1 more head-turns. The authors recommend care when choosing headphones for scenarios in which a listener is presented with external acoustic sources. Results for different headphone designs highlight that the use of electrostatic transducers could help maintain natural acoustical perception.","2015","2023-07-12 07:16:12","2023-07-19 04:46:48","","799–810","","10","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q92NDRBS","journalArticle","1989","Komiyama, Setsu","Subjective Evaluation of Angular Displacement between Picture and Sound Directions for HDTV Sound Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=6094","Several recent studies have suggested that ordinary stereophonic systems are not sufficient for HDTV use. These investigations have noted the localization error between picture and sound as a reason, but have not studied the extent to which this phenomenon causes viewer annoyance. The psychological experiment described was designed to investigate the acceptable extent of angular displacement between visual and auditory images for on-axis viewing. The results show that it is about 11 degrees for acoustic engineers and 20 degrees for the members of the general audience. In addition, the number of frontal channels in HDTV is discussed.","1989","2023-07-12 07:16:22","2023-07-19 04:15:58","","210–214","","4","37","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PJA7VF5K","journalArticle","2020","Howie, Will; Martin, Denis; Kim, Sungyoung; Kamekawa, Toru; King, Richard","Effect of Skill Level on Listener Performance in 3D Audio Evaluation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20894","A previous experiment (Part 1) found that, within the context of 3D audio evaluation, both audio production experience and musical training were significant predictors of listener consistency in making preference or attribute rating judgments of stimuli. In that study, 72 subjects ranging from highly experienced to na ¨ive listeners evaluated an excerpt of orchestral music captured by three different 3D music-recording techniques. Using the same data set from Part 1, the current study (Part 2) examines whether the results of skilled listeners can be generalized to the larger population of unskilled listeners within the context of 3D audio evaluation. Results show no significant changes in the rank order of recording technique attribute ratings or preferences as a function of listener skill. Results also show that using highly skilled participants will result in gains in sta- tistical power. This allows for the detection of subtler differences between stimuli or greater efficiency in the number of trials needed to achieve a significant result.","2020","2023-07-12 07:16:28","2023-07-19 04:07:11","","628–637","","9","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U5VFZXAI","journalArticle","2002","Larsen, Erik; Aarts, Ronald M.","Reproducing Low-Pitched Signals through Small Loudspeakers","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=11088","Ever since the invention of the electrodynamic loudspeaker there has been a need for greater acoustical output, especially at low frequencies. From a manufacturer's point of view it has been desirable for a long time to reduce the size of the loudspeaker (and cabinet). These two demands are physically contradictory. Options are being offered to evoke the illusion of a higher low-frequency response of the loudspeaker while the power radiated by the loudspeaker at those low frequencies remains the same, or is even lower. This is feasible by exploiting certain psychoacoustic phenomena. The required nonlinear signal processing is studied for a number of specific implementations. An elaborate analysis of the outcome of a listening test, aimed at assessing the subjective evaluation of the system presented, employing multidimensional scaling and biplots, is also presented.","2002","2023-07-12 07:16:37","2023-07-19 04:17:30","","147–164","","3","50","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PMR97R8C","journalArticle","1983","Penkov, Georgi","Power and Real Signals in an Audio System","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=4572","Power requirements for the amplifiers and drivers in a one-, two-, or three-channel active loudspeaker system have been determined by analyzing the moving-power maxima in 70 sequences of recorded musical and speech signals for different time constants and frequency bands. It was found that the distribution of these maxima follows a double exponential law. A computer program was used to process the data.","1983","2023-07-12 07:16:44","2023-07-19 04:37:36","","423–429","","6","31","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6TQDFSWW","journalArticle","2021","Rovithis, Emmanouel; Moustakas, Nikolaos; Vogklis, Konstantinos; Drossos, Konstantinos; Floros, Andreas","Design Recommendations for a Collaborative Game of Bird Call Recognition Based on Internet of Sound Practices","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21544","Citizen Science aims to engage people in research activities on important issues related to their well-being. Smart Cities aim to provide them with services that improve the quality of their life. Both concepts have seen significant growth in the last years and can be further enhanced by combining their purposes with Internet of Things technologies that allow for dynamic and large-scale communication and interaction. However, exciting and retaining the interest of participants is a key factor for such initiatives. In this paper we suggest that engagement in Citizen Science projects applied on Smart Cities infrastructure can be enhanced through contextual and structural game elements realized through augmented audio interactive mechanisms. Our inter-disciplinary framework is described through the paradigm of a collaborative bird call recognition game, in which users collect and submit audio data that are then classified and used for augmenting physical space. We discuss the Playful Learning, Internet of Audio Things, and Bird Monitoring principles that shaped the design of our paradigm and analyze the design issues of its potential technical implementation.","2021","2023-07-12 07:16:47","2023-07-19 04:42:21","","956–966","","12","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BIHDZTAI","journalArticle","2014","Künzel, Hermann J.; Alexander, Paul","Forensic Automatic Speaker Recognition with Degraded and Enhanced Speech","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17136","Various types of noise and other forms of degradation in the acoustic signal are typical of speech recordings used in forensic speaker recognition. The results of this study suggest that certain speech enhancement algorithms can be a useful tool for preprocessing speech samples before attempting automated recognition. This is particularly true for additive noise such as instrumental music and noise inside of a moving car. Comparing equal-error rates of identification experiments for ten male speakers based on the original, degraded, and enhanced voice signals, the performance of the speaker recognition system was most affected by pop music in both single-channel and 2-channel recordings. In contrast, road traffic and restaurant noise do not markedly degrade recognition performance.","2014","2023-07-12 07:17:12","2023-07-19 04:16:44","","244–253","","4","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LBE8KWPB","journalArticle","1993","Nielsen, Søren H.","Auditory Distance Perception in Different Rooms","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=6982","A series of listening experiments was carried out to investigate how auditory distance is perceived in different rooms-an anechoic room, an IEC listening room, and a classroom. Loudspeakers playing back a voice signal were placed in the rooms. The results show that in normal rooms the sound is perceived at about the same distance as the physical distance, regardless of the playback level. In the anechoic room there is no correspondence between physical and perceived distance.","1993","2023-07-12 07:17:16","2023-07-19 04:34:58","","755–770","","10","41","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"44LX9TXI","journalArticle","2022","Hayes, Ben; Saitis, Charalampos; Fazekas, György","Disembodied Timbres: A Study on Semantically Prompted FM Synthesis","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21740","Disembodied electronic sounds constitute a large part of the modern auditory lexicon, but research into timbre perception has focused mostly on the tones of conventional acoustic musical instruments. It is unclear whether insights from these studies generalize to electronic sounds, nor is it obvious how these relate to the creation of such sounds. This work presents an experiment on the semantic associations of sounds produced by FM synthesis with the aim of identifying whether existing models of timbre semantics are appropriate for such sounds. A novel experimental paradigm, in which experienced sound designers responded to semantic prompts by programming a synthesizer, was applied, and semantic ratings on the sounds they created were provided. Exploratory factor analysis revealed a five-dimensional semantic space. The first two factors mapped well to the concepts of luminance, texture, and mass. The remaining three factors did not have clear parallels, but correlation analysis with acoustic descriptors suggested an acoustical relationship to luminance and texture. The results suggest that further inquiry into the timbres of disembodied electronic sounds, their synthesis, and their semantic associations would be worthwhile and that this could benefit research into auditory perception and cognition and synthesis control and audio engineering.","2022","2023-07-12 07:17:19","2023-07-19 04:04:13","","373–391","","5","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"APNUEAFS","journalArticle","2023","Willemsen, Silvin; Nuijens, Helmer; Lasickas, Titas; Serafin, Stefania","The Sonic Interactions in Virtual Environments (SIVE) Toolkit","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22143","In this paper, the Sonic Interactions in Virtual Environments (SIVE) toolkit, a virtual reality (VR) environment for building musical instruments using physical models, is presented. The audio engine of the toolkit is based on finite-difference time-domain (FDTD) methods and works in a modular fashion. The authors show how the toolkit is built and how it can be imported in Unity to create VR musical instruments, and future developments and possible applications are discussed.","2023","2023-07-12 07:17:22","2023-07-19 10:55:51","","363–373","","6","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8C4F49DD","journalArticle","2015","Baumgartner, Robert; Majdak, Piotr","Modeling Localization of Amplitude-Panned Virtual Sources in Sagittal Planes","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17842","Vector-base amplitude panning (VBAP) aims at creating virtual sound sources at arbitrary directions within 3D multichannel sound reproduction systems. However, VBAP does not consistently produce listener-specific monaural spectral cues that are essential for localization of sound sources in sagittal planes, including the front-back and up-down dimensions. In order to better understand the limitations of VBAP, a functional model approximating human processing of spectro-spatial information was applied to assess accuracy in sagittal-plane localization of virtual sources created by means of VBAP. The model predicted a strong dependence on listeners’ individual head-related transfer functions, on virtual source directions, and on loudspeaker arrangements. In general, simulations showed a systematic degradation with increasing polar-angle span between neighboring loudspeakers.","2015","2023-07-12 07:17:35","2023-07-19 03:39:16","","562–569","","7/8","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4G4WZLJL","journalArticle","2016","Romblom, David; Depalle, Philippe; Guastavino, Catherine; King, Richard","Diffuse Field Modeling Using Physically-Inspired Decorrelation Filters and B-Format Microphones: Part I Algorithm","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18127","A reverberant diffuse sound field is characterized by incoherent energy arriving from all directions and is perceptually described as an auditory event that is heard everywhere. It is common practice for sound recording engineers to use differing microphone strategies for the direct and diffuse fields. While there are a variety of techniques to record and reproduce point sources, a systematic tool for diffuse sound fields does not exist. Diffuse Field Modeling (DFM) is a physically-inspired method for approximating a diffuse field in order to create a natural-sounding room effect for arbitrary loudspeaker configurations. It is intended to function in parallel with point source techniques. Using a statistical description of reverberation, the decorrelation filters in DFM are based on physical acoustics, and the resulting diffuse fields are validated with simulations incorporating the Kirchhoff/Helmholtz Integral. The resulting diffuse fields have the expected spatial autocorrelation, and the channels of the array have the expected frequency-dependent correlation. The filters can be tuned to introduce random variation that has physically-plausible frequency autocorrelation, which strongly influences the spatial impression.","2016","2023-07-12 07:17:39","2023-07-19 04:41:47","","177–193","","4","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"52WBCGCH","journalArticle","2009","Cobos, Maximo; Lopez, Jose J.","Resynthesis of Sound Scenes on Wave-Field Synthesis from Stereo Mixtures Using Sound Source Separation Algorithms","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14808","Because the vast majority of musical recordings are preserved in two-channel stereo format, special upconverters are required in order to use advanced spatial reproduction formats, such as wave-field synthesis. This paper evaluates the subjective quality of synthesized acoustic scenes when using virtual sources that were extracted as separate tracks from stereo mixes. Although wave-field synthesis has its own artifacts, the degradation produced by using separated sources includes timbre modification, burbling sounds, musical noise, and intersource residuals. However, masking effects make these artifacts less perceptible when the entire scene is being reproduced.","2009","2023-07-12 07:17:42","2023-07-19 03:48:57","","91–110","","3","57","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5XT4KJA2","journalArticle","2016","Mo, Ronald; So, Richard H. Y.; Horner, Andrew","An Investigation into How Reverberation Effects the Space of Instrument Emotional Characteristics","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18533","Previous research has shown that musical instruments have distinctive emotional characteristics and that these characteristics can be significantly changed with reverberation. This research examines if the changes in character are relatively uniform or dependent on the instrument. A comparison of eight sustained instrument tones with different amounts and lengths of simple parametric reverberation over eight emotional characteristics was performed. The results showed a remarkable consistency in listener rankings of the instruments for each of the different types of reverberation with strong correlations ranging from 90 to 95%. This indicates that the underlying instrument space for emotional characteristics does not change significantly with reverberation. Each instrument has a particular footprint of emotional characteristics. Tested instruments cluster into two fairly distinctive groups: those where the positive energetic emotional characteristics are strong (e.g., oboe, trumpet, violin), and those where the low-arousal characteristics are strong (e.g., bassoon, clarinet, lute, horn). The saxophone was an outlier, and is somewhat strong for most emotional characteristics.","2016","2023-07-12 07:17:45","2023-07-19 04:31:59","","988–1002","","12","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4AXR22AT","journalArticle","2023","Engel, Isaac; Daugintis, Rapolas; Vicente, Thibault; Hogg, Aidan O. T.; Pauwels, Johan; Tournier, Arnaud J.; Picinali, Lorenzo","The SONICOM HRTF Dataset","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22128","Immersive audio technologies, ranging from rendering spatialized sounds accurately to efficient room simulations, are vital to the success of augmented and virtual realities. To produce realistic sounds through headphones, the human body and head must both be taken into account. However, the measurement of the influence of the external human morphology on the sounds incoming to the ears, which is often referred to as head-related transfer function (HRTF), is expensive and time-consuming. Several datasets have been created over the years to help researcherswork on immersive audio; nevertheless, the number of individuals involved and amount of data collected is often insufficient for modern machine-learning approaches. Here, the SONICOM HRTF dataset is introduced to facilitate reproducible research in immersive audio. This dataset contains the HRTF of 120 subjects, as well as headphone transfer functions; 3D scans of ears, heads, and torsos; and depth pictures at different angles around subjects' heads.","2023","2023-07-12 07:17:48","2023-07-19 03:54:44","","241–253","","5","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DBHTVMPY","journalArticle","2015","Hendrickx, Etienne; Paquier, Mathieu; Koehl, Vincent","Audiovisual Spatial Coherence for 2D and Stereoscopic-3D Movies","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18049","In movie theaters, sound sources such as dialog are often reproduced on the center loudspeaker without regard to the visual position on screen. Some sound engineers and researchers have suggested that spatial audiovisual coherence could improve the audience experience, especially for stereoscopic-3D (s-3D) movies. In the experiment described, subjects were asked to judge the suitability of several soundtracks for eight s-3D sequences. Depending on the soundtrack, sound sources could be more or less coherent in azimuth and depth. Results showed that sound suitability could be significantly improved for most of the sequences when coherence in azimuth was achieved. An improvement in the experience of depth was only observed with one sequence. When sequences were presented in nonstereoscopic (2D) version, there was no significant effect of stereoscopy. Subjects quickly became accustomed to azimuthal coherence, which improved sound suitability throughout the experiment. This suggests that the audience adaptation to a new cinematographic convention regarding spatialization of sound objects would not be a burden.","2015","2023-07-12 07:17:51","2023-07-19 04:05:04","","889–899","","11","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JHVR7NRY","journalArticle","1984","Staffeldt, Henrik","Measurement and Prediction of the Timbre of Sound Reproduction","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=4499","Based on experimental observations, models are derived which make possible the measurement and prediction of the timbre produced by a sound-reproducing system. The importance of using ear-related measuring techniques is emphasized if conformity of sound reproduction from different sound systems is the aim.","1984","2023-07-12 07:17:58","2023-07-19 04:49:01","","410–414","","6","32","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DZVFLB6R","journalArticle","2021","Riionheimo, Janne; Lokki, Tapio","Movie Sound, Part 1: Perceptual Differences of Six Listening Environments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21016","The soundtracks of movies are composed and mixed in various listening environments and the final mix is reproduced in cinemas. The variation of electroacoustical properties between the rooms could be significant, and mixes do not translate easily from one location to another. This study aims to elicit the audible differences between six different movie listening environments, which are auralized to an anechoic listening room with 45 loudspeakers. A listening test was performed to determine the attributes that describe the alterations in the sound field between the rooms. Experienced listeners formulated a vocabulary and created an attribute set containing 19 descriptive attributes. The most important attribute was the sense of space when dialogue was evaluated. Moreover timbre and especially brightness were important when music was evaluated. Furthermore, the change of width and clarity of the sound field was considered important.","2021","2023-07-12 07:18:07","2023-07-19 04:40:45","","54–67","","1/2","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J7FNHF3T","journalArticle","1964","Olson, Harry F.","The RCA Victor DYNAGROOVE System","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=754","The RCA Victor DYNAGROOVE system is a comprehensive system of improvements in sound recording by means of disc records. All aspects of the process are taken into consideration, starting with the artist's conception of the music and ending with the reproduction of the sound in the listener's home.","1964","2023-07-12 07:18:11","2023-07-19 04:36:07","","98–114","","2","12","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9J72TBGK","journalArticle","2014","Borowiak, Adam; Reiter, Ulrich; Svensson, U. Peter","Momentary Quality of Experience: Users’ Audio Quality Preferences Measured Under Different Presentation Conditions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17135","Subjective evaluation of audio, video, and audiovisual material is typically performed in a static manner, after the sample has finished. This way, the possible temporal variability of the stimulus quality and its impact on quality perception are ignored. In this study, a new technique for momentary quality assessment was used in order to investigate the effect of content coherence/continuity as well as the influence of video stream on audio quality preferences. Obtained results show that the quality requirements are lower when the presented material is played in a continuous manner than when the same clip is cut into segments that are reproduced in a random order with short pauses in between. This may mean that subjective studies that use short audio stimuli extracted from long-duration content generate results that exaggerate the actual quality needs of consumers. Moreover, it has been shown that subjects’ quality preferences are higher when audio is played without the presence of an accompanying visual stimulus.","2014","2023-07-12 07:18:14","2023-07-19 03:44:15","","235–243","","4","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S5U9AL45","journalArticle","2020","Allan, Jon; Berg, Jan","Evaluation of the Momentary Time Scale for Live Loudness Metering","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20729","Different ballistic definitions for the momentary time scale used in live loudness measurement were evaluated. Definitions from the ITU and EBU were compared as well as a faster version of the ITU version, two asymmetric time scales and the deprecated ballistics, defined in EBU Tech 3205-E, for peak program meters. The goal was to identify the ballistics definition that would function as the best complementary tool to a short-term time scale. Engineers within the broadcast industry and students in audio technology performed an audio alignment task in a simulated live broadcast environment using one ballistics definition per trial. Fader movements and output levels were recorded. After each trial, a set of assessment scales were rated by the subjects. Some results were: a decay time constant of 250 ms yielded better representation of the low-level parts of the dynamics in the signal compared to a 400-ms time constant; the present ITU version of the momentary time scale yielded an estimated less eye fatigue; effects on the resulting output levels, related the gate in ITU-R BS.1770 in conjunction with live compensation of unadjusted audio material were shown.","2020","2023-07-12 07:18:18","2023-07-19 03:35:53","","193–222","","3","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y324SHCL","journalArticle","1988","Toole, Floyd E.; Olive, Sean E.","The Modification of Timbre by Resonances: Perception and Measurement","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=5163","Resonances are fundamental to the production of musical pitch and timbre. they are also the principle source of coloration when they are added in the processes of sound recording and reproduction. The traditional problem in the design and evaluation of audio products has been to find the measurements necessary to recognize the presence of a resonance, the interpretation necessary to characterize its audibility, and the judgment of how much its form must be modified in order for it not to cause objectionable coloration. A review of previous work and new experimental results describe the thresholds of audibility of resonances as a function of frequency. Q, relative amplitude, time delay, program material, listener hearing performance, loudspeaker directivity, and terms of the measured amplitude and time responses of the systems through which the audio signal is passed. While the emphasis is on reproduced sound, there are some interesting relationships to the perceived timbre of sound in live performances.","1988","2023-07-12 07:18:21","2023-07-19 04:53:39","","122–142","","3","36","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"35MGLUIZ","journalArticle","1991","Macpherson, Ewan A.","A Computer Model of Binaural Localization for Stereo Imaging Measurement","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=5969","A binaural localization model has been developed for measuring the stereo imaging properties of recording and playback techniques. Ear signals from a dummy head are passed through a model of the inner ear, and interaural time and amplitude differences are extracted and processed to give image location and diffuseness. Experiments show that the model emulates human performance under anechoic and reverberant conditions.","1991","2023-07-12 07:18:25","2023-07-19 04:25:51","","604–622","","9","39","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4WW39QMA","journalArticle","2016","Reiss, Joshua D.","A Meta-Analysis of High Resolution Audio Perceptual Evaluation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18296","Over the last decade, there has been considerable debate over the benefits of recording and rendering high resolution audio beyond standard CD quality audio. This research involved a systematic review and meta-analysis (combining the results of numerous independent studies) to assess the ability of test subjects to perceive a difference between high resolution and standard (16 bit, 44.1 or 48 kHz) audio. Eighteen published experiments for which sufficient data could be obtained were included, providing a meta-analysis that combined over 400 participants in more than 12,500 trials. Results showed a small but statistically significant ability of test subjects to discriminate high resolution content, and this effect increased dramatically when test subjects received extensive training. This result was verified by a sensitivity analysis exploring different choices for the chosen studies and different analysis approaches. Potential biases in studies, effect of test methodology, experimental design, and choice of stimuli were also investigated. The overall conclusion is that the perceived fidelity of an audio recording and playback chain can be affected by operating beyond conventional resolution.","2016","2023-07-12 07:18:28","2023-07-19 04:40:27","","364–379","","6","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2TRH4KIL","journalArticle","2006","Neher, Tobias; Brookes, Tim; Rumsey, Francis","A Hybrid Technique for Validating Unidimensionality of Perceived Variation in a Spatial Auditory Stimulus Set","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13675","Signal-processing algorithms that are meant to evoke a certain subjective effect often have to be perceptually equalized so that any unwanted artifacts are, as far as possible, eliminated. They can then be said to exhibit “unidimensionality of perceived variation.” Aiming to design a method that allows unidimensionality of perceived variation to be verified, established sensory evaluation approaches are examined in terms of their suitability for detailed, undistorted profiling and hence reliable validation of an algorithm’s subjective effects. It is found that a procedure combining multidimensional scaling with supplementary verbal elicitation constitutes the most appropriate approach. In the context of validating a signal-processing method intended to produce a specific spatial effect, this procedure is evaluated and some shortcomings are identified. However, following refinements, it is concluded that these can be overcome through additional data collection and analysis, resulting in a multistage hybrid validation technique.","2006","2023-07-12 07:18:32","2023-07-19 04:34:40","","259–275","","4","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CY9GW9VL","journalArticle","2021","Böhm, Christoph; Ackermann, David; Weinzierl, Stefan","A Multi-channel Anechoic Orchestra Recording of Beethoven’s Symphony No. 8 op. 93","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21011","For the quality of model-based, virtual acoustic environments, not only the room acoustic simulation but also the quality and suitability of the source material play an important role. An optimal recording of real sound sources is characterized by an anechoic production and a high signal-to-noise ratio and crosstalk attenuation between the different recording channels. Furthermore a recording in the far field of the source is necessary to use correct directivities for room acoustic simulations. From an artistic point of view, the recording situation with its technical boundary conditions must be designed in a way that the musical or vocal rendering of professional performers is impaired as little as possible. To provide a high-quality source signal for acoustic simulations of orchestral content, a professional symphony orchestra was recorded in the anechoic chamber of TU Berlin performing the 8th Symphony of L. v. Beethoven. Through a combination of groupwise and sequential recordings with individual monitor mixes via headphones and video recordings of the conductor and concertmaster, an optimal compromise was sought with regard to artistic and technical aspects. The article presents the recording process and processing chain as well as the results achieved with respect to technical and artistical quality criteria.","2021","2023-07-12 07:18:36","2023-07-19 03:43:15","","977–984","","12","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T7DVPIRC","journalArticle","2018","Woodcock, James; Davies, William J.; Melchior, Frank; Cox, Trevor J.","Elicitation of Expert Knowledge to Inform Object-Based Audio Rendering to Different Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19375","Object-based audio (OBA) is an approach to sound storage, transmission, and reproduction whereby individual audio objects contain associated metadata information that is rendered at the client side of the broadcast chain. For example, metadata may indicate the object’s position and the level or language of a dialogue track. An experiment was conducted to investigate how content creators perceive changes in perceptual attributes when the same content is rendered to different systems and how they would change the mix if they had control of it. The main aims of this experiment were to identify a small number of the most common mix processes used by sound designers when mixing object-based content to loudspeaker systems with different numbers of channels and to understand how the perceptual attributes of OBA content changes when it is rendered to different systems. The goal is to minimize perceived changes in the context of standard Vector Base Amplitude Panning and matrix-based downmixes. Text mining and clustering of the content creators’ responses revealed 6 general mix processes: the spatial spread of individual objects, EQ and processing, reverberation, position, bass, and level. Logistic regression models show the relationships between the mix processes, perceived changes in perceptual attributes, and the rendering method/speaker layout. The relative frequency of different mix processes was found to differ among categories of audio object, suggesting that any downmix rules should be object category specific. These results give insight into how OBA can be used to improve listener experience.","2018","2023-07-12 07:18:39","2023-07-19 10:56:56","","44–59","","1/2","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ED86SUT2","journalArticle","1999","Savioja, Lauri; Huopaniemi, Jyri; Lokki, Tapio; Väänänen, Ritta","Creating Interactive Virtual Acoustic Environments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12095","The theory and techniques for virtual acoustic modeling and rendering are discussed. The creation of natural sounding audiovisual environments can be divided into three main tasks: sound source, room acoustics, and listener modeling. These topics are discussed in the context of both non-real-time and real-time virtual acoustic environments. Implementation strategies are considered, and a modular and expandable simulation software is described.","1999","2023-07-12 07:18:45","2023-07-19 04:46:56","","675–705","","9","47","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7GLQ43HN","journalArticle","1985","Fielder, Louis D.","Pre- and Postemphasis Techniques as Applied to Audio Recording Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=4432","Audio recorders benefit from pre- and postemphasis, which reshapes the noise spectrum to match human audibility thresholds. A 10-dB increase in apparent dynamic range is realized for some digital audio systems. A first-order boost based on the CCITT J.17 preemphasis standard is shown to be appropriate for dynamic range expansion. A survey of peak acoustic levels present in 36 music performances is also included.","1985","2023-07-12 07:18:51","2023-07-19 03:57:37","","649–658","","9","33","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HJZE5SWN","journalArticle","2019","Rocamora, Martín; Cancela, Pablo; Biscainho, Luiz W.P.","Information Theory Concepts Applied to the Analysis of Rhythm in Recorded Music with Recurrent Rhythmic Patterns","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20449","This paper proposes a novel approach for rhythmic analysis of recorded percussion music based on information theory. Given an audio recording of a percussion music performance, the algorithm computes a lossy representation that captures much of its underlying regularity but tolerates some amount of distortion. Within a rate-distortion theory framework, the trade-off between rate and distortion allows for the extraction of some relevant information about the performance. Downbeat detection is addressed using lossy coding of an accentuation feature under rate-distortion criteria assuming the correct alignment produces the simplest explanation for the data. Experiments were conducted in order to assess the usefulness of the proposed approach when applied to a dataset of candombe drumming audio recordings. In particular, different performances were compared according to a measure of their overall complexity drawn from the operational rate-distortion curve, yielding results that roughly correspond to subjective judgment and correlate well with personal style and expertise.","2019","2023-07-12 07:18:55","2023-07-19 04:41:03","","160–173","","4","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RBNED2DK","journalArticle","2017","Frank, Matthias; Sontacchi, Alois","Case Study on Ambisonics for Multi-Venue and Multi-Target Concerts and Broadcasts","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19204","Ambisonics is a production format that can be used for 3D audio. It is based on the representation of the sound field by decomposing it into orthonormal basis functions, called spherical harmonics. This representation allows for a flexible production process that is independent of the target playback system, be it loudspeakers or headphones. The concert night at the International Conference on Spatial Audio 2015 employed the Ambisonics format to distribute the concert to different venues and broadcasts in real time: a concert venue with a 29-channel loudspeaker system, a monitor venue with a 25-channel loudspeaker system, an outside broadcasting van with a 23-channel loudspeaker system, as well as a 5.1 mixdown and a binaural headphone mixdown for national terrestrial and satellite radio broadcasting. This flexibility is achieved by the computation of suitable decoder matrices and weightings. The success of the sound system at ICSA 2015 encouraged the authors to carry out a live 3D concert with Al Di Meola in July 2016 that included spatial real-time effects and transmission to a neighboring venue.","2017","2023-07-12 07:18:59","2023-07-19 04:00:53","","749–756","","9","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DTPD2H29","journalArticle","2012","Lindau, Alexander; Kosanke, Linda; Weinzierl, Stefan","Perceptual Evaluation of Model- and Signal-Based Predictors of the Mixing Time in Binaural Room Impulse Responses","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16633","When creating virtual acoustic environments, the computational demands can be reduced by using generic late reverberation. Beyond the “mixing time,” the diffuse reverberation no longer contains details of the specific location. Therefore, a perceptually validated model for predicting the mixing time of different spaces will be helpful. This study evaluates various predictors of the perceptual mixing time using 9 different spaces. Both model- and signal-based estimators of mixing time were examined for their ability to predict the results of a group of expert listeners. For a shoebox-shaped room, the average perceptual mixing time can be predicted by the enclosure’s ratio of volume over surface area V/S and by vV, which serve as indicators of the mean free path length and the reflection density, respectively. Moreover, the “echo density profile” by Abel and Huang (AES paper 6985) can be used to predict the perceptual mixing time from measured data.","2012","2023-07-12 07:19:07","2023-07-19 04:20:25","","887–898","","11","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2TQTJBR9","journalArticle","2023","Doma, Shaimaa; Ermert, Cosima A.; Fels, Janina","A Magnitude-Based Parametric Model Predicting the Audibility of HRTF Variation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22038","This work proposes a parametric model for just noticeable differences of unilateral differences in head-related transfer functions (HRTFs). For seven generic magnitude-based distance metrics, common trends in their response to inter-individual and intra-individual HRTF differences are analyzed, identifying metric subgroups with pseudo-orthogonal behavior. On the basis of three representative metrics, a three-alternative forced-choice experiment is conducted, and the acquired discrimination probabilities are set in relation with distance metrics via different modeling approaches. A linear model, with coefficients based on principal component analysis and three distance metrics as input, yields the best performance, compared to a simple multi-linear regression approach or to principal component analysis--based models of higher complexity.","2023","2023-07-12 07:19:10","2023-07-19 03:53:32","","155–172","","4","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VMF435V4","journalArticle","1993","Horner, Andrew; Beauchamp, James; Haken, Lippold","Methods for Multiple Wavetable Synthesis of Musical Instrument Tones","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=7003","Spectrum matching of musical instrument tones is a fundamental problem in computer music. Two methods are presented for determining near-optimal parameters for the synthesis of harmonic musical instrument orvoice sounds using the addition of several fixed wavetables with time-varying weights. The overall objective is to find wavetable spectra and associated amplitude envelopes which together provide a close fit to an original time-varying spectrum. Techniques used for determining the wavetable spectra include a genetic algorithm (GA) and principal components analysis (PCA). In one study a GA was used to select spectra from the original signal at various time points. In another study PCA was used to obtain a set of orthogonal basis spectra for the wavetables. In both cases, least-squares solution is utilized to determine the associated amplitude envelopes. Both methods provide solutions which converge gracefully to the original as the number of tables is increased, but three to five wavetables frequently yield a good replica of the original sound. For the three instruments we analyzed, a trumpet, a guitar, and a tenor voice, the GA method seemed to offer the best results, especially when less than four wavetables were used. Comparative results using the methods are discussed and illustrated.","1993","2023-07-12 07:19:20","2023-07-19 04:06:44","","336–356","","5","41","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NKSELW2D","journalArticle","1972","Cable, Cecil R.","Acoustics and the Active Enclosure","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=2019","When a sound reinforcement is introduced into any passive acoustic enclosure, it becomes an integral element of the total acoustical network. Factors governing the -acoustics- become more controllable. Planning of the enclosure may profitably include the active elements to optimize the total network. Simplified formulas and a ready reckoner provide means to predict the final realization with greater accuracy.","1972","2023-07-12 07:19:22","2023-07-19 03:46:15","","823–826","","10","20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EAWL6HCB","journalArticle","1965","Clark, Melville; Luce, David","Intensities of Orchestral Instrument Scales Played at Prescribed Dynamic Markings","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=1209","The intensities of scales played on nonpercussive orchestral instruments at various dynamic markings and the resulting dynamic ranges are reported. We are able to deduce the size of quanta corresponding to one step in a dynamic marking and the numer of quanta in the dynamic range of a musical instrument using deviations from smoothed curves. The average dynamic range of the nonpercussive orchestral instruments is about 11 db between the dynamic markings of pianissimo and fortissimo; the average intensity level at 10 meters is about 59 db re 0.0002 dynes/cm/2.","1965","2023-07-12 07:19:33","2023-07-19 03:48:36","","151–157","","2","13","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BCHHZEZR","journalArticle","1985","Toole, Floyd E.","Subjective Measurements of Loudspeaker Sound Quality and Listener Performance","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=4465","With adequate attention to the details of experiment design and the selection of participants, listening tests on loudspeakers yielded sound-quality ratings that were both reliable and repeatable. Certain listeners differed in the consistency of their ratings and in the ratings themselves. These differences correlated with both hearing threshold levels and age. Listeners with near normal hearing thresholds showed the smallest individual variations and the closest agreement with each others. Sound-quality ratings changed as a function of the hearing threshold level and age of the listener. The amount and direction of the change depended upon the specific products; some products were rated similarly by all listeners, whereas others had properties that caused them to be rated differently. Stereophonic and monophonic tests yielded similar sound-quality ratings for highly rated products, but in stereo, listeners tended to be less consistent and less critifal of products with distinctive characteristics. Assessments of stereophonic spatial and image qualities were closely related to sound-quality ratings. The relationship between these results and objective performance data is being pursued.","1985","2023-07-12 07:19:40","2023-07-19 04:53:21","","2–32","","1/2","33","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K7QGM9UM","journalArticle","2013","Igumbor, Osedum P.; Foss, Richard","Control Protocol Command Translation for Device Interoperability on Ethernet AVB Networks","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16707","Audio/Video Bridging (AVB) on Ethernet refers to a suite of standards that allows for deterministic and guaranteed delivery of audio and video content on Virtual Local Area Networks (VLAN). A requirement for the networking of audio devices is to allow for remote establishment and destruction of audio stream connections. There are a number of sound control protocols that utilize AVB transport, and this paper describes an approach for common control and interoperability among them. To demonstrate this approach, this paper describes the design and implementation of a command translator that enables communication between AES64 and OSC-networked AVB devices. Results from a quantitative analysis reveal that when a proxy is used for connection management, noticeable delays are not added to a user’s visual and auditory perception.","2013","2023-07-12 07:19:54","2023-07-19 04:07:46","","224–234","","4","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ESFUNTRP","journalArticle","2009","Stone, Michael A.; Moore, Brian C. J.; Füllgrabe, Christian; Hinton, Andrew C.","Multichannel Fast-Acting Dynamic Range Compression Hinders Performance by Young, Normal-Hearing Listeners in a Two-Talker Separation Task","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14833","Because of the well-known benefits of dynamic range compression, sound engineers routinely use this tool as a normal part of their professional activities. Hearing aid designers also use compression. However, compression can impede perception of independent sound sources in a complex aural environment. When normal young listeners were given the task of recognizing keywords from two simultaneous talkers, their performance worsened when compression was applied to the mixed signal. Even when performance remained unchanged, the cognitive load, as measured using a secondary task, increased significantly. The results indicate that excessive compression can lead to increased listening effort.","2009","2023-07-12 07:19:58","2023-07-19 04:49:52","","532–546","","7/8","57","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UZ58CT84","journalArticle","2022","Liu, Lulu; Xie, Bosun","A High-Frequency–Band Timbre Equalization Method for Transaural Reproduction With Two Frontal Loudspeakers","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21549","Non-ideal conditions in actual transaural reproduction, such as a slightly off-central listening position and reflections in the listening room, can impair the perfect reconstruction of binaural pressures and give rise to perceived timbre coloration. In the present work, a high-frequency band equalization method is proposed to further reduce the timbre coloration in transaural reproduction with two frontal loudspeakers. The high-frequency responses of a pair of transaural filters are equalized by a frequency-dependent factor so that the overall power spectra of the responses remain constant, and the low-frequency responses of transaural filters are kept intact. An analysis using Moore's revised loudness model indicates that the proposed method reduces the deviation between the binaural loudness level spectra in transaural reproduction and those of the target sound source. A further psychoacoustic experiment validates that the proposed method reduces the timbre coloration in transaural reproduction without introducing an obvious perceivable directional distortion for virtual source in the frontal quadrants of the horizontal plane.","2022","2023-07-12 07:20:01","2023-07-19 04:21:07","","36–49","","1/2","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YT6NGJ5H","journalArticle","2006","Weisser, Adam; Rindel, Jens Holger","Evaluation of Sound Quality, Boominess, and Boxiness in Small Rooms","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13688","The acoustics of small rooms have been studied with emphasis on sound quality, boominess, and boxiness when the rooms are used for speech or music. Seven rooms with very different characteristics were used for the study. Subjective listening tests were made using binaural recordings of studio-produced music and anechoic speech. The test results were compared with a large number of objective acoustic parameters based on the frequencydependent reverberation times (RT) measured in the rooms. This has led to the proposal of three new acoustic parameters, which have shown high correlation with the subjective ratings. The classical bass ratio definitions showed poor correlation with all subjective ratings. The overall sound quality ratings gave different results for speech and music. For speech the preferred mean RT should be as low as possible, whereas for music a preferred range of between 0.3 and 0.5 s was found.","2006","2023-07-12 07:20:05","2023-07-19 10:54:48","","495–511","","6","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G7KYFK9Y","journalArticle","2018","Baird, Alice; Jørgensen, Stina Hasse; Parada-Cabaleiro, Emilia; Cummins, Nicholas; Hantke, Simone; Schuller, Björn","The Perception of Vocal Traits in Synthesized Voices: Age, Gender, and Human Likeness","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19393","As computer-generated voice synthesis has become a significant part of communications between computers and people, there is a need to understand the role of paralinguistic attributes of the voice, such as age, personality, and gender. In many cases, the synthesized voice is produced by concatenating segments of recorded human speech, which can be experienced as a lifeless voice that lacks free expression and fluidness. Technology companies have been developing their own unique synthesized voice identities without paying attention to the stereotypical traits being heard. This study evaluated the responses of 18 listeners who were asked to consider the paralinguistic traits of age, gender, and human likeness from 13 voices in IBM’s Watson corpus. The results of this study were similar to a previous study, with no voice achieving complete human likeness, no voice being perceived within a single age frequency band, and none tied solidly to their given binary gender.","2018","2023-07-12 07:20:08","2023-07-19 03:38:20","","277–285","","4","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T72GBYXK","journalArticle","2023","Lee, Ben; Rudzki, Tomasz; Skoglund, Jan; Kearney, Gavin","Context-Based Evaluation of the Opus Audio Codec for Spatial Audio Content in Virtual Reality","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22037","This paper discusses the evaluation of Opus-compressed Ambisonic audio content through listening tests conducted in a virtual reality environment.The aim of this studywas to investigate the effect that Opus compression has on the Basic Audio Quality (BAQ) of Ambisonic audio in different virtual reality contexts---gaming, music, soundscapes, and teleconferencing. The methods used to produce the test content, how the tests were conducted, the results obtained and their significance are discussed. Key findings were that in all cases, Ambisonic scenes compressed with Opus at 64 kbps/ch using Channel Mapping Family 3 garnered a median BAQ rating not significantly different than uncompressed audio. Channel Mapping Family 3 demonstrated the least variation in BAQ across evaluated contexts, although there were still some significant differences found between contexts at certain bitrates and Ambisonic orders.","2023","2023-07-12 07:20:14","2023-07-19 04:18:15","","145–154","","4","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LHDPM8QH","journalArticle","2011","George, Sunish; Zielinski, Slawomir; Rumsey, Francis; Jackson, Philip; Conetta, Robert; Dewhirst, Martin; Meares, David; Bech, Søren","Development and Validation of an Unintrusive Model for Predicting the Sensation of Envelopment Arising from Surround Sound Recordings","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15743","An objective prediction model for the sensation of sound envelopment in five-channel reproduction is important for evaluating spatial quality. Regression analysis was used to map the listening test scores on a variety of audio sources and the objective measures extracted from the recordings themselves. By following an iterative process, a prediction model with five features was constructed. The validity of the model was tested in a second set of subjective scores and showed a correlation coefficient of 0.9. Among the five features: sound distribution and interaural cross-correlation contributed substantially to the sensation of envelopment. The model did not require access to the original audio. Scales used for listening tests were defined by audible anchors.","2011","2023-07-12 07:20:20","2023-07-19 04:02:07","","1013–1031","","12","58","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YVTJIZ5J","journalArticle","2015","Lee, Hyunkook","2D-to-3D Ambience Upmixing based on Perceptual Band Allocation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18044","3D multichannel audio systems employ additional elevated loudspeakers in order to provide listeners with a vertical dimension to their auditory experience. Listening tests were conducted to evaluate the feasibility of a novel vertical upmixing technique called “perceptual band allocation (PBA),” which is based on a psychoacoustic principle of vertical sound localization, the “pitch height” effect. The practical feasibility of the method was investigated using 4-channel ambience signals recorded in a reverberant concert hall using the Hamasaki-Square microphone technique. Results showed that the PBA-upmixed 3D stimuli were significantly stronger than or similar to 9-channel 3D stimuli in 3D listener-envelopment (LEV), depending on the sound source and the crossover frequency of PBA. They also significantly produced greater 3D LEV than the 7-channel 3D stimuli. For the preference tests, the PBA stimuli were significantly preferred over the original 9-channel stimuli.","2015","2023-07-12 07:20:26","2023-07-19 04:18:24","","811–821","","10","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KTZJYHC9","journalArticle","2022","Agrawal, Sarvesh; Bech, Søren; De Moor, Katrien; Forchhammer, Søren","Influence of Changes in Audio Spatialization on Immersion in Audiovisual Experiences","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22009","Understanding the influence of technical system parameters on audiovisual experiences is important for technologists to optimize experiences. The focus in this study was on the influence of changes in audio spatialization (varying the loudspeaker configuration for audio rendering from 2.1 to 5.1 to 7.1.4) on the experience of immersion. First, a magnitude estimation experiment was performed to perceptually evaluate envelopment for verifying the initial condition that there is a perceptual difference between the audio spatialization levels. It was found that envelopment increased from 2.1 to 5.1 reproduction, but there was no significant benefit of extending from 5.1 to 7.1.4. An absolute-rating experimental paradigm was used to assess immersion in four audiovisual experiences by 24 participants. Evident differences between immersion scores could not be established, signaling that a change in audio spatialization and subsequent change in envelopment does not guarantee a psychologically immersive experience.","2022","2023-07-12 07:20:32","2023-07-19 03:34:32","","810–823","","10","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NZ3HYX63","journalArticle","2013","Kim, Chungeun; Mason, Russell; Brookes, Tim","Head Movements Made by Listeners in Experimental and Real-Life Listening Activities","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16833","Understanding the way in which listeners move their heads must be part of any objective model for evaluating and reproducing the sonic experience of space. Head movement is part of the listening experience because it allows for sensing the spatial distribution of parameters. In the first experiment, the head positions of subjects was recorded when they were asked to evaluate perceived source location, apparent source width, envelopment, and timbre of synthesis stimuli. Head motion was larger when judging source width than when judging direction or timbre. In the second experiment, head movement was observed in natural listening activities such as concerts, movies, and video games. Because the statistics of movement were similar to that observed in the first experiment, laboratory results can to be used as the basis of an objective model of spatial behavior. The results were based on 10 subjects.","2013","2023-07-12 07:20:35","2023-07-19 04:11:32","","425–438","","6","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YY6VTWW2","journalArticle","2013","Rohr, Lukas; Corteel, Etienne; Nguyen, Khoa-Van; Lissek, Hervé","Vertical Localization Performance in a Practical 3-D WFS Formulation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17077","An experimental study investigated the performance of an innovative Wave Field Synthesis (WFS) system in terms of a listener’s ability to localize sound sources in the median plane. Performance was measured by localization accuracy, precision and response time under a variety of conditions that included two seating positions, five levels of elevations, and two spatial precision settings. Localization precision was 6°– 9° with only 24 loudspeakers in the WFS system covering the frontal quarter of the upper half sphere of the listening space. Localization performance was good in comparison to other studies with denser loudspeaker arrays or with other reproduction techniques. The implemented 3-D WFS technique is a serious alternative to other state-of-the-art spatialization methods.","2013","2023-07-12 07:20:38","2023-07-19 04:41:30","","1001–1014","","12","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DYSURFGX","journalArticle","1992","Jason, M. Raymond","Design Considerations for Loudspeaker Preference Experiments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=7018","An overview is presented of some theoretical and practical problems in the experimental design of preference-based listening tests on loudspeakers. Measurement scales, statistical design techniques, and physical, procedural, and psychological variables are considered. Some consequences of present limitations in observation and theory are discussed. Specific improvements in design strategy are suggested.","1992","2023-07-12 07:20:41","2023-07-19 04:08:22","","979–996","","12","40","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9XEL6WI5","journalArticle","2021","Arend, Johannes M.; Brinkmann, Fabian; Pörschmann, Christoph","Assessing Spherical Harmonics Interpolation of Time-Aligned Head-Related Transfer Functions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21019","High-quality spatial audio reproduction over headphones requires head-related transfer functions (HRTFs) with high spatial resolution. However, acquiring datasets with a large number of (individual) HRTFs is not always possible, and using large datasets can be problematic for real-time applications with limited resources. Consequently, interpolation methods for sparsely sampled HRTFs are of great interest, with spherical harmonics (SH) interpolation becoming increasingly popular. However, the SH representation of sparse HRTFs suffers from spatial aliasing and order truncation errors. To mitigate this, preprocessing methods have been introduced that time-align the sparse HRTFs before SH interpolation. This reduces the effective SH order and thus the number of HRTFs required for SH interpolation. In this paper, we present a physical evaluation of four state-of-the-art preprocessing methods, which showed very similar performance of the methods with notable differences only at low SH orders and contralateral HRTFs. We also performed a listening experiment with one selected method to determine the minimum required SH order required for perceptually transparent interpolation. For the selected method, a sparse HRTF set of order N ˜ 7 is sufficient for interpolating a frontal source presenting speech or percussion. Higher orders are, however, required for a lateral source and noise.","2021","2023-07-12 07:20:44","2023-07-19 03:37:11","","104–117","","1/2","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9RK8QBNN","journalArticle","2005","Knox, Don; Bailey, Nicholas; Stewart, Iain","A Simple Hybrid Approach to the Time-Scale Modification of Speech","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13429","Time-domain methods of time-scale modification (TSM) are attractive from the point of view of computational effort. However, they suffer from audible artifacts for larger timestretch ratios (greater than 1.3 times the original duration). The occurrence of these artifacts is often the main justification for the use of more involved analysis/synthesis methods at these ratios. For speech signals these artifacts take the form of transient repetition—causing a “stuttering” effect and roughness due to spectral mismatch at segment boundaries—most obvious during voiced signal periods. These phenomena are not addressed by existing timedomain methods. A simple hybrid algorithm utilizing both time-domain and analysis/synthesis methods is presented which illustrates how these distortions may be minimized. Results of formal listening tests illustrate an improvement in basic audio quality for timestretched speech signals when compared to equivalent samples processed by the synchronized overlap and add (SOLA) algorithm.","2005","2023-07-12 07:20:51","2023-07-19 04:15:41","","612–619","","7/8","53","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D7P65JGV","journalArticle","2018","Mäkivirta, Aki; Liski, Juho; Välimäki, Vesa","Modeling and Delay-Equalizing Loudspeaker Responses","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19869","This paper focuses on the modeling of the linear properties of loudspeakers. The impulse response of a generalized multi-way loudspeaker is modeled and delay-equalized using digital filters. The dominant features of a loudspeaker are its low- and high-frequency roll-off characteristics and its behavior at the crossover points. The proposed loudspeaker model also characterizes the main effects of the mass-compliance resonant system. The impulse response, its logarithm and spectrogram, and the magnitude and group-delay responses are visualized and compared with those measured from a high-quality two-way loudspeaker. The model explains the typical local group-delay variations and magnitude-response deviations from a flat response in the passband. The group-delay equalization of a three-way loudspeaker is demonstrated with three different methods. Time-alignment of the tweeter and midrange elements using a bulk delay is shown to cause ripple in the magnitude response. The frequency-sampling method for the design of an FIR group-delay equalizer is detailed and is used to flatten the group delay of the speaker model in both the whole and limited audio range. The full-band equalization is shown to lead to preringing in the impulse response. In contrast, group-delay equalization at mid- and high-frequencies only reduces the length of the loudspeaker impulse response without introducing preringing.","2018","2023-07-12 07:20:54","2023-07-19 04:26:25","","922–934","","11","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N7RU7ATZ","journalArticle","2016","Sunder, Kaushik; Gan, Woon-Seng","Individualization of Head-Related Transfer Functions in the Median Plane using Frontal Projection Headphones","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18536","Using nonindividualized HRTFs in virtual audio synthesis produces front-back confusions, up-down reversals, in-head localization, and timbral coloration. Elevation and frontal localization are found to be most affected. In contrast, obtaining individualized HRTFs is a tedious process that involves complex acoustical measurements for each individual. Having a model of HRTF that does not involve tedious acoustical measurements would make the process much easier. In this research, individualization of the median plane HRTFs is explored using frontal projection headphones with a spherical head model because the frontal positioning of the headphone transducer inherently captures the idiosyncratic frontal spectral cues. To create the HRTFs, the important peaks (P1) and notches (N1, N2) are extracted first from the frontal headphone response and then shifted in frequency in accordance with the elevation angle. Detailed subjective experiments indicated that subjects were able to localize the virtual sound sources accurately with modeled HRTFs with results similar to individualized HRTFs.","2016","2023-07-12 07:20:57","2023-07-19 04:50:48","","1026–1041","","12","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SZBBZHZ6","journalArticle","2001","Czerwinski, Eugene; Voishvillo, Alexander; Alexandrov, Sergei; Terekhov, Alexander","Multitone Testing of Sound System Components'Some Results and Conclusions, Part 2: Modeling and Application","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=10168","An historical retrospective analysis of the measurement of nonlinearities in audio is carried out. A quantitative analysis of the responses of various nonlinear systems (theoretical and experimental) to a multitone signal is made, and multitone testing is compared to conventional harmonic and intermodulation measurements. The multitone test provides more accurate information about the behavior of nonlinear systems when compared to standard harmonic, two-tone intermodulation, and total harmonic distortion measurements. Modeling of the nonlinear reaction of various sound system components to a multitone signal is described.","2001","2023-07-12 07:21:01","2023-07-19 03:51:18","","1181–1192","","12","49","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NSF3AICZ","journalArticle","1994","Han, H. L.","Measuring a Dummy Head in Search of Pinna Cues","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=6964","Extensive measurements were performed on a dummy head to investigate how the pinnae encode directional information. Transfer functions and impulse responses were examined to identify the features that contribute to localization. The reults suggest that near-vertical spectral edges play a major role. This finding sheds a new light on the relationship between the characteristics of the external car and the results of earlier psychophysical experiments by Blauert and by Hebrank and Wright.","1994","2023-07-12 07:21:04","2023-07-19 04:03:20","","15–37","","1/2","42","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LVY9ZCUA","journalArticle","1995","Oomen, Werner; Groenewegen, Marc E.; van der Waal, Robbert G.; Veldhuis, Raymond","A Variable-Bit-Rate Buried-Data Channel for Compact Disc","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=7963","Recently an elegant method was published to add buried data to a CD signal in a compatible way. This method is based on subtractively dithered noise-shaped quantization and provides a fixed-rate buried-data channel. An adaptive extension to this method, resulting in a variable rate of higher average value, is described.","1995","2023-07-12 07:21:08","2023-07-19 04:36:26","","23–28","","1/2","43","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3Z78F8BF","journalArticle","1969","Klipsch, Paul W.","Modulation Distortion in Loudspeakers","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=1593","When comparing a loudspeaker with direct radiator bass system to one with horn loaded bass, the subjective judgment is that the one with the horn loaded bass is -cleaner.- The difference in listening quality appears to be due to modulation distortion. The mathematical analysis of modulation distortion is reviewed and spectrum analyzer measurements are described which have been correlated with listening tests. The spectrum analyses corroborate the mathematical analysis and the listening tests offer a subjective evaluation. It is concluded that frequency modulation in loudspeakers accounts in large measure for the masking of -inner voices.- Reduction of diaphragm excursions at low frequencies reduces FM distortion. Horn loading, properly applied, offers greatest reduction, while simultaneously improving bass power output capability.","1969","2023-07-12 07:21:12","2023-07-19 04:15:31","","194, 196, 198, 200, 202, 204, 206","","2","17","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3PXY92IU","journalArticle","2018","Agus, Natalie; Anderson, Hans; Chen, Jer-Ming; Lui, Simon; Herremans, Dorien","Minimally Simple Binaural Room Modeling Using a Single Feedback Delay Network","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19862","The widespread adoption of acoustic modeling in such applications as 3D gaming and virtual reality simulation are generally hindered by the complexity of the implementation. The most efficient binaural acoustic modeling systems use a multi-tap delay to generate early reflections, which are then combined with a feedback delay network to produce generic late reverberation. This report presents a method of binaural acoustic simulation that uses one feedback delay network to simultaneously model both first-order reflections and late reverberation. The advantages are simplicity and efficiency. The proposed method is compared to existing methods for modeling binaural early reflections using a multi-tap delay line. Measurements of ISO standard evaluators including interaural correlation coefficient, decay time, clarity, definition, and center time indicate that the proposed method achieves a comparable accuracy to less efficient methods. The proposed method is implemented as an iOS application, and is able to auralize input signal directly without convolution and update in real time. This significantly reduces the computational time because it does not need to produce an impulse response after every parameter update.","2018","2023-07-12 07:21:15","2023-07-19 03:35:10","","791–807","","10","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9ERQQ7G8","journalArticle","1999","Preis, Douglas; Georgopoulos, Voula Chris","Wigner Distribution Representation and Analysis of Audio Signals: An Illustrated Tutorial Review","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12082","The Wigner distribution provides a visual disply of quantitative information about how a signal's energy is distributed in both time and frequency. Through its low-order moments the Wigner distribution embodies the fundamentally important concepts of both Fourier analysis and time-domain analysis. Signal energy is distributed in such a way that specific frequencies are localized in time by the group-delay time (from classical filter Theory), and at specific instants in time the frequency is given by the instanteous frequency (from classical modulation theory). The energy spectrum (energy per frequency) and the instantaneous power (energy per time) are specified by the zero-order moments of the distribution. The net positive volume of the Wigner distribution is numerically equal to the signal's total energy. While the theoretical underpinnings of the Wigner distibution are mathematically elegant and do merit in-depth study, a substantial amount of practical insignt, understanding, and interpretive skill can be gained by carefully examining a wide variety of computed Wigner distributions such as those of the audio signals presented.","1999","2023-07-12 07:21:19","2023-07-19 04:39:01","","1043–1053","","12","47","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y9D5TQR9","journalArticle","2006","Corteel, Etienne","Equalization in an Extended Area Using Multichannel Inversion and Wave Field Synthesis","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13892","Wave field synthesis (WFS) targets the synthesis of the physical characteristics of a sound field in an extended listening area. This synthesis is, however, accompanied by noticeable reconstruction artifacts. They are due to both loudspeaker radiation characteristics and approximations to the underlying physical principles. These artifacts may introduce coloration, which must be compensated for over the entire listening area. Multichannel equalization techniques allow for the control of the sound field produced by a loudspeaker array at a limited number of positions. The control can be extended to a large portion of space by employing a new method that combines multichannel equalization with a linear microphone array–based description of the sound field and accounts for WFS rendering characteristics and limitations. The proposed method is evaluated using an objective coloration criterion. Its benefits compared to conventional equalization techniques are pointed out for both ideal omnidirectional loudspeakers and multi-actuator panels.","2006","2023-07-12 07:21:22","2023-07-19 03:50:03","","1140–1161","","12","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D6MSAC8W","journalArticle","2020","Agrawal, Sarvesh; Simon, Adèle; Bech, Søren; Bæntsen, Klaus; Forchhammer, Søren","Defining Immersion: Literature Review and Implications for Research on Audiovisual Experiences","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20857","The use of the term immersion to describe a multitude of varying experiences in the absence of a definitional consensus has obfuscated and diluted the term. The non-exhaustive literature review presented in this paper indicates that immersion is a psychological concept as opposed to being a property of the system or technology that facilitates an experience. An adaptable definition of immersion is synthesized based on the findings from the literature review: a state of deep mental in- volvement in which the individual may experience disassociation from the awareness of the physical world due to a shift in their attentional state. This definition is used to contrast and differentiate interchangeably used terms such as presence from immersion and outline the implications for conducting immersion research on audiovisual experiences. A new methodology for quantifying immersion is proposed and avenues for future work are briefly discussed.","2020","2023-07-12 07:21:31","2023-07-19 03:34:48","","404–417","","6","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UXB9JZ6V","journalArticle","2022","Gareis, Michael; Maas, Jürgen","Buckling Dielectric Elastomer Transducers as Loudspeakers","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22012","In recent decades, dielectric elastomers (DE) have emerged as a promising transducing principle for various applications. They promise to be lightweight, efficient, and affordable alternatives to conventional electrodynamic or piezoelectric transducers and show large deformations at fast rates. In this work a loudspeaker concept is proposed, which relies on the elastic instability of a DE membrane. A multilayered DE membrane is clamped in a circular ring. Upon applying a DC voltage, its area increases, and themembrane buckles up. A superimposed signal voltage induces vibration and generates sound. To model the device mechanically, a system of partial differential equations is derived from Hamilton's principle. The mechanical model is then coupled to the linear assumed electrical and acoustical domains. Static, dynamic, and acoustic experiments on buckling DE transducers of three different diameters (10, 15, and 20 mm) and different thicknesses (0.4mmto 0.6 mm) as multilayer configurations are conducted to validate the model. Sound pressure levels of about 70 dB above 1 kHz are reached. Small loudspeakers like this may find application in mobile or array systems.","2022","2023-07-12 07:21:35","2023-07-19 04:01:27","","858–870","","10","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VBZUU85T","journalArticle","2015","Conetta, Robert; Brookes, Tim; Rumsey, Francis; Zielinski, Slawomir; Dewhirst, Martin; Jackson, Philip; Bech, Søren; Meares, David; George, Sunish","Spatial Audio Quality Perception (Part 1): Impact of Commonly Encountered Processes","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17557","Spatial audio processes (SAPs) commonly encountered in consumer audio reproduction systems are known to produce a range of impairments to spatial quality. By way of two listening tests, this paper investigated the degree of degradation of the spatial quality of six 5-channel audio recordings resulting from 48 such SAPs. Perceived degradation also depends on the particular listeners, the program content, and the listening location. For example, combining off-center listener with another SAP can reduce spatial quality significantly when compared to listening to that SAP from a central location. The choice of the SAP can have a large influence on the degree of degradation. Taken together these findings and the quality-annotated database can guide the development of a regression model of perceived overall spatial audio quality, incorporating previously developed spatially-relevant feature-extraction algorithms. The results can guide the development of an artificial-listener-based evaluation system.","2015","2023-07-12 07:21:45","2023-07-19 03:49:34","","831–846","","12","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XY9L8WJB","journalArticle","2020","Poirier-Quinot, David; Katz, Brian F.G.","Assessing the Impact of Head-Related Transfer Function Individualization on Task Performance: Case of a Virtual Reality Shooter Game","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20731","This paper presents the results of an extended experiment to assess the impact of individualized binaural rendering on player performance in an ecologically valid use context, specifically that of a VR “shooter game,” as part of a larger project to characterize the impact of binaural rendering quality in various VR type applications. Participants played a simple game in which they were faced with successive targets approaching from random directions on a sphere. While audio-visual cues allowed for general target localization, only sections of the game that relied on audio cues were used for analysis. Two HRTF exposure protocols were used, comprising best and worst-match HRTFs from a “perceptually orthogonal” optimized set of HRTFs, during the course of six game sessions. Two groups performed the game sessions exclusively using either their best or worst-match HRTF. Two additional groups performed the game sessions alternating between best and worst-match HRTFs. Results suggest that HRTF quality had minimal general impact on in-game participant performance and improvement rate. However, performance for extreme elevation target positions was affected by the quality of HRTF matching. In addition, a subgroup of participants showed higher sensitivity to HRTF choice than others.","2020","2023-07-12 07:21:48","2023-07-19 04:38:27","","248–260","","4","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"86SW6Y3D","journalArticle","2010","Bispo, Bruno C.; Esquef, Paulo A. A.; Biscainho, Luiz W. P.; Lima, Amaro A. de; Freeland, Fabio P.; Jesus, Rafael A. de; Said, Amir; Lee, Bowon; Schafer, Ronald W.; Kalker, Ton","EW-PESQ: A Quality Assessment Method for Speech Signals Sampled at 48 kHz","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15252","In order to broaden the utility of objective methods for perceptual evaluation of ultrawide-band (sampled at 48 kHz) speech, two extensions to the W-PESQ standard are proposed. In one approach the psychoacoustic model of W-PESQ is expanded to cover higher frequencies by means of data extrapolation. In the alternative method the psychoacoustic model is replaced with that of PEAQ. A performance analysis of both methods reveals that their predictions strongly correlate with measured mean opinion scores (MOS), bearing a cross-correlation coefficient around 97%. Tests used speech signals corrupted with white and broad-band environmental noises.","2010","2023-07-12 07:21:52","2023-07-19 03:42:22","","251–268","","4","58","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MWG7FIZS","journalArticle","2008","Herre, Jürgen; Kjörling, Kristofer; Breebaart, Jeroen; Faller, Christof; Disch, Sascha; Purnhagen, Heiko; Koppens, Jeroen; Hilpert, Johannes; Rödén, Jonas; Oomen, Werner; Linzmeier, Karsten; Chong, Kok Seng","MPEG Surround-The ISO/MPEG Standard for Efficient and Compatible Multichannel Audio Coding","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14643","Finalized in 2006, the MPEG Surround specification enables the transmission of multichannel audio signals at data rates close to those of one- and two-channel systems. This paper describes the technical architecture and capabilities of the specification. Verification tests include several operational modes as they would be used in typical application scenarios. In order to achieve backward compatibility with legacy devices that are not compliant with MPEG Surround, the spatial side information is embedded in either an ancillary part of the downmix bit stream or in a separate stream. This approach is vastly superior to a matrixed system at the comparable data rates.","2008","2023-07-12 07:22:09","2023-07-19 04:05:24","","932–955","","11","56","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"28ZS78DE","journalArticle","2000","Karjalainen, Matti; Välimäki, Vesa; Penttinen, Henri; Saastamoinen, Harri","DSP Equalization of Electret Film Pickup for the Acoustic Guitar","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12032","The response of a guitar bridge pickup does not correspond to the acoustic radiation of the instrument. Digital signal processing is applied to obtain high-quality acoustic sound using only an elastic electret film pickup and properly designed digital filtering for equalization. The most promising results are obtained with long FIR filters with possibly one or more body resonances modeled with separate digital resonators, or with warped IIR filters.","2000","2023-07-12 07:22:13","2023-07-19 04:10:23","","1183–1193","","12","48","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4VCED7LL","journalArticle","2015","Tervo, Sakari; Pätynen, Jukka; Kaplanis, Neofytos; Lydolf, Morten; Bech, Søren; Lokki, Tapio","Spatial Analysis and Synthesis of Car Audio System and Car Cabin Acoustics with a Compact Microphone Array","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18051","This research proposes a spatial sound analysis and synthesis approach for automobile sound systems, where the acquisition of the measurement data is much faster than with the Binaural Car Scanning method. This approach avoids the problems that are typically found with binaural reproduction and dummy head measurements. In combination with a compact microphone array, the approach is based on the recently introduced parametric spatial sound analysis method, called the Spatial Decomposition Method (SDM). An objective analysis of the sound field with respect to direction and energy enables the synthesis of multichannel loudspeaker reproduction. Because of the extreme acoustics of an automobile cabin, the authors recommend several steps to improve both the objective and perceptual performance. Because SDM is a parametric approach to spatial impulse response analysis, this allows automobile audio systems to be tuned in a laboratory environment instead of in-situ.","2015","2023-07-12 07:22:16","2023-07-19 04:52:34","","914–925","","11","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WJLA4MVZ","journalArticle","2000","Tan, Chong-Jin; Gan, Woon-Seng","Direct Concha Excitation for Introduction of Individualized Hearing Cues","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12055","Nonindividualized head-related transfer functions (HRTFs) are known to cause significant front-back and elevation confusions when used for three-dimensional sound systems. In addition headphones tend to induce in-head localization and rear perceptual bias. A method of concha excitation is proposed as a means of introducing individual cues without the need of individual HRTF measurements, which may overcome some of these difficulties. A prototype headphone was designed and tested.","2000","2023-07-12 07:22:23","2023-07-19 04:51:33","","642–653","","7/8","48","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VD49PBYN","journalArticle","2022","Ragano, Alessandro; Benetos, Emmanouil; Hines, Andrew","Automatic Quality Assessment of Digitized and Restored Sound Archives","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21562","Archiving digital audio is conducted to preserve and make records accessible. However techniques for assessing the quality of experience (QoE) of sound archives are usually neglected. This paper presents a framework to assess the QoE of sound archives in an automatic fashion. The QoE influence factors, stakeholders, and audio archive degradations are described, and the above concepts are explored through a case study on the NASA Apollo audio archive. Each component of the framework is described in the audio archive life cycle based on digitization, restoration, and consumption. Insights and real-world examples are provided on why digitized and restored audio archives benefit from QoE assessment techniques similar to other multimedia applications, such as video calling and streaming services. The reasons why stakeholders, such as archivists, broadcasters, or public listeners, would benefit from the proposed framework are also provided.","2022","2023-07-12 07:22:26","2023-07-19 04:39:52","","252–270","","4","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DAVXAF94","journalArticle","2001","Müller, Swen; Massarani, Paulo","Transfer-Function Measurement with Sweeps","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=10189","Transfer-function measurements using sweeps as excitation signals rather than pseudo-noise signals show significantly higher immunity against distortion and time variance. Capturing binaural room impulse responses for high-quality auralization purposes requires a signal-to-noise ratio (SNR) greater that 90 dB, which is unattainable with maximum-length sequence (MLS) measurements because of loudspeaker nonlinearity, but it is fairly easy to reach with sweeps due to the possibility of complete rejection of harmonic distortion. Before investigating the differences and practical problems of measurements with MLS and sweeps and arguing why sweeps are the preferable choice for the majority of measurement tasks, the existing methods of obtaining transfer functions are reviewed. The continual need to use preemphasized excitation signals in acoustical measurements is also addressed. A method to create sweeps with arbitrary spectral contents, but constant orprescribed frequency-dependent temporal envelope, is presented. Finally, the possibility of simultaneously analyzing transfer functions and harmonics is investigated.","2001","2023-07-12 07:22:33","2023-07-19 04:34:14","","443–471","","6","49","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VF97BC3W","journalArticle","1982","Toole, Floyd E.","Listening Tests-Turning Opinion into Fact","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=3833","Listening tests of many kinds take place regularly in the audio industry. Most of them lack the necessary psychological and acoustical controls to produce results that are of real significance, yet the course of audio is regularly influenced by opinions derived from such tests. Some familiar and some not so familiar sources of variability in subjective evaluations of sound quality are reviewed, and the standardization of certain basic technical factors currently being chosen in an arbitrary manner is proposed.","1982","2023-07-12 07:22:41","2023-07-19 04:53:12","","431–445","","6","30","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JC8KTIBZ","journalArticle","2002","Fitz, Kelly; Haken, Lippold","On the Use of Time: Frequency Reassignment in Additive Sound Modeling","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=11058","A method of reassignment in sound modeling to produce a sharper, more robust additive representation is introduced. The reassigned bandwidth-enhanced additive model follows ridges in a time-frequency analysis to construct partials having both sinusoidal and noise characteristics. This model yields greater resolution in time and frequency than is possible using conventional additive techniques, and better preserves the temporal envelope of transient signals, even in modified reconstruction, without introducing new component types or cumbersome phase interpolation algorithms.","2002","2023-07-12 07:22:45","2023-07-19 03:58:10","","879–893","","11","50","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MPQY2B4J","journalArticle","1990","Wagenaars, Wil M.","Localization of Sound in a Room with Reflecting Walls","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=6050","The ability of subjects to localize sounds in a typical living room was tested for both direction and distance. A subject was seated in the center of two concentric circular arrays of eight loudspeakers, one having a 1-m and the other a 2-m radius. Eleven different stimuli were used. these were white noise and sinusoids with different temporal envelopoes, a 100-µs pulse, and music. Results show that noise, pulses, and music can be localized very well. Sinusoids with at least one abrupt transition (onset or offset) are localized reasonably well, but sinusoids with slow, 1-s onsets and offsets are localized poorly.","1990","2023-07-12 07:22:48","2023-07-19 10:54:12","","99–110","","3","38","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4B5VFXFI","journalArticle","2006","Bai, Mingsian R.; Lin, Wan-chi","Synthesis and Implementation of Virtual Bass System with a Phase-Vocoder Approach","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13888","A bass enhancement technique based on a phase-vocoder approach is presented. Instead of direct bass boosting, the proposed method creates a bass impression by exploiting the psychoacoustic properties of humans. This technique is most useful in audio reproduction using small loudspeakers that have no low-frequency capability, where direct boosting will likely result in nonlinear distortions. In light of psychoacoustics, the bass effect is synthesized by augmenting the original signals with high-frequency harmonics. Unlike conventional methods that rely on nonlinear processing, the proposed method performs the required frequency transformation by using a phase-vocoder approach. Apart from frequency transformation, another key element of the proposed technique is the magnitude adjustment of the generated harmonics. The underlying principle for magnitude adjustment is based on a polynomial model of equal-loudness contours. The method is implemented on a digital signal processor with the aid of multirate signal processing. To validate the proposed technique, objective and subjective experiments are conducted for PC multimedia loudspeakers and handset loudspeakers. The subjective listening experiment followed the procedure of multistimuli with the hidden reference and anchor (MUSHRA), and the data were analyzed by using the multianalysis of variance (MANOVA) method. As indicated by the results, the proposed technique proved effective in rendering bass impression with acceptable audio quality.","2006","2023-07-12 07:22:52","2023-07-19 03:37:59","","1077–1091","","11","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZD8Q9MWN","journalArticle","2003","Moore, Brian C. J.; Glasberg, Brian R.; Stone, Michael A.","Why Are Commercials so Loud? ' Perception and Modeling of the Loudness of Amplitude-Compressed Speech*","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12190","The level of broadcast sound is usually limited to prevent overmodulation of the transmitted signal. To increase the loudness of broadcast sounds, especially commercials, fastacting amplitude compression is often applied. This allows the root-mean-square (rms) level of the sounds to be increased without exceeding the maximum permissible peak level. In addition, even for a fixed rms level, compression may have an effect on loudness. To assess whether this was the case, we obtained loudness matches between uncompressed speech (short phrases) and speech that was subjected to varying degrees of four-band compression. All rms levels were calculated off line. We found that the compressed speech had a lower rms level than the uncompressed speech (by up to 3 dB) at the point of equal loudness, which implies that, at equal rms level, compressed speech sounds louder than uncompressed speech. The effect increased as the rms level was increased from 50 to 65 to 80 dB SPL. For the largest amount of compression used here, the compression would allow about a 58% increase in loudness for a fixed peak level (equivalent to a change in level of about 6 dB). With a slight modification, the model of loudness described by Glasberg and Moore [1] was able to account accurately for the results.","2003","2023-07-12 07:22:55","2023-07-19 04:33:37","","1123–1132","","12","51","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WGCIRMH6","journalArticle","2020","McCormack, Leo; Pulkki, Ville; Politis, Archontis; Scheuregger, Oliver; Marschall, Marton","Higher-Order Spatial Impulse Response Rendering: Investigating the Perceived Effects of Spherical Order, Dedicated Diffuse Rendering, and Frequency Resolution","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20852","This article details an investigation into the perceptual effects of different rendering strategies when synthesizing loudspeaker array room impulse responses (RIRs) using microphone array RIRs in a parametric fashion. The aim of this rendering task is to faithfully reproduce the spatial characteristics of a captured space, encoded within the input microphone array RIR (or the spherical harmonic RIR derived from it), over a loudspeaker array. For this study, a higherorder formulation of the Spatial Impulse Response Rendering (SIRR) method is introduced and subsequently employed to investigate the perceptual effects of the following rendering configurations: the spherical harmonic input order, frequency resolution, and utilizing ded- icated diffuse stream rendering. Formal listening tests were conducted using a 64-channel loudspeaker array in an anechoic chamber, where simulated reference scenarios were compared against the outputs of different methods and rendering con- figurations. The test results indicate that dedicated diffuse stream rendering and higher analysis orders both yield noticeable perceptual improvements, particularly when employing problematic transient stimuli as input. Additionally, it was found that the frequency resolution employed during rendering has only a minor influence over the perceived accuracy of the reproduc- tion in comparison to the other two tested attributes.","2020","2023-07-12 07:22:59","2023-07-19 04:29:28","","338–354","","5","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"85H7KH74","journalArticle","2021","Villegas, Julián; Fukasawa, Naoki; Arevalo, Camilo","The Presence of a Floor Improves Subjective Elevation Accuracy of Binaural Stimuli Created With Non-Individualized Head-Related Impulse Responses","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21534","We report the effect of the presence of floor on elevation estimation of audio spatialized with non-individualized Head-Related Impulse Responses (HRIRs). The results of two experiments (n = 21 and n = 39) suggest that using HRIRs captured when a floor simulator was present improved assessors' accuracy when judging elevation in the sagittal and coronal planes, especially at high elevations. Such improvements were not observed when signals delayed according to their computed first reflection were mixed with signals convolved with anechoic HRIRs. These findings suggest that capturing non-individualized HRIRs in hemi-anechoic rooms could improve accuracy of audio spatialization in virtual environments.","2021","2023-07-12 07:23:05","2023-07-19 10:53:08","","849–859","","11","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G72SE4X9","journalArticle","2017","Francombe, Jon; Brookes, Tim; Mason, Russell","Evaluation of Spatial Audio Reproduction Methods (Part 1): Elicitation of Perceptual Differences","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18555","An experiment was performed to determine the attributes that contribute to listener preference for a range of spatial audio reproduction methods. Experienced and inexperienced listeners made preference ratings for combinations of seven program items replayed over eight reproduction systems, and reported the reasons for their judgments. Automatic text clustering reduced redundancy in the responses by approximately 90%, thereby facilitating subsequent group discussions that produced clear attribute labels, descriptions, and scale end-points. Twenty-seven and twenty-four attributes contributed to preference for the experienced and inexperienced listeners respectively. The two sets of attributes contain a degree of overlap (ten attributes from the two sets were closely related); the experienced listeners used more technical terms while the inexperienced listeners used broader descriptive categories.","2017","2023-07-12 07:23:10","2023-07-19 03:58:37","","198–211","","3","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6KCN5ZR6","journalArticle","1999","Algazi, V. Ralph; Avendano, Carlos; Thompson, Dennis","Dependence of Subject and Measurement Position in Binaural Signal Acquisition","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12085","A detailed evaluation of the dependence of the head-related transfer functions (HRTFs) on the position of the recording microphone along the ear canal for blocked and unblocked conditions is presented. Measurements at 250 source locations for a KEMAR mannequin and at 125 locations for two human subjects were performed. Conditions under which the magnitude transfer function from the HRTF acquisition position to the eardrum is essentially independent of the source location are shown quantitatively. These results are in agreement with and provide an extensive validation of other studies. The cause and magnitude of measurement errors are also analyzed in detail, and it is shown that these errors are subject dependent.","1999","2023-07-12 07:23:13","2023-07-19 03:35:19","","937–947","","11","47","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QV29HNE2","journalArticle","1975","Torick, Emil L.; DiMattia, Alfred L.; Staruk, Walter E., Jr.; Milner, Paul","A Wearable Master Hearing Aid","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=2689","An experimental wearable master hearing aid device has been developed for clinical studies of methods to improve perception capabilities of subjects having varied hearing impairments. In accordance with a specification of the National Institutes of Health, a multiplicity of parameter adjustments and transducer options provides more than 500 million permutations of operational modes.","1975","2023-07-12 07:23:18","2023-07-19 04:53:49","","361–368","","5","23","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9QPVR3JM","journalArticle","2006","Ramos, Germán; López, José J.","Filter Design Method for Loudspeaker Equalization Based on IIR Parametric Filters","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13893","A novel method for the equalization of loudspeakers and other audio systems using IIR (infinite impulse response) parametric filters is presented. The main characteristic of the proposed filter design method resides in the fact that the equalization structure is planned from the beginning as a chain of SOSs (second-order sections), where each SOS is a low-pass, high-pass, or peak filter, defined by its parameters. The algorithm combines a direct search method with a heuristic parametric optimization process where constraints on the values could be imposed in order to obtain practical implementations. A psychoacoustic model based on the detection of peaks and dips in the frequency response has been employed to determine which ones need to be equalized, reducing the filter order without noticeable effect. The first computed sections of the designed filter are the ones that correct the response more effectively, allowing scalable solutions when hardware limitations exist or different degrees of correction are needed. The method has been validated with subjective testing and compared with other methods. Its results could be applied to passive and multiway active loudspeakers.","2006","2023-07-12 07:23:21","2023-07-19 04:40:01","","1162–1178","","12","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IH2YFCWB","journalArticle","2005","Loutridis, Spyros J.","Decomposition of Impulse Responses Using Complex Wavelets","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13434","The application of the continuous wavelet transform, implemented with complex wavelets, to the decomposition of loudspeaker and room impulse responses is discussed. The wavelet transform possesses adaptive time–frequency resolution and is very well suited to the analysis of transient signals. It has the important property that significant signal information is concentrated on certain regions called ridges. Applications include separation of modal components with subsequent damping estimation and low-frequency coloration detection. Wavelets form filter banks and can be designed to have any desirable filter bandwidth. Wavelet filters are used for reverberation-time estimation. It is also suggested that complex wavelets might be used for envelope extraction and calculation of the instantaneous frequency and instantaneous spectral density of a signal.","2005","2023-07-12 07:23:25","2023-07-19 04:21:26","","796–811","","9","53","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2MLI2SYB","journalArticle","1994","van de Par, Steven L.; ten Kate, Warner R.; Kohlrausch, Armin; Houtsma, Adrianus J.","Bit-Rate Saving in Multichannel Sound: Using a Band-Limited Channel to Transmit the Center Signal","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=6934","A method is proposed to achieve full-frequency-range three-channel (left, right, and center) sound reproduction in systems that have only two full-range sound channels and some band-limited commentary channels. The low-frequency part of the center signal, which matches the bandwidth of the commentary channels, is added to the (multilingual) speech signals in each of the commentary channels. The remaining high-frequency part is added in the left and right channels as in conventional mixdowns. Sound reproduction of this signal by a conventional two-channel receiver remains unaltered. The low-frequency part of the center signal is mixed to the left and right signals together with the speech once the user has selected a commentary channel. Three-channel reproduction is obtained by routing the selected commentary channel to a central loudspeaker. Listening tests revealed that sound reproduction according to the proposed scheme could not be distinguished from original three-channel reproduction. This scheme can be applied to proposed standards such as D2MAC and MPEG2.","1994","2023-07-12 07:23:28","2023-07-19 04:55:04","","555–564","","7/8","42","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6Y9GPW59","journalArticle","2012","Nikunen, Joonas; Virtanen, Tuomas; Vilermo, Miikka","Multichannel Audio Upmixing by Time-Frequency Filtering Using Non-Negative Tensor Factorization","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16553","The expanding use of portable multimedia devices has intensified the need for better forms of scalable spatial audio coding (SAC) that match the connectivity rate and multichannel playback capabilities of the receiving device. A new SAC method is based on the parameterization of multichannel audio by representing it as a linear combination of objects composed of fixed spectral bases with time-varying gain and channel-dependent spatial gain. Spatial parameters can be estimated from the original multichannel signal using psychoacoustic properties of sound source localization. The base audio can be monophonic or downmixed stereophonic. Listening tests showed that the proposed SAC algorithm achieved the performance of conventional spatial audio coding methods with similar bit rates. The sound separation performance was evaluated and found applicable for separating sound sources in the coding domain directly.","2012","2023-07-12 07:23:32","2023-07-19 04:35:06","","794–806","","10","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YS2TNLNP","journalArticle","2016","Ben Ali, Faten; Djaziri-Larbi, Sonia; Girin, Laurent","Low Bit-Rate Speech Codec Based on a Long-Term Harmonic Plus Noise Model","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18522","In speech/music coders and analysis/synthesis systems, spectral modeling is generally performed on a short-term (ST) frame-by-frame basis, which is justified by the fact that the signal is only locally (quasi-) stationary. The vocal tract configuration moves slowly and smoothly thereby resulting in a high correlation between the spectral parameters of successive frames: this correlation property is exploited in long-term modeling of the ST parameters, which however results in longer modeling/coding delays. The short delay constraint can be relaxed in many applications, such as text-to-speech modification/synthesis, telephony surveillance data, digital answering machines, electronic voicemail, digital voice logging, electronic toys, and video games. The long-term harmonic plus noise model (LT-HNM) for speech shows additional data compression possibilities since it exploits the smooth evolution of the time trajectories of the short-term harmonic plus noise model parameters by applying a discrete cosine model (DCM). In this paper, the authors extend the LT-HNM to a complete low bit-rate speech coder that is based on a long-term approach ca. 200ms. The proposed LT-HNM coder reaches a bit-rate of 2.7kbps for wideband speech.","2016","2023-07-12 07:23:35","2023-07-19 03:41:22","","844–857","","11","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HZD5VWXW","journalArticle","2003","Fielder, Louis D.","Analysis of Traditional and Reverberation-Reducing Methods of Room Equalization","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12249","Traditionally, electronic equalization has used linear filters of low complexity. The nature of spectral and temporal distortions of rooms limits useful equalization to minimum-phase filters of relatively low order, despite the existence of new and powerful digital signal processing tools. The high Q and non-minimum-phase nature of the room loudspeaker 'listener transfer function, caused by wave interference effects, creates severe problems for more complete equalization. A typical professional listening room and three cinema acoustic environments were used to investigate the difficulties inherent for more ambitious equalization approaches.","2003","2023-07-12 07:23:39","2023-07-19 03:57:54","","3–26","","1/2","51","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JMWZSV9Y","journalArticle","2018","Francombe, Jon; Brookes, Tim; Mason, Russell","Determination and Validation of Mix Parameters for Modifying Envelopment in Object-Based Audio","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19381","With object-based audio transmission, a scene is distributed as a set of audio objects, as opposed to discrete audio channels. An object comprises an audio stream for a particular aspect of the scene, accompanied by some metadata, such as the desired level and spatial position of the object. Object-based audio offers the possibility of altering the rendering of an audio scene in order to modify or maintain perceptual attributes if the relationships between attributes and mix parameters are known. This research aims to determine the relationship between parameters of an object-based mix and the perception of envelopment (an important attribute of spatial audio reproduction systems), and to develop and test a system for manipulating envelopment in object-based audio in a perceptually relevant manner. An experiment was performed in which mixing engineers were asked to create mixes of object-based content at three levels of envelopment (low, medium, and high) while keeping the overall mix quality at an acceptable level. This enabled analysis of parameter values in order to assess how participants created different levels of envelopment. It was shown in a validation experiment that these parameters could be used to adjust envelopment to a target level.","2018","2023-07-12 07:23:42","2023-07-19 04:00:16","","127–145","","3","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GPGGHDYL","journalArticle","2020","Tylka, Joseph G.; Choueiri, Edgar Y.","Performance of Linear Extrapolation Methods for Virtual Sound Field Navigation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20725","Performance errors are characterized for two representative linear extrapolation methods for virtual navigation of higher-order ambisonics sound fields. For such methods, navigation is theoretically restricted to within the so-called region of validity, which extends spherically from the recording ambisonics microphone to its nearest source, but the precise consequences of violating that restriction has not been previously established. To that end, the errors introduced by each method are objectively evaluated in terms of metrics for sound level, spectral coloration, source localization, and diffuseness, through numerical simulations over a range of valid and invalid conditions. Under valid conditions, results show that the first method, based on translating along plane-waves, accurately reproduces both the level and localization of a source, whereas the second method, based on ambisonics translation coefficients, incurs significant errors in both level and spectral content that increase steadily with translation distance. Under invalid conditions, two common features of the performance of both methods are identified: significant localization errors are introduced and the reproduced level is too low. It is argued that these penalties are inherent to all methods that are bound by the region of validity restriction, including all linear extrapolation methods.","2020","2023-07-12 07:23:50","2023-07-19 04:54:07","","138–156","","3","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3FVATVVR","journalArticle","2011","Hur, Yoomi; Abel, Jonathan S.; Park, Young-Cheol; Youn, Dae Hee","Techniques for Synthetic Reconfiguration of Microphone Arrays","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15937","Methods are presented for transforming signals from a specific microphone array into those that would have been recorded at a different array at the same location. In a nonparametric method, beams are formed at fixed directions using a low-sidelobe beamforming technique. In a parametric method, beams are formed adaptively using a direction-finding algorithm. In a hybrid method, point source signals and spatially diffuse residual signals are separately processed. Results show good agreement between measured and synthesized array outputs with signal correlation coefficients near 1.0 for all three methods. Informal listening tests confirmed effective sound field resynthesis.","2011","2023-07-12 07:23:53","2023-07-19 04:07:37","","404–418","","6","59","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X8Y4FUKB","journalArticle","2019","Ma, Xiaohui; Hohnerlein, Christoph; Ahrens, Jens","Concept and Perceptual Validation of Listener-Position Adaptive Superdirective Crosstalk Cancellation Using a Linear Loudspeaker Array","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20701","This paper presents a multiband approach for crosstalk cancellation based on superdirective near-field beamforming (SDB) that adapts dynamically to a change in the listener position. SDB requires the computation of a separate set of beamformer weights for each listener position. The beamformer uses weights that exhibit a smooth evolution for listening positions along a linear trajectory parallel to the array. The beamformer weights can therefore be parameterized by using only a few parameters for each frequency. Upon real-time execution, the beamformer weights are determined efficiently for any position from the parameters with negligible error. Simulations and measurements show that the proposed method provides high channel separation and is robust with respect to small uncertainties of the listener position. A user study with 20 subjects and binaural signals shows consistent auditory localization accuracy across the different tested listening positions that are comparable to the localization accuracy of headphone rendering. The study also confirms the previously informal observation that fewer front-back confusions are observed when the listeners face away from the loudspeaker array compared to the listeners facing toward the array.","2019","2023-07-12 07:24:16","2023-07-19 04:25:43","","871–881","","11","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9YKIWHJN","journalArticle","1996","Cao, Yuchang; Sridharan, Sridha; Moody, Miles","Co-talker Separation Using the 'Cocktail Party Effect'","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=7875","An artificial neural network (ANN) speech-classifier-controlled iterative filtering system is described, which simulates the cocktail party effect for speech separation. The ANN speech classifier controls a modified iterative Wiener filter to cancel the interference by setting the filter's parameter's and the convergence criterion for the iteration. The proposed system has been employed successfully with multiple-microphone speech acquisition systems for co-talker speech separation. The simulation results have shown that the iterative processing controlled by the neural network consistently provides speech of good quality and intelligibility.","1996","2023-07-12 07:24:23","2023-07-19 03:47:16","","1084–1096","","12","44","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"27XUI7KI","journalArticle","1959","Somerville, T.; Gilford, C. L. S.","Acoustics of Large Orchestral Studios and Concert Halls","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=558","In order to establish design criteria for orchestral studios, which should be similar in performance to good ocncert halls, an extensive investigation has been in progress for many years. The effects of shape on the subjective acoustic qualities of a large enclosure are here examined with reference to a large number of concert halls and music studios, and a comparison is made, in particular, between concert halls of the traditional type and thos which have been built during the last few decades. The former were generally of rectangular plan with walls and ceilings overlaid with ornamentation, whereas most recent designs have fan-shaped plans, reflecting canopies and comparatively smooth surfaces. Measurements of sound levels in different parts of concert halls during orchestral concerts show that, for a given sound level in the neighborhood of the platform, the intensity at the back of the hall is no greater in halls with fan-shaped plans and reflectors than with the traditional rectangular shape. In the former case, the gain in the intensity of the first few reflections which results from the shape of the hall is offset by a reduced reverberant sound level. The authors conclude that the modern fashion of directing the early reflections towards the back of a concert hall, although it may improve the hearing of speech, has an adverse effect on the quality of music.","1959","2023-07-12 07:24:33","2023-07-19 04:48:43","","160–172","","4","7","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H2IJHL2U","journalArticle","2017","Brandt, Matthias; Doclo, Simon; Gerkmann, Timo; Bitzer, Joerg","Impulsive Disturbances in Audio Archives: Signal Classification for Automatic Restoration","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19356","Historic recordings usually have degraded audio quality because of their age, improper storage, and the shortcomings of the original media. One typical problem is the presence of impulsive disturbances. Recordings that suffer from clicks and crackles can be processed by impulse-restoration algorithms to improve their audio quality. This report presents a new algorithm that classifies one-second frames of an audio recording based on the existence of impulsive disturbances. The algorithm uses supervised learning. It is shown that existing impulse-restoration algorithms suffer from degradation of the desired signal if the input SNR is high and if no manual parameter adjustment is possible. This would make automatic restoration of large amounts of diverse archive audio material unfeasible. The proposed classification algorithm can be used as a supplement to an existing impulse-restoration algorithm to alleviate this drawback. An evaluation using a large number of test signals shows that high classification accuracy can be achieved, making automatic impulse restoration possible. Results show that prewhitening of the input signal by means of a phase-only transform serves to increase the detectability of disturbance impulses, which can also be used as a detection enhancement method for impulse-restoration algorithms.","2017","2023-07-12 07:24:36","2023-07-19 03:44:33","","826–840","","10","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MVX5VVGU","journalArticle","2003","Esquef, Paulo A. A.; Biscainho, Luiz W. P.; Välimäki, Vesa","An Efficient Algorithm for the Restoration of Audio Signals Corrupted with Low-Frequency Pulses","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12221","Digital audio restoration of old recordings is addressed, with a focus on the removal of long pulses with low-frequency content. The main drawback of the state-of-the-art method, which is based on the separation of autoregressive (AR) processes, is its high computational complexity. A method is proposed in which the pulse tails are first estimated via a nonlinear scheme called two-pass split-window (TPSW) filtering, followed by a polynominal smoothing stage. After removing the tail of each pulse by subtraction, the remaining initial clicks are suppressed through a model-based declicking algorithm. The proposed procedure is as effective for pulse removal as the AR-based method but has a substantially lower computational complexity. Moreover, from a user point of view, its processing parameters are more intuitive and easier to adjust.","2003","2023-07-12 07:24:39","2023-07-19 03:54:53","","502–517","","6","51","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"24EH3F3U","journalArticle","2015","Conetta, Robert; Brookes, Tim; Rumsey, Francis; Zielinski, Slawomir; Dewhirst, Martin; Jackson, Philip; Bech, Søren; Meares, David; George, Sunish","Spatial Audio Quality Perception (Part 2): A Linear Regression Model","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17558","The QESTRAL (Quality Evaluation of Spatial Transmission and Reproduction using an Artificial Listener) system is intended to be an artificial-listener-based evaluation system capable of predicting the perceived spatial quality degradations resulting from SAPs (Spatial Audio Processes) commonly encountered in consumer audio reproduction. A generalizable model employing just five metrics and two principal components performs well in its prediction of the quality over a range of program types. Commonly-encountered SAPs can have a large deleterious effect on several spatial attributes including source location, envelopment, coverage angle, ensemble width, and spaciousness. They can also impact timbre, and changes to timbre can then influence spatial perception. Previously obtained data was used to build a regression model of perceived spatial audio quality in terms of spatial and timbral metrics. In conjunction with two simple probe signals, the resulting model can form the core of an evaluation system.","2015","2023-07-12 07:24:44","2023-07-19 03:49:44","","847–860","","12","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"38EFXB88","journalArticle","2005","Naqvi, Amber; Rumsey, Francis","The Active Listening Room. A Novel Approach to Early Reflection Manipulation in Critical Listening Rooms","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13418","A novel method of controlling reflections, using flat panel loudspeakers, in a reference listening room is described. Models and implementations are presented in the case of single-transducer monophonic reproduction as well as two-channel and five-channel stereophonic arrangements. The results of a pilot listening test showed that differences in reflection patterns were readily detected by a panel of experienced listeners.","2005","2023-07-12 07:24:51","2023-07-19 04:34:31","","385–402","","5","53","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I6IEW342","journalArticle","2007","Menzies, Dylan; Al-akaidi, Marwan","Ambisonic Synthesis of Complex Sources","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14175","Exterior expansions of complex sound sources are presented as flexible objects for producing Ambisonic sound-field encodings. The sources can be synthesized or recorded directly, rotated, and positioned in space. Related techniques can also be used to efficiently add high-quality reverberation, depending on the orientation and location of the source and listener.","2007","2023-07-12 07:24:54","2023-07-19 04:30:50","","864–876","","10","55","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PJ94JFJA","journalArticle","2002","Bech, Søren","Requirements for Low-Frequency Sound Reproduction, Part I: The Audibility of Changes in Passband Amplitude Ripple and Lower System Cutoff Frequency and Slope","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=11070","The audibility of changes in passband amplitude ripple and lower system cutoff frequency and slope has been investigated for a loudspeaker system for two situations: a real loud-speaker in an anechoic chamber and a simulated system reproduced via headphones. The signals were standard program material, selected to ensure a sufficient energy content at the relevant frequencies. The experiments were conducted with six subjects with normal hearing using a paired-comparison procedure. The subjects assessed the attributes ""lower bass"" and ""upper bass"" in relation to a fixed reference condition. The first experiment investigated the influence of high-pass filter order (second, fourth, and sixth) and lower cutoff frequency (20, 35, and 50 Hz) at three reproduction levels and for four program items. The second experiment examined the influence of amplitude ripple corresponding to four reverberation times at three reproduction levels and for four program items. The results of the first experiment showed that the lower cutoff frequency has a significant influence on the perceived level of lower bass reproduction if the reproduction level is above the hearing threshold in the relevant frequency bands. The influence of high-pass filter order was not significant for the conditions investigated. The results of the second experiment showed that the amplitude ripple has a significant influence on the perceived level of lower and upper bass reproduction. The results also showed that there were no significant differences between the data produced by the two reproduction methods.","2002","2023-07-12 07:25:02","2023-07-19 03:39:46","","564–580","","7/8","50","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"84BSFG2H","journalArticle","2023","Kirsch, Christoph; Wendt, Torben; Van De Par, Steven; Hu, Hongmei; Ewert, Stephan D.","Computationally-Efficient Simulation of Late Reverberation for Inhomogeneous Boundary Conditions and Coupled Rooms","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22040","For computational efficiency, acoustic simulation of late reverberation can be simplified by generating a limited number of incoherent signals with frequency-dependent exponential decay radiated by spatially distributed virtual reverberation sources (VRS). A sufficient number of VRS and adequate spatial mapping are required to approximate spatially anisotropic late reverberation, e.g., in rooms with inhomogeneous distribution of absorption or for coupled volumes. For coupled rooms, moreover, a dual-slope decay might be required. Here, an efficient and perceptually plausible method to generate and spatially render late reverberation is suggested. Incoherent VRS signals for (sub-) volumes are generated based on room dimensions and frequencydependent absorption coefficients at the boundaries. For coupled rooms, (acoustic) portals account for effects of sound propagation and diffraction at the room connection and energy transfer during the reverberant decay process. The VRS are spatially distributed around the listener, with weighting factors representing the spatially subsampled distribution of absorption on the boundaries and the location and solid angle covered by portals. A technical evaluation and listening tests demonstrate the validity of the approach in comparison to measurements in real rooms.","2023","2023-07-12 07:25:05","2023-07-19 04:12:30","","186–201","","4","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M67A5FHZ","journalArticle","2014","Tiemounou, Sibiri; Jeannès, Régine Le Bouquin; Ewert, Vincent Barriac","Perception-Based Automatic Classification of Background Noise in Super-Wideband Telephony","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17552","Given the effort required to perform subjective tests of quality of service in super-wideband telephone networks, an objective model of perceived degradation was created. This paper describes an efficient classification model of background noise arising at the originating side of super-wideband telephony. Classification depends on the impact on the perceived voice quality at receiving side from originating noises. This approach extends the research findings of narrowband telephony. Subjective experiments showed that background noises can be divided into three relevant classes: environmental noise, breath noise, and crackling noise. Each of these corresponds respectively to spectral flow, acoustic power variations, and the noise spectral centroid. Based on three quality degradation indicators, tests showed 82% correct classification of unknown noise signals.","2014","2023-07-12 07:25:09","2023-07-19 04:53:01","","776–781","","11","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8NUA7MYI","journalArticle","1995","Boone, Marinus M.; Verheijen, Edwin N. G.; van Tol, Peter F.","Spatial Sound-Field Reproduction by Wave-Field Synthesis","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=7920","The wave-field synthesis method is used for a natural reproduction of sound in living rooms, cinemas, and theaters. Wave-field synthesis gives a much larger listening area with correct virtual directions and spatial impression than can be obtained with conventional reproduction methods. Starting with a summary of the theory for an infinitely long linear array of reproduction loudspeakers, simulations show the effects of finite array length and distance between the loudspeakers. A strategy is presented for practical realization of this new system by making use of the notional source concept, including compatibility with two-channel stereophony.","1995","2023-07-12 07:25:13","2023-07-19 03:43:57","","1003–1012","","12","43","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NNWB9AJU","journalArticle","2011","Parodi, Yesenia Lacouture; Rubak, Per","A Subjective Evaluation of the Minimum Channel Separation for Reproducing Binaural Signals over Loudspeakers","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15974","While there are a variety of methods and tradeoffs for creating the necessary crosstalk cancellation when reproducing a binaural signal through loudspeakers in a real environment, there are no studies that reveal the minimum required crosstalk. In the current study the authors simulated varying degrees of crosstalk in order to determine the required threshold. For most audio signals crosstalk should be below –15 dB, and for broadband signals the crosstalk should be below –20 dB. Off-center locations require even lower crosstalk.","2011","2023-07-12 07:25:16","2023-07-19 04:37:02","","487–497","","7/8","59","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SBPTEPMB","journalArticle","2002","Karjalainen, Matti; Esquef, Paulo A. A.; Antsalo, Poju; Mäkivirta, Aki; Välimäki, Vesa","Frequency-Zooming ARMA Modeling of Resonant and Reverberant Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=11054","Discrete-time analysis and modeling of reverberant and resonating systems has many applications in audio and acoustics. The methodology of ARMA modeling by pole-zero filters for measured impulse responses was investigated. In addition to an overview of the standard AR and ARMA techniques, a spectral zooming technique is proposed, which is useful for resolving very closely positioned modes and high-density modal clusters. Application cases related to the analysis and modeling of room responses, loudspeaker room equalization, and the estimation of parameters for musical instrument modeling are studied.","2002","2023-07-12 07:25:22","2023-07-19 04:10:02","","1012–1029","","12","50","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8NP5GQAJ","journalArticle","2020","Pörschmann, Christoph; Arend, Johannes M.","A Method for Spatial Upsampling of Voice Directivity by Directional Equalization","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20896","To describe the sound radiation of the human voice into all directions, measurements need to be performed on a spherical grid. However, the resolution of such captured directivity patterns is limited and methods for spatial upsampling are required, for example by interpolation in the spherical harmonics (SH) domain. As the number of measurement directions limits the resolvable SH order, the directivity pattern suffers from spatial aliasing and order-truncation errors. We present an approach for spatial upsampling of voice directivity by spatial equalization. It is based on preprocessing, which equalizes the sparse directivity pattern by spectral division with corresponding directional rigid sphere transfer functions, resulting in a time-aligned and spectrally matched directivity pattern that has a significantly reduced spatial complexity. The directivity pattern is then transformed into the SH domain, interpolated to a dense grid by an inverse spherical Fourier transform and subsequently de-equalized by spectral multiplication with corresponding rigid sphere transfer functions. Based on measurements of a dummy head with an integrated mouth simulator, we compare this approach to reference measurements on a dense grid. The results show that the method significantly decreases errors of spatial undersampling and this allows a meaningful high-resolution voice directivity to be determined from sparse measurements.","2020","2023-07-12 07:25:25","2023-07-19 04:38:44","","649–663","","9","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F5GCK7P7","journalArticle","2021","Mathews, Jonathan; Braasch, Jonas","Sparse Iterative Beamforming Using Spherical Microphone Arrays for Low-Latency Direction of Arrival Estimation in Reverberant Environments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21545","Acoustic direction of arrival estimation methods allows positional information about sound sources to be transmitted over a network using minimal bandwidth. For these purposes,methods that prioritize low computational overhead and consistent accuracy under non-ideal conditions are preferred. The estimation method introduced in this paper uses a set of steered beams to estimate directional energy at sparsely distributed orientations around a spherical microphone array. By iteratively adjusting beam orientations based on the orientation of maximum energy, an accurate orientation estimate of a sound source may be produced with minimal computational cost. Incorporating conditions based on temporal smoothing and diffuse energy estimation further refines this process. Testing under simulated conditions indicates favorable accuracy under reverberation and source discrimination when compared with several other contemporary localization methods. Outcomes include an average localization error of less than 10? under 2 s of reverberation time (T60) and the potential to separate up to four sound sources under the same conditions. Results from testing in a laboratory environment demonstrate potential for integration into real-time frameworks.","2021","2023-07-12 07:25:29","2023-07-19 04:29:11","","967–977","","12","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CM4ZKGTA","journalArticle","2018","He, Jianjun; Ranjan, Rishabh; Gan, Woon-Seng; Chaudhary, Nitesh Kumar; Hai, Nguyen Duy; Gupta, Rishabh","Fast Continuous Measurement of HRTFs with Unconstrained Head Movements for 3D Audio","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19866","Head-related transfer function (HRTF) is an essential component of a system for creating an immersive listening experience over headphones in multimedia and virtual and augmented reality applications. A critical requirement is the measure of the HRTFs for each individual to accommodate ear idiosyncrasies. Conventional static stop-and-go HRTF measurement methods are tedious and time-consuming. Recently proposed continuous HRTF acquisition methods could improve the acquisition efficiency but they still restrict the movements of the subjects and must be conducted in a controlled environment. In this paper the authors propose a fast and continuous HRTF measurement system with an embedded head tracker that can drastically reduce the measurement duration, obtaining HRTFs at high resolution while still allowing unconstrained head movements in both azimuth and elevation directions. To extract the HRTFs from such dynamic binaural measurements with random head movements, an improved adaptive filtering algorithm is proposed by integrating direction quantization, variable step size, and including optimal HRTF selection into the progressive-based normalized least-mean-squares algorithm. Objective evaluations and subjective listening tests were conducted using measurement data obtained from human subjects. The experimental results demonstrate that the proposed system and algorithm can yield HRTFs that are very close to HRTFs obtained with conventional static methods.","2018","2023-07-12 07:25:35","2023-07-19 04:04:21","","884–900","","11","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RNJMQBBB","journalArticle","1998","Zacharov, Nick","Subjective Appraisal of Loudspeaker Directivity for Multichannel Reproduction","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12155","A group of subjective experiments considering the loudspeaker directivity characteristics for five-channel audiovisual reproduction was examined. A large number of tests were performed with a screened and trained listening panel. The effects of listening position, loudspeaker directivity, and program were examined. Frontal and surround loudspeaker directivity characteristics were considered separately. Certain findings suggest that de facto standard approaches may not be optimal in terms of loudspeaker directivity.","1998","2023-07-12 07:25:38","2023-07-19 10:59:06","","288–303","","4","46","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UNRNXS56","journalArticle","1992","D'Antonio, Peter; Konnert, John","The QRD Diffractal: A New One- or Two-Dimensional Fractal Sound Diffusor","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=7057","Number theoretic diffusors, as described by Schroeder, are characterized by a periodic grouping of a series of wells of equal width, but different depths, separated by thin dividers. Over the past 7 years the quadratic residue depth sequence has been used in over 1000 applications, including recording studios, concert halls, worship spaces, conference rooms, music education, and even home theater and music listening rooms. The frequency bandwidth is determined principally by the depth of the deepest well, which determines the low-frequency limit, and the well width, which determines the upper frequency limit. Physical manufacturing constraints pose a space limitation of approximately 1 in (25.4 mm) on the well width and a depth limitation of approximately 16 in (406.4 mm), after which the units become diaphragmetic and defeat the purpose of increasing the depth further. In an effort to provide full-spectrum sound diffusion in a single integrated diffusor, the self-similarity property of fractals was combined with the uniform scattering property of the number theoretic reflection phase grating to produce a patented new diffusing fractal called the Diffractal. The Diffractal is in effect a diffusor within a diffusor, with each covering a different frequency range, much like a multiway loudspeaker. The mid-high-frequency diffusors are placed on the wells of a dedicated stiff and massive low-frequency diffusor.Thepassband of the nested band-limited diffusors and the crossover points are completely calculable, and the overall bandwidth is limited only by the available depth and the surface detail. The theory of the one- and two-dimensional diffusor, fractal geometry, and the one- and two-dimensional Diffractal is reviewed. Two installations at Real World Studios, Bath, UK, and Crawford Post, Atlanta, GA, USA, are presented.","1992","2023-07-12 07:25:50","2023-07-19 03:51:28","","117–129","","3","40","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"35NLMSRE","journalArticle","2015","Zheng, Jianwen; Zhu, Tianyi; Lu, Jing; Qiu, Xiaojun","A Linear Robust Binaural Sound Reproduction System with Optimal Source Distribution Strategy","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17875","When binaural audio signals containing directional cues are presented with loudspeakers, the listener should still be able to localize sound images and experience a realistic three-dimensional sound environment. Because acoustic crosstalk between loudspeaker channels degrades performance, it is necessary to preprocess the binaural signals with a crosstalk cancellation system. However, a limited sweet spot is still a challenge especially when the head is laterally misaligned. To increase the robustness of the reproduction system, a direct multidrive array configuration with multiple control points is combined into the optimal source distribution strategy. The research goal is to find a good trade-off between the crosstalk cancellation performance and a high tolerance to head misalignment. Both simulations and experiments demonstrate the efficacy of the method, and the system is well suited to large display devices.","2015","2023-07-12 07:25:54","2023-07-19 10:59:22","","725–735","","9","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X4IKBTZE","journalArticle","2006","Aarts, Ronald M.; Nieuwendijk, Joris A.; Ouweltjes, Okke","Efficient Resonant Loudspeakers with Large Form-Factor Design Freedom","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13882","Small cabinet loudspeakers with a flat response are quite inefficient. Assuming that the frequency response can be manipulated electronically, systems that have a nonflat soundpressure level (SPL) response can provide greater usable efficiency. Such a nonflat design can deal with very compact housing, but for small drivers it would require a relatively large cone excursion to obtain a high SPL. A new solution is presented that uses a resonant combination of a coupling volume and a long pipe-shaped port. In this structure the efficient resonant coupling of the driver to the acoustic load enables small drivers with modest cone displacement to achieve a high SPL. Due to the high and narrow peak in the frequency response, the normal operating range of the driver decreases considerably. This makes the driver unsuitable for normal use. To overcome this, a second measure is applied. Nonlinear processing essentially compresses the bandwidth of a 20–120-Hz 2.5-octave bass signal down to a much narrower span, which is centered where the driver efficiency is maximum. This system allows very compact loudspeakers. An experimental example of such a design is described, and a working prototype is discussed. The new loudspeaker is compared with a closed cabinet and a bass-reflex cabinet using the same drivers. It appears that the new loudspeaker has the highest output in its working range.","2006","2023-07-12 07:25:58","2023-07-19 03:33:15","","940–953","","10","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3TTLGQ5M","journalArticle","2013","Wierstorf, Hagen; Raake, Alexander; Geier, Matthias; Spors, Sascha","Perception of Focused Sources in Wave Field Synthesis","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16663","Wave Field Synthesis (WFS) can synthesize virtual sound sources that are perceived to be at locations between loudspeakers and the listener, called focused sources. Because of practical limitations in the density of loudspeakers, there are artifacts. This research explores the amount of perceptual artifacts and the localization of the focused sources. The results from a variety of listening configurations illustrate the trade-offs. The truncation of loudspeaker arrays creates two opposite effects: (a) fewer additional wave fronts reduce the perception of artifacts, (b) stronger diffraction reduces the size of the listening area with adequate binaural cues.","2013","2023-07-12 07:26:02","2023-07-19 10:55:43","","5–16","","1/2","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4VRTD7Z6","journalArticle","1986","Toole, Floyd E.","Loudspeaker Measurements and Their Relationship to Listener Preferences: Part 2","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=5270","Using the highly reliable subjective ratings from an earlier study, loudspeaker measurements have been examined for systematic relationships to listener preferences. The resuls has been a logical and orderly organization of measurements that can be used to anticipate listener opinion. With the restriction to listeners with near-normal hearing and loudspeakers of the conventional forward-facing configuration, the data offer convincing proof that a reliable ranking of loudspeaker sound quality can be achieved with specific combinations of high-resolution free-field amplitude-response data. Using such data obtained at several orientations it is possible to estimate loudspeaker performance in the listening room. Listening-room and sound-power measurements alone appear to be susceptible to error in that while truly poor loudspeakers can generally be identified, excellence may not be recognized. High-quality stereo reproduction is compatible with those loudspeakers yielding high sound quality; however, there appears to be an inherent trade-off between the illusions of specific image localization and the sense of spatial involvement.","1986","2023-07-12 07:26:12","2023-07-19 04:53:30","","323–348","","5","34","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VCTL3X8T","journalArticle","2021","Falcón Pérez, Ricardo; Götz, Georg; Pulkki, Ville","Spherical Maps of Acoustic Properties as Feature Vectors in Machine-Learning-Based Estimation of Acoustic Parameters","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21460","This work suggests a method of presenting information about the acoustical and geometric properties of a room as spherical images to a machine-learning algorithm to estimate acoustical parameters of the room. The approach has the advantage that the spatial distribution of the properties can be presented in a generic and potentially compact way to machine learning methods. The estimation of reverberation time T60 is used as a proof-of-concept study here. The distribution of absorptive material is presented as a spherical map of feature values in which each value is formed by calculating the equivalent absorption area visible through the corresponding facet of a polyhedron as seen from the polyhedron’s center point. The pixel values are then used as feature vectors and the real measured T60 values of corresponding rooms are used as target data. This work presents the method and trains a set of neural networks with different spherical map resolutions using a dataset composed of real-world acoustical measurements of a single room with 831 different configurations of furniture and absorptive materials. The estimation of reverberation time using the proposed approach exhibits a much higher accuracy compared to simple analytic methods, which proves the validity of the approach.","2021","2023-07-12 07:26:16","2023-07-19 03:55:19","","632–643","","9","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AJMNK3BG","journalArticle","2015","Gauthier, Philippe-Aubert; Camier, Cédric; Padois, Thomas; Pasco, Yann; Berry, Alain","Sound Field Reproduction of Real Flight Recordings in Aircraft Cabin Mock-Up","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17562","A mock-up of an airplane cabin can be a good simulation tool for the study, prediction, demonstration, and jury testing of interior aircraft sound quality. The authors used real flight recordings created with vibrational and microphone transducer arrays in combination with multichannel equalization with least-mean-square optimization techniques. The paper presents physical evaluations of reproduced sound fields on the basis of real flight recordings using an 80-channel microphone array and 41 reproduction sources in the cabin mock-up. To provide a faithfully reproduced sound environment, time, frequency, and spatial characteristics should be preserved. Results are satisfactory given the various limitations and constraints.","2015","2023-07-12 07:26:19","2023-07-19 04:01:36","","6–20","","1/2","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VAGIX3PW","journalArticle","2006","Welti, Todd; Devantier, Allan","Low-Frequency Optimization Using Multiple Subwoofers","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13680","At low frequencies the listening environment has a significant impact on the sound quality of an audio system. Standing waves within the room cause large frequency-response variations at the listening locations. Furthermore, the frequency response changes significantly from one listening location to another; therefore the system cannot be equalized effectively. However, through the use of multiple subwoofers the seat-to-seat variation in the frequency response can be reduced significantly, allowing subsequent equalization to be more effective. Three methods to reduce seat-to-seat variation are described, including a novel approach based on simple signal processing. The desired result in each case is to allow the system to be equalized over a seating area rather than just one seat. Results are shown for several listening rooms.","2006","2023-07-12 07:26:29","2023-07-19 10:54:57","","347–364","","5","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"37UGCZ99","journalArticle","2007","Muraoka, Teruo; Nakazato, Tomoaki","Examination of Multichannel Sound-Field Recomposition Utilizing Frequency-Dependent Interaural Cross Correlation (FIACC)","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14158","The locations of loudspeakers were examined utilizing frequency-dependent interaural cross correlation (FIACC) for optimum sound-field recomposition in multichannel recording and reproduction processes. Experiments were conducted and sets of FIACC measurements compared, with one taken in an original sound field, the other in the reproduced sound field. Several sound-field reproduction methods using different microphone-array geometries and matching loudspeaker-array geometries were considered. The results presented support the ITU recommendation for loudspeaker arrangements in five-channel systems (BS.775-1).","2007","2023-07-12 07:26:34","2023-07-19 04:34:23","","236–256","","4","55","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W3QZAINS","journalArticle","2008","Kim, Sunmin; Kong, Donggeon; Jang, Seongcheol","Adaptive Virtual Surround Sound Rendering System for an Arbitrary Listening Position","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14383","A smart virtual reality presentation system, using only two loudspeakers, allows for a dynamically selected sweet spot that can be positioned at the arbitrary location of the listener. An efficient asymmetric crosstalk cancellation creates the needed filters to reposition the sweet spot; and a specialized remote control held by the listener allows the system to determine the angle and distance to the listener. Informal listening tests showed that the proposed approach can enhance the surround sound effect for television listening. Compared to conventional playback, listeners preferred this system at every location.","2008","2023-07-12 07:26:44","2023-07-19 04:12:11","","243–254","","4","56","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GUVQ95UU","journalArticle","2002","Bolaños, Fernando","Experimental Evidence of Cooperation Phenomena Application to a Loudspeaker with Rub","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=11052","An experimental investigation into cooperation phenomena is presented, which in this case can be described as the contribution of energy from a random signal to the energy of a periodic signal under certain conditions. Previously cooperation phenomena have been described mainly from a theoretical point of view. However, well-known practical examples exist. For the present experiment a loudspeaker was used that had a slightly rubbing voice coil subjected simultaneously to excitation from a random signal and from a periodic signal. Test results clearly show the influence of cooperation on both the random and the periodic signals. The random signal acquires additional ""agility,"" allowing some of its energy to be transferred to the periodic signal. Experimental results are compared with theoretical analysis based on limit cycles and Van der Pol's oscillators. The application of limit cycles is then extended to friction-induced phenomena. The subject matter is of general interest in the dynamics of nonlinear systems and of more specific interest in the behavior and testing of loudspeakers.","2002","2023-07-12 07:26:48","2023-07-19 03:43:27","","1039–1053","","12","50","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W3I23B8Q","journalArticle","2005","Zielinski, Slawomir K.; Rumsey, Francis; Kassier, Rafael; Bech, Søren","Comparison of Basic Audio Quality and Timbral and Spatial Fidelity Changes Caused by Limitation of Bandwidth and by Down-mix Algorithms in 5.1 Surround Audio Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13407","The effect on audio quality of controlled multichannel audio bandwidth limitation and selected down-mix algorithms was quantified using one generic attribute (basic audio quality) and three specific attributes (timbral fidelity, frontal spatial fidelity, and surround spatial fidelity). The investigation was focused on the standard 5.1 multichannel audio setup (ITU-R BS.775-1) and was limited to the optimum listening position. The results obtained from a panel of experienced listeners indicate that the basic audio quality of multichannel recordings is more affected by timbral fidelity than by spatial fidelities. Therefore it can be concluded that in the case of broadcasting multichannel audio under highly restricted transmission conditions, it is better, in terms of basic audio quality, to sacrifice spatial fidelity by downmixing original multichannel audio material to a lower number of broadcast audio channels than to sacrifice the timbral fidelity by transmitting all channels with limited bandwidths.","2005","2023-07-12 07:26:51","2023-07-19 10:59:48","","174–192","","3","53","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"37BEE9A9","journalArticle","2022","Kaleris, Konstantinos; Stelzner, Bjoern; Hatziantoniou, Panagiotis; Trimis, Dimosthenis; Mourjopoulos, John","Laser-Sound Transduction From Digital ΣΔ Streams","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21550","In this work, a novel optoacoustic transducer prototype capable of reproducing continuous sound waves from single or multi-bit ∑∆ digital audio streams is presented. The prototype is based on a pulsed nanosecond laser that generates acoustic N-waves via Laser-Induced Breakdown in air or indirect sound via the laser ablation effect on solid targets. Technical aspects of the prototype platform are presented, with emphasis on the laser control system and audio signal–processing techniques deployed for the modulation of the pulsed laser radiation. Experimental results from acoustic measurements of reproduced test audio signals are presented within the frequency range allowed by the specifications of the specific laser. The system’s audio performance characteristics are derived along with its impulse and frequency responses. The experimental results are compared with simulations of the optoacoustic transducer’s response via a computational model, showing good agreement.","2022","2023-07-12 07:26:56","2023-07-19 04:09:26","","50–61","","1/2","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YG9LXGQY","journalArticle","2021","Agrawal, Sarvesh; Bech, Søren; Bærentsen, Klaus; De Moor, Katrien; Forchhammer, Søren","Method for Subjective Assessment of Immersion in Audiovisual Experiences","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21462","Studying immersion in audiovisual experiences can help technologists deliver engaging and enhanced experiences. As a first step toward this goal this paper details an investigation conducted to establish an experimental paradigm for quantifying immersion and determining the influence of immersive tendency (susceptibility to become immersed) on immersion. A balanced incomplete block design was employed where 21 assessors rated 15 commercially available stimuli (representative of the highest quality encountered in domestic AV applications) without repetitions and simultaneous comparisons. The assessors were instructed to rate immersion on a graphic line scale and document their familiarity with the content. A questionnaire was administered to measure the immersive tendency after the rating experiment. The results show that the assessors can comprehend the description of immersion and follow the experimental protocol. It is found that immersion is a graded experience and the correlation between immersive tendencies and immersion ratings is predominantly statistically insignificant. The experimental paradigm presented in this paper can form the framework for assessing immersion and developing novel methods to thoroughly explore the concept of immersion in audiovisual experiences.","2021","2023-07-12 07:27:03","2023-07-19 03:34:21","","656–671","","9","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IYC9JUUC","journalArticle","2010","Faller II, Kenneth John; Barreto, Armando; Adjouadi, Malek","Augmented Hankel Total Least-Squares Decomposition of Head-Related Transfer Functions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15238","Filters based on head-related transfer functions (HRTFs) allow a monaural source to be localized in a three-dimensional virtual space. There are three ways to obtain HRTF impulse responses: (a) individually measured on each specific person, which requires extensive effort; (b) generic standardized responses, such as dummy heads, which are often inaccurate; and (c) derived responses based on anthropometric measurements of an individual’s head. This article proposes an approach for decomposing HRTFs that will facilitate the development of systems based on the third method. This method offers the potential to produce high accuracy without the extensive effort.","2010","2023-07-12 07:27:06","2023-07-19 03:55:37","","3–21","","1/2","58","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GWEV43XX","journalArticle","2017","Kodrasi, Ina; Cauchi, Benjamin; Goetze, Stefan; Doclo, Simon","Instrumental and Perceptual Evaluation of Dereverberation Techniques Based on Robust Acoustic Multichannel Equalization","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18549","Speech signals recorded in an enclosed space by microphones at a distance from the speaker are often corrupted by reverberation, which arises from the superposition of many delayed and attenuated copies of the source signal. Because reverberation degrades the signal, removing reverberation would enhance quality. Dereverberation techniques based on acoustic multichannel equalization are known to be sensitive to room impulse response perturbations. In order to increase robustness, several methods have been proposed, as for example, using a shorter reshaping filter length, incorporating regularization, or applying a sparsity-promoting penalty function. This paper focuses on evaluating the performance of these methods for single-source multi-microphone scenarios, using instrumental performance measures as well as using subjective listening tests. By analyzing the correlation between the instrumental and the perceptual results, it is shown that signal-based performance measures are more advantageous than channel-based performance measures to evaluate the perceptual speech quality of signals that were dereverberated by equalization techniques. Furthermore, this analysis also demonstrates the need to develop more reliable instrumental performance measures.","2017","2023-07-12 07:27:09","2023-07-19 04:15:49","","117–129","","1/2","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TR9GXJ6V","journalArticle","1999","Møller, Henrik; Hammershøi, Dorte; Johnson, Clemen Boje; Sørensen, Michael Friis","Evaluation of Artificial Heads in Listening Tests","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12115","The localization performance was studied when subjects listened 1) to a real sound field and 2) to artificial-head recordings of the same sound field. The experiments took place in a standard listening room where each stimulus (female speech) was emitted from one of 19 loudspeakers, and the subjects were to indicate the perceived sound source. The artificial-head recordings were made a) by the artificial heads' built-in microphones and b) by blocked ear canal microphones. The reproduction was carried out by carefully equalized headphones. Eight artificial heads were included in the investigation, and 20 subjects participated, except for the experiment with recordings from built-in microphones, which was performed for eight subjects. When compared to real life, the localization performance with the artificial heads resulted in an increased number of errors independent of the recording technique. In general, the directions in the median plane were frequently confused, not only with nearby directions, but also with directions further away. For some artificial heads there was also an increase in confusions of directions outside the median plane. A much better performance is obtainable with binaural recordings made in the ears of humans. This encourages the design and production of improved artificial heads.","1999","2023-07-12 07:27:13","2023-07-19 04:32:32","","83–100","","3","47","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H8D8GJGV","journalArticle","2023","Vidal, Adrien; Herzog, Philippe; Lambourg, Christophe; Chatron, Jacques","Comparison of Transaural Configurations Inside Usual Rooms","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22041","This paper deals with the design of transaural systems in usual rooms, whose response has a strong influence on sound reproduction. The paper proposes to select configurations for the best perceptual rendering. However, realistic perceptive experiments cannot deal with the many possible room and loudspeaker configurations. Therefore, the authors propose to assess them using objective scores that are extrapolated from the results of perceptive tests assessing a suitable selection of rooms and loudspeaker configurations. This extrapolation then allows comparison of a much larger set of combinations, leading to the conclusion that close-to-ears configurations allow reduction of the room influence, leading to a good perceived fidelity---even inside usual rooms. Closer loudspeakers are, however, likely to be more sensitive to listener position, so the robustness of loudspeaker configurations to listenermisplacement were investigated. A suitable objective score, again based on a perceptive test, led to the surprising conclusion that some close-to-ears configurations are also robust to listener position.","2023","2023-07-12 07:27:16","2023-07-19 04:55:20","","202–215","","4","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2QMP5NQE","journalArticle","2021","Bernier, Antoine; Bouserhal, Rachel E.; Voix, Jérémie; Herzog, Philippe","Design and Assessment of an Active Musician's Hearing Protection Device With Occlusion Effect Reduction","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21459","This paper presents the design and assessment of an active musician’s hearing protection device focusing on occlusion effect reduction through active noise control. The system is designed to adapt its feedback compensation to achieve a specified target performance across users, regardless of their ear canal acoustic properties. The detailed design process of the prototype earpiece and feedback compensation algorithm are presented, validated, and implemented. Experimental measurements show the system is able to maintain robust and stable occlusion effect reduction despite great variation in the acoustic properties of an adjustable ear simulator.","2021","2023-07-12 07:27:19","2023-07-19 03:42:05","","618–631","","9","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RSMAP3IQ","journalArticle","2008","James, B. St.; Hawksford, M. O. J.","Corpuscular Streaming and Parametric Modification Paradigm for Spatial Audio Teleconferencing","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14639","This paper presents a new paradigm for spatial audio teleconferencing that exploits the corpuscular nature of general audio parametric coding for low bit-rate audio compression. The paradigm consists of several processing tools. The spatial aspect is enacted by a dynamic rendition tool that optimizes placement of speech sources within the soundscape; the temporal aspects of a phoneme protection tool misalign corruptive and vulnerable speech elements in a multitalking environment. A novel means of echo suppression eliminates the need for adaptive filtering and real-time echo cancellation, which is difficult for multichannel audio.","2008","2023-07-12 07:27:23","2023-07-19 04:08:06","","823–843","","10","56","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6YE4VXRF","journalArticle","2019","Stefanakis, Nikolaos; Mastorakis, Yannis; Alexandridis, Anastasios; Mouchtaris, Athanasios","Automating Mixing of User-Generated Audio Recordings from the Same Event","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20452","When users attend the same public event, there may be multiple audiovisual recordings that are then posted on social media and websites. The availability of such a massive amount of user-generated recordings (UGR) has triggered new research directions related to the search, organization, and management of this content. And it has provided inspiration for new business models for content storage, retrieval, and consumption. The authors propose an approach to combine the available recordings based on a normalization step and a mixing step. The normalization step defines a fixed-with-time gain that is specific to each UGR. In the mixing step, a mechanism that reduces the master gain in accordance with the number of activated inputs at each time is employed. An approach called orthogonal mixing is presented, which is based on the assumption that the mixture components are mutually independent. The presented mixing process allows the combination of multiple short-duration UGRs to produce a longer audio stream, with potentially better quality than any one of its constituent parts. This property is exploited in the design of an automatic mixing process that exploits all the available audio recordings at each moment. Automatic mixing is then possible.","2019","2023-07-12 07:27:42","2023-07-19 04:49:09","","201–212","","4","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DBASJICJ","journalArticle","1987","Julstrom, Stephen","A High-Performance Surround Sound Process for Home Video","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=5192","Video disks, high-fidelity video cassettes, and stereo television bring to the consumer a substantial library of video entertainment with high-quality stereo sound tracks. A growing portion of these are encoded with surround sound using a 4-2-4 matrix-based method particularly suited to front-stage oriented film/video presentation. Home reproduction is accomplished with a processor incorporating dynamic matrix modification to stabilize and enhance directional effects and a delayed surround channel to aid forward localization of front sound.","1987","2023-07-12 07:27:46","2023-07-19 04:09:18","","536–549","","7/8","35","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DJNSA93M","journalArticle","2005","Aarts, Ronald M.","High-Efficiency Low-Bl Loudspeakers","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13427","Normally, low-frequency sound reproduction with small transducers is quite inefficient. This is shown by calculating the efficiency and voltage sensitivity for loudspeakers with high, medium, and, in particular, low force factors. For these low-force-factor loudspeakers a practically relevant and analytically tractable optimality criterion, involving the loudspeaker parameters, will be defined. Actual prototype bass drivers are assessed according to this criterion. Because the magnet can be considerably smaller than usual, the loudspeaker can be of the moving-magnet type with a stationary coil. These so-called low-Bl drivers have a high efficiency, however, only in a limited frequency region. To deal with that, nonlinear processing essentially compresses the bandwidth of a 20–120-Hz bass signal down to a much more narrow span. This span is centered at the resonance of the low-Bl driver, where its efficiency is maximum. The signal processing preserves the temporal envelope modulations of the original bass signal. The compression is at the expense of a decreased sound quality and requires some additional electronics. This new, optimal design has a much higher power efficiency as well as a higher voltage sensitivity than current bass drivers, while the cabinet may be much smaller.","2005","2023-07-12 07:27:54","2023-07-19 03:32:59","","579–592","","7/8","53","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8QEUGQMZ","journalArticle","2022","Johansson, Jaan; Mäkivirta, Aki; Malinen, Matti; Saari, Ville","Interaural Time Difference Prediction Using Anthropometric Interaural Distance","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22011","This paper studies the feasibility of predicting the interaural time difference (ITD) in azimuth and elevation once the personal anthropometric interaural distance is known, proposing an enhancement for spherical head ITD models to increase their accuracy. The method and enhancement are developed using data in a Head-Related Impulse Response (HRIR) data set comprising photogrammetrically obtained personal 3D geometries for 170 persons and then evaluated using three acoustically measured HRIR data sets containing 119 persons in total. The directions include 360° in azimuth and –15° to 60° in elevation. The prediction error for each data set is described, the proportion of persons under a given error in all studied directions is shown, and the directions in which large errors occur are analyzed. The enhanced spherical head model can predict the ITD such that the first and 99th percentile levels of the ITD prediction error for all persons and in all directions remains below 122 µs. The anthropometric interaural distance could potentially be measured directly on a person, enabling personalized ITD without measuring the HRIR. The enhanced model can personalize ITD in binaural rendering for headphone reproduction in games and immersive audio applications.","2022","2023-07-12 07:27:58","2023-07-19 04:08:49","","843–857","","10","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MRZLMXK3","journalArticle","1998","Gander, Mark R.","Fifty Years of Loudspeaker Developments as Viewed Through the Perspective of the Audio Engineering Society","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12162","An exhaustive review of over 450 AES Journal loudspeaker papers and other select references is presented and categorized by subject area. The names and affiliations of the authors are included. Perspective is given on the technical significance, degree of influence, and historical context of the contributions and contributors. Except where otherwise noted, all references are from the Journal of the Audio Engineering Society and, where applicable, the volumes of the AES Loudspeakers anthology.","1998","2023-07-12 07:28:01","2023-07-19 04:01:20","","43–58","","1/2","46","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5KS8C42R","journalArticle","2019","Lee, Hyunkook","Capturing 360° Audio Using an Equal Segment Microphone Array (ESMA)","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19883","Microphone array techniques for surround sound recording can be broadly classified into two groups: those that attempt to produce the continuous phantom imaging around 360° in the horizontal plane and those that treat the front and rear channels separately. The equal segment microphone array (ESMA) is a multichannel microphone technique that attempts to capture a sound field in 360° without any overlap between the stereophonic recording angle of each pair of adjacent microphones. This study investigated the optimal microphone spacing for a quadraphonic ESMA using cardioid microphones. Recordings of a speech source were made using the ESMAs with four different microphone spacings of 0 cm, 24 cm, 30 cm, and 50 cm, based on different psychoacoustic models for microphone array design. Multichannel and binaural stimuli were created with the reproduced sound field rotated over 45° intervals. Listening tests were conducted to examine the accuracy of phantom image localization for each microphone spacing, in both loudspeaker and binaural headphone reproductions. The results generally indicated that the 50-cm spacing, which was derived from an interchannel time and level trade-off model that is perceptually optimized for 90° loudspeaker base angle, produced more accurate localization results than the 24-cm and 30-cm ones, which were based on conventional models derived from the standard 60° loudspeaker setup. The 0-cm spacing produced the worst accuracy with the most frequent bimodal distributions of responses between the front and back regions. Findings from this study are expected to be useful for acoustic recording for virtual reality applications as well as for multichannel surround sound.","2019","2023-07-12 07:28:05","2023-07-19 04:18:52","","13–26","","1/2","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S6Q6VZS4","journalArticle","2003","Paatero, Tuomas; Karjalainen, Matti","Kautz Filters and Generalized Frequency Resolution: Theory and Audio Applications","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12248","Frequency-warped filters have recently been applied successfully to a number of audio applications. The idea of all-pass delay elements replacing unit delays in digital filters allows for focusing enhanced frequency resolution on the lowest (or highest) frequencies and enables a good match to the psychoacoustical Bark scale. Kautz filters can be seen as a further generalization, where each all-pass element may be different, allowing also complexconjugate poles. This enables an arbitrary allocation of frequency resolution to filter design, such as modeling and equalization (inverse modeling) of linear systems. Strategies for using Kautz filters in audio applications are formulated. Case studies of loudspeaker equalization, room response modeling, and guitar body modeling for sound synthesis are presented.","2003","2023-07-12 07:28:10","2023-07-19 04:36:43","","27–44","","1/2","51","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LTZ3VVKX","journalArticle","1976","Preis, Douglas","Linear Distortion","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=2618","Linear distortion is defined and exemplified. Important early contributions dealing with its fundamental properties as well as more recent work showing its occurrence in practical audio devices are mentioned. Necessary conventions for steady-state phase-shift measurements are presented along with a discussion and resolution of some associated ambiguities. The behavior of the phase-shift versus frequency characteristic is approximated within a frequency band by a two-term Taylor series, then a catalog of experimentally measured steady-state and transient response properties of a general minimum-phase system is given and qualitatively related to this approximation. Apart from known mathematical relationships, this establishes practical and intuitive relationships between time-domain and frequency-domain measurements. Definitions, limitations, and specific examples of phase delay and group delay are given, which are based upon the same Taylor series approximation. The importance of the group-delay versus frequency characteristic-including its flatness and distortion-is discussed. A significant early experiment which illustrates group-delay distortion is reviewed. Specific examples are given of various measured and calculated group-delay characteristics for a wide variety of practical audio devices, including power amplifiers, magnetic tape recorders, bandpass filters as well as wideband systems, and the general second-order system which is often representative of transducers such as microphones, loudspeakers, and phonograph cartridges. Intercept distortion and echo distortion are also discussed. Differential group-delay distortion is defined, and its occurrence in multichannel systems is both predicted theoretically and measured. Some contemporary methods for correcting linear distortion are briefly reviewed. It is concluded that complete specification of magnitude, phase and group-delay characteristics is essential to the proper technical assessment of audio quality.","1976","2023-07-12 07:28:22","2023-07-19 04:38:53","","346–367","","5","24","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YCHYGCQY","journalArticle","1997","Bosi, Marina; Brandenburg, Karlheinz; Quackenbush, Schuyler; Fielder, Louis; Akagiri, Kenzo; Fuchs, Hendrik; Dietz, Martin","ISO/IEC MPEG-2 Advanced Audio Coding","Journal of the Audio Engineering Society","","","","https://www.aes.org/e-lib/browse.cfm?elib=10271","The ISO/IEC MPEG-2 advanced audio coding (AAC) system was designed to provide MPEG-2 with the best audio quality without any restrictions due to compatibility requirements. The main features of the AAC system (ISO/IEC 13818-7) are described. MPEG-2 AAC combines the coding efficiency of a high-resolution filter bank, production techniques, and Huffman coding with additional functionalities aimed to deliver very high audio quality at a variety of data rates.","1997-10-01","2023-07-17 05:10:16","2023-07-17 05:10:16","2023-07-17 05:10:16","789-814","","10","45","","JAES","","","","","","","","English","","","","","www.aes.org","","Publisher: Audio Engineering Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""