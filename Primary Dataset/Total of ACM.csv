"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"TRUMCWY5","conferencePaper","2016","Goudarzi, Visda","Exploration of sonification design process through an interdisciplinary workshop","Proceedings of the Audio Mostly 2016","978-1-4503-4822-5","","10.1145/2986416.2986422","https://dl.acm.org/doi/10.1145/2986416.2986422","In sonification of scientific data, designers know very little about the domain science and domain scientists are not familiar with the sonification methodology. The knowledge about the domain science is not given, but evolved during the problem-solving process. We discuss design challenges in auditory display design regarding user-centered approaches and suggest a method to involve domain scientists throughout sonification designs. We explore this within a workshop in which sonification experts, domain experts, and programmers worked together to better understand and solve problems collaboratively. The sonification framework that is used during the workshops is briefly described and the workshop process and how each group worked together during the workshop sessions are examined. Participants worked on pre-defined and exploratory tasks to sonify climate data. Furthermore, they grasped each other's domains; climate scientists especially became more open to use auditory display and sonification as a tool in their data mining tasks. Resulting sonification prototypes and workshop sessions are documented on a wiki to be used by the sonification community. To get started, we used some of the sonification designs created during the workshop for an online study where participants from science, engineering, and humanities were asked questions about the data behavior by listening to sonifcations of bivariate time series. Results indicate that sonic representation of data from resulting sonification allows most users (even with little or no knowledge of sound and music) to successfully complete some common data exploration tasks.","2016-10-04","2023-07-06 04:36:04","2023-07-06 04:36:04","2023-07-05","147–153","","","","","","","AM '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/CM7EMMGG/Goudarzi - 2016 - Exploration of sonification design process through.pdf","","","Sonification; Auditory Display; Participatory Design; User Experience","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9IKFFJZX","conferencePaper","2021","Rönnberg, Niklas","Sonification for Conveying Data and Emotion","Proceedings of the 16th International Audio Mostly Conference","978-1-4503-8569-5","","10.1145/3478384.3478387","https://dl.acm.org/doi/10.1145/3478384.3478387","In the present study a sonification of running data was evaluated. The aim of the sonification was to both convey information about the data and convey a specific emotion. The sonification was evaluated in three parts, firstly as an auditory graph, secondly together with additional text information, and thirdly together with an animated visualization, with a total of 150 responses. The results suggest that the sonification could convey an emotion similar to that intended, but at the cost of less good representation of the data. The addition of visual information supported understanding of the sonification, and the auditory representation of data. The results thus suggest that it is possible to design sonification that is perceived as both interesting and fun, and convey an emotional impression, but that there may be a trade off between musical experience and clarity in sonification.","2021-10-15","2023-07-06 04:36:13","2023-07-06 04:36:13","2023-07-05","56–63","","","","","","","AM '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/PEF22AXJ/Rönnberg - 2021 - Sonification for Conveying Data and Emotion.pdf","","","sonification; model of affect; user evaluation; visualization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9VXKPQ4W","conferencePaper","2015","Seibert, Gabriela; Hug, Daniel; Cslovjecsek, Markus","Towards an Enactive Swimming Sonification: Exploring Multisensory Design and Musical Interpretation","Proceedings of the Audio Mostly 2015 on Interaction With Sound","978-1-4503-3896-7","","10.1145/2814895.2814902","https://dl.acm.org/doi/10.1145/2814895.2814902","In this paper we present a design method that integrates the exploration of visual representations and musical expertise in the process of creating a swimming sonification, and initial results of the method's application in an explorative study. Our focus lies on the creation of a sonic representation that facilitates the affective, intuitive reproduction of the crawl swim movement. The method integrates artistic creativity and a systematic design process. By combining the linguistic-conceptual, visual and auditory representation of the (imagined) movement, we aim to advance the expressive quality of the sonic representation as well as the design method in a crossmodal, holistic way. Finally we report on a qualitative evaluation of the potential of this approach to support the affective, intuitive re-enactment of the swimming movement.","2015-10-07","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","1–8","","","","","","Towards an Enactive Swimming Sonification","AM '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/8QBGUSQZ/Seibert et al. - 2015 - Towards an Enactive Swimming Sonification Explori.pdf","","","Enactive Design; Movement Sonification; Multisensory Design; Musical Improvisation; Sonic Representation; Swimming Sport Training; Visual Musical Score","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZESVXFMY","conferencePaper","2008","Last, Mark; Gorelik, Anna","Using sonification for mining time series data","Proceedings of the 9th International Workshop on Multimedia Data Mining: held in conjunction with the ACM SIGKDD 2008","978-1-60558-261-0","","10.1145/1509212.1509220","https://dl.acm.org/doi/10.1145/1509212.1509220","In recent years, there is a growing interest in mining time series databases by both automated and interactive tools. In this paper, we present an interactive methodology for mining of time series data using a novel sonification technique which uses some important properties of time series and tonal music to achieve effective (accurate) and efficient (fast) results. We have created an experimental website, where participants were asked to perform some basic data exploration and mining tasks by listening to a musical display of several time series. The initial results indicate that the proposed methodology for musical representation of data allows, on one hand, to efficiently perform some decision-making tasks ""on the fly"" - by only listening to some short music examples, and on the other hand, it provides an alternative data representation for blind or visually impaired users or users who are due to their professional or personal activities (e.g., driving) cannot use their sense of vision for watching a visual display of data, but still need to get some important time-based information by using their other senses.","2008-08-24","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","63–72","","","","","","","MDM '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/YDEHBT8S/Last and Gorelik - 2008 - Using sonification for mining time series data.pdf","","","sonification; data mining; sound display; time series","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FJ3T8BZ8","conferencePaper","2022","Senan, Toros; Hengeveld, Bart; Eggen, Berry","Sounding Obstacles for Social Distance Sonification","Proceedings of the 17th International Audio Mostly Conference","978-1-4503-9701-8","","10.1145/3561212.3561239","https://dl.acm.org/doi/10.1145/3561212.3561239","This article reports the results of an experiment (N = 10) that employs continuous auditory feedback to influence participants’ routing choices while walking between two points by sonifying their interactions with invisible obstacles. A relative distance parameter, proximity, is defined and mapped simultaneously to perceived loudness and amplitude modulation frequencies of sine tones. The proximity parameter is divided into three sections: slow modulation, border zone, and fast modulation. The slow and fast modulation sections generate a monotonic relationship between proximity values and the resulting psychoacoustic parameters: fluctuation strength and roughness. A social distance sonification case study in a laboratory experiment evaluated the effectiveness of the generated hearing sensations and explored participants’ experiences through a semi-structured interview. The quantitative results show that the non-spatial, psychoacoustically-inspired sonification mappings successfully influenced participants’ routing choices during the experiental task of walking. On the other hand, the semi-structured interview revealed that participants ascribed a pleasantness/annoyance attribute to presented sounds, which was not intended.","2022-10-10","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","187–194","","","","","","","AM '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/GA4PJEMP/Senan et al. - 2022 - Sounding Obstacles for Social Distance Sonificatio.pdf","","","Sonification; Psychoacoustics; Guidance technologies","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QIIE8WU8","conferencePaper","2011","Ramos, Daniel; Folmer, Eelke","Supplemental sonification of a bingo game","Proceedings of the 6th International Conference on Foundations of Digital Games","978-1-4503-0804-5","","10.1145/2159365.2159388","https://dl.acm.org/doi/10.1145/2159365.2159388","Visual cues are typically used in video games to indicate to the player what input to provide and when. Cues represented in multiple modalities that are presented simultaneously can be detected at lower thresholds, faster and more accurately than when presented separately in each modality. This characteristic has not been explored in playing video games to reduce errors. This paper explores the use of supplemental audio feedback to reduce errors in playing Bingo, a game which is typically played in crowded and noisy environments by a demographic, which -due to their age- are more likely to suffer from sensory impairments such as low vision or hearing impairments. A user study explored three different types of sonification (pitch, timbre, and audio icons) versus using no sonification and found that supplemental sonification using timbre or audio icons significantly reduces player's errors.","2011-06-29","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","168–173","","","","","","","FDG '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/YPJVGBHY/Ramos and Folmer - 2011 - Supplemental sonification of a bingo game.pdf","","","sonification; games; multimodal feedback","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FQ4Y6RNT","conferencePaper","2020","Quinton, Michael; McGregor, Iain; Benyon, David","Sonification of an exoplanetary atmosphere","Proceedings of the 15th International Audio Mostly Conference","978-1-4503-7563-4","","10.1145/3411109.3411117","https://dl.acm.org/doi/10.1145/3411109.3411117","This study investigates the effectiveness of user design methods to create a sonification for an astronomer who analyses exoplanet meteorological data situated in habitable zones. Requirements about the astronomer's work, the dataset and how to sonify it utilising Grounded Theory were identified. Parameter mapping sonification was used to represent effective transiting radii measurements through subtractive synthesis and spatialization. The design was considered to be effective, allowing the instantaneous identification of a water feature overlooked on a visual graph, even when noise within the dataset overlapped the source signal. The results suggest that multiple parameter mappings provide richer auditory stimuli and semantic qualities in order to allow an improved understanding of the dataset.","2020-09-16","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","191–198","","","","","","","AM '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/WR8PC7CI/Quinton et al. - 2020 - Sonification of an exoplanetary atmosphere.pdf","","","sonification; astronomy; exoplanet atmospheres; grounded theory; parameter mapping sonification; user centred design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I67DEFUK","conferencePaper","2021","Enge, Kajetan; Rind, Alexander; Iber, Michael; Höldrich, Robert; Aigner, Wolfgang","It’s about Time: Adopting Theoretical Constructs from Visualization for Sonification","Proceedings of the 16th International Audio Mostly Conference","978-1-4503-8569-5","","10.1145/3478384.3478415","https://dl.acm.org/doi/10.1145/3478384.3478415","Both sonification and visualization convey information about data by effectively using our human perceptual system, but their ways to transform the data could not be more different. The sonification community has demanded a holistic perspective on data representation, including audio-visual analysis, several times during the past 30 years. A design theory of audio-visual analysis could be a first step in this direction. An indispensable foundation for this undertaking is a terminology that describes the combined design space. To build a bridge between the domains, we adopt two of the established theoretical constructs from visualization theory for the field of sonification. The two constructs are the spatial substrate and the visual mark. In our model, we choose time to be the temporal substrate of sonification. Auditory marks are then positioned in time, such as visual marks are positioned in space. The proposed definitions allow discussing visualization and sonification designs as well as multi-modal designs based on a common terminology. While the identified terminology can support audio-visual analytics research, it also provides a new perspective on sonification theory itself.","2021-10-15","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","64–71","","","","","","It’s about Time","AM '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/ZPACRWP4/Enge et al. - 2021 - It’s about Time Adopting Theoretical Constructs f.pdf","","","Audio-Visual Data Analysis; Sonification Theory; Visualization Theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BEPCKVNX","conferencePaper","2022","Joo, Woohun","Graphic-to-Sound Sonification for Visual and Auditory Communication Design","Proceedings of the 17th International Audio Mostly Conference","978-1-4503-9701-8","","10.1145/3561212.3561214","https://dl.acm.org/doi/10.1145/3561212.3561214","I designed two sonification platforms designed for visual/auditory communication design studies and audiovisual art. The purpose of this study was to examine whether test participants can associate visuals and sound without any prior training and sonification approaches in this paper can be utilized as an interactive musical expression. The platform for the communication design study was developed first and the artistic audiovisual platform with the same sonification methodology followed next. In this paper, I introduce the (former) sonification platform designed for the image-to-sound association studies, their sonification methodologies, and present the study results. The object-oriented sonification method that I newly developed describes each shape sonically. The five image-sound association studies were conducted to see whether people can successfully associate sounds and fundamental shapes (i.e., a circle, a triangle, a square, lines, curves, and other custom shapes). Regardless of age and educational background, the correct answer rate was high.","2022-10-10","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","1–6","","","","","","","AM '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/T44QUYT3/Joo - 2022 - Graphic-to-Sound Sonification for Visual and Audit.pdf","","","Sonification; Auditory Icon; Auditory Symbol; Graphic Sonification; Image-Sound Sonification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q736TKGZ","conferencePaper","2021","Quinton, Michael; McGregor, Iain; Benyon, David","Sonification of Planetary Orbits in Asteroid Belts","Proceedings of the 16th International Audio Mostly Conference","978-1-4503-8569-5","","10.1145/3478384.3478390","https://dl.acm.org/doi/10.1145/3478384.3478390","This study investigates the design and evaluation of a sonification designed to detect any planets orbiting within an asteroid belt of an exosolar system. The interface was designed for an astronomer who studies this phenomenon. User centered design methods were applied to create an accurate sonification of the data that could allow the astronomer to perceive possible planetary movements within an asteroid belt. The sonification was developed over three stages: A requirements gathering exercise inquiring about the data that the astronomer uses in her work. A design and development stage based on the findings of the requirements gathering and the third stage, an evaluation of the sonification design. The sonification effectively allowed the astronomer to immediately detect a planet orbiting within an asteroid belt. Multiple parameter mappings provide richer auditory stimuli that are more semantical to the user. The use of more familiar, natural sounding sound design led to a clearer comprehension of the dataset. The use of spatial mapping and movement allowed for immediate identification and understanding of the planet's course through the asteroid belt.","2021-10-15","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","72–80","","","","","","","AM '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/RYPYS3A3/Quinton et al. - 2021 - Sonification of Planetary Orbits in Asteroid Belts.pdf","","","Sonification; Astronomy; Exoplanet hunting; Grounded theory; Parameter mapping sonification; User centered design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q7GGXC6Q","conferencePaper","2017","Ferguson, Jamie; Brewster, Stephen A.","Evaluation of psychoacoustic sound parameters for sonification","Proceedings of the 19th ACM International Conference on Multimodal Interaction","978-1-4503-5543-8","","10.1145/3136755.3136783","https://dl.acm.org/doi/10.1145/3136755.3136783","Sonification designers have little theory or experimental evidence to guide the design of data-to-sound mappings. Many mappings use acoustic representations of data values which do not correspond with the listener's perception of how that data value should sound during sonification. This research evaluates data-to-sound mappings that are based on psychoacoustic sensations, in an attempt to move towards using data-to-sound mappings that are aligned with the listener's perception of the data value's auditory connotations. Multiple psychoacoustic parameters were evaluated over two experiments, which were designed in the context of a domain-specific problem - detecting the level of focus of an astronomical image through auditory display. Recommendations for designing sonification systems with psychoacoustic sound parameters are presented based on our results.","2017-11-03","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","120–127","","","","","","","ICMI '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/LIZTJAG3/Ferguson and Brewster - 2017 - Evaluation of psychoacoustic sound parameters for .pdf","","","Sonification; Psychoacoustics; Auditory Display","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZK3SCJC3","conferencePaper","2022","Seiça, Mariana; Roque, Licínio; Martins, Pedro; Cardoso, F. Amílcar","An Illustrative Design Case of Systemic Sonification","Proceedings of the 17th International Audio Mostly Conference","978-1-4503-9701-8","","10.1145/3561212.3561224","https://dl.acm.org/doi/10.1145/3561212.3561224","The experience of sonification as a living system is a recent proposal for designing audio-centred communication. Drawing concepts from embodied perception and phenomenology of interaction, the systemic sonification approach has been characterised as a dynamic, evolving auditory community of sound beings that act and are acted upon by humans through interactive exchanges. In this study, we take on this theoretical proposal to explore a tentative design and develop a proof-of-concept of how this approach can be realised in practice. Departing from a previous sonification exercise of retail consumption data and adopting the proposed foundations of systemism, we develop an illustrative design case for the system’s composition, its environment, its structure, and the mechanisms which translate the behaviour of the evolving system to human interaction. While discussing the particular results, we debate the generativity of such a perspective towards envisioning alternative design spaces and novel interactive experiences.","2022-10-10","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","171–178","","","","","","","AM '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/K2N66EDL/Seiça et al. - 2022 - An Illustrative Design Case of Systemic Sonificati.pdf","","","Sonification; Sound Design; Interacting with Sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PN3RIP9J","conferencePaper","2019","Kalampratsidou, Vilelmini; Torres, Elizabeth B.","Bodily Signals Entrainment in the Presence of Music","Proceedings of the 6th International Conference on Movement and Computing","978-1-4503-7654-9","","10.1145/3347122.3347125","https://dl.acm.org/doi/10.1145/3347122.3347125","As a user listens to music, his bodily biorhythms can entrain with the music's rhythms. This work describes a human computer interface used to characterize the evolution of the stochastic signatures of physiological rhythms across the central and the peripheral nervous systems in the presence (or absence) of music. We track the heart, EEG and kinematics' variability under different music-driven conditions to identify the parameter manifold and context with maximal signal to noise ratio as well as to identify regions of maximal and minimal statistical co-dependencies of present events from past events.","2019-10-10","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","1–8","","","","","","","MOCO '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/VSCJYGJ3/Kalampratsidou and Torres - 2019 - Bodily Signals Entrainment in the Presence of Musi.pdf","","","sonification; music; autocorrelation; biorhythms; bodily signals; body movement; heart activity; micro-movements; musical features; stochastic signatures","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QRLHLRUB","conferencePaper","2020","van Rheden, Vincent; Grah, Thomas; Meschtscherjakov, Alexander","Sonification approaches in sports in the past decade: a literature review","Proceedings of the 15th International Audio Mostly Conference","978-1-4503-7563-4","","10.1145/3411109.3411126","https://dl.acm.org/doi/10.1145/3411109.3411126","The raise of novel mobile sensor and actuator technologies in the past decade enabled sonification of sport related data in real time for the athlete. We have systematically analysed scientific literature with respect to sports domains, feedback goals, user types, used sensor technology, and how the captured sensor data was sonified. We describe these aspects of 34 selected papers. Our analysis enables researchers to grasp prevalent approaches and to identify blind spots for new ways how sonification can not only aid athletes to improve their performance, but also to enhance the experience of various sports practitioners through sonification.","2020-09-16","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","199–205","","","","","","Sonification approaches in sports in the past decade","AM '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/ZIRUMGXM/van Rheden et al. - 2020 - Sonification approaches in sports in the past deca.pdf","","","sonification; human-computer interaction; literature analysis; sports","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VUVYYQKQ","conferencePaper","2012","Hermann, Thomas; Neumann, Alexander; Zehe, Sebastian","Head gesture sonification for supporting social interaction","Proceedings of the 7th Audio Mostly Conference: A Conference on Interaction with Sound","978-1-4503-1569-2","","10.1145/2371456.2371469","https://dl.acm.org/doi/10.1145/2371456.2371469","In this paper we introduce two new methods for real-time sonification of head movements and head gestures. Head gestures such as nodding or shaking the head are important non-verbal back-channelling signals which facilitate coordination and alignment of communicating interaction partners. Visually impaired persons cannot interpret such non-verbal signals, same as people in mediated communication (e.g. on the phone), or cooperating users whose visual attention is focused elsewhere. We introduce our approach to tackle these issues, our sensing setup and two different sonification methods. A first preliminary study on the recognition of signals shows that subjects understand the gesture type even without prior explanation and can estimate gesture intensity and frequency with no or little training.","2012-09-26","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","82–89","","","","","","","AM '12","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/V82EB5YC/Hermann et al. - 2012 - Head gesture sonification for supporting social in.pdf","","","sonification; auditory display; assistive technology; head gestures; interaction technology; mediated communication; social interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CNTIYAQT","conferencePaper","2015","Smith, Thomas; Bowen, Simon J.; Nissen, Bettina; Hook, Jonathan; Verhoeven, Arno; Bowers, John; Wright, Peter; Olivier, Patrick","Exploring Gesture Sonification to Support Reflective Craft Practice","Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems","978-1-4503-3145-6","","10.1145/2702123.2702497","https://dl.acm.org/doi/10.1145/2702123.2702497","Much of the knowing employed in skilled craft practice is difficult to communicate solely through written or verbal description. Consequently, the reflection and development of a craft practice in this manner may miss important nuances of practitioners' skills and experiences. We created digital technologies to sonify (using audio to perceptualize data) a group of craft practitioners' gestures to explore how we can aid their reflection in and on their craft, and consequently develop it. Over a number of workshops, the design of these sonifications were iterated based on how the practitioners responded to them. We found that direct sonification of gesture (sounds generated directly from motion sensor data) helped practitioners understand and reflect upon their own and each other's practice, encouraged discussion and enabled modification of craft technique.","2015-04-18","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","67–76","","","","","","","CHI '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/2EELQD7L/Smith et al. - 2015 - Exploring Gesture Sonification to Support Reflecti.pdf","","","sonification; craft skills; reflective practice","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CS3U6UXY","conferencePaper","2020","Giomi, Andrea","Somatic Sonification in Dance Performances. From the Artistic to the Perceptual and Back","Proceedings of the 7th International Conference on Movement and Computing","978-1-4503-7505-4","","10.1145/3401956.3404226","https://dl.acm.org/doi/10.1145/3401956.3404226","Since the end of the 1980s, interactive musical systems have played an increasingly relevant role in dance performances. More recently, the use of interactive auditory feedback for sensorimotor learning such as movement sonification has gained currency and scientific attention in a variety of fields ranging from rehabilitation to sport training, neuroscience and product design. This paper investigates the convergence between interactive music/dance systems and movement sonification in the field of dance. The main question we address is whether the emergence of the notion of sonification can foster new perspectives for practice-based artistic research. In this context, we highlight a fundamental shift of perspective from musical interactivity per se to the somatic knowledge provided by the real time sonification of movement, which can be considered as a major somatic-sonification turn.","2020-07-15","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","1–8","","","","","","","MOCO '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/E6VWIYUE/Giomi - 2020 - Somatic Sonification in Dance Performances. From t.pdf","","","movement sonification; dance technology; dance/music interactive systems; somatic sonification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XB7UKN3J","conferencePaper","2014","Cruz, Pablo","A parameter mapping sonification for price differences in market research studies","Proceedings of the 7th Euro American Conference on Telematics and Information Systems","978-1-4503-2435-9","","10.1145/2590651.2590777","https://dl.acm.org/doi/10.1145/2590651.2590777","Differences in prices registered in market research studies are common. Visualization of the price differences is difficult to develop and understand as several attributes are considered by the analysts. An alternative to visualization of the price differences is sonification, a kind of auditory display, in which these attributes and relations are translated into sound. In this paper we present a parameter mapping sonification for differences of prices in market research studies. As a proof of concept, we show results from an implementation of the sonification which creates four samples, one for each category of differences defined in this work. Also we describe its use and its benefits in a real market research environment.","2014-04-02","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","1–6","","","","","","","EATIS '14","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/Z4UR6F8C/Cruz - 2014 - A parameter mapping sonification for price differe.pdf","","","sonification; auditory displays; numerical sound synthesis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NFNYFANN","conferencePaper","2016","Dietz, Michael; Garf, Maha El; Damian, Ionut; André, Elisabeth","Exploring Eye-Tracking-Driven Sonification for the Visually Impaired","Proceedings of the 7th Augmented Human International Conference 2016","978-1-4503-3680-2","","10.1145/2875194.2875208","https://dl.acm.org/doi/10.1145/2875194.2875208","Most existing sonification approaches for the visually impaired restrict the user to the perception of static scenes by performing sequential scans and transformations of visual information to acoustic signals. This takes away the user's freedom to explore the environment and to decide which information is relevant at a given point in time. As a solution, we propose an eye tracking system to allow the user to choose which elements of the field of view should be sonified. More specifically, we enhance the sonification approaches for color, text and facial expressions with eye tracking mechanisms. To find out how visually impaired people might react to such a system we applied a user centered design approach. Finally, we explored the effectiveness of our concept in a user study with seven visually impaired persons. The results show that eye tracking is a very promising input method to control the sonification, but the large variety of visual impairment conditions restricts the applicability of the technology.","2016-02-25","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","1–8","","","","","","","AH '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/46E62CX9/Dietz et al. - 2016 - Exploring Eye-Tracking-Driven Sonification for the.pdf","","","Sonification; Eye Tracking; Signal Processing; Sound Synthesis; Visually Impaired","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UESPQR7B","conferencePaper","2018","Ferguson, Jamie; Brewster, Stephen A.","Investigating Perceptual Congruence between Data and Display Dimensions in Sonification","Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","978-1-4503-5620-6","","10.1145/3173574.3174185","https://dl.acm.org/doi/10.1145/3173574.3174185","The relationships between sounds and their perceived meaning and connotations are complex, making auditory perception an important factor to consider when designing sonification systems. Listeners often have a mental model of how a data variable should sound during sonification and this model is not considered in most data:sound mappings. This can lead to mappings that are difficult to use and can cause confusion. To investigate this issue, we conducted a magnitude estimation experiment to map how roughness, noise and pitch relate to the perceived magnitude of stress, error and danger. These parameters were chosen due to previous findings which suggest perceptual congruency between these auditory sensations and conceptual variables. Results from this experiment show that polarity and scaling preference are dependent on the data:sound mapping. This work provides polarity and scaling values that may be directly utilised by sonification designers to improve auditory displays in areas such as accessible and mobile computing, process-monitoring and biofeedback.","2018-04-21","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","1–9","","","","","","","CHI '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/8HHC9VIZ/Ferguson and Brewster - 2018 - Investigating Perceptual Congruence between Data a.pdf","","","sonification; auditory display; mental models; psychoacoustics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YXNVLGTA","conferencePaper","2022","Ziemer, Tim; Schultheis, Holger","Both Rudimentary Visualization and Prototypical Sonification can Serve as a Benchmark to Evaluate New Sonification Designs","Proceedings of the 17th International Audio Mostly Conference","978-1-4503-9701-8","","10.1145/3561212.3561228","https://dl.acm.org/doi/10.1145/3561212.3561228","Comparing sonification with visualization is like comparing apples and oranges. While visualizations are ubiquitous to the public and have established names, principles, application areas, and sophisticated designs, sonifications tend to be unique, self-made and completely new to users. In this study we developed a rudimentary visualization that is related closely to the principle of the sonification designs that we want to evaluate. In addition, we implemented a prototypical sonification that uses the most common mapping principles. Experiment results show that participants perform similarly well using the rudimentary visualization and the prototypical sonification, which is much better than chance but significantly worse than using our new sonification design. We therefore argue that both rudimentary visualization and prototypical sonifications can serve as a suitable benchmark to evaluate new sonifications designs against.","2022-10-10","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","24–31","","","","","","","AM '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/9EFBAEH4/Ziemer and Schultheis - 2022 - Both Rudimentary Visualization and Prototypical So.pdf","","","visualization; auditory display evaluation; sonification evaluation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZAUQJP7B","conferencePaper","2013","Seibert, Gabriela; Hug, Daniel","Bringing musicality to movement sonification: design and evaluation of an auditory swimming coach","Proceedings of the 8th Audio Mostly Conference","978-1-4503-2659-9","","10.1145/2544114.2544127","https://dl.acm.org/doi/10.1145/2544114.2544127","In this paper we describe a novel approach to the sonification of crawl swim movement. The design method integrates task and data analysis from a sport science perspective with subjective experience of swimmers and swimming coaches, and strongly relies on the skills of musicians in order to define the basic sonic design. We report on the design process, and on the implementation and evaluation of a first prototype.","2013-09-18","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","1–6","","","","","","Bringing musicality to movement sonification","AM '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/WG93NNGQ/Seibert and Hug - 2013 - Bringing musicality to movement sonification desi.pdf","","","movement sonification; musical improvisation; swimming sport training","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YEYYNZVN","conferencePaper","2015","Gerino, Andrea; Picinali, Lorenzo; Bernareggi, Cristian; Alabastro, Nicolò; Mascetti, Sergio","Towards Large Scale Evaluation of Novel Sonification Techniques for Non Visual Shape Exploration","Proceedings of the 17th International ACM SIGACCESS Conference on Computers & Accessibility","978-1-4503-3400-6","","10.1145/2700648.2809848","https://dl.acm.org/doi/10.1145/2700648.2809848","There are several situations in which a person with visual impairment or blindness needs to extract information from an image. Examples include everyday activities, like reading a map, as well as educational activities, like exercises to develop visuospatial skills. In this contribution we propose a set of 6 sonification techniques to recognize simple shapes on touchscreen devices. The effectiveness of these sonification techniques is evaluated though Invisible Puzzle, a mobile application that makes it possible to conduct non-supervised evaluation sessions. Invisible Puzzle adopts a gamification approach and is a preliminary step in the development of a complete game that will make it possible to conduct a large scale evaluation with hundreds or thousands of blind users. With Invisible Puzzle we conducted 131 tests with sighted subjects and 18 tests with subjects with blindness. All subjects involved in the process successfully completed the evaluation session, with high engagement, hence showing the effectiveness of the evaluation procedure. Results give interesting insights on the differences among the sonification techniques and, most importantly, show that, after a short training, subjects are able to identify many different shapes.","2015-10-26","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","13–21","","","","","","","ASSETS '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/KX323MZU/Gerino et al. - 2015 - Towards Large Scale Evaluation of Novel Sonificati.pdf","","","sonification; accessibility; gamification; image recognition; remote evaluation; touch-screen devices; visual impairment or blindness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"USHVYIU3","conferencePaper","2013","Hermann, Thomas; Neumann, Alexander; Schnier, Christian; Pitsch, Karola","Sonification for supporting joint attention in dyadic augmented reality-based cooperation","Proceedings of the 8th Audio Mostly Conference","978-1-4503-2659-9","","10.1145/2544114.2597651","https://dl.acm.org/doi/10.1145/2544114.2597651","This paper presents a short evaluation of auditory representations for object interactions as support for cooperating users of an Augmented Reality(AR) system. Particularly head-mounted AR displays limit the field of view and thus cause users to miss relevant activities of their interaction partner, such as object interactions or deictic references that normally would be effective to establish joint attention. We start from an analysis of the differences between face-to-face interaction and interaction via the AR system, using interaction linguistic conversation analysis. From that we derive a set of features that are relevant for interaction partners to co-ordinate their activities. We then present five different interactive sonifications which make object manipulations of interaction partners audible by sonification that convey information about the kind of activity.","2013-09-18","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","1–6","","","","","","","AM '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/3HR8KCJN/Hermann et al. - 2013 - Sonification for supporting joint attention in dya.pdf","","","sonification; auditory display; assistive technology; mediated communication; social interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q7NKBBNV","conferencePaper","2022","Holloway, Leona M; Goncu, Cagatay; Ilsar, Alon; Butler, Matthew; Marriott, Kim","Infosonics: Accessible Infographics for People who are Blind using Sonification and Voice","Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems","978-1-4503-9157-3","","10.1145/3491102.3517465","https://dl.acm.org/doi/10.1145/3491102.3517465","Data visualisations are increasingly used online to engage readers and enable independent analysis of the data underlying news stories. However, access to such infographics is problematic for readers who are blind or have low vision (BLV). Equitable access to information is a basic human right and essential for independence and inclusion. We introduce infosonics, the audio equivalent of infographics, as a new style of interactive sonification that uses a spoken introduction and annotation, non-speech audio and sound design elements to present data in an understandable and engaging way. A controlled user evaluation with 18 BLV adults found a COVID-19 infosonic enabled a clearer mental image than a traditional sonification. Further, infosonics prove complementary to text descriptions and facilitate independent understanding of the data. Based on our findings, we provide preliminary suggestions for infosonics design, which we hope will enable BLV people to gain equitable access to online news and information.","2022-04-29","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","1–13","","","","","","Infosonics","CHI '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/EPMY2E3Z/Holloway et al. - 2022 - Infosonics Accessible Infographics for People who.pdf","","","sonification; Blind; accessible graphics; information access; low vision","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y9AEJB3E","conferencePaper","2017","Pigrem, Jon; Barthet, Mathieu","Datascaping: Data Sonification as a Narrative Device in Soundscape Composition","Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences","978-1-4503-5373-1","","10.1145/3123514.3123537","https://dl.acm.org/doi/10.1145/3123514.3123537","Soundscape composition is an art form that has grown from acoustic ecology and soundscape studies. Current practices foster a wide range of approaches, from the educational and documentary function of the world soundscape project (WSP) to the creation of imaginary sonic worlds supported by theories of acousmatic and electroacoustic music. Sonification is the process of rendering audio in response to data, and is often used in scenarios where visual representations of data are impractical. The field of auditory display has grown in isolation to soundscape composition, however fosters conceptual similarities in its representation of information in sonic form. This paper investigates the use of data sonification as a narrative tool in soundscape composition. A soundscape has been created using traditional concrete sounds (fixed media recorded sound objects), augmented with sonified real-time elements. An online survey and listening experiment was conducted, which asked participants to rate the soundscape on its ability to communicate specific detail with regard to environmental and social elements contained within. Research data collected shows a strong ability in participants to decode and comprehend additional layers of narrative information communicated through the soundscape.","2017-08-23","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","1–8","","","","","","Datascaping","AM '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/DMU4YXZW/Pigrem and Barthet - 2017 - Datascaping Data Sonification as a Narrative Devi.pdf","","","data sonification; acoustic ecology; generative music; soundscape composition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MFCCAYZ8","conferencePaper","2014","Françoise, Jules; Fdili Alaoui, Sarah; Schiphorst, Thecla; Bevilacqua, Frederic","Vocalizing dance movement for interactive sonification of laban effort factors","Proceedings of the 2014 conference on Designing interactive systems","978-1-4503-2902-6","","10.1145/2598510.2598582","https://dl.acm.org/doi/10.1145/2598510.2598582","We investigate the use of interactive sound feedback for dance pedagogy based on the practice of vocalizing while moving. Our goal is to allow dancers to access a greater range of expressive movement qualities through vocalization. We propose a methodology for the sonification of Effort Factors, as defined in Laban Movement Analysis, based on vocalizations performed by movement experts. Based on the experiential outcomes of an exploratory workshop, we propose a set of design guidelines that can be applied to interactive sonification systems for learning to perform Laban Effort Factors in a dance pedagogy context.","2014-06-21","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","1079–1082","","","","","","","DIS '14","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/BV4QIR3S/Françoise et al. - 2014 - Vocalizing dance movement for interactive sonifica.pdf","","","interactive sonification; multimodal; dance; effort factors; laban movement analysis; motion-sound mapping; movement qualities; vocalization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FUUVS7U9","conferencePaper","2013","Schuett, Jonathan H.; Walker, Bruce N.","Measuring comprehension in sonification tasks that have multiple data streams","Proceedings of the 8th Audio Mostly Conference","978-1-4503-2659-9","","10.1145/2544114.2544121","https://dl.acm.org/doi/10.1145/2544114.2544121","When the goal of an auditory display is to provide inference or intuition to a listener, it is important for researchers and sound designers to gauge users' comprehension of the display to determine if they are, in fact, receiving the correct message. This paper discusses an approach to measuring listener comprehension in sonifications that contain multiple concurrently presented data series. We draw from situation awareness research that has developed measures of comprehension within environments or scenarios, based on our view that an auditory scene is similar to a virtual or mental representation of the listener's environment.","2013-09-18","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","1–6","","","","","","","AM '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/3E2BEDNY/Schuett and Walker - 2013 - Measuring comprehension in sonification tasks that.pdf","","","sonification; auditory displays; comprehension; multiple data series; situation awareness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"86NWBA99","conferencePaper","2008","Song, Hong Jun; Beilharz, Kirsty","Aesthetic and auditory enhancements for multi-stream information sonification","Proceedings of the 3rd international conference on Digital Interactive Media in Entertainment and Arts","978-1-60558-248-1","","10.1145/1413634.1413678","https://dl.acm.org/doi/10.1145/1413634.1413678","Sonification is an emerging modality of information representation, the auditory equivalent of visualization employing non-speech sound to display attributes of form, pattern, recurrence and trends in abstract data. Like data-art or visual and auditory art-forms driven by data content directly mapped to their rendering, sonification shares the goal of aesthetic representation (auditory graphing) in a way to better and more accessibly convey the message to broader consumer audiences. Often, the simple re-contextualization of dense abstract data in an auditory graph (or sonification) is sufficient to highlight long-term trends, to hear regularities (patterns) and anomalies in periodicity of time-series data and to assimilate very subtle and fine transformations. Sonification is also optimal for certain working or ambient situations that are visually rich or visually saturated, when we seek to command topical and peripheral attention with relevant cues. Auditory display is also an alternative to visualization for people with visual impairments. Exploring the premise that sonification should be both aesthetic and informative, i.e. listenable, attractive and engaging, this paper summarises the findings of 3 experiments conducted to determine ways to better represent and access dense information mapped on to more than one concurrent stream of information. Specifically, we show evidence that spatialization of informative events coinciding in time can be more clearly distinguished and that timbre (or tone colour / tone quality) characteristics can serve to further reinforce spatial and stream separation. These findings combine to develop comprehensible methods for representing complex data-sets. We consider human cognition, auditory perception and audio reproduction technologies that each influence the ability to display information sonically.","2008-09-10","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","224–231","","","","","","","DIMEA '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/ZCKTG3VP/Song and Beilharz - 2008 - Aesthetic and auditory enhancements for multi-stre.pdf","","","auditory stream segregation; concurrent auditory graphing; information sonification; spatialization; timbre","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5VKQ4S43","conferencePaper","2020","Kalampratsidou, Vilelmini; Torres, Elizabeth B.","Sonification of heart rate variability can entrain bodies in motion","Proceedings of the 7th International Conference on Movement and Computing","978-1-4503-7505-4","","10.1145/3401956.3404186","https://dl.acm.org/doi/10.1145/3401956.3404186","In this work, we introduce a co-adaptive closed-loop interface driven by audio augmented with a parameterization of the dancer's heart-rate in near real-time. In our set-up, two salsa dancers perform their routine dance (previously choreographed and well-trained) and a spontaneously improvised piece lead by the male dancer. They firstly dance their pieces while listening to the original version of the song (baseline condition). Then, we ask them to dance while listening to the music, as altered by the heart rate extracted from the female dancer in near real-time. Salsa dancing is always led by the male. As such, their challenge is to adapt, their movements, as a dyad, to the real-time change induced by the female's heart activity. Our work offers a new co-adaptive set up for dancers, new data types and analytical methods to study two forms of dance: well-rehearsed choreography and improvisation. We show that the small variations in heart activity, despite its robustness for autonomic function, can distinguish well between these two modes of dance.","2020-07-15","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","1–8","","","","","","","MOCO '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/74UASUJ8/Kalampratsidou and Torres - 2020 - Sonification of heart rate variability can entrain.pdf","","","Audio augmentation; Autocorrelation; Bodily signal entrainment; Cross-correlation; Dancers; Heart rate; Heart sonification; Movement; Musical feature extraction; Partnering; Salsa; Stochastic signatures","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L6LQNR35","conferencePaper","2013","Banf, Michael; Blanz, Volker","Sonification of images for the visually impaired using a multi-level approach","Proceedings of the 4th Augmented Human International Conference","978-1-4503-1904-1","","10.1145/2459236.2459264","https://dl.acm.org/doi/10.1145/2459236.2459264","This paper presents a system that strives to give visually impaired persons direct perceptual access to images via an acoustic signal. The user explores the image actively on a touch screen and receives auditory feedback about the image content at the current position. The design of such a system involves two major challenges: what is the most useful and relevant image information, and how can as much information as possible be captured in an audio signal. We address both problems, and propose a general approach that combines low-level information, such as color, edges, and roughness, with mid- and high-level information obtained from Machine Learning algorithms. This includes object recognition and the classification of regions into the categories ""man made"" versus ""natural"". We argue that this multi-level approach gives users direct access to what is where in the image, yet it still exploits the potential of recent developments in Computer Vision and Machine Learning.","2013-03-07","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","162–169","","","","","","","AH '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/ESPVLP7G/Banf and Blanz - 2013 - Sonification of images for the visually impaired u.pdf","","","sonification; computer vision; exploration; machine learning; sound synthesis; visually impaired","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WKQ966WR","conferencePaper","2022","Luo, Gang; Chen, Hao; Li, Zhengxiu; Wang, Mingxun","Music generation based on emotional EEG","2022 the 6th International Conference on Innovation in Artificial Intelligence (ICIAI)","978-1-4503-9550-2","","10.1145/3529466.3529492","https://dl.acm.org/doi/10.1145/3529466.3529492","Transforming electroencephalogram (EEG) into music has been playing an important role in social life. How to generate music that can express a certain emotion state is a challenge for most of the existing generative models in the studies. To address the problem, a music generation method based on emotional EEG is proposed in this paper. In this method, sequence to sequence long-short term memory is utilized to train the emotional music to obtain emotional music generators, and support vector machine is used to get emotional information. The features related to emotion are extracted to map into musical parameters and emotion music generator is used to generate the emotional EEG music. The experimental results show that the music generated by the proposed method achieves a high performance with respect to both emotion-expressing and musicality.","2022-06-04","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","143–147","","","","","","","ICIAI 2022","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/7AYAFUXH/Luo et al. - 2022 - Music generation based on emotional EEG.pdf","","","EEG; emotion recognition; LSTM; music generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HVNARRIY","conferencePaper","2021","Thorn, Seth","Telematic Wearable Music: Remote Ensembles and Inclusive Embodied Education","Proceedings of the 16th International Audio Mostly Conference","978-1-4503-8569-5","","10.1145/3478384.3478386","https://dl.acm.org/doi/10.1145/3478384.3478386","The author discusses telematic wearable music, recounting the design and evolution of improvised techniques and approaches in a remotely taught course offered to undergraduates. This new contribution to interactive interfaces for remote ensembles is musically motivated and inclusive for non-specialists who apply musical instincts they discover through participation. Students are introduced to wearable ”Internet of Things” (IoT) computing, synthesis, and sound design, with the goal of developing rich, movement responsive, individually and/or collectively playable wearable instruments. The course facilitates practice-situated investigation of accessible, agile, and inexpensive modes of distributed creativity in musical interaction through experiential inquiry and tinkering. Telematic wearable music aspires to enact shared, situated spaces and less ocularcentric modes of learning through embodied sonic telepresence, emphasizing and enhancing embodied participation in synchronous remote learning.","2021-10-15","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","188–195","","","","","","Telematic Wearable Music","AM '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/4YTJM8UA/Thorn - 2021 - Telematic Wearable Music Remote Ensembles and Inc.pdf","","","music; Internet of Things; wearables; distance learning; distributed creativity; remote ensembles; telematic music","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GD697KIZ","conferencePaper","2009","Chemseddine, Maher; Noirhomme-Fraiture, Monique","Classification des émotions et des données pour la « sonification » étude de cas: événements d'un jeu vidéo","Proceedings of the 21st International Conference on Association Francophone d'Interaction Homme-Machine","978-1-60558-461-4","","10.1145/1629826.1629856","https://dl.acm.org/doi/10.1145/1629826.1629856","Our ultimate goal is to ""sonify"" different applications, particularly video games. To achieve this aim we choose to rely on user emotion experience with some applications. In this paper we present a classification of emotions and game events (scenarios) of a classic arcade video game (attack, enemy approaching, destroying etc.) in a two-dimensional space. We selected 18 words of emotion in different languages (Arabic, English and French) and 14 events. Our statistical analysis showed a distribution of emotions in 8 major classes in the two-dimensional space (valence, arousal). We associated each class with a set of game events based on 72 user's own emotional experience.","2009-10-13","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","193–196","","","","","","Classification des émotions et des données pour la « sonification » étude de cas","IHM '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/KWTCNTLK/Chemseddine and Noirhomme-Fraiture - 2009 - Classification des émotions et des données pour la.pdf","","","sonification; emotion; classification; game event","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A6IF7ATG","conferencePaper","2020","Li, Yana","Technologies and Music Therapy from the Perspective of Music Therapists","Proceedings of the Fourth International Conference on Biological Information and Biomedical Engineering","978-1-4503-7709-6","","10.1145/3403782.3403789","https://dl.acm.org/doi/10.1145/3403782.3403789","Music therapy has been used to treat individuals with Autism Spectrum Disorders (ASD) for a long period of time. As the development of technologies, both patients and therapists benefit from technological applications. This literature reviews music therapy and its effects on autism people, along with the classifications of technologies used by music therapists and their applications. With regard to the review work, discussion is drawn on challenges that therapists are facing toward technologies, including lack of education and training as well as complicated interface of technologies.","2020-07-21","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","1–5","","","","","","","BIBE2020","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/MAKW2QCA/Li - 2020 - Technologies and Music Therapy from the Perspectiv.pdf","","","Music; Music therapy; Autism Spectrum Disorder; Review; Technology development","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HUTE6IBU","conferencePaper","2018","Bilalpur, Maneesh; Kankanhalli, Mohan; Winkler, Stefan; Subramanian, Ramanathan","EEG-based Evaluation of Cognitive Workload Induced by Acoustic Parameters for Data Sonification","Proceedings of the 20th ACM International Conference on Multimodal Interaction","978-1-4503-5692-3","","10.1145/3242969.3243016","https://dl.acm.org/doi/10.1145/3242969.3243016","Data Visualization has been receiving growing attention recently, with ubiquitous smart devices designed to render information in a variety of ways. However, while evaluations of visual tools for their interpretability and intuitiveness have been commonplace, not much research has been devoted to other forms of data rendering, \eg, sonification. This work is the first to automatically estimate the cognitive load induced by different acoustic parameters considered for sonification in prior studies~\citeferguson2017evaluation,ferguson2018investigating. We examine cognitive load via (a) perceptual data-sound mapping accuracies of users for the different acoustic parameters, (b) cognitive workload impressions explicitly reported by users, and (c) their implicit EEG responses compiled during the mapping task. Our main findings are that (i) low cognitive load-inducing (ıe, more intuitive) acoustic parameters correspond to higher mapping accuracies, (ii) EEG spectral power analysis reveals higher α band power for low cognitive load parameters, implying a congruent relationship between explicit and implicit user responses, and (iii) Cognitive load classification with EEG features achieves a peak F1-score of 0.64, confirming that reliable workload estimation is achievable with user EEG data compiled using wearable sensors.","2018-10-02","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","315–323","","","","","","","ICMI '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/PPZGU7H6/Bilalpur et al. - 2018 - EEG-based Evaluation of Cognitive Workload Induced.pdf","","","data sonification; acoustic parameters; cognitive workload; eeg","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PWKNZ8DB","conferencePaper","2021","Nath, Surabhi","Hear Her Fear: Data Sonification for Sensitizing Society on Crime Against Women in India","Proceedings of the 11th Indian Conference on Human-Computer Interaction","978-1-4503-8944-0","","10.1145/3429290.3429307","https://dl.acm.org/doi/10.1145/3429290.3429307","Data sonification is a means of representing data through sound and has been utilized in a variety of applications. Crime against women has been a rising concern in India. We explore the potential of data sonification to provide an immersive engagement with sensitive data on crime against women in Indian states. The data for nine crime categories covering thirty-five Indian states over a period of twelve years is acquired from National records. Sonification techniques of parameter mapping and auditory icons are adopted: sound parameters such as frequencies, amplitudes and timbres are incorporated to represent the crime data, and audio sounds of women screams are employed as auditory icons to emphasize the traumatic experience. Higher crime rates are assigned higher frequencies, harsher scream textures and larger amplitudes. A user-friendly interface is developed with multiple options for sequential and comparative data sonification. Through the interface, a user can evaluate and compare the extent of crime against women in different states, years or crime categories. Sound spatialization is used to immerse the listener in the sound and further intensify the sonification experience. To assess and validate effectiveness, a user study on twenty participants is conducted with feedback obtained through questionnaires. The responses indicate that the participants could comprehend trends in the data easily and found the data sonification experience impactful. Sonification may therefore prove to be a valuable tool for data representation in fields related to social and human studies.","2021-12-27","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","86–91","","","","","","Hear Her Fear","IndiaHCI '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/4M55NYWU/Nath - 2021 - Hear Her Fear Data Sonification for Sensitizing S.pdf","","","Parameter mapping; Auditory icons; Crime against women; Data sonification; Sound spatialization; User interface; User study","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7A5YV5XH","conferencePaper","2017","Roddy, S.","Composing The Good Ship Hibernia and the Hole in the Bottom of the World","Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences","978-1-4503-5373-1","","10.1145/3123514.3123520","https://dl.acm.org/doi/10.1145/3123514.3123520","This paper explores topics in embodied cognition, soundscape composition and sonification. It explains the compositional decisions and technical considerations that went into the composition of the piece The Good Ship Hibernia, which is an example of embodied soundscape sonification. This explanation is undertaken within the context of an approach to both sonification design and music composition that accounts for and exploits the embodied aspects of meaning-making in auditory cognition as described in the embodied cognitive science literature.","2017-08-23","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","1–6","","","","","","","AM '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/5C4ST2QD/Roddy - 2017 - Composing The Good Ship Hibernia and the Hole in t.pdf","","","Sonification; Music; Composition; Data-driven; Financial Data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ME8M8CEJ","conferencePaper","2014","Donnarumma, Marco","Notes on Bimodal Muscle Sensing for the Sonification of Indeterminate Motion","Proceedings of the 2014 International Workshop on Movement and Computing","978-1-4503-2814-2","","10.1145/2617995.2618028","https://dl.acm.org/doi/10.1145/2617995.2618028","This article offers an overview of a musical performance instrument that leverages bimodal muscle sensing for the sonification of motion. Namely, the instrument a) captures the sound produced by a performer's muscles and makes it available for real-time audio processing, and b) enables a performer to drive the processing parameters using high-level features extracted from muscle activity. This enables the performer to produce and finely shape sound with gestures that do not need to be specified beforehand as a vocabulary of a finite number of movements. This allows the performer and the software to create an open-ended range of sonified movements which arise from the interplay of bodily mechanisms and software processes.","2014-06-16","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","170–171","","","","","","","MOCO '14","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/9CQIV4NN/Donnarumma - 2014 - Notes on Bimodal Muscle Sensing for the Sonificati.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MNKQBP7W","conferencePaper","2015","Troiano, Giovanni Maria; Pedersen, Esben Warming; Hornbæk, Kasper","Deformable Interfaces for Performing Music","Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems","978-1-4503-3145-6","","10.1145/2702123.2702492","https://dl.acm.org/doi/10.1145/2702123.2702492","Deformable interfaces offer new possibilities for gestures, some of which have been shown effective in controlled laboratory studies. Little work, however, has attempted to match deformable interfaces to a demanding domain and evaluate them out of the lab. We investigate how musicians use deformable interfaces to perform electronic music. We invited musicians to three workshops, where they explored 10 deformable objects and generated ideas on how to use these objects to perform music. Based on the results from the workshops, we implemented sensors in the five preferred objects and programmed them for controlling sounds. Next, we ran a performance study where six musicians performed music with these objects at their studios. Our results show that (1) musicians systematically map deformations to certain musical parameters, (2) musicians use deformable interfaces especially to filter and modulate sounds, and (3) musicians think that deformable interfaces embody the parameters that they control. We discuss what these results mean to research in deformable interfaces.","2015-04-18","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","377–386","","","","","","","CHI '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/B784NWHU/Troiano et al. - 2015 - Deformable Interfaces for Performing Music.pdf","","","music; controller; deformable interfaces; usefulness; user interfaces; user study","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WDCBMJJK","conferencePaper","2013","Favilla, Stu; Pedell, Sonja","Touch screen ensemble music: collaborative interaction for older people with dementia","Proceedings of the 25th Australian Computer-Human Interaction Conference: Augmentation, Application, Innovation, Collaboration","978-1-4503-2525-7","","10.1145/2541016.2541088","https://dl.acm.org/doi/10.1145/2541016.2541088","This paper presents new touch-screen collaborative interaction models for people with dementia. The authors argue that dementia technology has yet to focus on group musical interactions. The project aims to contribute to dementia care while addressing a significant gap in current literature. Research includes observations and two system trials exploring contrasting musical scenarios: the performance of abstract electronic music and the distributed performance of J. S. Bach's Goldberg Variations. Findings presented in this paper suggest that dementia people are able to successfully perform and engage in collaborative music performance activities with little or no scaffolded instruction.","2013-11-25","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","481–484","","","","","","Touch screen ensemble music","OzCHI '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/THCADMVD/Favilla and Pedell - 2013 - Touch screen ensemble music collaborative interac.pdf","","","music; collaborative interaction; dementia; touch screen","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4ZUU8RWV","conferencePaper","2020","Wang, Yujia; Liang, Wei; Li, Wanwan; Li, Dingzeyu; Yu, Lap-Fai","Scene-Aware Background Music Synthesis","Proceedings of the 28th ACM International Conference on Multimedia","978-1-4503-7988-5","","10.1145/3394171.3413894","https://dl.acm.org/doi/10.1145/3394171.3413894","In this paper, we introduce an interactive background music synthesis algorithm guided by visual content. We leverage a cascading strategy to synthesize background music in two stages: Scene Visual Analysis and Background Music Synthesis. First, seeking a deep learning-based solution, we leverage neural networks to analyze the sentiment of the input scene. Second, real-time background music is synthesized by optimizing a cost function that guides the selection and transition of music clips to maximize the emotion consistency between visual and auditory criteria, and music continuity. In our experiments, we demonstrate the proposed approach can synthesize dynamic background music for different types of scenarios. We also conducted quantitative and qualitative analysis on the synthesized results of multiple example scenes to validate the efficacy of our approach.","2020-10-12","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","1162–1170","","","","","","","MM '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/8WIYLNNI/Wang et al. - 2020 - Scene-Aware Background Music Synthesis.pdf","","","background music synthesis; music transition; scene sentiment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PW7ME83N","conferencePaper","2023","Guarese, Renan; Zambetta, Fabio; van Schyndel, Ron","Evaluating micro-guidance sonification methods in manual tasks for Blind and Visually Impaired people","Proceedings of the 34th Australian Conference on Human-Computer Interaction","9798400700248","","10.1145/3572921.3572929","https://dl.acm.org/doi/10.1145/3572921.3572929","This paper presents a user evaluation of seven sonification methods in two-dimensional (2D) manual micro-guidance tasks, which can be used as building blocks for spatialized audio in Mixed and Virtual Reality to model next-generation guidance aids for the Blind and Visually Impaired (BVI). The methods were tested in comparable interactive sonifications of 2D positions in a series of hand-navigation assessments with BVI and blindfolded sighted users, to validate the different approaches in environments without any visual feedback. Results highlighted that alternation and spatiality can be useful resources in sonified guidance, and that users accustomed to faster-than-regular audio speed replay tend to have more precise performances, while musical literacy only had a performance effect on methods highly dependent on aural skills. Ultimately, this work corroborates the notion that sonification may help BVI users perform better in day-to-day manual micro-guidance tasks such as retrieving items from a pantry, handling kitchen appliances, and properly discarding trash.","2023-04-06","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","260–271","","","","","","","OzCHI '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/VNMUEMCR/Guarese et al. - 2023 - Evaluating micro-guidance sonification methods in .pdf","","","assistive technologies; blind and visually impaired guidance; micro-guidance; mixed reality accessibility","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ABJMCZEQ","conferencePaper","2018","Ghisio, Simone; Kolykhalova, Ksenia; Volpe, Gualtiero; Amadeo, Beatrice; Coletta, Paolo; Ferrari, Nicola; Tacchino, Chiara; Fiscon, Sofia; Primavera, Ludovica; Moretti, Paolo; Camurri, Antonio","Designing a Platform for Child Rehabilitation Exergames Based on Interactive Sonification of Motor Behavior","Proceedings of the 4th EAI International Conference on Smart Objects and Technologies for Social Good","978-1-4503-6581-9","","10.1145/3284869.3284919","https://dl.acm.org/doi/10.1145/3284869.3284919","Designing multimodal systems for interactive computer play, as tools for supporting rehabilitation of children with motor and/or cognitive impairment is a complex process, requiring interaction of many stakeholders, including clinicians, therapists, engineers, human factors experts, parents, and children themselves. This paper discusses the participatory design process, leading us to conceive, develop, and iteratively refine a flexible and modular open platform for nonverbal interactive child rehabilitation exergames, with a specific focus on interactive sonification of motor behavior. The process was implemented through a collection of case studies carried out using an early prototype of the platform.","2018-11-28","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","208–213","","","","","","","Goodtechs '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/RP4FEIK5/Ghisio et al. - 2018 - Designing a Platform for Child Rehabilitation Exer.pdf","","","participatory design; exergames; interactive computer play; Multimodal systems; rehabilitation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GVPFWG4B","conferencePaper","2020","Mitchell, Thomas J.; Jones, Alex J.; O'Connor, Michael B.; Wonnacott, Mark D.; Glowacki, David R.; Hyde, Joseph","Towards molecular musical instruments: interactive sonifications of 17-alanine, graphene and carbon nanotubes","Proceedings of the 15th International Audio Mostly Conference","978-1-4503-7563-4","","10.1145/3411109.3411143","https://dl.acm.org/doi/10.1145/3411109.3411143","Scientists increasingly rely on computational models of atoms and molecules to observe, understand and make predictions about the microscopic world. Atoms and molecules are in constant motion, with vibrations and structural fluctuations occurring at very short time-scales and corresponding length-scales. But can these microscopic oscillations be converted into sound? And, what would they sound like? In this paper we present our initial steps towards a generalised approach for sonifying data produced by a real-time molecular dynamics simulation. The approach uses scanned synthesis to translate real-time geometric simulation data into audio. The process is embedded within a stand alone application as well as a variety of audio plugin formats to enable the process to be used as an audio synthesis method for music making. We review the relevant background literature before providing an overview of our system. Simulations of three molecules are then considered: 17-alanine, graphene and a carbon nanotube. Four examples are then provided demonstrating how the technique maps molecular features and parameters onto the auditory character of the resulting sound. A case study is then provided in which the sonification/synthesis method is used within a musical composition.","2020-09-16","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","214–221","","","","","","Towards molecular musical instruments","AM '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/X4YZCMW4/Mitchell et al. - 2020 - Towards molecular musical instruments interactive.pdf","","","sonification; virtual reality; sonic interaction design; augmented reality; game audio; musicology; sound art; spatial audio","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2N6JUI4L","conferencePaper","2008","Pohle, Tim; Knees, Peter; Widmer, Gerhard","Sound/tracks: real-time synaesthetic sonification and visualisation of passing landscapes","Proceedings of the 16th ACM international conference on Multimedia","978-1-60558-303-7","","10.1145/1459359.1459440","https://dl.acm.org/doi/10.1145/1459359.1459440","When travelling on a train, many people enjoy looking out of the window at the landscape passing by. We present sound/tracks, an application that translates the perceived movement of the scenery and other visual impressions, such as passing trains, into music. The continuously changing view outside the window is captured with a camera and translated into MIDI events that are replayed instantaneously. This allows for a reflection of the visual impression, adding a sound dimension to the visual experience and deepening the state of contemplation. The application is intended to be run on both mobile phones (with built-in camera) and on laptops (with a connected Web-cam). We propose and discuss different approaches to translating the video signal into an audio stream, present different application scenarios, and introduce a method to visualise the dynamics of complete train journeys by ""re-transcribing"" the captured video frames used to generate the music.","2008-10-26","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","599–608","","","","","","Sound/tracks","MM '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/249YW8ZG/Pohle et al. - 2008 - Soundtracks real-time synaesthetic sonification .pdf","","","sonification; mobile music generation; train journey","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z8XA249I","conferencePaper","2021","Sugawa, Moe; Furukawa, Taichi; Chernyshov, George; Hynds, Danny; Han, Jiawen; Padovani, Marcelo; Zheng, Dingding; Marky, Karola; Kunze, Kai; Minamizawa, Kouta","Boiling Mind: Amplifying the Audience-Performer Connection through Sonification and Visualization of Heart and Electrodermal Activities","Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction","978-1-4503-8213-7","","10.1145/3430524.3440653","https://dl.acm.org/doi/10.1145/3430524.3440653","In stage performances, an invisible wall in front of the stage often weakens the connections between the audience and performers. To amplify this performative connection, we present the concept ”Boiling Mind”. Our design concept is based on streaming sensor data related to heart and electrodermal activities from audience members and integrating this data into staging elements, such as visual projections, music, and lighting. Thus, the internal states of the audience directly influence the staging. Artists can have a more direct perception of the inner reactions of audience members and can create physical expressions in response to them. In this paper, we present the wearable sensing system as well as design considerations of mapping heart and electrodermal activity to changes in the staging elements. We evaluated our design and setup over three live performances.","2021-02-14","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","1–10","","","","","","Boiling Mind","TEI '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/C62ISXTH/Sugawa et al. - 2021 - Boiling Mind Amplifying the Audience-Performer Co.pdf","","","Embodiment; Audience engagement; Interactive Dance performance; Physiological signals","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9MUZXIL9","conferencePaper","2015","Schacher, Jan C.","Music means movement: musings on methods for movement analysis in music","Proceedings of the 2nd International Workshop on Movement and Computing","978-1-4503-3457-0","","10.1145/2790994.2791001","https://dl.acm.org/doi/10.1145/2790994.2791001","This article addresses the intersection of technical, analytical and artistic approaches to perceiving and measuring musical movement. The point of view taken is situated between the development and application of technological tools, the design and running of exploratory experiments, and the musical performance moment, where perception of the body and its movements constitutes an integral part of the experience. Through a use-case that is shared with other artists and researchers, a wide range of necessary developments, both conceptually and in software is shown. The tools and the methods generated are juxtaposed with the realisation that movement analysis is merely one possible usage of acquired data. Artistic translations provide alternate ways of generating meaning from movement data, in particular when translating musical actions to pieces that span multiple modalities. With the proposed multi-perspective methodology, ways and means are sketched out that address the inherent multiplicity of domains involved in music performance and perception.","2015-08-14","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","132–139","","","","","","Music means movement","MOCO '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/9DERVANW/Schacher - 2015 - Music means movement musings on methods for movem.pdf","","","motion capture; machine learning; artistic appropriation; comparative; methods; movement analysis; tools","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LAXY3MSM","conferencePaper","2016","Bergström, Ilias; Jonsson, Martin","Sarka: Sonification and Somaesthetic Appreciation Design","Proceedings of the 3rd International Symposium on Movement and Computing","978-1-4503-4307-7","","10.1145/2948910.2948922","https://dl.acm.org/doi/10.1145/2948910.2948922","We often take for granted that we have immediate access to our perception and experience of and through our bodies. But inward listening is a demanding activity and thus not easy to learn to perform or design for. With the Sarka mat we want to support the ability to direct attention by providing sound feedback linked to the weight distribution and motion intensity of different parts of the body, and to provide an exemplar for how such design may be conducted. The process of Sarka's creation is informed by Somaesthetic Appreciation Design. We discuss how a sonic feedback signal can influence listeners, followed by how we, in this design, worked to navigate the complex design space presented to us. We detail the design process involved, and the very particular set of limitations which this interactive sonification presented.","2016-07-05","2023-07-06 04:39:31","2023-07-06 04:39:31","2023-07-05","1–8","","","","","","Sarka","MOCO '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/XA5KPY96/Bergström and Jonsson - 2016 - Sarka Sonification and Somaesthetic Appreciation .pdf","","","Sonification; Biofeedback; Carpet; Feldenkrais; Somaesthetics; Somatic Appreciation Design; Somatic practices","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CMPUZRNA","conferencePaper","2021","Winters, R. Michael; Walker, Bruce N.; Leslie, Grace","Can You Hear My Heartbeat?: Hearing an Expressive Biosignal Elicits Empathy","Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems","978-1-4503-8096-6","","10.1145/3411764.3445545","https://dl.acm.org/doi/10.1145/3411764.3445545","Interfaces designed to elicit empathy provide an opportunity for HCI with important pro-social outcomes. Recent research has demonstrated that perceiving expressive biosignals can facilitate emotional understanding and connection with others, but this work has been largely limited to visual approaches. We propose that hearing these signals will also elicit empathy, and test this hypothesis with sounding heartbeats. In a lab-based within-subjects study, participants (N = 27) completed an emotion recognition task in different heartbeat conditions. We found that hearing heartbeats changed participants’ emotional perspective and increased their reported ability to “feel what the other was feeling.” From these results, we argue that auditory heartbeats are well-suited as an empathic intervention, and might be particularly useful for certain groups and use-contexts because of its musical and non-visual nature. This work establishes a baseline for empathic auditory interfaces, and offers a method to evaluate the effects of future designs.","2021-05-07","2023-07-06 04:41:06","2023-07-06 04:41:06","2023-07-05","1–11","","","","","","Can You Hear My Heartbeat?","CHI '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/Y7AUFTTY/Winters et al. - 2021 - Can You Hear My Heartbeat Hearing an Expressive .pdf","","","music; emotion; physiology; AAC; affect; ASD; communication; empathy; heart rate; rhythm; sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S2V29NKK","conferencePaper","2020","Lutz, Otto Hans-Martin; Kröger, Jacob Leon; Schneiderbauer, Manuel; Kopankiewicz, Jan Maria; Hauswirth, Manfred; Hermann, Thomas","That password doesn't sound right: interactive password strength sonification","Proceedings of the 15th International Audio Mostly Conference","978-1-4503-7563-4","","10.1145/3411109.3412299","https://dl.acm.org/doi/10.1145/3411109.3412299","Despite 2-factor authentication and other modern approaches, authentication by password is still the most commonly used method on the Internet. Unfortunately, as analyses show, many users still choose weak and easy-to-guess passwords. To alleviate the significant effects of this problem, systems often employ textual or graphical feedback to make the user aware of this problem, which often falls short on engaging the user and achieving the intended user reaction, i.e., choosing a stronger password. In this paper, we introduce auditory feedback as a complementary method to remedy this problem, using the advantages of sound as an affective medium. We investigate the conceptual space of creating usable auditory feedback on password strength, including functional and non-functional requirements, influences and design constraints. We present web-based implementations of four sonification designs for evaluating different characteristics of the conceptual space and define a research roadmap for optimization, evaluation and applications.","2020-09-16","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","206–213","","","","","","That password doesn't sound right","AM '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/UNHLNSBX/Lutz et al. - 2020 - That password doesn't sound right interactive pas.pdf","","","sonification; interactive sonification; auditory feedback; auditory display; password security; password strength; usable security","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UJM3CILF","conferencePaper","2016","Weber, Maximilian; Kuhn, Marco","KONTRAKTION: Sonification of Metagestures with electromyographic Signals","Proceedings of the Audio Mostly 2016","978-1-4503-4822-5","","10.1145/2986416.2986421","https://dl.acm.org/doi/10.1145/2986416.2986421","'Kontraktion' is an embodied musical interface using biosignals to create an immersive sonic performance setup. It explores the energetic coupling between digital synthesis and musical expression by reducing the interface to an embodied instrument and therefore tightening the connection between intention and sound. By using the setup as a biofeedback system the user explores his own subconscious gestures with a heightened sensitivity. Even subtle, usually unaware neural impulses are brought to conscious awareness by sensing muscle contractions with an armband and projecting them outward into space with sound in realtime. The users gestural expressions are embodied in sound and allow for an expressive energetic coupling between the users body and a virtual agent. Utilizing the newly adopted awareness of his body the user can take control of the sound and perform with it using the metagestures of his body as an embodied interface. The body itself is transformed into a musical instrument, controlled by neurological impulses and sonified by a virtual interpreter.","2016-10-04","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","132–138","","","","","","KONTRAKTION","AM '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/TSXQUGRX/Weber and Kuhn - 2016 - KONTRAKTION Sonification of Metagestures with ele.pdf","","","sonification; biofeedback; embodiment; emg; gesture; musical interface; surround sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KMQBI2TS","conferencePaper","2010","Rossoff, Sam; Tzanetakis, George; Gooch, Bruce","Adapting personal music for synesthetic game play","Proceedings of the Fifth International Conference on the Foundations of Digital Games","978-1-60558-937-4","","10.1145/1822348.1822370","https://dl.acm.org/doi/10.1145/1822348.1822370","Music can significantly effect game play and help players understand underlying patterns in the game, or the effects of their actions on the characters. Conversely, inappropriate music can have a negative effect on players by creating additional difficulties. While game makers recognize the effects of music on game play, solutions that provide users with a choice in personal music are not forthcoming. We design, implement and evaluate an algorithm for automatically adapting an arbitrary music track from a personal library and synchronizing play back to the user, without requiring any access to the video game source code.","2010-06-19","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","163–170","","","","","","","FDG '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/MY7GQYWC/Rossoff et al. - 2010 - Adapting personal music for synesthetic game play.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4D7P3CVM","conferencePaper","2019","Presti, Giorgio; Ahmetovic, Dragan; Ducci, Mattia; Bernareggi, Cristian; Ludovico, Luca; Baratè, Adriano; Avanzini, Federico; Mascetti, Sergio","WatchOut: Obstacle Sonification for People with Visual Impairment or Blindness","Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility","978-1-4503-6676-2","","10.1145/3308561.3353779","https://dl.acm.org/doi/10.1145/3308561.3353779","Independent mobility is one of the main challenges for blind or visually impaired (BVI) people. In particular, BVI people often need to identify and avoid nearby obstacles, for example a bicycle parked on the sidewalk. This is generally achieved with a combination of residual vision, hearing and haptic sensing using the white cane. However, in many cases, BVI people can only perceive obstacles at short distance (typically about 1m, i.e., the white cane detection range), in other situations obstacles are hard to detect (e.g., those elevated from the ground), while others should not be hit by the white cane (e.g., a standing person). Thus, some time and effort are required to identify the object in order to understand how to avoid it. A solution to these problems can be found in recent computer vision techniques that can run on mobile and wearable devices to detect obstacles at a distance. However, in addition to detecting obstacles, it is also necessary to convey information about them to a BVI user. This contribution presents WatchOut, a sonification technique for conveying real-time information about the main characteristics of an obstacle to a BVI person, who can then use this additional feedback to safely navigate in the environment. WatchOut was designed with a user-centric approach, involving two iterations of online questionnaires with BVI participants in order to define, improve and evaluate the sonification technique. WatchOut was implemented and tested as a module of a mobile app that detects obstacles using state-of-the-art computer vision technology. Results show that the system is considered usable, and can guide the users to avoid more than 85% of the obstacles.","2019-10-24","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","402–413","","","","","","WatchOut","ASSETS '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/BBJ2335F/Presti et al. - 2019 - WatchOut Obstacle Sonification for People with Vi.pdf","","","sonification; visual impairment; navigation assistive technologies; obstacle avoidance","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WS4UZNS2","conferencePaper","2014","Rovithis, Emmanouel; Mniestris, Andreas; Floros, Andreas","Educational audio game design: sonification of the curriculum through a role-playing scenario in the audio game 'Kronos'","Proceedings of the 9th Audio Mostly: A Conference on Interaction With Sound","978-1-4503-3032-9","","10.1145/2636879.2636902","https://dl.acm.org/doi/10.1145/2636879.2636902","Audio-Games (AGs) are electronic games that feature partially or completely auditory interfaces to express the game's plot and mechanics. The required concentration on sonic information makes AGs a suitable medium not only for entertainment, but also for education on (and not limited to) music and sound studies curricula. This paper presents a novel educational AG entitled Kronos that implements a role-playing scenario to facilitate the sonification of the relevant curriculum and to create an educational platform that combines an audio-based gaming environment with a musical instrument. In that process a methodology suggested by the authors has been used. The sonic symbols assigned to create the game's narrative content will be explained and future developments will be mentioned.","2014-10-01","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","1–6","","","","","","Educational audio game design","AM '14","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/UJEGJERM/Rovithis et al. - 2014 - Educational audio game design sonification of the.pdf","","","sonification; composition; education; audio games; game design; role-playing games","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P2AXNH7W","conferencePaper","2019","Dassani, Vansh; Bird, Jon; Cliff, Dave","Automated Composition of Picture-Synched Music Soundtracks for Movies","Proceedings of the 16th ACM SIGGRAPH European Conference on Visual Media Production","978-1-4503-7003-5","","10.1145/3359998.3369405","https://dl.acm.org/doi/10.1145/3359998.3369405","We describe the implementation of and early results from a system that automatically composes picture-synched musical soundtracks for videos and movies. We use the phrase picture-synched to mean that the structure of the automatically composed music is determined by visual events in the input movie, i.e. the final music is synchronised to visual events and features such as cut transitions or within-shot key-frame events. Our system combines automated video analysis and computer-generated music-composition techniques to create unique soundtracks in response to the video input, and can be thought of as an initial step in creating a computerised replacement for a human composer writing music to fit the picture-locked edit of a movie. Working only from the video information in the movie, key features are extracted from the input video, using video analysis techniques, which are then fed into a machine-learning-based music generation tool, to compose a piece of music from scratch. The resulting soundtrack is tied to video features, such as scene transition markers and scene-level energy values, and is unique to the input video. Although the system we describe here is only a preliminary proof-of-concept, user evaluations of the output of the system have been positive.","2019-12-17","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","1–10","","","","","","","CVMP '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/2I3RDTEP/Dassani et al. - 2019 - Automated Composition of Picture-Synched Music Sou.pdf","","","machine learning; automated music composition; video soundtracks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"76SBWB4C","conferencePaper","2018","Godbout, Andrew; Popa, Iulius A. T.; Boyd, Jeffrey E.","Emotional Musification","Proceedings of the Audio Mostly 2018 on Sound in Immersion and Emotion","978-1-4503-6609-0","","10.1145/3243274.3243303","https://dl.acm.org/doi/10.1145/3243274.3243303","We present a method for emotional musification that utilizes the musical game MUSE. We take advantage of the strong links between music and emotion to represent emotions as music. While we provide a prototype for measuring emotion using facial expression and physiological signals our sonification is not dependent on this. Rather we identify states within MUSE that elicit certain emotions and map those onto the arousal and valence spatial representation of emotion. In this way our efforts are compatible with emotion detection methods which can be mapped to arousal and valence. Because MUSE is based on states and state transitions we gain the ability to transition seamlessly from one state to another as new emotions are detected thus avoiding abrupt changes between music types.","2018-09-12","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","1–6","","","","","","","AM'18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/PA3W4IX6/Godbout et al. - 2018 - Emotional Musification.pdf","","","Sonification; Emotion; Process Music","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2TR5U52T","conferencePaper","2020","Jarvis, Robert; Verhagen, Darrin","Composing in spacetime with rainbows: spatial metacomposition in the real world","Proceedings of the 15th International Audio Mostly Conference","978-1-4503-7563-4","","10.1145/3411109.3411136","https://dl.acm.org/doi/10.1145/3411109.3411136","There exists a long tradition of incorporating acoustic space as a creative parameter in musical composition and performance. This creative potential has been extended by way of modern sensing and computing technology which allows the position of the listener to act as an input to interactive musical works in immersive, digital environments. Furthermore, the sophistication of sensing technology has reached a point where barriers to implementing these digital interactive musical systems in the physical world are dissolving. In this research we have set out to understand what new modes of artistic performance might be enabled by these interactive spatial musical systems, and what the analysis of these systems can tell us about the compositional principles of arranging musical elements in space as well as time. We have applied a practice-based approach, leveraging processes of software development, composition, and performance to create a complete system for composing and performing what we refer to as spatial metacompositions. The system is tested at scale in the realisation of a musical work based upon the path of a sailplane in flight. Analysis of the work and the supporting system leads us to suggest opportunities exist for extending existing intermodal composition theory through the analysis of audiovisual renderings of performed spatial works. We also point to unique challenges posed by spatial arrangement, such as effective strategies for structuring musical notes in three dimensions as to produce strong harmonic movement. Beyond enabling new modes of artistic expression, the understanding garnered from these musical structures may help inform a more generalisable approach to non-linear composition, leveraging virtual representations of musical space that respond to arbitrary input data.","2020-09-16","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","175–182","","","","","","Composing in spacetime with rainbows","AM '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/R8MTXFH7/Jarvis and Verhagen - 2020 - Composing in spacetime with rainbows spatial meta.pdf","","","sonification; music; composition; flight; metacomposition; music technology; musical performance; performing arts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z8EHKGAH","conferencePaper","2020","Constantinescu, Angela; Müller, Karin; Haurilet, Monica; Petrausch, Vanessa; Stiefelhagen, Rainer","Bring the Environment to Life: A Sonification Module for People with Visual Impairments to Improve Situation Awareness","Proceedings of the 2020 International Conference on Multimodal Interaction","978-1-4503-7581-8","","10.1145/3382507.3418874","https://dl.acm.org/doi/10.1145/3382507.3418874","Digital navigation tools for helping people with visual impairments have become increasingly popular in recent years. While conventional navigation solutions give routing instructions to the user, systems such as GoogleMaps, BlindSquare, or Soundscape offer additional information about the surroundings and, thereby, improve the orientation of people with visual impairments. However, these systems only provide information about static environments, while dynamic scenes comprising objects such as bikes, dogs, and persons are not considered. In addition, both the routing and the information about the environment are usually conveyed by speech. We address this gap and implement a mobile system that combines object identification with a sonification interface. Our system can be used in three different scenarios of macro and micro navigation: orientation, obstacle avoidance, and exploration of known and unknown routes. Our proposed system leverages popular computer vision methods to localize 18 static and dynamic object classes in real-time. At the heart of our system is a mixed reality sonification interface which is adaptable to the user's needs and is able to transmit the recognized semantic information to the user. The system is designed in a user-centered approach. An exploratory user study conducted by us showed that our object-to-sound mapping with auditory icons is intuitive. On average, users perceived our system as useful and indicated that they want to know more about their environment, apart from wayfinding and points of interest.","2020-10-22","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","50–59","","","","","","Bring the Environment to Life","ICMI '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/6NX7PQ8E/Constantinescu et al. - 2020 - Bring the Environment to Life A Sonification Modu.pdf","","","sonification; computer vision; assistive technologies; evaluations of intelligent user interfaces; mixed reality; object localization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6QFVG7E4","conferencePaper","2022","Ingebritsen, Ryan; Knowlton, Christopher","And She Will Sound The Alarm: Performance and Demo of the Body Sample Player","Proceedings of the 8th International Conference on Movement and Computing","978-1-4503-8716-3","","10.1145/3537972.3538019","https://dl.acm.org/doi/10.1145/3537972.3538019","Kinesthetic empathy, in the context of interactive design, is defined as the ability to encode and decode the input of other users of the system or to sense a shift in the system itself [7]. Achieving this between two players or between players and observers in a virtual performance system involving sound is difficult to achieve and requires that players and audience have an existing connection with the system that marries the sonic and kinesthetic sense known as “auditory kinesthesia” [11]. These sensibilities can be facilitated through the design of the system itself following the principles of usability and kinesthetic interaction, or through performer training [9]. In traditional music and dance, kinesthetic empathy exists both between players and between players and audience. It is much more difficult to achieve this in the context of interactive performance as the audience does not have the same culturally specific empathetic response when observing digitally mediated performance as they do when viewing forms of music and dance they are already familiar with. The Body Sample Player is a digital musical instrument and performance platform that uses real-time data gathered by body tracking cameras to control the volumes of looping sound samples, turning the body itself into a sound controller. The system has evolved over the years, facilitating several different musical works ranging from solo ”DJ” controller style musical performances to large scale interactive music and dance works for multiple performers. It has also been used to create interactive installations that are accessible to the audiences of these large-scale works. It is our contention that by giving audience members first-hand experience engaging with this instrument before witnessing a performance, they will more easily be able to recognize expressivity in the performance, and in doing so, will experience a higher level of kinesthetic empathy. Therefore, we propose a performance of And She Will Sound the Alarm, a piece for a single performer created for the Body Sample Player system, to be preceded by a demonstration in which conference attendees can interact with the system itself.","2022-06-30","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","1–4","","","","","","And She Will Sound The Alarm","MOCO '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/C8DIFITH/Ingebritsen and Knowlton - 2022 - And She Will Sound The Alarm Performance and Demo.pdf","","","sonification; auditory kinesthesia; contemporary dance; digital musical instruments; entrainment; kinesthetic empathy","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EKJASRJ4","conferencePaper","2013","Oh, Uran; Kane, Shaun K.; Findlater, Leah","Follow that sound: using sonification and corrective verbal feedback to teach touchscreen gestures","Proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility","978-1-4503-2405-2","","10.1145/2513383.2513455","https://dl.acm.org/doi/10.1145/2513383.2513455","While sighted users may learn to perform touchscreen gestures through observation (e.g., of other users or video tutorials), such mechanisms are inaccessible for users with visual impairments. As a result, learning to perform gestures can be challenging. We propose and evaluate two techniques to teach touchscreen gestures to users with visual impairments: (1) corrective verbal feedback using text-to-speech and automatic analysis of the user's drawn gesture; (2) gesture sonification to generate sound based on finger touches, creating an audio representation of a gesture. To refine and evaluate the techniques, we conducted two controlled lab studies. The first study, with 12 sighted participants, compared parameters for sonifying gestures in an eyes-free scenario and identified pitch + stereo panning as the best combination. In the second study, 6 blind and low-vision participants completed gesture replication tasks with the two feedback techniques. Subjective data and preliminary performance findings indicate that the techniques offer complementary advantages.","2013-10-21","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","1–8","","","","","","Follow that sound","ASSETS '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/AYAZGH6Y/Oh et al. - 2013 - Follow that sound using sonification and correcti.pdf","","","sonification; blindness; gestures; touchscreen; visual impairments","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q4XUIZ5Q","conferencePaper","2021","Kari, Mohamed; Grosse-Puppendahl, Tobias; Jagaciak, Alexander; Bethge, David; Schütte, Reinhard; Holz, Christian","SoundsRide: Affordance-Synchronized Music Mixing for In-Car Audio Augmented Reality","The 34th Annual ACM Symposium on User Interface Software and Technology","978-1-4503-8635-7","","10.1145/3472749.3474739","https://dl.acm.org/doi/10.1145/3472749.3474739","Music is a central instrument in video gaming to attune a player’s attention to the current atmosphere and increase their immersion in the game. We transfer the idea of scene-adaptive music to car drives and propose SoundsRide, an in-car audio augmented reality system that mixes music in real-time synchronized with sound affordances along the ride. After exploring the design space of affordance-synchronized music, we design SoundsRide to temporally and spatially align high-contrast events on the route, e. g., highway entrances or tunnel exits, with high-contrast events in music, e. g., song transitions or beat drops, for any recorded and annotated GPS trajectory by a three-step procedure. In real-time, SoundsRide 1) estimates temporal distances to events on the route, 2) fuses these novel estimates with previous estimates in a cost-aware music-mixing plan, and 3) if necessary, re-computes an updated mix to be propagated to the audio output. To minimize user-noticeable updates to the mix, SoundsRide fuses new distance information with a filtering procedure that chooses the best updating strategy given the last music-mixing plan, the novel distance estimations, and the system parameterization. We technically evaluate SoundsRide and conduct a user evaluation with 8 participants to gain insights into how users perceive SoundsRide in terms of mixing, affordances, and synchronicity. We find that SoundsRide can create captivating music experiences and positively as well as negatively influence subjectively perceived driving safety, depending on the mix and user.","2021-10-12","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","118–133","","","","","","SoundsRide","UIST '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/NQ3FVIS6/Kari et al. - 2021 - SoundsRide Affordance-Synchronized Music Mixing f.pdf","","","auditory augmented reality; context-adaptive music; sound affordances","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KE9SZS2V","conferencePaper","2022","Mainsbridge, Mary","Feeling movement in live electronic music: An embodied autoethnography","Proceedings of the 8th International Conference on Movement and Computing","978-1-4503-8716-3","","10.1145/3537972.3537989","https://dl.acm.org/doi/10.1145/3537972.3537989","Movement-based interaction relies on the measurement and abstraction of human motion. Yet reducing physical experience to quantitative and visual representations overlooks the inner perceptions and sensations that accompany movement. Rather than treating the body as an object to be observed and analysed, soma-based design approaches instead reflect first-person perspectives of felt experience, providing insight into how the body experiences and interacts with the world. Applying these methods to the musical realm, this paper offers a personal account of working with motion-sensing interfaces, exploring the underlying qualities and meanings of performance actions in live and recorded contexts. The embodied authethnographic study serves as an opportunity to question normative approaches to technology that favour techno-scientific narratives of external control and surveillance. Alternative values of agency, autonomy, empathy and transparency are investigated using first-person methodologies.","2022-06-30","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","1–7","","","","","","Feeling movement in live electronic music","MOCO '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/3GNXEKTJ/Mainsbridge - 2022 - Feeling movement in live electronic music An embo.pdf","","","Autoethnography; Embodied musical interaction; Gestural system; Kinaesthetic awareness; Soma-based design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EJTZLKAY","conferencePaper","2021","Ley-Flores, Judith; Turmo Vidal, Laia; Berthouze, Nadia; Singh, Aneesha; Bevilacqua, Frédéric; Tajadura-Jiménez, Ana","SoniBand: Understanding the Effects of Metaphorical Movement Sonifications on Body Perception and Physical Activity","Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems","978-1-4503-8096-6","","10.1145/3411764.3445558","https://dl.acm.org/doi/10.1145/3411764.3445558","Negative body perceptions are a major predictor of physical inactivity, a serious health concern. Sensory feedback can be used to alter such body perceptions; movement sonification, in particular, has been suggested to affect body perception and levels of physical activity (PA) in inactive people. We investigated how metaphorical sounds impact body perception and PA. We report two qualitative studies centered on performing different strengthening/flexibility exercises using SoniBand, a wearable that augments movement through different sounds. The first study involved physically active participants and served to obtain a nuanced understanding of the sonifications’ impact. The second, in the home of physically inactive participants, served to identify which effects could support PA adherence. Our findings show that movement sonification based on metaphors led to changes in body perception (e.g., feeling strong) and PA (e.g., repetitions) in both populations, but effects could differ according to the existing PA-level. We discuss principles for metaphor-based sonification design to foster PA.","2021-05-07","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","1–16","","","","","","SoniBand","CHI '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/GH9YBHJP/Ley-Flores et al. - 2021 - SoniBand Understanding the Effects of Metaphorica.pdf","","","movement sonification; auditory body perception; interaction styles; multimodal interfaces; physically inactive population; sensory feedback","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NTG7AKCP","conferencePaper","2007","Ng, Kia C.; Weyde, Tillman; Larkin, Oliver; Neubarth, Kerstin; Koerselman, Thijs; Ong, Bee","3d augmented mirror: a multimodal interface for string instrument learning and teaching with gesture support","Proceedings of the 9th international conference on Multimodal interfaces","978-1-59593-817-6","","10.1145/1322192.1322252","https://dl.acm.org/doi/10.1145/1322192.1322252","Multimodal interfaces can open up new possibilities for music education, where the traditional model of teaching is based predominantly on verbal feedback. This paper explores the development and use of multimodal interfaces in novel tools to support music practice training. The design of multimodal interfaces for music education presents a challenge in several respects. One is the integration of multimodal technology into the music learning process. The other is the technological development, where we present a solution that aims to support string practice training with visual and auditory feedback. Building on the traditional function of a physical mirror as a teaching aid, we describe the concept and development of an ""augmented mirror"" using 3D motion capture technology.","2007-11-12","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","339–345","","","","","","3d augmented mirror","ICMI '07","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/W4866V6N/Ng et al. - 2007 - 3d augmented mirror a multimodal interface for st.pdf","","","sonification; music; multimodal; education; motion capture; visualisation; visualization; gesture; 3d; feedback; interface","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5LXH23T9","conferencePaper","2016","Knees, Peter; Andersen, Kristina","Searching for Audio by Sketching Mental Images of Sound: A Brave New Idea for Audio Retrieval in Creative Music Production","Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval","978-1-4503-4359-6","","10.1145/2911996.2912021","https://dl.acm.org/doi/10.1145/2911996.2912021","We propose a new paradigm for searching for sound by allowing users to graphically sketch their mental representation of sound as query. By conducting interviews with professional music producers and creators, we find that existing, text-based indexing and retrieval methods based on file names and tags to search for sound material in large collections (e.g., sample databases) do not reflect their mental concepts, which are often rooted in the visual domain and hence are far from their actual needs, work practices, and intuition. As a consequence, when creating new music on the basis of existing sounds, the process of finding these sounds is cumbersome and breaks their work flow by being forced to resort to browsing the collection. Prior work on organizing sound repositories aiming at bridging this conceptual gap between sound and vision builds upon psychological findings (often alluding to synaesthetic phenomena) or makes use of ad-hoc, technology-driven mappings. These methods foremost aim at visualizing the contents of collections or individual sounds and, again, facilitating browsing therein. For the purpose of indexing and querying, such methods have not been applied yet. We argue that the development of a search system that allows for visual queries to audio collections is desired by users and should inform and drive future research in audio retrieval. To explore this notion, we test the idea of a sketch interface with music producers in a semi-structured interview process by making use of a physical non-functional prototype. Based on the outcomes of this study, we propose a conceptual software prototype for visually querying sound repositories using image manipulation metaphors.","2016-06-06","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","95–102","","","","","","Searching for Audio by Sketching Mental Images of Sound","ICMR '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/BV2K38QQ/Knees and Andersen - 2016 - Searching for Audio by Sketching Mental Images of .pdf","","","audio retrieval; cross-domain retrieval; music production; retrieval by sketch","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D63NYN5G","conferencePaper","2017","Fitzpatrick, J.; Neff, F.","The Data-Driven Algorithmic Composer","Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences","978-1-4503-5373-1","","10.1145/3123514.3123549","https://dl.acm.org/doi/10.1145/3123514.3123549","The Data-Driven Algorithmic Composer (D-DAC) is an application designed to output data-driven algorithmically composed music via MIDI. The application requires input data to be in tab-separated format to be compatible. Each dataset results in a unique piece of music that remains consistent with each iteration of the application. The only varying elements between each iteration of the same dataset are factors defined by the user: tempo, scale, and intervals between rows. Each measure of the melody, harmony and bassline is derived from each row of the dataset. By utilizing this non-random algorithmic application, users can create a unique and predefined musical iteration of their dataset. The overall aim of the D-DAC is to inspire musical creativity from scientific data and encourage the sharing of datasets between various research communities.","2017-08-23","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","1–6","","","","","","","AM '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/JMZ29T5M/Fitzpatrick and Neff - 2017 - The Data-Driven Algorithmic Composer.pdf","","","sonification; algorithmic composition; Dataset","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EZ983LAT","conferencePaper","2016","Chernyshov, George; Chen, Jiajun; Lai, Yenchin; Noriyasu, Vontin; Kunze, Kai","Ambient Rhythm: Melodic Sonification of Status Information for IoT-enabled Devices","Proceedings of the 6th International Conference on the Internet of Things","978-1-4503-4814-0","","10.1145/2991561.2991564","https://dl.acm.org/doi/10.1145/2991561.2991564","In this paper we explore how to embed status information of IoT-enabled devices in the acoustic atmosphere using melodic ambient sounds while limiting obtrusiveness for the user. The user can use arbitrary sound samples to represent the devices he wants to monitor. Our system combines these sound samples into a melodic ambient rhythm that contains information on all the processes or variables that user is monitoring. We focus on continuous rather than binary information (e.g. ""monitoring progress status"" rather then ""new message received""). We evaluate our system in a machine monitoring scenario focusing on 5 distinct machines/processes to monitor with 6 priority levels for each. 9 participants use our system to monitor these processes with an up to 92.44% detection rate, if several levels are combined. Participants had no previous experience with this or similar systems and had only 5-10 minute training session before the tests.","2016-11-07","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","1–6","","","","","","Ambient Rhythm","IoT'16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/SWDLHX5M/Chernyshov et al. - 2016 - Ambient Rhythm Melodic Sonification of Status Inf.pdf","","","Sonification; Acoustic Atmosphere; Interfaces; IoT; Process monitoring","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B4FC7S2F","conferencePaper","2017","Jeon, Myounghoon; FakhrHosseini, Maryam; Vasey, Eric; Nees, Michael A.","Blueprint of the Auditory Interactions in Automated Vehicles: Report on the Workshop and Tutorial","Proceedings of the 9th International Conference on Automotive User Interfaces and Interactive Vehicular Applications Adjunct","978-1-4503-5151-5","","10.1145/3131726.3131743","https://dl.acm.org/doi/10.1145/3131726.3131743","Vehicle automation is becoming more widespread. As automation increases, new opportunities and challenges have emerged. Given that vision is heavily taxed in driving, much research has been conducted on an auditory channel. To identify new design spaces and solutions, we have organized successive workshops and tutorial at International Conference on Auditory Display (ICAD) and AutoUI Conference for several years. In this report, we address novel opportunities and directions of auditory interactions in automated vehicles that we have discussed in the workshop and tutorial to design better driver user experience and secure road safety.","2017-09-24","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","178–182","","","","","","Blueprint of the Auditory Interactions in Automated Vehicles","AutomotiveUI '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/MKRJSB8R/Jeon et al. - 2017 - Blueprint of the Auditory Interactions in Automate.pdf","","","sonification; Auditory displays; situation awareness; exterior sound; sonic branding; speech; take-over request","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QV7AQF4N","conferencePaper","2020","Barrett, Natasha","Deepening presence: probing the hidden artefacts of everyday soundscapes","Proceedings of the 15th International Audio Mostly Conference","978-1-4503-7563-4","","10.1145/3411109.3411120","https://dl.acm.org/doi/10.1145/3411109.3411120","Sound penetrates our outdoor spaces. Much of it we ignore amidst our fast passage from place to place, its qualities may be too quiet or fleeting to pay heed to above the bustle of our own thoughts, or we may experience the sounds as an annoyance. Manoeuvring our listening to be excited by its features is not so easy. This paper presents new artistic research that probes the hidden artefacts of everyday soundscapes - the sounds and details which we ignore or fail to engage - and draws them into a new audible reality. The work focuses on the affordances of spatial information in a novel combination of art and technology: site-specific composition and the ways of listening established by Schaeffer and his successors are combined with the technology of beam-forming from high resolution (Eigenmike) Ambisonics recordings, Ambisonics sound-field synthesis and the deployment of a new prototype loudspeaker. Underlying the artistic and scientific research is the hypothesis that spatially distributed information offers new opportunities to explore, isolate and musically develop features of interest, and that composition should address the same degree of spatiality as the real landscape. The work is part of the 'Reconfiguring the Landscape' project investigating how 3-D electroacoustic composition and sound-art can incite a new awareness of outdoor sound environments.","2020-09-16","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","77–84","","","","","","Deepening presence","AM '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/K3S6FF2Q/Barrett - 2020 - Deepening presence probing the hidden artefacts o.pdf","","","sonification; composition; acoustic ecology; acoustics; feature extraction; higher-order ambisonics; loudspeaker technology; soundscapes","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XA2DQLRE","conferencePaper","2022","Kalampratsidou, Vilelmini; Diamantides, Pandelis; Stergiou, Marina; El Raheb, Katerina; Ioannidis, Yannis; Issari, Philia; Georgaca, Eugenie; Skali, Dora; Koliouli, Flora; Karydi, Evangelia; Giokas, Panos; Vassilakou, Virginia; Pappas, Yannis","From capturing the embodied social experience to music composition: data as mediation","Proceedings of the 8th International Conference on Movement and Computing","978-1-4503-8716-3","","10.1145/3537972.3538002","https://dl.acm.org/doi/10.1145/3537972.3538002","This work presents the first steps of a journey from capturing the embodied social experience to creating contemporary sound art through digital means. In the framework of the Transition to 8 project, residents of the Greek City of Eleusis expressed their feelings and perspectives on the social issues of their community through organized sociodrama sessions. The participants used wearable technology, resembling that of smart-watches, that enables the recording of various bodily data, such as heart rate, temperature, and skin conductance (affiliated with emotional arousal). The data collected are organised and analysed by applying a mixed-methods approach, and shared with artists to be used as inspiration and source material for their artistic productions. In Transition to 8 project we aim to build a platform and methodology that uses data as a means to communicate local social issues and citizen perspectives to an international community of artists. As a proof of concept one sound composition has been created so far, as an artistic piece, to guide this process, while the development of the platform and further experimentation are still an ongoing endeavor.","2022-06-30","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","1–4","","","","","","From capturing the embodied social experience to music composition","MOCO '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/F5U48FZQ/Kalampratsidou et al. - 2022 - From capturing the embodied social experience to m.pdf","","","heart rate; algorithmic music; biosignals; body temperature; music composition; skin conductance; Social issues; sociodrama","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XN3FVEAT","conferencePaper","2022","Groß-Vogt, Katharina; Svoronos-Kanavas, Iason; Weger, Marian; Amon, Clemes","The Augmented Floor - Assessing Auditory Augmentation","Proceedings of the 17th International Audio Mostly Conference","978-1-4503-9701-8","","10.1145/3561212.3561219","https://dl.acm.org/doi/10.1145/3561212.3561219","The Augmented Floor is a wooden floor with in-built PyzoFlex® printed electronics. It measures any change of pressure on its surface. For users walking on this floor, we created an auditory augmentation of their footsteps. Different sound designs were implemented, both realistic and synthetic ones. We developed a method based on envelope reusing and compared it to the triggering of step-sounds. The envelope method shows a higher correlation between the input signal and the playback sound; it was rated better. Besides, realistic sounds were preferred over synthetic ones.","2022-10-10","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","7–14","","","","","","","AM '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/779MM4CG/Groß-Vogt et al. - 2022 - The Augmented Floor - Assessing Auditory Augmentat.pdf","","","sonification; interactive sonification; auditory augmentation; onset detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z7THJDVQ","conferencePaper","2014","Yaremchuk, Vanessa; Wanderley, Marcelo M.","Brahms, Bodies and Backpropagation: Artificial Neural Networks for Movement Classification in Musical Performance","Proceedings of the 2014 International Workshop on Movement and Computing","978-1-4503-2814-2","","10.1145/2617995.2618011","https://dl.acm.org/doi/10.1145/2617995.2618011","Two types of artificial neural networks are used to determine a sufficient subset of data for reasonable classification of musical instrument based on performance data from motion capture. Feedforward and recurrent networks are trained on subsets of joint angles and centre of mass from performances by violists and clarinettists. A successfully learned mapping from the reduced set of input data to the correct instrument classification implies that the data subset carries sufficient information to identify an instrument. After training, cross-validation is performed by testing networks on previously unseen data. Finally, performance is compared with that of humans performing similar classification tasks based on the same data. Feedforward and recurrent networks are found to perform similarly when classifying test data. Instrument recognition rates by networks are comparable with human recognition rates over the various data subset conditions. The methods demonstrated here could also be applied to other movement analysis domains (e.g. gait, posture, kinematics, clinical/rehabilitation work).","2014-06-16","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","88–93","","","","","","Brahms, Bodies and Backpropagation","MOCO '14","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/DE5ED9T8/Yaremchuk and Wanderley - 2014 - Brahms, Bodies and Backpropagation Artificial Neu.pdf","","","music; motion capture; performance; movement analysis; artificial neural network; feedforward; recurrent","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DUYMAU2J","conferencePaper","2020","Greindl, Andreas; Heidegger, Patrick; Groß-Vogt, Katharina; Weger, Marian","Expergefactor: sonic interaction design for an alarm clock app","Proceedings of the 15th International Audio Mostly Conference","978-1-4503-7563-4","","10.1145/3411109.3411149","https://dl.acm.org/doi/10.1145/3411109.3411149","We present a prototype for an alarm clock app that is based on sonic interaction design, called the Expergefactor. It consists of two parts that solely rely on rotating the phone and receiving auditory feedback. In an awakening challenge, the mobile phone has to be rotated to find a random position following sonic clues. For setting the snoozing timer, the phone is turned and gives swelling sonic feedback for each minute more.","2020-09-16","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","241–244","","","","","","Expergefactor","AM '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/WT5E2RTV/Greindl et al. - 2020 - Expergefactor sonic interaction design for an ala.pdf","","","sonification; sonic interaction design; auditory app","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B6V8GLS6","conferencePaper","2019","Hazzard, Adrian; Greenhalgh, Chris","Adaptive Musical Soundtracks: from in-game to on the street","Proceedings of the 14th International Audio Mostly Conference: A Journey in Sound","978-1-4503-7297-8","","10.1145/3356590.3356597","https://dl.acm.org/doi/10.1145/3356590.3356597","We describe the challenges of creating location-based adaptive musical soundtracks, which we try to address via the iterative development of daoplayer, a tool to create such media experiences. Daoplayer is broadly modelled on the functionality of computer game middleware, functionality we argue is missing from most tools freely available for location-based experiences. We chart the development progress and use in practice, identifying the emerging challenges and outlining our responses to these along the way. Our focus is broad, taking in technical concepts and infrastructure, in addition to the creative concerns of the composer. We reveal that our tool was largely successful in delivering fine grained 'musical' soundtracks, which on the one hand highlights the similarities between computer game soundtracks and location-based soundtracks, but on the other hand we note a number of key distinctions.","2019-09-18","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","31–38","","","","","","Adaptive Musical Soundtracks","AM'19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/XS53NRTI/Hazzard and Greenhalgh - 2019 - Adaptive Musical Soundtracks from in-game to on t.pdf","","","Music; Composition; Computer Games; Location-Based Soundtracks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J7DZV6HS","conferencePaper","2017","Krome, Sven; Batty, Joshua; Greuter, Stefan; Holopainen, Jussi","AutoJam: Exploring Interactive Music Experiences in Stop-and-Go Traffic","Proceedings of the 2017 Conference on Designing Interactive Systems","978-1-4503-4922-2","","10.1145/3064663.3064758","https://dl.acm.org/doi/10.1145/3064663.3064758","AutoJam is an interactive music listening experience played on an inactive steering wheel of an autonomous car. By designing AutoJam, our aim was to make in-car music interactive and align music creation with the progression of the traffic situation. Besides being a fun and creative activity for frustrating stop-and-go traffic, AutoJam helps to understand the building blocks of in-car performances. In order to explore the impact of AutoJam, we conducted an in-situ user study with 14 participants. From the findings, we constructed a map of experiential dimensions of AutoJam and found that player's experience, constitute itself through a continuous shifting between those dimensions. Reflecting on this process, we discuss four building blocks of in-car performances. Ultimately, this work aims to frame autonomous driving in difficult traffic as a creative and playful activity promoting situational awareness and musical expression.","2017-06-10","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","441–450","","","","","","AutoJam","DIS '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/B7K7LIHU/Krome et al. - 2017 - AutoJam Exploring Interactive Music Experiences i.pdf","","","music; games; autonomous driving; design; improvisation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D5LSPJEI","conferencePaper","2020","Ingebritsen, Ryan; Knowlton, Christopher; Sato, Hugh; Mott, Erica","Social Movements: A Case Study in Dramaturgically-Driven Sound Design for Contemporary Dance Performance to Mediate Human-Human Interaction","Proceedings of the Fourteenth International Conference on Tangible, Embedded, and Embodied Interaction","978-1-4503-6107-1","","10.1145/3374920.3374955","https://dl.acm.org/doi/10.1145/3374920.3374955","We present the iterative design and final implementation of a real-time full-body control system of the sound score by performers in an immersive contemporary dance performance. Digital musical instruments for dance require different considerations than for music, particularly in contemporary, non-proscenium and participatory audience contexts. Arising from dramaturgical research around social movement and civic participation in the digital age, this case study presents a fusion of design choices that consider artistic themes, performer interaction and audience experience. Three generations of sound and technical design are presented, with intermittent performances, challenges and learnings at each stage. We believe that the collaborative evolution of compositional choices, computer vision techniques, sound mapping, performer interaction and staging reveals important insights into developing interaction systems for kinesthetic empathy. Rather than placing the focus on human-computer interaction, our final production employed technology as a bridge to encourage human-human interaction around themes of occupation, resistance and resilience.","2020-02-09","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","227–237","","","","","","Social Movements","TEI '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/75UBQZFL/Ingebritsen et al. - 2020 - Social Movements A Case Study in Dramaturgically-.pdf","","","sonification; contemporary dance; digital musical instruments; kinesthetic empathy; audience participation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5J4BN4GX","conferencePaper","2017","Palacio, Pablo; Bisig, Daniel","Piano&Dancer: Interaction Between a Dancer and an Acoustic Instrument","Proceedings of the 4th International Conference on Movement Computing","978-1-4503-5209-3","","10.1145/3077981.3078052","https://dl.acm.org/doi/10.1145/3077981.3078052","Piano&Dancer is an interactive piece for a dancer and an electromechanical acoustic piano. The piece presents the dancer and the piano as two performers on stage whose bodily movements are mutually interdependent. This interdependence reveals a close relationship between physical and musical gestures. Accordingly, the realisation of the piece has been based on creative processes that merge choreographic and compositional methods. In order to relate the expressive movement qualities of a dancer to the creation of musical material, the piece employs a variety of techniques. These include methods for movement tracking and feature analysis, generative algorithms for creating musical structures, and the application of non-conventional scales and chord transformations to shape the modal characteristics of the music. The publication contextualises Piano&Dancer by relating its creation to concepts of embodiment, interactivity and musical structure and by discussing opportunities for creative cross-fertilisation between dance choreography and musical composition. It also provides some details about the challenges and potentials of integrating a mechanical musical instrument into an interactive setting for a dance performance. Finally, the paper highlights some of the technical and aesthetic principles that were used in order to connect expressive qualities of body movements to the creation of music structures.","2017-06-28","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","1–8","","","","","","Piano&Dancer","MOCO '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/L6HP5B4E/Palacio and Bisig - 2017 - Piano&Dancer Interaction Between a Dancer and an .pdf","","","Embodiment; Automated Analysis of Movement Qualities; Dance Technology; Gesture-based Interaction; Interactive Sonification; Music and Movement","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FUVQMESD","conferencePaper","2008","Eslambolchilar, Parisa; Murray-Smith, Rod","Interact, excite, and feel","Proceedings of the 2nd international conference on Tangible and embedded interaction","978-1-60558-004-3","","10.1145/1347390.1347419","https://dl.acm.org/doi/10.1145/1347390.1347419","This paper presents a dynamic system approach to the design of multimodal interactive systems. We use an example where we support human behavior in a browsing task, by adapting the dynamics of navigation using speed-dependent automatic zooming (SDAZ), allowing the user to switch smoothly among different modes of control. We show how the user's intention is coupled to the browsing technique via the dynamic model, and how the SDAZ method couples the document structure to audio samples using a model-based sonification. We demonstrate that this approach is well suited to mobile and wearable applications, and audio feedback provides valuable information, supporting intermittent interaction, i.e. allowing movement-based interaction techniques to continue while the user is simultaneously involved with real life tasks.","2008-02-18","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","131–138","","","","","","","TEI '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/WV8KD7D6/Eslambolchilar and Murray-Smith - 2008 - Interact, excite, and feel.pdf","","","sonification; dynamic continuous interaction; mobile devices; mode switching; multimodality; speed-dependent automatic zooming","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SQZ8XTYD","conferencePaper","2021","Hermann, Thomas; Reinsch, Dennis","sc3nb: a Python-SuperCollider Interface for Auditory Data Science","Proceedings of the 16th International Audio Mostly Conference","978-1-4503-8569-5","","10.1145/3478384.3478401","https://dl.acm.org/doi/10.1145/3478384.3478401","This paper introduces sc3nb, a Python package for audio coding and interactive control of the SuperCollider programming environment. sc3nb supports Jupyter notebooks, enables flexible means for sound and music computing such as sound synthesis and analysis and is particularly tailored for sonification. We present the main concepts and interfaces and illustrate how to use sc3nb at hand of selected code examples for basic sonification approaches, such as audification and parameter-mapping sonification. Finally, we introduce TimedQueues which enable coordinated audiovisual displays, e.g. to synchronize matplotlib data and sc3nb-based sound rendition. sc3nb enables interactive sound applications right in the center of the pandas/numpy/scipy data science ecosystem. The open source package is hosted at GitHub and available via the Python Package Index PyPI.","2021-10-15","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","208–215","","","","","","sc3nb","AM '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/H8JUBIDQ/Hermann and Reinsch - 2021 - sc3nb a Python-SuperCollider Interface for Audito.pdf","","","sonification; audio coding; auditory data science; interactive programming; Python","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JX85VX32","conferencePaper","2017","Pon, Aura; Pattison, Eric; Fyfe, Lawrence; Radford, Laurie; Carpendale, Sheelagh","Torrent: Integrating Embodiment, Physicalization and Musification in Music-Making","Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction","978-1-4503-4676-4","","10.1145/3024969.3024974","https://dl.acm.org/doi/10.1145/3024969.3024974","In this paper, we present Torrent, for Flutes and Water. Torrent is an embodied electroacoustic music composition written to commemorate a catastrophic local flood and the spirit of the citizens who united to overcome it. To create this music, we designed and built an interactive computer system to make audible and physical that which is usually hidden but ever-present in music-making: the musicians' muscle tension as they perform, think about, and feel their music. The Torrent system musifies and physicalizes the flutists' muscle tension as live water sounds and movement that accompany the musicians as they perform. Semi-structured interviews and surveys were conducted to under-stand the experiences of the performers and audience during one performance. The use of embodiment, as integral to the music itself, aspires to emphasize means and end, body and mind, effort and achievement in the struggle of people unit-ed against the elements.","2017-03-20","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","209–216","","","","","","Torrent","TEI '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/ZSWRVYIY/Pon et al. - 2017 - Torrent Integrating Embodiment, Physicalization a.pdf","","","electroacoustic music; electromyography; embodied interaction; musification; physicalization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E5BUYRIV","conferencePaper","2008","Schloss, W. Andrew; Stammen, Dale","Ambient media in public spaces","Proceedings of the 1st ACM international workshop on Semantic ambient media experiences","978-1-60558-314-3","","10.1145/1461912.1461916","https://dl.acm.org/doi/10.1145/1461912.1461916","As artists working in public art and media, we have repeatedly found that the usual parameters and characteristics for a work of art have to be heavily modified to be successful in an ambient environment. Art does not normally strive to be innocuous or inconspicuous; however every artist who does public art quickly learns to deal with the impact of their art in a public space, especially if the public's exposure to the art is continuous. We have recently created four public art installations, three in Seattle and one in New York, varying tremendously in style and content. In all such work, the issues of ambient sound and its semantics had to be specifically and carefully addressed; in our case, one installation is in a public library, two others are in a workplace where the staff would be exposed to the sound for extended periods, and the fourth is in a gallery that allowed the public to experience the artwork one person at a time. It is the auditory aspect of the artworks that will be the focus here, in the context of ubiquitous ambient sonic environments. Lessons learned from the world of public art may have some resonance in the coming world of ubiquitous computing. Essentially, these installations use the forces and sounds of nature (wind, sun, rain, ocean waves) to generate sounds and images to amplify reality, to draw attention to our natural surroundings that so often get lost in urban environments.","2008-10-31","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","17–20","","","","","","","SAME '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/F8ZLRK8P/Schloss and Stammen - 2008 - Ambient media in public spaces.pdf","","","sonification; auditory displays; acoustic ecology; computer music; information interfaces; public art installations","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IGY2XQJ7","conferencePaper","2022","Potluri, Venkatesh; Thompson, John; Devine, James; Lee, Bongshin; Morsi, Nora; De Halleux, Peli; Hodges, Steve; Mankoff, Jennifer","PSST: Enabling Blind or Visually Impaired Developers to Author Sonifications of Streaming Sensor Data","Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology","978-1-4503-9320-1","","10.1145/3526113.3545700","https://dl.acm.org/doi/10.1145/3526113.3545700","We present the first toolkit that equips blind and visually impaired (BVI) developers with the tools to create accessible data displays. Called PSST (Physical computing Streaming Sensor data Toolkit), it enables BVI developers to understand the data generated by sensors from a mouse to a micro:bit physical computing platform. By assuming visual abilities, earlier efforts to make physical computing accessible fail to address the need for BVI developers to access sensor data. PSST enables BVI developers to understand real-time, real-world sensor data by providing control over what should be displayed, as well as when to display and how to display sensor data. PSST supports filtering based on raw or calculated values, highlighting, and transformation of data. Output formats include tonal sonification, nonspeech audio files, speech, and SVGs for laser cutting. We validate PSST through a series of demonstrations and a user study with BVI developers.","2022-10-28","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","1–13","","","","","","PSST","UIST '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/78297ZQA/Potluri et al. - 2022 - PSST Enabling Blind or Visually Impaired Develope.pdf","","","Accessibility; Blind or Visually Impaired (BVI) Programmers; Physical Computing; Toolkit","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M5FRSTI7","conferencePaper","2018","Ramchurn, Richard; Chamberlian, Alan; Benford, Steve","Designing Musical Soundtracks for Brain Controlled Interface (BCI) Systems","Proceedings of the Audio Mostly 2018 on Sound in Immersion and Emotion","978-1-4503-6609-0","","10.1145/3243274.3243288","https://dl.acm.org/doi/10.1145/3243274.3243288","This paper presents research based on the creation and development of two Brain Controlled Interface (BCI) based film experiences. The focus of this research is primarily on the audio in the films; the way that the overall experiences were designed, the ways in which the soundtracks were specifically developed for the experiences and the ways in which the audience perceived the use of the soundtrack in the film. Unlike traditional soundtracks the adaptive nature of the audio means that there are multiple parts that can be interacted with and combined at specific moments. The design of such adaptive audio systems is something that is yet to be fully understood and this paper goes someway to presenting our initial findings. We think that this research will be of interest and excite the Audio-HCI community.","2018-09-12","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","1–8","","","","","","","AM'18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/493GEE3C/Ramchurn et al. - 2018 - Designing Musical Soundtracks for Brain Controlled.pdf","","","Music; Composition; Creativity; Art; HCI; Audience; Audio; Brain Controlled Interface (BCI); Design; Film; New Media","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZEK4BZ2D","conferencePaper","2023","Hoque, Md Naimul; Ehtesham-Ul-Haque, Md; Elmqvist, Niklas; Billah, Syed Masum","Accessible Data Representation with Natural Sound","Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","978-1-4503-9421-5","","10.1145/3544548.3581087","https://dl.acm.org/doi/10.1145/3544548.3581087","Sonification translates data into non-speech audio. Such auditory representations can make data visualization accessible to people who are blind or have low vision (BLV). This paper presents a sonification method for translating common data visualization into a blend of natural sounds. We hypothesize that people’s familiarity with sounds drawn from nature, such as birds singing in a forest, and their ability to listen to these sounds in parallel, will enable BLV users to perceive multiple data points being sonified at the same time. Informed by an extensive literature review and a preliminary study with 5 BLV participants, we designed an accessible data representation tool, Susurrus, that combines our sonification method with other accessibility features, such as keyboard interaction and text-to-speech feedback. Finally, we conducted a user study with 12 BLV participants and report the potential and application of natural sounds for sonification compared to existing sonification tools.","2023-04-19","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","1–19","","","","","","","CHI '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/EXZ69P43/Hoque et al. - 2023 - Accessible Data Representation with Natural Sound.pdf","","","Sonification; Data visualization; Accessibility; Natural sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W4V9VFQY","conferencePaper","2013","McFarlane, Stuart; Feltham, Frank; Verhagen, Darrin","Exploring internet CO2 emissions as an auditory display","Proceedings of the 25th Australian Computer-Human Interaction Conference: Augmentation, Application, Innovation, Collaboration","978-1-4503-2525-7","","10.1145/2541016.2541081","https://dl.acm.org/doi/10.1145/2541016.2541081","This research project explores the effectiveness of an auditory display (AD) prototype for the sonification of perceived internet e-waste of CO2 emissions to a small user group within their office context. To date, methods do not exist for the reporting of e-waste to users of personal computing while they perform simple internet enquiries. Underpinning the theoretical development of this project is a focus on AD guided by a soundscape theory, and on approaches to sonification to convey subtle, unobtrusive, and useful information. Evaluation of the prototype takes place as a field study in an office context. The following paper gives an account of the design and development of the AD prototype and its respective sonification, the design methodology employed and the research findings, and concludes with recommendations for further exploration of the balance between ambient and salient information.","2013-11-25","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","225–228","","","","","","","OzCHI '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/NL3AZ9BM/McFarlane et al. - 2013 - Exploring internet CO2 emissions as an auditory di.pdf","","","sonification; auditory display; e-waste; internet CO2 emissions","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L5G3GAGT","conferencePaper","2015","North, Kevin J.; Bolan, Shane; Sarma, Anita; Cohen, Myra B.","GitSonifier: using sound to portray developer conflict history","Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering","978-1-4503-3675-8","","10.1145/2786805.2803199","https://dl.acm.org/doi/10.1145/2786805.2803199","There are many tools that help software engineers analyze data about their software, projects, and teams. These tools primarily use visualizations to portray data in a concise and understandable way. However, software engineering tasks are often multi-dimensional and temporal, making some visualizations difficult to understand. An alternative for representing data, which can easily incorporate higher dimensionality and temporal information, is the use of sound. In this paper we propose the use of sonification to help portray collaborative development history. Our approach, GitSonifier, combines sound primitives to represent developers, days, and conflicts over the history of a program's development. In a formative user study on an open source project's data, we find that users can easily extract meaningful information from sound clips and differentiate users, passage of time, and development conflicts, suggesting that sonification has the potential to provide benefit in this context.","2015-08-30","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","886–889","","","","","","GitSonifier","ESEC/FSE 2015","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/WNGRZRQY/North et al. - 2015 - GitSonifier using sound to portray developer conf.pdf","","","sonification; conflicts; version control history","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EA2S2VI2","conferencePaper","2010","Dulyan, Aram; Edmonds, Ernest","AUXie: initial evaluation of a blind-accessible virtual museum tour","Proceedings of the 22nd Conference of the Computer-Human Interaction Special Interest Group of Australia on Computer-Human Interaction","978-1-4503-0502-0","","10.1145/1952222.1952280","https://dl.acm.org/doi/10.1145/1952222.1952280","Remotely accessible audio-based virtual tours can offer great utility for blind or vision impaired persons, eliminating the difficulties posed by travel to unfamiliar locations, and allowing truly independent exploration. This paper draws upon sonification techniques used in previous implementations of audio-based 3D environments to develop a prototype of blind-accessible virtual tours specifically tailored to the needs of cultural sites. A navigable 3D world is presented using spatially positioned musical earcons, accompanied by synthesised speech descriptions and navigation aids. The worlds are read from X3D models enhanced with metadata to identify and describe the rooms and exhibits, thus enabling an audio modality for existing 3D worlds and simplifying the tour creation process. The prototype, named AUXie, was evaluated by 11 volunteers with total blindness to establish a proof of concept and identify the problematic aspects of the interface. The positive response obtained confirmed the validity of the approach and yielded valuable insight into how such tours can be further improved.","2010-11-22","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","272–275","","","","","","AUXie","OZCHI '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/4DM6I68W/Dulyan and Edmonds - 2010 - AUXie initial evaluation of a blind-accessible vi.pdf","","","sonification; accessibility; audio-based 3D environments; inclusive design; X3D","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AQIAGBDB","conferencePaper","2014","Schuett, Jonathan H.; Winton, Riley J.; Batterman, Jared M.; Walker, Bruce N.","Auditory weather reports: demonstrating listener comprehension of five concurrent variables","Proceedings of the 9th Audio Mostly: A Conference on Interaction With Sound","978-1-4503-3032-9","","10.1145/2636879.2636898","https://dl.acm.org/doi/10.1145/2636879.2636898","Displaying multiple variables or data sets within a single sonification has been identified as a challenge for the field of auditory display research. We discuss our recent study that evaluates the usability of a sonification that contains multiple variables presented in a way that encouraged perception across multiple auditory streams. We measured listener comprehension of weather sonifications that include the variables of temperature, humidity, wind speed, wind direction, and cloud cover. Listeners could accurately identify trends in five concurrent variables presented together in a single sonification. This demonstrates that it is indeed possible to include multiple variables together within an auditory stream and thus a greater number of variables within a sonification.","2014-10-01","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","1–7","","","","","","Auditory weather reports","AM '14","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/HDGTP3VZ/Schuett et al. - 2014 - Auditory weather reports demonstrating listener c.pdf","","","sonification; auditory displays; comprehension; auditory scene analysis; multiple streams; stream segregation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DPHZ83XA","conferencePaper","2021","Woodring, Ira; Owen, Charles","An Empirical Study of User Perception of Audio Stimuli in Relation to a Cartesian Space","The 14th PErvasive Technologies Related to Assistive Environments Conference","978-1-4503-8792-7","","10.1145/3453892.3453897","https://dl.acm.org/doi/10.1145/3453892.3453897","The field of Sonification has led to a variety of projects aimed at conveying graphical information via audition. Most of these projects are aimed at specific domains or contexts, and few have studied general audio presentation techniques empirically. This paper details the results of a multipart study of user perception of selected audio stimuli as users are asked to interpret those stimuli as points in a generic Cartesian space. Results will be used in future work designing methods of portraying technical drawings such as flowcharts and UML via audio.","2021-06-29","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","8–15","","","","","","","PETRA 2021","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/S5TJQSWH/Woodring and Owen - 2021 - An Empirical Study of User Perception of Audio Sti.pdf","","","Sonification; audio interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HTNPR8A6","conferencePaper","2016","North, Kevin J.; Sarma, Anita; Cohen, Myra B.","Understanding Git history: a multi-sense view","Proceedings of the 8th International Workshop on Social Software Engineering","978-1-4503-4397-8","","10.1145/2993283.2993285","https://dl.acm.org/doi/10.1145/2993283.2993285","Version control systems archive data about the development history of a project, which can be used to analyze and understand different facets of a software project. The project history can be used to evaluate the development process of a team, as an aid in bug fixing, or to help new members get on track with development. However, state of the art techniques for analyzing version control data provide only partial views into this information, and lack an easy way to present all the dimensions of the data. In this paper we present GitVS, a hybrid view that incorporates visualization and sonification to represent the multiple dimensions of version control data - development time line, conflicts, etc. In a formative user study comparing the GitHub Network Graph, GitVS, and a version of GitVS without sound, we show GitVS improves over the GitHub Network Graph and that while sound makes it easier to correctly understand version history for some tasks, it is more difficult for others.","2016-11-14","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","1–7","","","","","","Understanding Git history","SSE 2016","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/ZIWPJFV8/North et al. - 2016 - Understanding Git history a multi-sense view.pdf","","","Sonification; Conflicts; Version Control History","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZBUQKREI","conferencePaper","2021","Bakogiannis, Konstantinos; Andreopoulou, Areti; Georgaki, Anastasia","The development of a dance-musification model with the use of machine learning techniques under COVID-19 restrictions","Proceedings of the 16th International Audio Mostly Conference","978-1-4503-8569-5","","10.1145/3478384.3478407","https://dl.acm.org/doi/10.1145/3478384.3478407","Interactive technologies enable dancers to control the music in real-time with their movement. This paper presents the design and development of a model which takes as input a dancer’s movement and outputs music, structurally related to dance, with the use of machine learning techniques. Both the technical and artistic aspects of the model development are described in detail. In particular, the paper compares the use of machine learning techniques to traditional coding, in interactive dance and music applications. Moreover, it describes the significant discrimination between movement sonification and dance musification and explains why the model presented here falls into the second category. Special focus is given to the implications of the COVID-19 restrictions regarding the established collaboration with the dancer.","2021-10-15","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","81–88","","","","","","","AM '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/9FS4WGKF/Bakogiannis et al. - 2021 - The development of a dance-musification model with.pdf","","","algorithmic music; choreomusicology; interactive dance; interactive machine learning; interactive technology; music composition for dance","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WXQ47625","conferencePaper","2020","Brown, Dom; Nash, Chris; Mitchell, Thomas J.","Was that me? exploring the effects of error in gestural digital musical instruments","Proceedings of the 15th International Audio Mostly Conference","978-1-4503-7563-4","","10.1145/3411109.3411137","https://dl.acm.org/doi/10.1145/3411109.3411137","Traditional Western musical instruments have evolved to be robust and predictable, responding consistently to the same player actions with the same musical response. Consequently, errors occurring in a performance scenario are typically attributed to the performer and thus a hallmark of musical accomplishment is a flawless musical rendition. Digital musical instruments often increase the potential for a second type of error as a result of technological failure within one or more components of the instrument. Gestural instruments using machine learning can be particularly susceptible to these types of error as recognition accuracy often falls short of 100%, making errors a familiar feature of gestural music performances. In this paper we refer to these technology-related errors as system errors, which can be difficult for players and audiences to disambiguate from performer errors. We conduct a pilot study in which participants repeat a note selection task in the presence of simulated system errors. The results suggest that, for the gestural music system under study, controlled increases in system error correspond to an increase in the occurrence and severity of performer error. Furthermore, we find the system errors reduce a performer's sense of control and result in the instrument being perceived as less accurate and less responsive.","2020-09-16","2023-07-06 04:43:36","2023-07-06 04:43:36","2023-07-05","168–174","","","","","","Was that me?","AM '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/GLTADFR8/Brown et al. - 2020 - Was that me exploring the effects of error in ges.pdf","","","sonification; virtual reality; sonic interaction design; augmented reality; game audio; musicology; sound art; spatial audio","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QI85KHN5","conferencePaper","2018","Torsi, Silvia; Ardito, Carmelo","Strolling across the City: Geo-Tagged Sound Loops for Augmenting the Urban Spaces","Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers","978-1-4503-5966-5","","10.1145/3267305.3274168","https://dl.acm.org/doi/10.1145/3267305.3274168","We present the activity of strolling as an artistic practice. With this we make an invitation to be lost in the geography of an urban setting. The main concept is to provide the physical space with a communication level that would be pleasant to discover while wandering without a destination. We want to support flânerie as way for recapturing the dimensions of both time and space against the modern division between work, conviviality, rest and family life which are typical of modernism. Therefore the Post-Modern paradigm of valuing heterotopias drives this contribution. Along with considerations about semiotics, sonification, ecological psychology and digital jewellery we would like to furnish a possible way to imbue the sub-urban areas of genius loci, the spirit of a place.","2018-10-08","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","1121–1129","","","","","","Strolling across the City","UbiComp '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/WWJ73YS7/Torsi and Ardito - 2018 - Strolling across the City Geo-Tagged Sound Loops .pdf","","","sonification; digital jewellery; flânerie; neighborhoods; physical space; post-modernism; Strolling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EH226HW8","conferencePaper","2018","Smith, Brian A.; Nayar, Shree K.","The RAD: Making Racing Games Equivalently Accessible to People Who Are Blind","Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","978-1-4503-5620-6","","10.1145/3173574.3174090","https://dl.acm.org/doi/10.1145/3173574.3174090","We introduce the racing auditory display (RAD), an audio-based user interface that allows players who are blind to play the same types of racing games that sighted players can play with an efficiency and sense of control that are similar to what sighted players have. The RAD works with a standard pair of headphones and comprises two novel sonification techniques: the sound slider for understanding a car's speed and trajectory on a racetrack and the turn indicator system for alerting players of the direction, sharpness, length, and timing of upcoming turns. In a user study with 15 participants (3 blind; the rest blindfolded and analyzed separately), we found that players preferred the RAD's interface over that of Mach 1, a popular blind-accessible racing game. We also found that the RAD allows an avid gamer who is blind to race as well on a complex racetrack as casual sighted players can, without a significant difference between lap times or driving paths.","2018-04-21","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","1–12","","","","","","The RAD","CHI '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/WMPSMJHU/Smith and Nayar - 2018 - The RAD Making Racing Games Equivalently Accessib.pdf","","","sonification; accessibility; audio games; accessible games","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GBQLJGUM","conferencePaper","2022","Kantan, Prithvi Ravi; Dahl, Sofia; Spaich, Erika G.","Sound-Guided 2-D Navigation: Effects of Information Concurrency and Coordinate System","Nordic Human-Computer Interaction Conference","978-1-4503-9699-8","","10.1145/3546155.3546688","https://dl.acm.org/doi/10.1145/3546155.3546688","Auditory guidance conveying positional information through concurrent variations in properties of synthesized sound has previously been investigated. Auditory guidance may be more effective if multidimensional tasks are divided into unidimensional tasks where the user sequentially tackles each dimension and sound property. User performance may also depend on the coordinate system used for providing guidance. We compared concurrent and sequential guidance presentations in Cartesian and polar coordinate systems in a computer-based 2-D target-finding experiment with 15 participants. Sequential guidance was superior regarding completion time and number of interruptions with less cognitive burden than concurrent guidance. Participants were slower with the polar coordinate system than the Cartesian. These findings can contribute to the development of more efficacious guidance systems.","2022-10-08","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","1–11","","","","","","Sound-Guided 2-D Navigation","NordiCHI '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/CLPQ6JCJ/Kantan et al. - 2022 - Sound-Guided 2-D Navigation Effects of Informatio.pdf","","","sonification; auditory guidance; concurrency; navigation; perceptual test","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3LRH86KV","conferencePaper","2019","Kim, Kyung Jin; Jang, Sangsu; Kim, Bomin; Kwon, Hyosun; Park, Young-Woo","muRedder: Shredding Speaker for Ephemeral Musical Experience","Proceedings of the 2019 on Designing Interactive Systems Conference","978-1-4503-5850-7","","10.1145/3322276.3322362","https://dl.acm.org/doi/10.1145/3322276.3322362","The experience of sound may be seen as fleeting or ephemeral, as it naturally disperses through space in waveforms unless recorded by media. We designed muRedder to reinstate the ephemerality of sound by shredding a song ticket that embeds a sound source while playing the song simultaneously. In this study, we explored ordinary music listening activities by turning intangible music content into tangible artefacts, making the music unable to be replayed, and representing the sound-fading process by shredding the ticket. We conducted a field study with 10 participants over seven days. The results showed that muRedder enabled users to focus solely on the music content and to actively find times to enjoy the music. We also found that limitedness of the media draws prudent decision in selecting music. By showing the process of consuming the invisible auditory content in a way that is tangibly perceivable, our findings imply new value for slow consumption of digital content and musical participation in public spaces.","2019-06-18","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","127–134","","","","","","muRedder","DIS '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/V32PI5IK/Kim et al. - 2019 - muRedder Shredding Speaker for Ephemeral Musical .pdf","","","music; ephemeral interface; materialization; physicality","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6VZKH4ZK","conferencePaper","2017","Klauer, Giorgio; Metus, Annalisa; Polotti, Pietro","Sonic Interaction Design for Paper Wearables","Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences","978-1-4503-5373-1","","10.1145/3123514.3123533","https://dl.acm.org/doi/10.1145/3123514.3123533","The paper reports a workshop on sonic interaction design conceived and led by the authors in the context of a living lab born from the collaboration between a music conservatory and an IT university department. The main subject was the application of non-verbal sound in the process of product design, focusing on the augmentation of clothes and wearable accessories. The workshop resulted in exercises exploring the interactive role of the sound within three different scenarios: (a) abstract/relational, (b) strictly functional, (c) aesthetic/performative. Each exercise was carried on by a group of four to five participants working as a team. The presentation of the exercises comes along considerations regarding the participatory approach, that matched design techniques and tools with practices and theoretical foundations of electroacoustic music. Results are briefly discussed and improvements and further steps are accounted for.","2017-08-23","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","1–7","","","","","","","AM '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/2NU377UV/Klauer et al. - 2017 - Sonic Interaction Design for Paper Wearables.pdf","","","sonic interaction design; paper design; product design; prototyping; Wearable systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MVZXRFK2","conferencePaper","2014","Losing, Viktor; Rottkamp, Lukas; Zeunert, Michael; Pfeiffer, Thies","Guiding visual search tasks using gaze-contingent auditory feedback","Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication","978-1-4503-3047-3","","10.1145/2638728.2641687","https://dl.acm.org/doi/10.1145/2638728.2641687","In many applications it is necessary to guide humans' visual attention towards certain points in the environment. This can be to highlight certain attractions in a touristic application for smart glasses, to signal important events to the driver of a car or to draw the attention of a user of a desktop system to an important message of the user interface. The question we are addressing here is: How can we guide visual attention if we are not able to do it visually? In the presented approach we use gaze-contingent auditory feedback (sonification) to guide visual attention and show that people are able to make use of this guidance to speed up visual search tasks significantly.","2014-09-13","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","1093–1102","","","","","","","UbiComp '14 Adjunct","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/2PMMCJQT/Losing et al. - 2014 - Guiding visual search tasks using gaze-contingent .pdf","","","sonification; auditory feedback; eye-tracking; gaze-contingent; guidance; visual search","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AZTZW4K4","conferencePaper","2009","Baharin, Hanif; Mühlberger, Ralf","Living with the sound of the past: experiencing sonic atomic interaction using the sound diary","Proceedings of the 10th International Conference NZ Chapter of the ACM's Special Interest Group on Human-Computer Interaction","978-1-60558-574-1","","10.1145/1577782.1577800","https://dl.acm.org/doi/10.1145/1577782.1577800","This paper proposes a new interaction paradigm, atomic interaction, that aims at creating and maintaining contact without the transmission of content beyond the fact that an interaction is occurring. Atomic interactions can be represented using sound, which we term sonic atomic interaction. Since this is a new application of sonification, a prototype called the Sound Diary was created to simulate sonic atomic interaction for a pilot trial study. The experience of living with the Sound Diary is described in this paper from the first author point of view. The lessons learned from the experience are discussed in the light of previous literature. It was found that auditory icons can be easily confused for environmental sounds, and that dead metaphor sound may reduce confusion and startled reactions if auditory icons are to be used in unattended devices.","2009-07-06","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","101–104","","","","","","Living with the sound of the past","CHINZ '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/9ZR2DX62/Baharin and Mühlberger - 2009 - Living with the sound of the past experiencing so.pdf","","","sonification; atomic interaction; auditory icons; earcons; first-person research; sonic atomic interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HZFFVF8B","conferencePaper","2022","Payne, William; Ahmed, Fabiha; Gardell, Michael; DuBois, R. Luke; Hurst, Amy","SoundCells: designing a browser-based music technology for braille and print notation","Proceedings of the 19th International Web for All Conference","978-1-4503-9170-2","","10.1145/3493612.3520462","https://dl.acm.org/doi/10.1145/3493612.3520462","Technologies for notating music pose usage barriers to blind and visually impaired musicians requiring many to overcome a significant learning curve and/or rely on complicated tool chains with limited screen reader support. To address a need for accessible music notation software, we present SoundCells, a browser-based system designed to make music notation easy, intuitive, and accessible to screen reader users, and output music in audio, print, and braille formats. We share findings from a co-design process, in which two experienced musicians used SoundCells for two months guided by four remote meetings, and from a Design Probe, in which five other musicians tried SoundCells with a screen reader and reflected on its usability and accessibility in the context of their current practices. Finally, we discuss design recommendations relevant to a broader ecosystem of creative technologies, including how text-editing and multi-modal output capabilities could be extended and improved, how SoundCells' current design facilitated remote collaboration between sighted researchers and blind musicians, and future opportunities for learning and sharing music on the web.","2022-04-27","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","1–12","","","","","","SoundCells","W4A '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/6IA23IHZ/Payne et al. - 2022 - SoundCells designing a browser-based music techno.pdf","","","accessibility; music technology; visual impairments; braille; co-design; music notation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KYJNXEIK","conferencePaper","2013","Kaklanis, Nikolaos; Votis, Konstantinos; Tzovaras, Dimitrios","A mobile interactive maps application for a visually impaired audience","Proceedings of the 10th International Cross-Disciplinary Conference on Web Accessibility","978-1-4503-1844-0","","10.1145/2461121.2461152","https://dl.acm.org/doi/10.1145/2461121.2461152","Existing spatial information resources on the Web are graphically-orientated and are usually presented through interactive maps. As a consequence, in most of the cases, visually impaired users and especially blind users have very restricted access while they find it extremely difficult to recognize this kind of visual representation. Multimodal and mobile interactive maps could be a solution for presenting the spatial information resources to visually impaired people. In this paper, we present an interactive multimodal map application by investigating ways of presenting alternative formats that would replace visual information. ""Open Touch/Sound Maps"" is an Android mobile application, which enforces the accessibility of interactive maps for the visually impaired users. Multimodal interaction including sonification, Text-To-Speech (TTS) and vibration feedback enables access to OpenStreetMap data for the visually impaired and blind users using a common mobile device.","2013-05-13","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","1–2","","","","","","","W4A '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/QBMADSKM/Kaklanis et al. - 2013 - A mobile interactive maps application for a visual.pdf","","","sonification; visually impaired; haptic exploration accessibility; map exploration; multimodal map; OpenStreetMap; text-to-speech; touch interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3L8IYVJZ","conferencePaper","2016","Bisig, Daniel; Palacio, Pablo","Neural Narratives: Dance with Virtual Body Extensions","Proceedings of the 3rd International Symposium on Movement and Computing","978-1-4503-4307-7","","10.1145/2948910.2948925","https://dl.acm.org/doi/10.1145/2948910.2948925","From the context of two dance productions, the Neural Narratives project has started to emerge as a comprehensive exploration of simulation-based approaches that enable the creation of artificial body extensions for dancers. The simulation, visualisation and sonification of these body extensions allow a dancer to alter and enlarge his or her bodily presence and movement possibilities. The main focus of this publication lies in the contextualisation and discussion of a number of questions that have arisen during the realisation of the dance productions. These questions relate to concepts of embodiment, agency, and creativity and their possible implications for the realisation of interactive systems for dance. We try to address these questions by drawing from ideas that originate from a wide range of fields including dance and technology, cognitive science, systems science, and medical engineering. By connecting our own practical activities to a broad disciplinary context, we hope to contribute to a discourse concerning future directions for research and creation that deepen the integration of technology and dance.","2016-07-05","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","1–8","","","","","","Neural Narratives","MOCO '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/RWZ9599Y/Bisig and Palacio - 2016 - Neural Narratives Dance with Virtual Body Extensi.pdf","","","Sonification; Embodiment; Interaction; Creativity; Visualization; Dance Technology; Agency; Artificial Evolution; Artificial Neural Network; Body Extension; Mass-Spring Simulation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LQ5Q3EWF","conferencePaper","2011","Harada, Susumu; Takagi, Hironobu; Asakawa, Chieko","On the audio representation of radial direction","Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","978-1-4503-0228-9","","10.1145/1978942.1979354","https://dl.acm.org/doi/10.1145/1978942.1979354","We present and evaluate an approach towards eyes-free auditory display of spatial information that considers radial direction as a fundamental type of value primitive. There are many benefits to being able to sonify radial directions, such as indicating the heading towards a point of interest in a direct and dynamic manner, rendering a path or shape outline by sonifying a continual sequence of tangent directions as the path is traced, and providing direct feedback of the direction of motion of the user in a physical space or a pointer in a virtual space. We propose a concrete mapping of vowel-like sounds to radial directions as one potential method to enable sonification of such information. We conducted a longitudinal study with five sighted and two blind participants to evaluate the learnability and effectiveness of this method. Results suggest that our directional sound mapping can be learned within a few hours and be used to aurally perceive spatial information such as shape outlines and path contours.","2011-05-07","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","2779–2788","","","","","","","CHI '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/L57AW744/Harada et al. - 2011 - On the audio representation of radial direction.pdf","","","sonification; visual impairment; navigation; audio display; non-speech; non-verbal; radial direction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RAEWEZN3","conferencePaper","2009","Berman, Lewis I.; Gallagher, Keith B.","Using sound to understand software architecture","Proceedings of the 27th ACM international conference on Design of communication","978-1-60558-559-8","","10.1145/1621995.1622019","https://dl.acm.org/doi/10.1145/1621995.1622019","Use of non-speech sound can facilitate the understanding of a software program. Non-speech sound has been shown to be useful in dynamic program comprehension, that is, understanding the dynamic behavior of a program. We have developed a sonification scheme to describe static software entities in Java programs, and we show that it is useful in static program comprehension, notably concerning low-level architecture. The scheme is implemented via a tool in which an Eclipse IDE is integrated with a CSound synthesis engine. The tool is intended for use by sighted software developers in a static browsing/editing environment. A validation study of the concept has been performed via one-on-one sessions with experienced software developers. Preliminary results indicate that software developers are easily able to learn and recognize sonified characteristics of software entities and their relationships by listening to sequences of mapped sound constructs. Identification of specific entities is more problematic. Developers have indicated that they would find the tool useful during both exploration and more focused programming activities. Their additional perceptions have been collected using grounded qualitative means.","2009-10-05","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","127–134","","","","","","","SIGDOC '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/WCZSBRA7/Berman and Gallagher - 2009 - Using sound to understand software architecture.pdf","","","sonification; multimodal; auditory display; comprehension; architecture; eclipse; non-visual representations; program comprehension; software architecture","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NYZEILRJ","conferencePaper","2008","Reid, Peter; Plimmer, Beryl","A collaborative multimodal handwriting training environment for visually impaired students","Proceedings of the 20th Australasian Conference on Computer-Human Interaction: Designing for Habitus and Habitat","978-0-9803063-4-7","","10.1145/1517744.1517808","https://dl.acm.org/doi/10.1145/1517744.1517808","The spatial motor skills used for handwriting are particularly difficult for visually impaired people to develop. These skills are required in order to sign an aesthetically pleasing and repeatable signature, which is often required for documents such as legal agreements and job applications. Our multimodal system with haptic guidance, sonification and tactile feedback is designed to assist when teaching visually impaired students to form letters, and eventually, a signature. As tactile technologies become commonplace, appearing even in mobile phones, our system may also provide useful insight into the use of nonvisual feedback for a variety of applications.","2008-12-08","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","195–202","","","","","","","OZCHI '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/5AS8QUW4/Reid and Plimmer - 2008 - A collaborative multimodal handwriting training en.pdf","","","sonification; handwriting; haptic guidance; signature; tactile; visually-impaired","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HE3LKSZ9","conferencePaper","2016","Tomlinson, Brianna J.; Schuett, Jonathan H.; Shortridge, Woodbury; Chandran, Jehoshaph; Walker, Bruce N.","Talkin' about the weather: incorporating TalkBack functionality and sonifications for accessible app design","Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services","978-1-4503-4408-1","","10.1145/2935334.2935390","https://dl.acm.org/doi/10.1145/2935334.2935390","As ubiquitous as weather is in our daily lives, individuals with vision impairments endure poorly designed user experiences when attempting to check the weather on their mobile devices. This is primarily caused by a mismatch between the visually based information layout on screen and the order in which a screen reader, such as TalkBack or VoiceOver, presents the information to users with visual impairments. Additionally, any image or icon included on the screen presents no information to the user if they are not able to see it. Therefore we created the Accessible Weather App to run on Android and integrate with the TalkBack accessibility feature that is already available on the operating system. We also included a set of auditory weather icons which use sound, rather than visuals, to convey current weather conditions to users in a fast and pleasant way. This paper discusses the process for determining what features the users' would want and require, as well as our methodology for evaluating the beta version of our app.","2016-09-06","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","377–386","","","","","","Talkin' about the weather","MobileHCI '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/NRIQAVU8/Tomlinson et al. - 2016 - Talkin' about the weather incorporating TalkBack .pdf","","","sonification; accessibility; blind; assistive technology; visually impaired; app; talkback; VoiceOver; weather","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"93NWSCSQ","conferencePaper","2016","Russell, Spencer; Dublon, Gershon; Paradiso, Joseph A.","HearThere: Networked Sensory Prosthetics Through Auditory Augmented Reality","Proceedings of the 7th Augmented Human International Conference 2016","978-1-4503-3680-2","","10.1145/2875194.2875247","https://dl.acm.org/doi/10.1145/2875194.2875247","In this paper we present a vision for scalable indoor and outdoor auditory augmented reality (AAR), as well as HearThere, a wearable device and infrastructure demonstrating the feasibility of that vision. HearThere preserves the spatial alignment between virtual audio sources and the user's environment, using head tracking and bone conduction headphones to achieve seamless mixing of real and virtual sounds. To scale between indoor, urban, and natural environments, our system supports multi-scale location tracking, using fine-grained (20-cm) Ultra-WideBand (UWB) radio tracking when in range of our infrastructure anchors and mobile GPS otherwise. In our tests, users were able to navigate through an AAR scene and pinpoint audio source locations down to 1m. We found that bone conduction is a viable technology for producing realistic spatial sound, and show that users' audio localization ability is considerably better in UWB coverage zones than with GPS alone. HearThere is a major step towards realizing our vision of networked sensory prosthetics, in which sensor networks serve as collective sensory extensions into the world around us. In our vision, AAR would be used to mix spatialized data sonification with distributed, livestreaming microphones. In this concept, HearThere promises a more expansive perceptual world, or umwelt, where sensor data becomes immediately attributable to extrinsic phenomena, externalized in the wearer's perception. We are motivated by two goals: first, to remedy a fractured state of attention caused by existing mobile and wearable technologies; and second, to bring the distant or often invisible processes underpinning a complex natural environment more directly into human consciousness.","2016-02-25","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","1–8","","","","","","HearThere","AH '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/Y7JND9C8/Russell et al. - 2016 - HearThere Networked Sensory Prosthetics Through A.pdf","","","sonification; auditory augmented reality; bone conduction; sensory augmentation; UWB","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K73N6LA4","conferencePaper","2008","Dodiya, Janki; Alexandrov, Vassil N.","Use of auditory cues for wayfinding assistance in virtual environment: music aids route decision","Proceedings of the 2008 ACM symposium on Virtual reality software and technology","978-1-59593-951-7","","10.1145/1450579.1450615","https://dl.acm.org/doi/10.1145/1450579.1450615","This paper addresses the crucial problem of wayfinding assistance in the Virtual Environments (VEs). A number of navigation aids such as maps, agents, trails and acoustic landmarks are available to support the user for navigation in VEs, however it is evident that most of the aids are visually dominated. This work-in-progress describes a sound based approach that intends to assist the task of 'route decision' during navigation in a VE using music. Furthermore, with use of musical sounds it aims to reduce the cognitive load associated with other visually as well as physically dominated tasks. To achieve these goals, the approach exploits the benefits provided by music to ease and enhance the task of wayfinding, whilst making the user experience in the VE smooth and enjoyable.","2008-10-27","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","171–174","","","","","","Use of auditory cues for wayfinding assistance in virtual environment","VRST '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/LIHEE7YN/Dodiya and Alexandrov - 2008 - Use of auditory cues for wayfinding assistance in .pdf","","","virtual environments; auditory navigation; sound and music perception; wayfinding aids","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"89LB7VF4","conferencePaper","2010","McAdam, Christopher; Pinkerton, Craig; Brewster, Stephen A.","Novel interfaces for digital cameras and camera phones","Proceedings of the 12th international conference on Human computer interaction with mobile devices and services","978-1-60558-835-3","","10.1145/1851600.1851625","https://dl.acm.org/doi/10.1145/1851600.1851625","Camera phones are now very common but there are some usability issues that affect their use. These can occur because the users look through the LCD to frame the image and can often miss the icons displayed around the edges that present important information about the status of the camera. This may lead to shots being missed or poorly exposed. Most camera phones do not take full advantage of the features of the underlying phone platform to enhance their interfaces. We created a camera application for the Nokia N95 that featured novel interface elements and made use of the features of the platform to provide a rich variety of information in more usable forms, such as: sonifications of the luminance histogram to ensure better exposure before a picture is taken; phone orientation to give a level indicator to ensure the camera is straight; measuring phone movement to ensure the phone is being held steady; and the detection of image motion to support panning We also present a scenario for how these features could be used in conjunction with each other during the photo taking process.","2010-09-07","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","143–152","","","","","","","MobileHCI '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/RS893M7K/McAdam et al. - 2010 - Novel interfaces for digital cameras and camera ph.pdf","","","sonification; tactile; camera phone; luminance histogram; motion; orientation; panning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6JWSGRU9","conferencePaper","2022","Samaruga, Lucas; Riera, Pablo","A port of the SuperCollider’s class library to Python","Proceedings of the 17th International Audio Mostly Conference","978-1-4503-9701-8","","10.1145/3561212.3561250","https://dl.acm.org/doi/10.1145/3561212.3561250","This paper presents a fully functional port of the client side SuperCollider’s class library from sclang to Python. It discusses the fundamental architectural features that make the original library unique compared to other Python clients which differ in approach or scope. An exploration of how general purpose programming languages techniques and paradigms are used to represent DSP algorithms and musical structures is made while exposing the different components of the library, technical decisions and a few needed adaptations to the resources of the target language. It also presents the implementation of a non real time mode to create OSC scores from the same code used in real time. Finally, the advantages and disadvantages of the library itself are discussed.","2022-10-10","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","137–142","","","","","","","AM '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/PRQVXB7Y/Samaruga and Riera - 2022 - A port of the SuperCollider’s class library to Pyt.pdf","","","Music; Sound; Python; SuperCollider","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BY4KQGQI","conferencePaper","2020","Friske, Mikhaila; Wirfs-Brock, Jordan; Devendorf, Laura","Entangling the Roles of Maker and Interpreter in Interpersonal Data Narratives: Explorations in Yarn and Sound","Proceedings of the 2020 ACM Designing Interactive Systems Conference","978-1-4503-6974-9","","10.1145/3357236.3395442","https://dl.acm.org/doi/10.1145/3357236.3395442","To explore how materials, data, and humans collaborate to produce physical data representations, we created a series of artefacts from personal data we collected (about commuting, forgetting, and busy-ness) in different media---yarn and sound. We exchanged these artefacts without providing guidelines for how to interpret them in order to study where the boundary between maker and interpreter emerges. Through creating hand-crafted physicalizations and sonifications, we present three themes on making personal data narratives: matching data to the materials (and vice versa), accepting the materials' will to co-author, and negotiating between the experience of the data and data of the experience. In exchanging the artefacts, we explored the role of the interpreter as a re-maker and how multiple narratives can productively co-exist. We conclude with a discussion about how reimagining the roles of maker and interpreter might lead to new interactions with personal data narratives.","2020-07-03","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","297–310","","","","","","Entangling the Roles of Maker and Interpreter in Interpersonal Data Narratives","DIS '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/7Q37HRB4/Friske et al. - 2020 - Entangling the Roles of Maker and Interpreter in I.pdf","","","sonification; physicalization; personal informatics; research through design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HJA6UH8C","conferencePaper","2021","Reed, Courtney N.; McPherson, Andrew P.","Surface Electromyography for Sensing Performance Intention and Musical Imagery in Vocalists","Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction","978-1-4503-8213-7","","10.1145/3430524.3440641","https://dl.acm.org/doi/10.1145/3430524.3440641","Through experience, the techniques used by professional vocalists become highly ingrained and much of the fine muscular control needed for healthy singing is executed using well-refined mental imagery. In this paper, we provide a method for observing intention and embodied practice using surface electromyography (sEMG) to detect muscular activation, in particular with the laryngeal muscles. Through sensing the electrical neural impulses causing muscular contraction, sEMG provides a unique measurement of user intention, where other sensors reflect the results of movement. In this way, we are able to measure movement in preparation, vocalised singing, and in the use of imagery during mental rehearsal where no sound is produced. We present a circuit developed for use with the low voltage activations of the laryngeal muscles; in sonification of these activations, we further provide feedback for vocalists to investigate and experiment with their own intuitive movements and intentions for creative vocal practice.","2021-02-14","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","1–11","","","","","","","TEI '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/JEBVQLGQ/Reed and McPherson - 2021 - Surface Electromyography for Sensing Performance I.pdf","","","Biofeedback; Electromyography; First-Person Perspectives; Mental Imagery; Performer Intention","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6DSUTC2E","conferencePaper","2017","Ordiales, Hernán; Bruno, Matías Lennie","Sound recycling from public databases: Another BigData approach to sound collections","Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences","978-1-4503-5373-1","","10.1145/3123514.3123550","https://dl.acm.org/doi/10.1145/3123514.3123550","Discovering new sounds from large databases or Internet is a tedious task. Standard search tools and manual exploration fails to manage the actual amount of information available. This paper presents a new approach to the problem which takes advantage of grown technologies like Big Data and Machine Learning, keeping in mind compositional concepts and focusing on artistic performances. Among several different distributed systems useful for music experimentation, a new workflow is proposed based on analysis techniques from Music Information Retrieval (MIR) combined with massive online databases, dynamic user interfaces, physical controllers and real-time synthesis. Based on Free Software tools and standard communication protocols to classify, cluster and segment sound. The control architecture allows multiple clients request the API services concurrently enabling collaborative work. The resulting system can retrieve well defined or pseudo-aleatory audio samples from the web, mix and transform them in real-time during a live-coding performance, play like another instrument in a band, as a solo artist combined with visual feedback or working alone as automated multimedia installation.","2017-08-23","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","1–8","","","","","","Sound recycling from public databases","AM '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/U7N4KVNT/Ordiales and Bruno - 2017 - Sound recycling from public databases Another Big.pdf","","","User Interface; Audio discovery; BigData; Collaborative; Experimental; Live coding; Machine Learning; Music Information Retrieval; NetMusic; Performance; Realtime","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FBMW7HIU","conferencePaper","2010","Ketabdar, Hamed; Jahanbekam, Amirhossein; Yuksel, Kamer Ali; Hirsch, Tobias; Haji Abolhassani, Amin","MagiMusic: using embedded compass (magnetic) sensor for touch-less gesture based interaction with digital music instruments in mobile devices","Proceedings of the fifth international conference on Tangible, embedded, and embodied interaction","978-1-4503-0478-8","","10.1145/1935701.1935749","https://dl.acm.org/doi/10.1145/1935701.1935749","Playing musical instruments such as chordophones, percussions and keyboard types accompany with harmonic interaction of player's hand with the instruments. In this work, we present a novel approach that enables the user to imitate the music playing gestures around mobile devices. In our approach, touch-less gestures, which change magnetic field around the device, are employed for interaction. The activity of playing an instrument can be transparently pursued by moving a tiny magnet in hand around new generation of mobile phones equipped with embedded digital compass (magnetic sensor). The phonation intentions of the user can be simulated on the mobile device by capturing the gestural pattern using magnetic sensor. The proposed method allows digital imitation of a broad number of instruments while still being able to sense musical hits and relative plectrum gestures. It provides a framework for extending interaction space with music applications beyond physical boundaries of small mobile devices, and to 3D space around the device. This can allow for a more natural, comfortable and flexible interaction. We present several mobile music applications developed based on the proposed method for Apple iPhone 3GS.","2010-01-22","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","241–244","","","","","","MagiMusic","TEI '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/RCTLJARX/Ketabdar et al. - 2010 - MagiMusic using embedded compass (magnetic) senso.pdf","","","mobile devices; 3d magnetic gestures; around device interaction; digital music instruments; magnetic field sensor","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B9ZM7VET","conferencePaper","2019","Young, Emma; Marsden, Alan; Coulton, Paul","Making the Invisible Audible: Sonifying Qualitative Data","Proceedings of the 14th International Audio Mostly Conference: A Journey in Sound","978-1-4503-7297-8","","10.1145/3356590.3356610","https://dl.acm.org/doi/10.1145/3356590.3356610","We describe how Embosonic Sketching, a novel approach to qualitative data sonification, was employed in the design of a sound art installation. The method was conceived to minimise designer bias in the creation of sound artworks which seek to faithfully represent the lived experience of a sample group. The 'Her[sonifications]' project was driven by the notion that much exists beyond sight, yet remains undiscovered, and explored bringing the unseen to light through the transformative medium of sound. We ran a series of workshop and focus group sessions to engage with women from a range of backgrounds including artists, writers, designers and researchers to explore how we could harness the emotive qualities of sound to communicate the visceral experience of womanhood. We contribute insight into how Embosonic Sketching can be employed to enrich the process of qualitative data collection in group-based settings; to produce sound sketches to inform the sound design process; and most significantly, to create self-actualised sonifications of visceral and experiential phenomena. Our findings demonstrate that Embosonic Sketching is a useful and accessible tool for exchanging ideas about sound and promotes heightened participant engagement with participatory design processes.","2019-09-18","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","124–130","","","","","","Making the Invisible Audible","AM'19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/7X5ITC5P/Young et al. - 2019 - Making the Invisible Audible Sonifying Qualitativ.pdf","","","data sonification; participatory design; sound art; Embodied sound design; interactive sound installation; soundwalk","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y4RYY7A9","conferencePaper","2013","Wessolek, Daniel","Switching sensory domains: exploring the possibilities of a flickerfon","Proceedings of the 7th International Conference on Tangible, Embedded and Embodied Interaction","978-1-4503-1898-3","","10.1145/2460625.2460700","https://dl.acm.org/doi/10.1145/2460625.2460700","This paper presents results from experimentation with and subsequent possibilities of using an ambient light sensor connected to an audio amplifier, as well as a derived DIY kit: the Flickerfon. From the perspective of Interaction Design and Media Architecture, this paper focuses on presenting three different scenarios: a 'friendly neighborhood' or silent party scenario, an exhibition scenario, and the use of the Flickerfon as an instrument for exploring visual phenomena where the perception speed of the visual sense is slower than that of the auditory. By sending sound through visual light it becomes possible to mix different sound sources and benefit from the directional properties of light.","2013-02-10","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","373–374","","","","","","Switching sensory domains","TEI '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/ZVY3TTUP/Wessolek - 2013 - Switching sensory domains exploring the possibili.pdf","","","sonification; DIY kit; exhibition design; media architecture; photophone","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X5UEHAAB","conferencePaper","2008","Brewster, Stephen A.; Johnston, Jody","Multimodal interfaces for camera phones","Proceedings of the 10th international conference on Human computer interaction with mobile devices and services","978-1-59593-952-4","","10.1145/1409240.1409295","https://dl.acm.org/doi/10.1145/1409240.1409295","Camera phones are now very common but there are some issues that affect their usability. These can occur because users look through the LCD to frame the image and can often miss the icons displayed around the edge that present important information about exposure, battery life, number of shots remaining, etc. This may lead to shots being missed or poorly exposed. We created a sonified luminance histogram to present exposure information, a sound cue to indicate memory card space remaining and a tactile cue for battery charge status. A user study showed that participants were able to use the sonified histogram to identify exposure successfully and could recognise the status of the battery and memory card well, suggesting that alternative forms of output could free-up the screen for framing the image.","2008-09-02","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","387–390","","","","","","","MobileHCI '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/4GR8GKPZ/Brewster and Johnston - 2008 - Multimodal interfaces for camera phones.pdf","","","sonification; tactile; camera phone; luminance histogram","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NX5J8U66","conferencePaper","2016","Berdahl, Edgar; Blandino, Michael; Baker, David; Shanahan, Daniel","An Approach for Using Information Theory to Investigate Continuous Control of Analog Sensors by Humans","Proceedings of the Audio Mostly 2016","978-1-4503-4822-5","","10.1145/2986416.2986450","https://dl.acm.org/doi/10.1145/2986416.2986450","For applications in HCI and sonic interaction design, the accuracy with which humans can continuously control analog sensors is investigated. The field of information theory suggests that a human together with a user interface can be modeled as a communication channel. Specifically, the Shannon-Hartley theorem implies that the channel capacity/throughput can be estimated by asking human subjects to ""perform"" gestures that match idealized, bandlimited Gaussian ""target gestures."" Then, the signal-to-noise ratio of the recorded gestures determines the channel capacity/throughput. This approach is tested on human users alternately operating one of four simple analog sensors. In contrast with prior work in HCI, joint probability density functions do not need to be estimated nor must geometrically ""impossible"" gestures be eliminated from consideration. Suggestions are made for creating knowledge about user interfaces that could potentially transmit an enhanced amount of information to a computer.","2016-10-04","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","85–90","","","","","","","AM '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/A6VM3PKS/Berdahl et al. - 2016 - An Approach for Using Information Theory to Invest.pdf","","","channel capacity; continuous control; information theory; mutual information; Shannon-Hartley theorem; sound and music computing; throughput; User interface design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HK5LHRUS","conferencePaper","2011","Jylhä, Antti; Erkut, Cumhur","Auditory feedback in an interactive rhythmic tutoring system","Proceedings of the 6th Audio Mostly Conference: A Conference on Interaction with Sound","978-1-4503-1081-9","","10.1145/2095667.2095683","https://dl.acm.org/doi/10.1145/2095667.2095683","We present the recent developments in the design of audio-visual feedback in iPalmas, the interactive Flamenco rhythm tutor. Based on evaluation of the original implementation, we have re-designed the interface to better support the user in learning and performing rhythmic patterns. The system measures the performance parameters of the user and provides auditory feedback on the performance with different sounds corresponding to different performance attributes. The design of these sounds is informed by several attributes derived from the evaluation. We propose informative, non-intrusive. and archetypal sounds to be used in the system.","2011-09-07","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","109–115","","","","","","","AM '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/J9HCBN5H/Jylhä and Erkut - 2011 - Auditory feedback in an interactive rhythmic tutor.pdf","","","sonification; auditory feedback; hand clapping; rhythmic interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2MDKN7MS","conferencePaper","2013","Hajinejad, Nassrin; Vatterrott, Heide-Rose; Grüter, Barbara; Bogutzky, Simon","GangKlang: designing walking experiences","Proceedings of the 8th Audio Mostly Conference","978-1-4503-2659-9","","10.1145/2544114.2544130","https://dl.acm.org/doi/10.1145/2544114.2544130","As mobile game researchers we focus on playful experiences emerging in everyday life interactions. In this paper we present GangKlang, a particular sonic interaction design (SID) to support and facilitate the activity of walking with reference to Csikszentmihalyi's concept of flow. Tightly coupled with movement, sound becomes an element of sensorimotor cycles of hearing, proprioception and action.","2013-09-18","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","1–6","","","","","","GangKlang","AM '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/AGW4TJSS/Hajinejad et al. - 2013 - GangKlang designing walking experiences.pdf","","","sonification; walking; gait; sonic interaction design; activity; flow","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"37MNCL9Z","conferencePaper","2016","Leichsenring, Christian; Yang, Jiajun; Hammerschmidt, Jan; Hermann, Thomas","Challenges for smart environments in bathroom contexts","Proceedings of the 1st Workshop on Embodied Interaction with Smart Environments","978-1-4503-4555-2","","10.1145/3008028.3008033","https://dl.acm.org/doi/10.1145/3008028.3008033","Smart homes have been mostly treated as homogeneous environments where each room is distinguished by the activities performed there but not by any fundamentally different basic parameters for systems to operate in. We argue that at least for bathroom environments, things like the extensive presence of liquid water and humidity and special privacy considerations challenge these assumptions. We discuss typical and unique challenges for ubiquitous computing interfaces in bathroom environments and we look at how actual and conceptual systems confront these challenges. We review bathroom systems in the literature and present two systems of our own to exemplify the unique challenges to smart environments the bathroom provides, one of which is presented here for the first time.","2016-11-16","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","1–7","","","","","","","EISE '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/6PRST37C/Leichsenring et al. - 2016 - Challenges for smart environments in bathroom cont.pdf","","","sonification; ambient intelligence; smart environments; smart home; soundscape; tangible interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RPPWEV2U","conferencePaper","2010","Valenti, Roberto; Jaimes, Alejandro; Sebe, Nicu","Sonify your face: facial expressions for sound generation","Proceedings of the 18th ACM international conference on Multimedia","978-1-60558-933-6","","10.1145/1873951.1874219","https://dl.acm.org/doi/10.1145/1873951.1874219","We present a novel visual creativity tool that automatically recognizes facial expressions and tracks facial muscle movements in real time to produce sounds. The facial expression recognition module detects and tracks a face and outputs a feature vector of motions of specific locations in the face. The feature vector is used as input to a Bayesian network which classifies facial expressions into several categories (e.g., angry, disgusted, happy, etc.). The classification results are used along with the feature vector to generate a combination of sounds that change in real time depending on the person's facial expressions. We explain the artistic motivation behind the work, the basic components of our tool, and possible applications in the arts (performance, installation) and in the medical domain. Finally, we report on the experience of approximately 25 users of our system at a conference demonstration session, of 9 participants in a pilot study to assess the system's usability, and discuss our experience installing the work at an important digital arts festival (RE-NEW 2009).","2010-10-25","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","1363–1372","","","","","","Sonify your face","MM '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/JBT5JUEA/Valenti et al. - 2010 - Sonify your face facial expressions for sound gen.pdf","","","sonification; affective computing; facial expressions; facial therapy interface; gesture-based interaction; multimodal interface","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9CE6TRGW","conferencePaper","2010","Costanza, Enrico; Panchard, Jacques; Zufferey, Guillaume; Nembrini, Julien; Freudiger, Julien; Huang, Jeffrey; Hubaux, Jean-Pierre","SensorTune: a mobile auditory interface for DIY wireless sensor networks","Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","978-1-60558-929-9","","10.1145/1753326.1753675","https://dl.acm.org/doi/10.1145/1753326.1753675","Wireless Sensor Networks (WSNs) allow the monitoring of activity or environmental conditions over a large area, from homes to industrial plants, from agriculture fields to forests and glaciers. They can support a variety of applications, from assisted living to natural disaster prevention. WSNs can, however, be challenging to setup and maintain, reducing the potential for real-world adoption. To address this limitation, this paper introduces SensorTune, a novel mobile interface to support non-expert users in iteratively setting up a WSN. SensorTune uses non-speech audio to present to its users information regarding the connectivity of the network they are setting up, allowing them to decide how to extend it. To simplify the interpretation of the data presented, the system adopts the metaphor of tuning a consumer analog radio, a very common and well known operation. A user study was conducted in which 20 subjects setup real multi-hop networks inside a large building using a limited number of wireless nodes. Subjects repeated the task with SensorTune and with a comparable mobile GUI interface. Experimental results show a statistically significant difference in the task completion time and a clear preference of users for the auditory interface.","2010-04-10","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","2317–2326","","","","","","SensorTune","CHI '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/FU52PR83/Costanza et al. - 2010 - SensorTune a mobile auditory interface for DIY wi.pdf","","","sonification; wireless sensor network; user study; mobile hci; network deployment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BRPKXIJY","conferencePaper","2016","Cunningham, Stuart; Weinel, Jonathan","The Sound of the Smell (and taste) of my Shoes too: Mapping the Senses using Emotion as a Medium","Proceedings of the Audio Mostly 2016","978-1-4503-4822-5","","10.1145/2986416.2986456","https://dl.acm.org/doi/10.1145/2986416.2986456","This work discusses basic human senses: sight; sound; touch; taste; and smell; and the way in which it may be possible to compensate for lack of one, or more, of these by explicitly representing stimuli using the remaining senses. There may be many situations or scenarios where not all five of these base senses are being stimulated, either because of an optional restriction or deficit or because of a physical or sensory impairment such as loss of sight or touch sensation. Related to this there are other scenarios where sensory matching problems may occur. For example: a user immersed in a virtual environment may have a sense of smell from the real world that is unconnected to the virtual world. In particular, this paper is concerned with how sound can be used to compensate for the lack of other sensory stimulation and vice-versa. As a link is well established already between the visual, touch, and auditory systems, more attention is given to taste and smell, and their relationship with sound. This work presents theoretical concepts, largely oriented around mapping other sensory qualities to sound, based upon existing work in the literature and emerging technologies, to discuss where particular gaps currently exist, how emotion could be a medium to cross-modal representations, and how these might be addressed in future research. It is postulated that descriptive qualities, such as timbre or emotion, are currently the most viable routes for further study and that this may be later integrated with the wider body of research into sensory augmentation.","2016-10-04","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","28–33","","","","","","The Sound of the Smell (and taste) of my Shoes too","AM '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/FDJQ8QQY/Cunningham and Weinel - 2016 - The Sound of the Smell (and taste) of my Shoes too.pdf","","","sonification; cross-modality; Human senses; interaction; sensory perception","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PK3Y9N9J","conferencePaper","2010","Chambel, Teresa; Neves, Sérgio; Sousa, Celso; Francisco, Rafael","Synesthetic video: hearing colors, seeing sounds","Proceedings of the 14th International Academic MindTrek Conference: Envisioning Future Media Environments","978-1-4503-0011-7","","10.1145/1930488.1930515","https://dl.acm.org/doi/10.1145/1930488.1930515","In this paper we present Synesthetic Video, an interactive video that allows to experience video in cross-sensorial ways, to hear its colors and to influence its visual properties with sound and music, through user interaction or ambient influence. Our main motivations include accessibility, enriching users experiences, stimulating and supporting users creativity, and to learn more about synesthesia and how videos can influence and be influenced by users and the ambient, at the crossroads of art, science and technology.","2010-10-06","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","130–133","","","","","","Synesthetic video","MindTrek '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/B2X32NCX/Chambel et al. - 2010 - Synesthetic video hearing colors, seeing sounds.pdf","","","accessibility; music; ambient media; audio; color; digital art; immersive interactive interfaces; neuroscience; perception; synesthesia; video","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I9RGPDL3","conferencePaper","2011","Riener, A.; Jeon, M.; Tscheligi, M.; Fellner, J.","Subliminal perception in cars","Proceedings of the 3rd International Conference on Automotive User Interfaces and Interactive Vehicular Applications","978-1-4503-1231-8","","10.1145/2381416.2381449","https://dl.acm.org/doi/10.1145/2381416.2381449","Following laws and provisions passed on the national and international level, the most relevant goal of future vehicular interfaces is to increase road safety. To alleviate the cognitive load associated with the interaction with the variety of emerging information and assistance systems in the car (and to increase driving performance as well), subliminal persuasion is assumed to be a promising technique to reduce the amount of information the driver must store and recall. Subliminal cues could be provided across appropriate sensory modalities, according to the specific nature of the current task, and corresponding to drivers' cognitive abilities. The central objective of this workshop is to provoke a lively debate on the adequacy of information provided below active awareness and to discuss how to resolve potential problems in this highly risky research field. This approach exhibits exciting challenges, which can -- once fully understood -- impact on society at large, making significant contributions toward a more natural, convenient, and even relaxing future style of driving. Therefore, and to further strengthen significance of results, the workshop is directed at researchers from a range of disciplines, such as engineering, neuroscience, computer science, and psychophysiology.","2011-11-30","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","203–206","","","","","","","AutomotiveUI '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/BVRAAH7N/Riener et al. - 2011 - Subliminal perception in cars.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DDWXA58E","conferencePaper","2022","Lee, YeaJi; Wyatt, Ariana; Dong, Jiayuan; Upthegrove, Tanner; Hale, Brandon; Lyles, Chelsea H.; Choi, Koeun; Kim, Jisun; Yu, Shuqi; Vajir, Devanshu; Newbill, Phyllis; Jeon, Myounghoon","Robot Musical Theater for Climiate Change Education","Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction","","","","","The use of social robots has recently been investigated in various areas, including STEM (Science, Technology, Engineering, and Mathematics) education and artistic performances. To inform children of the seriousness of climate change and awareness that they can make change, we created the Robot Musical Theater performance. In this project, natural elements (wind, earth, fire, and water) were anthropomorphized and represented by humanoid robots (Pepper, Milo, and Nao). The robots were designed to motivate audience to participate in the action to prevent climate change. Because of COVID, only fourteen visitors as a single group were allowed to participate in real-time and posted to YouTube, where at the time of submission, 141 people have viewed the performance. The participants provided positive comments on the performance and showed their willingness to participate in the movement to prevent climate change, and expressed their further interest in STEM learning. This performance is expected to contribute to enhancing informal STEM and robotics learning, as well as advancing robotic arts.","2022-03-07","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","870–874","","","","","","","HRI '22","","","","IEEE Press","Sapporo, Hokkaido, Japan","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/NRRSJXXX/Lee et al. - 2022 - Robot Musical Theater for Climiate Change Educatio.pdf","","","alienation effect; child-robot interaction; interactive theater; robot theater; robotic art; stem education","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V4P99CSY","conferencePaper","2020","Bisig, Daniel; Palacio, Pablo","Sounding feet","Proceedings of the 15th International Audio Mostly Conference","978-1-4503-7563-4","","10.1145/3411109.3411112","https://dl.acm.org/doi/10.1145/3411109.3411112","The project emphSounding Feet explores the creative possibilities of interactively controlling sound synthesis through pressure sensitive shoe inlays that can monitor minute body movements. The project is motivated by the authors' own experience of working with interactive technologies in the context of dance. This experience has led to the desire to more closely relate the sensing capabilities of an interactive system to a dancer's own body awareness which prominently involve aspects of inner perception. The outcome of this project demonstrates that such an approach can help to establish interactive musical scenarios for dance that are not only more intuitive to work with for dancers but that also offer new possibilities for composers to tap into aspects of the dancers' expressivity that are normally hidden for an audience.","2020-09-16","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","222–228","","","","","","","AM '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/3R5WKXEL/Bisig and Palacio - 2020 - Sounding feet.pdf","","","movement sonification; body awareness; dance and technology; wearable interface","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C8SRNA6T","conferencePaper","2018","Dalmazzo, David; Tassani, Simone; Ramírez, Rafael","A Machine Learning Approach to Violin Bow Technique Classification: a Comparison Between IMU and MOCAP systems","Proceedings of the 5th International Workshop on Sensor-based Activity Recognition and Interaction","978-1-4503-6487-4","","10.1145/3266157.3266216","https://dl.acm.org/doi/10.1145/3266157.3266216","Motion Capture (MOCAP) Systems have been used to analyze body motion and postures in biomedicine, sports, rehabilitation, and music. With the aim to compare the precision of low-cost devices for motion tracking (e.g. Myo) with the precision of MOCAP systems in the context of music performance, we recorded MOCAP and Myo data of a top professional violinist executing four fundamental bowing techniques (i.e. Détaché, Martelé, Spiccato and Ricochet). Using the recorded data we applied machine learning techniques to train models to classify the four bowing techniques. Despite intrinsic differences between the MOCAP and low-cost data, the Myo-based classifier resulted in slightly higher accuracy than the MOCAP-based classifier. This result shows that it is possible to develop music-gesture learning applications based on low-cost technology which can be used in home environments for self-learning practitioners.","2018-09-20","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","1–8","","","","","","A Machine Learning Approach to Violin Bow Technique Classification","iWOAR '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/FV8S5BUD/Dalmazzo et al. - 2018 - A Machine Learning Approach to Violin Bow Techniqu.pdf","","","Gesture; Machine Learning; Audio Descriptors; MOCAP; Myo Armband","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TB3RRGAC","conferencePaper","2023","Bang, Tove Grimstad; Fdili Alaoui, Sarah; Schwartz, Elisabeth","Designing in Conversation With Dance Practice","Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","978-1-4503-9421-5","","10.1145/3544548.3581543","https://dl.acm.org/doi/10.1145/3544548.3581543","We present a long-term collaboration between dancers and designers, centred around the transmission of the century-old repertoire of modern dance pioneer Isadora Duncan. We engaged in a co-design process with a Duncanian dancer consisting of conversations and participation in her transmission of Duncan’s choreographies and technique. We then articulated experiential qualities central to Duncan’s repertoire and used them to guide the design of the probes, the sounding scarfs. Our probes sonify dancers’ movements using temporal sensors embedded in the fabric of the scarfs, with the goal of evoking Duncan’s work and legacy. We shared the probes with our Duncan dance community and found that they deepen the dancers’ engagement with the repertoire. Finally, we discuss how co-designing with slowness and humility were key to the dialogue created between the practitioners, allowing for seamless integration of design research and dance practice.","2023-04-19","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","1–16","","","","","","","CHI '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/S3MGF6KT/Bang et al. - 2023 - Designing in Conversation With Dance Practice.pdf","","","sonification; co-design; Dance; first-person perspectives; longitudinal study; soma design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HGE7Y5L2","conferencePaper","2021","Schlagowski, Ruben; Mertes, Silvan; André, Elisabeth","Taming the Chaos: Exploring Graphical Input Vector Manipulation User Interfaces for GANs in a Musical Context","Proceedings of the 16th International Audio Mostly Conference","978-1-4503-8569-5","","10.1145/3478384.3478411","https://dl.acm.org/doi/10.1145/3478384.3478411","Generative Adversarial Networks (GANs) are a widely used tool for generating highly realistic artificial data. As the output of these networks can show high diversity and novelty, GANs have the potential to be used as creative tools. However, using GANs in this context poses major challenges due to their unpredictability and lack of controllability, making it difficult for creative people to realize their artistic vision. To address this problem, we present two graphical user interfaces that visually order the (otherwise chaotic) latent input space of a GAN that was trained to generate drum samples. Further, these GUIs provide convergent search functions that allow users to fine-tune generated sounds. By doing so, we provide the ability to create sounds more purposefully to sound-affine users such as musicians or sound engineers. Additionally, we present the results of a user study that we conducted in order to explore our approach in accuracy-oriented and creative tasks. Our results indicate that usability and pragmatic qualities play a more important role for users than aesthetic-oriented aspects. Although not improving the accuracy within reproductive tasks, we observed that convergent search functions, if available, were used significantly more often than divergent/randomized search functions.","2021-10-15","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","216–223","","","","","","Taming the Chaos","AM '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/A2Q76US5/Schlagowski et al. - 2021 - Taming the Chaos Exploring Graphical Input Vector.pdf","","","Sound Design; Audio Synthesis; Generative Adversarial Networks; Human-Computer Interaction; Interactive Sound Generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H4A9ZJ8B","conferencePaper","2020","Sardana, Disha; Joo, Woohun; Bukvic, Ivica Ico; Earle, Gregory","Perception of spatial data properties in an immersive multi-layered auditory environment","Proceedings of the 15th International Audio Mostly Conference","978-1-4503-7563-4","","10.1145/3411109.3411134","https://dl.acm.org/doi/10.1145/3411109.3411134","We present a study of spatial sonification of multidimensional data using a spatial mask and an immersive high-density loudspeaker array. The study participants are asked to identify edges and perceived center of 2D shapes projected across the perimeter of an exocentric environment. The results show that the phase modulation technique results in less accurate user responses than the amplitude modulation or combined modulation techniques. No significant differences are found between stationary and mobile-user scenarios when comparing the angular miss distances of the perceived center of sonified shapes, but significant differences are identified in locating their left and top edges. Further research is warranted to determine why properties of some shapes are easier to pinpoint than others, and how sonification may be improved to minimize such discrepancies.","2020-09-16","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","30–37","","","","","","","AM '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/DTPYPFXN/Sardana et al. - 2020 - Perception of spatial data properties in an immers.pdf","","","data sonification; spatial audio; perception; immersive environments","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S4355J37","conferencePaper","2014","Bermudez, Bertha; Ziegler, Chris","Pre-Choreographic Movement Kit","Proceedings of the 2014 International Workshop on Movement and Computing","978-1-4503-2814-2","","10.1145/2617995.2617997","https://dl.acm.org/doi/10.1145/2617995.2617997","This paper describes the concept and development of the interactive installation ""Pre-Choreographic Movement Kit"". It utilizes a dance concept as a starting point to use physical objects for generating movements, using sensor-based motion tracking. The ""Pre-Choreographic Movement Kit"" is part of the interdisciplinary research project ""Pre-Choreographic Alphabet"", initiated by Bertha Bermudez and Emio Greco, ICKAmsterdam (International Choreographic Center in Amsterdam) joined by Chris Ziegler (School of Arts Media Engineering ASU, Tempe) for design and development under LABO21. The objective of the research project, is to reflect upon previous artistic work, define methodologies for the articulation of dance experiences using interactive environments for transmission and dissemination of knowledge. Choreographer Emio Greco and Pieter C. Scholten have developed over many years an understanding of dancer's technologies to generate movement material for dance creation. Chris Ziegler cites the idea of ""art in a box"" first developed by George Maciunas ""Fluxkit"" (1964), by designing a playful environment for dancers and non-dancers alike. The ""Pre-Choreographic Movement Kit"" proposes interactive Objects, generating a physical encounter to""Pre-choreographic Knowledge"" of thinking and performing movements.","2014-06-16","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","7–12","","","","","","","MOCO '14","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/Y8LBQLC5/Bermudez and Ziegler - 2014 - Pre-Choreographic Movement Kit.pdf","","","Sonification; Interaction; Sound; Dance; Installation; interactive Objects; Movement Principles; Notation; Sculpture","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RTJCAET3","conferencePaper","2019","Tajadura-Jiménez, Ana; Newbold, Joseph; Zhang, Linge; Rick, Patricia; Bianchi-Berthouze, Nadia","As Light as You Aspire to Be: Changing Body Perception with Sound to Support Physical Activity","Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","978-1-4503-5970-2","","10.1145/3290605.3300888","https://dl.acm.org/doi/10.1145/3290605.3300888","Supporting exercise adherence through technology remains an important HCI challenge. Recent works showed that altering walking sounds leads people perceiving themselves as thinner/lighter, happier and walking more dynamically. While this novel approach shows potential for physical activity, it raises critical questions impacting technology design. We ran two studies in the context of exertion (gym-step, stairs-climbing) to investigate how individual factors impact the effect of sound and the duration of the after-effects. The results confirm that the effects of sound in body-perception occur even in physically demanding situations and through ubiquitous wearable devices. We also show that the effect of sound interacted with participants' body weight and masculinity/femininity aspirations, but not with gender. Additionally, changes in body-perceptions did not hold once the feedback stopped; however, body-feelings or behavioural changes appeared to persist for longer. We discuss the results in terms of malleability of body-perception and highlight opportunities for supporting exercise adherence.","2019-05-02","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","1–14","","","","","","As Light as You Aspire to Be","CHI '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/NI9ACHAC/Tajadura-Jiménez et al. - 2019 - As Light as You Aspire to Be Changing Body Percep.pdf","","","sonification; emotion; auditory body perception; interaction styles; multimodal interfaces; evaluation method","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FW3Q46ZW","conferencePaper","2021","Hollerweger, Florian","Streaaam: A fully automated experimental audio streaming server","Proceedings of the 16th International Audio Mostly Conference","978-1-4503-8569-5","","10.1145/3478384.3478426","https://dl.acm.org/doi/10.1145/3478384.3478426","Streaaam is a fully automated experimental audio streaming server, built from Free/Libre Open Source Software in a higher education context. Some of its features include an automatic synthesized-speech moderator; real-time data integration via web APIs; and automatic loading and playing of generative music patches in Pd, SuperCollider, and Csound. These features are creatively combined in our stream’s program, such as in the form of a robot comedy show. Streaaam was conceived of and continues to be developed as a pedagogical vehicle in the context of an undergraduate education in audio arts, music technology, and sound design. Its goals are to inspire students to work creatively in the sonic arts, to introduce them to the development of audio applications for the web, and to showcase student art works at our department and college. This paper discusses the project’s technical implementation, our software development and program curation cycles, pedagogical experiences with the project thus far, and plans for future work.","2021-10-15","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","161–168","","","","","","Streaaam","AM '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/FPPM5GAQ/Hollerweger - 2021 - Streaaam A fully automated experimental audio str.pdf","","","music technology; audio streaming; FLOSS; sonic arts; sound design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B2SMMK7Q","conferencePaper","2019","Cherng, Fu-Yin; Lee, Yi-Chen; King, Jung-Tai; Lin, Wen-Chieh","Measuring the Influences of Musical Parameters on Cognitive and Behavioral Responses to Audio Notifications Using EEG and Large-scale Online Studies","Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","978-1-4503-5970-2","","10.1145/3290605.3300639","https://dl.acm.org/doi/10.1145/3290605.3300639","Prior studies have evaluated various designs for audio notifications. However, calls for more in-depth research on how such notifications work, especially at the level of users' cognitive states, have gone unanswered; and studies evaluating audio notifications with large numbers of participants in multiple environments have been rare. This study conducted an electroencephalography study (N=20) and an online study (N=967) to enhance understandings of how three musical parameters - melody (simple, complex), pitch (high, low), and tempo (fast, slow) - influenced users' cognition and behaviors. There are eight different notifications with different combinations of these parameters. The online study analyzed the effects of user-specific and environmental information on users' behaviors while they listened to these notifications. The results revealed that tempo and pitch have the main effect on the speed and strength (accuracy) of users' cognition and behaviors. The users' characteristics and environments influenced the effects of these musical parameters.","2019-05-02","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","1–12","","","","","","","CHI '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/E664KTDL/Cherng et al. - 2019 - Measuring the Influences of Musical Parameters on .pdf","","","audio notifications; brain-computer interface; neuroergonomics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8WZYLALX","conferencePaper","2021","Ferguson, Jamie; Freeman, Euan; Brewster, Stephen","Investigating the Effect of Polarity in Auditory and Vibrotactile Displays Under Cognitive Load","Proceedings of the 2021 International Conference on Multimodal Interaction","978-1-4503-8481-0","","10.1145/3462244.3479911","https://dl.acm.org/doi/10.1145/3462244.3479911","When users are undertaking mentally demanding visuals tasks, it can be beneficial to convey information through the auditory or tactile modality instead. A fundamental problem when mapping information to sound or vibration is establishing which polarity the mapping should use. Magnitude estimation is a popular method of establishing polarity preferences, however the effectiveness of this approach remains unclear, especially in more ecologically valid contexts. We investigate what impact the polarity of a data-sound or data-vibration mapping has on how well users can interpret these mappings, under two different levels of mental workload. Our results show that polarity does not affect error rate or cognitive workload, although may affect response time. We also found that induced cognitive load may influence usability. An implication of this is that commonly used methods of establishing data mappings need to be revisited, with cognitive load in mind, to help designers create more usable auditory and vibrotactile displays.","2021-10-18","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","379–386","","","","","","","ICMI '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/457ICH79/Ferguson et al. - 2021 - Investigating the Effect of Polarity in Auditory a.pdf","","","Sonification; Auditory Display; Cognitive Load; Polarity; Vibrotactile; Vibrotactile Display","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A6JRMCFD","conferencePaper","2011","Bikaki, Athina; Floros, Andreas","An RSS-feed auditory aggregator using earcons","Proceedings of the 6th Audio Mostly Conference: A Conference on Interaction with Sound","978-1-4503-1081-9","","10.1145/2095667.2095681","https://dl.acm.org/doi/10.1145/2095667.2095681","In this work we present a data sonification framework based on parallel/concurrent sonic earcons' representations for monitoring in real-time information related to stock market. The information under consideration is conveyed through the well-known Really Simple Syndication (RSS) feed Internet mechanism and includes both text and numeric values, converted to speech and earcons using existing speech synthesis techniques and sonic design guidelines. Due to the considered application characteristics, particular emphasis is provided on information representation concurrency, mainly achieved using sound source spatialization techniques and different timbre characteristics. Spatial positioning of sound sources is performed through typical binaural processing and reproduction. A number of systematic, subjective assessments performed have shown that the overall perceptual efficiency and sonic representation accuracy fulfills the overall application requirements, provided that the users are appropriately trained prior to using the proposed RSS-feed auditory aggregator.","2011-09-07","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","95–100","","","","","","","AM '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/UHTJEKAI/Bikaki and Floros - 2011 - An RSS-feed auditory aggregator using earcons.pdf","","","auditory displays; earcons; RSS-feed sonification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V7ANHXRI","conferencePaper","2022","Ragone, Grazia; Howland, Kate; Brulé, Emeline","Evaluating Interactional Synchrony in Full-Body Interaction with Autistic Children","Proceedings of the 21st Annual ACM Interaction Design and Children Conference","978-1-4503-9197-9","","10.1145/3501712.3529729","https://dl.acm.org/doi/10.1145/3501712.3529729","Interactional synchrony, the spontaneous coordination of movements during interaction, is increasingly considered important in research on the development of non-verbal communication by autistic children. There is evidence that interventions using embodied-interaction technologies to support interactional synchrony are possible, but we do not have a shared framework in Human-Computer Interaction (HCI) for designing and evaluating such systems. We discuss existing measurement and evaluation tools used in experimental psychology and consider how the prevalent approach could be adapted to naturalistic HCI study contexts, with input from domain experts. We report on an exploratory case study evaluating a full-body interactive musical system with a group of ten autistic children. We provide methodological recommendations for the evaluation of future systems focusing on interactional synchrony, highlight limitations of current measurement tools and suggest mitigations.","2022-06-27","2023-07-06 04:47:01","2023-07-06 04:47:01","2023-07-05","1–12","","","","","","","IDC '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/5K85UJ67/Ragone et al. - 2022 - Evaluating Interactional Synchrony in Full-Body In.pdf","","","music; motion capture; Autism; evaluation methods; interactional synchrony; Motion Energy Analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MZTQM6CW","conferencePaper","2018","Bozlak, Elif; Acar, Aybar Can; Surer, Elif","Design and Evaluation of an Interactive Art Installation to Introduce 'de novo' Mutations to Different Audiences: ""Music Within"" Project","Proceedings of the 4th EAI International Conference on Smart Objects and Technologies for Social Good","978-1-4503-6581-9","","10.1145/3284869.3284883","https://dl.acm.org/doi/10.1145/3284869.3284883","Installation projects are frequently used in education as they are functional in explaining complex scientific concepts by making them intuitive. Mutation is such a scientific concept---mostly misunderstood by those lacking a scientific background. The ""Music Within"" project explains the term ""mutation"" via an art installation which combines music and Light Emitting Diode (LED) signals. In the project, 'de novo' mutations, which are specific to an individual, are used in order to create music and light signals. Genomic positions of mutations from 5 different individuals were placed on a ""virtual"" guitar string and the corresponding frequency values were converted to sound and Red-Green-Blue (RGB) values. The comprehensibility of the system was tested using a short questionnaire which was applied to 92 volunteers from different backgrounds and age groups. Questions from System Usability Scale (SUS) and EGameFlow were used in the survey to measure user-friendliness. Survey results show that age or having different backgrounds do not affect the comprehensibility of the system and explaining the mutation concept using art installations is helpful and engaging.","2018-11-28","2023-07-06 04:48:05","2023-07-06 04:48:05","2023-07-05","82–87","","","","","","Design and Evaluation of an Interactive Art Installation to Introduce 'de novo' Mutations to Different Audiences","Goodtechs '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/55IC3B2M/Bozlak et al. - 2018 - Design and Evaluation of an Interactive Art Instal.pdf","","","Art Installation; Human-Object Interaction; Mutations; System Usability Scale","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TH7EVTPU","conferencePaper","2019","Schacher, Jan; Wei, Lia","Gesture-Ink-Sound: Linking Calligraphy Performance with Sound","Proceedings of the 6th International Conference on Movement and Computing","978-1-4503-7654-9","","10.1145/3347122.3347136","https://dl.acm.org/doi/10.1145/3347122.3347136","In calligraphy, a brush stroke is rooted in an inner image, breath and the uninterrupted flow of movement. The same can be said of a bow stroke on a string instrument or a note sounded on a wind instrument. This article documents the encounter between a specific, two-person form of calligraphic performance, movement analysis techniques, and the mapping of brush gestures to sound processes. It shows how, based on data obtained in motion-capture sessions, the link between gesture and sound is established. This enables different models of sound processes, their specific mode of operation, and the understanding of what makes a stroke. Questions and issues arising from this concrete work are collected and a reflective analysis is carried out via a diagrammatic process. A discussion of critical limitations and possible extensions in this configuration concludes the article.","2019-10-10","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","1–8","","","","","","Gesture-Ink-Sound","MOCO '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/MZ46NMKB/Schacher and Wei - 2019 - Gesture-Ink-Sound Linking Calligraphy Performance.pdf","","","Gesture; Calligraphy and Sound; Domain Translation; Mapping; Motion Capture; Movement Analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BYYVZENV","conferencePaper","2023","Reed, Courtney N.; McPherson, Andrew P.","The Body as Sound: Unpacking Vocal Embodiment through Auditory Biofeedback","Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction","978-1-4503-9977-7","","10.1145/3569009.3572738","https://dl.acm.org/doi/10.1145/3569009.3572738","Multi-sensory experiences underpin embodiment, whether with the body itself or technological extensions of it. Vocalists experience intensely personal embodiment, as vocalisation has few outwardly visible effects and kinaesthetic sensations occur largely within the body, rather than through external touch. We explored this embodiment using a probe which sonified laryngeal muscular movements and provided novel auditory feedback to two vocalists over a month-long period. Somatic and micro-phenomenological approaches revealed that the vocalists understand their physiology through its sound, rather than awareness of the muscular actions themselves. The feedback shaped the vocalists’ perceptions of their practice and revealed a desire for reassurance about exploration of one’s body when the body-as-sound understanding was disrupted. Vocalists experienced uncertainty and doubt without affirmation of perceived correctness. This research also suggests that technology is viewed as infallible and highlights expectations that exist about its ability to dictate success, even when we desire or intend to explore.","2023-02-26","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","1–15","","","","","","The Body as Sound","TEI '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/HXJA39Q2/Reed and McPherson - 2023 - The Body as Sound Unpacking Vocal Embodiment thro.pdf","","","auditory feedback; embodiment; musical interaction; sensory translation; tacit knowledge","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PDCK7BIB","conferencePaper","2011","Schaffert, Nina; Mattes, Klaus; Effenberg, Alfred O.","Examining effects of acoustic feedback on perception and modification of movement patterns in on-water rowing training","Proceedings of the 6th Audio Mostly Conference: A Conference on Interaction with Sound","978-1-4503-1081-9","","10.1145/2095667.2095685","https://dl.acm.org/doi/10.1145/2095667.2095685","This paper describes a concept for providing acoustic feedback during on-water training to elite rowers and its implementation into the training process. The final aim was to improve the mean boat velocity by a reduction of intra-cyclic interruptions in the boat acceleration. It was assumed to enhance athletes' perception for the modification of movement patterns and control in technique training because sound conveys time-critical structures subliminally. That is of crucial importance for the precision of modifying movements to improve their execution. Advances in technology allow the design for innovative feedback systems to communicate feedback information audibly to athletes. The acoustic feedback system Sofirow was designed and field-tested with elite athletes. The device presents the boat acceleration-time trace audibly and online to athletes and coaches. The results showed a significant increase in the mean boat velocity during the sections with acoustic feedback compared to the sections without. It is thus very supportive to implement acoustic feedback regularly into training processes for elite athletes. A behavioral dynamics approach was invoked to provide a theoretical basis for this concept.","2011-09-07","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","122–129","","","","","","","AM '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/V3ZLW7S6/Schaffert et al. - 2011 - Examining effects of acoustic feedback on percepti.pdf","","","interactive sonification; auditory display; movement sonification; acoustic feedback; auditory information; elite athletes; motion perception; movement optimization; multi-sensory information; online feedback training; rowing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3CAIPJTT","conferencePaper","2021","Mary Costello, Brigid","Paying attention to rhythm in HCI: Some thoughts on methods","Proceedings of the 32nd Australian Conference on Human-Computer Interaction","978-1-4503-8975-4","","10.1145/3441000.3441005","https://dl.acm.org/doi/10.1145/3441000.3441005","To focus on rhythms within the lived relations between humans and technology is to focus on unfolding processes, dynamic temporality, and patterns of change and continuity. Rhythms infuse the political, the social, the personal and the technological. Through anticipation and expectation rhythms also point towards the future and its tendencies. Drawing inspiration from Rhythmanalysis, Somaesthetic Design and Entanglement HCI this paper explores methods for analysing rhythm within HCI and discusses the challenges of recording the ephemeral, articulating experience and understanding the non-human. Like rhythm, the methods outlined involve process, temporality and difference. There are the embodied practices of developing sensitivity, paying attention and thinking by doing. Then there are the representational practices of revealing through sound, working with moving images, shifting temporal scales and expressing dynamics graphically. These methods are presented as suggestions and tentative principles for ways of thinking with and through rhythm and aim to inspire further exploration.","2021-02-15","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","471–480","","","","","","Paying attention to rhythm in HCI","OzCHI '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/FU2U2HDF/Mary Costello - 2021 - Paying attention to rhythm in HCI Some thoughts o.pdf","","","Sonification; Rhythm; Disruption; Embodied Research; Method; Non-human Relations","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N3HMPL4R","conferencePaper","2022","Artacho, Adrián; Horstmeyer, Leonhard","SmoothOperator : A Device for Characterizing Smoothness in Body Movement","Proceedings of the 8th International Conference on Movement and Computing","978-1-4503-8716-3","","10.1145/3537972.3538000","https://dl.acm.org/doi/10.1145/3537972.3538000","In our research we faced the problem of characterizing smoothness in human movement. Even though ’smooth’ is a common way to describe and conceptualize motion in the performing arts as well as in informal speech, we realized the need for a tool to differentiate between various degrees and modes of smoothness. We propose that smoothness operates at different interlocking orders. These appear only in aggregation, intertwined with other qualities of body movement. Akin to how a spectrometer splits light into a spectrum of frequencies, we developed a method to measure the degree of smoothness in each order, as an epistemic tool for dance practitioners to investigate the quality of body movement from a fresh perspective. To this end we have implemented a device that provides dancers with aural, haptic and visual feedback in real time, taking into account the constraints of a dance practice session.","2022-06-30","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","1–6","","","","","","SmoothOperator","MOCO '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/BHFS7WGW/Artacho and Horstmeyer - 2022 - SmoothOperator  A Device for Characterizing Smoot.pdf","","","movement analysis; contemporary dance; performance tool; smoothness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CRQ3Z4Z9","conferencePaper","2022","Chang, Ruei-Che; Ting, Chao-Hsien; Hung, Chia-Sheng; Lee, Wan-Chen; Chen, Liang-Jin; Chao, Yu-Tzu; Chen, Bing-Yu; Guo, Anhong","OmniScribe: Authoring Immersive Audio Descriptions for 360° Videos","Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology","978-1-4503-9320-1","","10.1145/3526113.3545613","https://dl.acm.org/doi/10.1145/3526113.3545613","Blind people typically access videos via audio descriptions (AD) crafted by sighted describers who comprehend, select, and describe crucial visual content in the videos. 360° video is an emerging storytelling medium that enables immersive experiences that people may not possibly reach in everyday life. However, the omnidirectional nature of 360° videos makes it challenging for describers to perceive the holistic visual content and interpret spatial information that is essential to create immersive ADs for blind people. Through a formative study with a professional describer, we identified key challenges in describing 360° videos and iteratively designed OmniScribe, a system that supports the authoring of immersive ADs for 360° videos. OmniScribe uses AI-generated content-awareness overlays for describers to better grasp 360° video content. Furthermore, OmniScribe enables describers to author spatial AD and immersive labels for blind users to consume the videos immersively with our mobile prototype. In a study with 11 professional and novice describers, we demonstrated the value of OmniScribe in the authoring workflow; and a study with 8 blind participants revealed the promise of immersive AD over standard AD for 360° videos. Finally, we discuss the implications of promoting 360° video accessibility.","2022-10-28","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","1–14","","","","","","OmniScribe","UIST '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/G7YWV25H/Chang et al. - 2022 - OmniScribe Authoring Immersive Audio Descriptions.pdf","","","sonification; Blind; accessibility; multimedia; visual impairment; virtual reality; computer vision; 360° video; audio description; mobile","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XX6MVJDN","conferencePaper","2019","Iber, Michael; Lechner, Patrik; Jandl, Christian; Mader, Manuel; Reichmann, Michael","Auditory Augmented Reality for Cyber Physical Production Systems","Proceedings of the 14th International Audio Mostly Conference: A Journey in Sound","978-1-4503-7297-8","","10.1145/3356590.3356600","https://dl.acm.org/doi/10.1145/3356590.3356600","We describe a proof-of-concept approach on the sonification of estimated operation states of 3D printing processes. The results of this study form the basis for the development of an ""intelligent"" noise protection headphone as part of Cyber Physical Production Systems, which provides auditorily augmented information to machine operators and enables radio communication between them. Further application areas are implementations in control rooms (equipped with multichannel loudspeaker systems) and utilization for training purposes. The focus of our research lies on situation-specific acoustic processing of conditioned machine sounds and operation related data with high information content, considering the often highly auditorily influenced working knowledge of skilled workers. As a proof-of-concept the data stream of error probability estimations regarding partly manipulated 3D printing processes was mapped to three sonification models, giving evidence about momentary operation states. The neural network applied indicates a high accuracy (>93%) concerning error estimation distinguishing between normal and manipulated operation states. None of the manipulated states could be identified by listening. An auditory augmentation, respectively sonification of these error estimations provides a considerable benefit to process monitoring.","2019-09-18","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","53–60","","","","","","","AM'19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/I68X3UP5/Iber et al. - 2019 - Auditory Augmented Reality for Cyber Physical Prod.pdf","","","auditory display; auditory augmentation; cyber physical production systems; error prediction estimation; process monitoring","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MVN7BMLU","conferencePaper","2017","Schlegel, Andreas; Honnet, Cedric","Digital Oxymorons: From Ordinary to Expressive Objects Using Tiny Wireless IMUs","Proceedings of the 4th International Conference on Movement Computing","978-1-4503-5209-3","","10.1145/3077981.3078040","https://dl.acm.org/doi/10.1145/3077981.3078040","In this paper we discuss the potential of ordinary objects acting as human computer interfaces with an Inertial Measurement Unit, the Twiz, to capture a body's orientation and acceleration. The motivation behind this research is to develop a toolkit that enables end users to quickly prototype custom interfaces for artistic expressions through movement. Through an iterative design process we have enhanced existing technical implementations such as wireless data transfer, battery lifespan, two-way communication and data analysis including machine-learning techniques. We conducted object-making sessions and developed software prototypes for audio and visual feedback. We explored a range of experiments related to visual arts, dance, and music by attaching the Twiz to different types of objects to allow users to carry out impromptu interactions. As a result of this process we have gained a better understand of an object's expressive potential whilst capturing and analyzing its movement.","2017-06-28","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","1–8","","","","","","Digital Oxymorons","MOCO '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/CK873TQL/Schlegel and Honnet - 2017 - Digital Oxymorons From Ordinary to Expressive Obj.pdf","","","tangible interfaces; embodied interaction; interactive object; motion sensing extensions; motion sonification; motion visualization; movement data; multimodal interactive performance","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9GELH7RU","conferencePaper","2017","Ucar, Ezgi","Eclipse: a wearable instrument for performance based storytelling","Proceedings of the Second International Conference on Internet of things, Data and Cloud Computing","978-1-4503-4774-7","","10.1145/3018896.3018951","https://dl.acm.org/doi/10.1145/3018896.3018951","This paper provides a presentation of the ideation, design, and implementation processes of a wearable musical instrument for performance based storytelling. This experimental project seeks ways to integrate technology into interactive dance performance and music creation with subtlety and practicality. Eclipse is a dance performance telling the Altai Shaman story of a solar eclipse. Through movement of a dancer, a unique soundscape is created, to which the dancer moves. This feedback mechanism tries to achieve a multisensory interaction where the sounds created by movements are supplementary to the visual perception of the movements.","2017-03-22","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","1–4","","","","","","Eclipse","ICC '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/I9QXUHFB/Ucar - 2017 - Eclipse a wearable instrument for performance bas.pdf","","","altai shamanism; interactive storytelling; multisensory perception; new musical instruments; wearable technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WXHFKK9K","conferencePaper","2018","Giomi, Andrea; Fratagnoli, Federica","Listening Touch: A Case Study about Multimodal Awareness in Movement Analysis with Interactive Sound Feedback","Proceedings of the 5th International Conference on Movement and Computing","978-1-4503-6504-8","","10.1145/3212721.3212815","https://dl.acm.org/doi/10.1145/3212721.3212815","During the last years1, motion sensing technologies have been proven to be a useful mean for movement's analysis in relation to dance and music performances. The case study presented is part of a research project that involves dance pedagogy and new technologies at Nice University. This paper focus on description of an interactive experience based on motion sensing technologies allowing dance students to explore the connection between to relate kinesthesia, sense of touch and listening via interactive sound feedbacks.","2018-06-28","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","1–8","","","","","","Listening Touch","MOCO '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/RKDNHRY8/Giomi and Fratagnoli - 2018 - Listening Touch A Case Study about Multimodal Awa.pdf","","","Auditory feedback; movement analysis; gesture sonification; multimodal integration; somatic awareness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LEQJHS7D","conferencePaper","2011","Floros, Andreas; Tatlas, Nicolas-Alexander; Potirakis, Stylianos","Sonic perceptual crossings: a tic-tac-toe audio game","Proceedings of the 6th Audio Mostly Conference: A Conference on Interaction with Sound","978-1-4503-1081-9","","10.1145/2095667.2095680","https://dl.acm.org/doi/10.1145/2095667.2095680","The development of audio-only computer games imposes a number of challenges for the sound designer, as well as for the human machine interface design approach. Modern sonification methods can be used for effective data and game-environment or conditions representation through sound, including earcons and auditory icons. In this work we take advantage of earcons fundamental characteristics, such as spatialization usually employed for concurrent/parallel reproduction, in order to implement a tic-tac-toe audio game prototype. The proposed sonic design is transparently integrated with a novel user control/interaction mechanism that can be easily implemented in state-of-the-art mobile devices incorporating movement sensors (i.e. accelerometers and gyroscope). The overall prototype design efficiency is assessed in terms of the employed sonification accuracy, while the playability achieved through the integration of the sonic design and the employed auditory user interface is assessed in real game-play conditions.","2011-09-07","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","88–94","","","","","","Sonic perceptual crossings","AM '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/FFF3LM7Y/Floros et al. - 2011 - Sonic perceptual crossings a tic-tac-toe audio ga.pdf","","","sonic interaction design; audio games; earcons; eye-free interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ULWK6NI7","conferencePaper","2019","Lepri, Giacomo; McPherson, Andrew","Making Up Instruments: Design Fiction for Value Discovery in Communities of Musical Practice","Proceedings of the 2019 on Designing Interactive Systems Conference","978-1-4503-5850-7","","10.1145/3322276.3322353","https://dl.acm.org/doi/10.1145/3322276.3322353","The design of a new technology entails the materialisation of values emerging from the specific community, culture and context in which that technology is created. Within the domain of musical interaction, HCI research often examines new digital tools and technologies which can carry unstated cultural assumptions. This paper takes a step back to present a value discovery exercise exploring the breadth of perspectives different communities might have in relation to the values inscribed in fictional technologies for musical interaction. We conducted a hands-on activity in which musicians active in different contexts were invited to envision not-yet-existent musical instruments. The activity revealed several sources of influence on participants' artefacts, including cultural background, instrumental training, and prior experience with music technology. Our discussion highlights the importance of cultural awareness and value rationality for the design of interactive systems within and beyond the musical domain.","2019-06-18","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","113–126","","","","","","Making Up Instruments","DIS '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/9WAQ6HCJ/Lepri and McPherson - 2019 - Making Up Instruments Design Fiction for Value Di.pdf","","","communities of practice; design fiction; musical instrument design; non-functional prototyping; value discovery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4AEWDLAH","conferencePaper","2022","Weger, Marian; Svoronos-Kanavas, Iason; Höldrich, Robert","Schrödinger’s box: an artifact to study the limits of plausibility in auditory augmentations","Proceedings of the 17th International Audio Mostly Conference","978-1-4503-9701-8","","10.1145/3561212.3561222","https://dl.acm.org/doi/10.1145/3561212.3561222","For every physical interaction with our environment, we have some expectations concerning the resulting sound. As these expectations are quite rough, the auditory feedback can be modulated to convey additional information, without restricting the object’s original purpose. Such auditory augmentation is calm and unobtrusive as long it stays plausible with respect to the performed action. The plausibility range defines a hard limit for the information capacity of the auditory display. In order to maximize the information capacity of auditory augmentations, an estimate of the plausibility range of augmented auditory feedback is required. Here we present Schrödinger’s box, a mobile hardware- and software-platform that is designed for exploring the limits of plausibility of auditory feedback for unknown sounding objects. It renders augmented auditory feedback for its one and only affordance: striking it with a mallet. While hiding all electronics from the users, it meets the extreme requirements of latency that are necessary so that the original auditory feedback is effectively masked by the synthetic auditory feedback. With Schrödinger’s box, we now have a valuable research tool, not only for optimizing auditory augmentations, but also for investigating the plausibility of auditory feedback in general.","2022-10-10","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","15–23","","","","","","Schrödinger’s box","AM '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/XRRJUIH5/Weger et al. - 2022 - Schrödinger’s box an artifact to study the limits.pdf","","","auditory feedback; sonic interaction design; augmented reality; auditory augmentation; onset detection; plausibility; sounding object","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4QCDQCCX","conferencePaper","2019","Tsaknaki, Vasiliki; Elblaus, Ludvig","A Wearable Nebula Material Investigations of Implicit Interaction","Proceedings of the Thirteenth International Conference on Tangible, Embedded, and Embodied Interaction","978-1-4503-6196-5","","10.1145/3294109.3295623","https://dl.acm.org/doi/10.1145/3294109.3295623","In this paper, we present the Nebula, a garment that translates intentional gestures and implicit interaction into sound.Nebula is a studded cloak made from a heavy fabric that envelopes the wearer with many pendulous folds. We describe the design process, and specifically highlight three material investigations that show particularly important material connections that were fundamental to the experience of the garment: How the draping and construction of the garment allowed for implicit interaction, how the studs were used both as a computational sensing material and a strong visual component, and how the sound design exploited tangible material qualities in the garment. Finally, we discuss how such material investigations in general can be put to use. Both as a way to produce evocative connections in the materials available in design work, but also as a way to extract legible design intentions for other designers and researchers.","2019-03-17","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","625–633","","","","","","","TEI '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/A8UPMV8S/Tsaknaki and Elblaus - 2019 - A Wearable Nebula Material Investigations of Impli.pdf","","","sound and music computing; wearable technology; crafting; implicit interaction; materials","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TIINTFNS","conferencePaper","2021","Despond, Anne; Reeves, Nicolas; Cusson, Vincent","Atmosphéries and the poetics of the in situ: the role and impact of sensors in data-to-sound transposition installations","Proceedings of the 16th International Audio Mostly Conference","978-1-4503-8569-5","","10.1145/3478384.3478422","https://dl.acm.org/doi/10.1145/3478384.3478422","Atmosphéries is a research-creation program which produced a series of art installations originally called “Cloud Harps”, that has known multiple transformations since their first instantiation in 1997. They are made of sculptural wooden “buffets” integrating different sensors that probe the sky and the surroundings to collect various atmospheric data. An internal process tuned by a human composer then maps these data to different audio parameters from a custom-made synthesis environment to create continuous sound and musical sequences. The present paper addresses conceptual and technological considerations about real-time sensing of clouds in a sonic-artistic installation context. Particular attention is given to the limitations and artefacts produced by such specialized sensors, and how those can be embraced to remain consistent with the spatio-temporal aspects of in situ works.","2021-10-15","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","48–55","","","","","","Atmosphéries and the poetics of the in situ","AM '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/QDQLYAGE/Despond et al. - 2021 - Atmosphéries and the poetics of the in situ the r.pdf","","","sound art; cloud sensing; environmental monitoring; musical expression interfaces; real-time systems; science art; sound installation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XSSJV3X9","conferencePaper","2018","Cibrian, Franceli L.; Mercado, Jose; Escobedo, Lizbeth; Tentori, Monica","A Step towards Identifying the Sound Preferences of Children with Autism","Proceedings of the 12th EAI International Conference on Pervasive Computing Technologies for Healthcare","978-1-4503-6450-8","","10.1145/3240925.3240958","https://dl.acm.org/doi/10.1145/3240925.3240958","Music-based therapies are increasingly being used to support children with autism with promising clinical results. However, we have a little understanding of which are the most appropriate sounds to be used in music-based interventions for children with autism. In this paper, we describe a pilot study to understand the attention and emotions of children with autism when listening to different sounds. We measured participants' attention using a wearable brain-computer headband, and a psychologist scored their emotions through direct observation. We compared 15 sounds including three natural sounds, and a melody and a single note being played with cello, clarinet, and piano, in a low and high pitch. Our results show children with autism stay more focused when listening to a melody being played with a cello in a low pitch. Additionally, they were more distracted and felt more negative emotions when listening to natural sounds. We close discussing how our results could be a first step towards a potential understanding of what are the proper selection of sounds that could inform the design of pervasive healthcare applications for this population.","2018-05-21","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","158–167","","","","","","","PervasiveHealth '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/XXUJ2BXN/Cibrian et al. - 2018 - A Step towards Identifying the Sound Preferences o.pdf","","","brain-computer interfaces; autism; music-therapy; Sounds feedback","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KYVRYHQX","conferencePaper","2020","Gupfinger, Reinhard; Kaltenbrunner, Martin","Animal-Centred Sonic Interaction Design: Musical Instruments and Interfaces for Grey Parrots","Proceedings of the Sixth International Conference on Animal-Computer Interaction","978-1-4503-7693-8","","10.1145/3371049.3371062","https://dl.acm.org/doi/10.1145/3371049.3371062","This paper describes our research and the methodology used to design musical instruments and interfaces aimed at providing auditory enrichment for grey parrots living in captivity. Based on the cognitive, physiological, and acoustic abilities of grey parrots, and their intrinsic interest in acoustic and physical interactions, we have developed and tested various interactive instrument prototypes from an animal-centered design perspective. In a previous study, we analyzed the physical and musical skills of a group of grey parrots, and here we present our design results for auditory enrichment in the context of Animal Computer Interaction (ACI) and artistic research. Our investigation should lead to a better understanding of how grey parrots interact with technological mediators, respond to sound devices, and create ""parrot music,"" with potential benefits for their wellbeing while living in captivity.","2020-01-10","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","1–11","","","","","","Animal-Centred Sonic Interaction Design","ACI '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/I58EMAC7/Gupfinger and Kaltenbrunner - 2020 - Animal-Centred Sonic Interaction Design Musical I.pdf","","","Sonic interaction design; animal-centered design; animal-computer interaction; auditory enrichment; grey parrots; interactive musical interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"87VXK6WU","conferencePaper","2022","Gilling, Joseph","Haunted by the Glitch: Technological Malfunction - Critiquing the Media of Innovation","10th International Conference on Digital and Interactive Arts","978-1-4503-8420-9","","10.1145/3483529.3483667","https://dl.acm.org/doi/10.1145/3483529.3483667","The glitch: an unexpected moment in a system resulting in the appearance of a malfunction. Commonly found within software, video games, digital images, film, and audio. In a world of digital perfectionism and sleek sonic design, why do artists now seek to re-associate the sounds of computational error into contemporary music? The “aesthetics of failure”, and subsequent glitch music sub-genre, emerged in the 1990s, deliberately using and exploiting technological systems as sound materials for composition. Whilst exploring the technological innovations and artistic movements compounding the rise of this new musical phenomenon, we will also investigate the genres link to French philosopher Jacques Derrida's concept of hauntology. This will give us an insight into the human anxieties associated with a wavering faith in technological progress and modernity when it came to the turn of the millennium. Whilst the last century was spent perfecting the sound recording, the availability of the glitch has revolutionised modern music. We now seem to be entering a “post-digital” era, concerned primarily with the rapidly changing relationships we have with technologies, as well as our increasing dependence on them. The internet has provided the ultimate archival space to reminisce and re-contextualise all sound materials, even those once seen as defective.","2022-02-20","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","1–7","","","","","","Haunted by the Glitch","ARTECH 2021","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/BIDMFB67/Gilling - 2022 - Haunted by the Glitch Technological Malfunction -.pdf","","","cancellation of the future; digitalisation; Hauntology; post-digital; retro-fetishisation; technological failure","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WYIUJAY3","conferencePaper","2020","Hebling, Eduardo D.; Partesotti, Elena; Santana, Charles P.; Figueiredo, André; Dezotti, Cássio G.; Botechia, Tales; da Silva, César A. Pereira; da Silva, Micael A.; Rossetti, Danilo; de Oliveira, Victor A. W.; Cielavin, Sandra; Moroni, Artemis S.; Manzolli, Jônatas","MovieScape: Audiovisual Landscapes for Silent Movie: Enactive Experience in a Multimodal Installation","Proceedings of the 9th International Conference on Digital and Interactive Arts","978-1-4503-7250-3","","10.1145/3359852.3359883","https://dl.acm.org/doi/10.1145/3359852.3359883","The multimodal installation MovieScape articulates sound, image, and movement with the poetics of silent film. It is an immersive mixed-reality environment in which the boundaries between the real and the virtual are imprecise and ambiguous. In MovieScape, audiovisuals generated in real time induce the emergence of recurring motor patterns performed by the visitor. An audiovisual-scape is selected with an imaginary steering wheel that is activated when the visitor closes both fists. After this initial gesture, a sphere covered with images of the silent movie is modified with rotations and displacements that alter the sequence of scenes, music track and the sound landscape. The interaction between perception, action and movement articulates with the concept of Embodied Cognition, bringing about an enactive experience in which the participant immerses in the possibilities of the silent movie. The basic setup of the installation is presented, as well as technical aspects concerning the network communication, motion capture, sonification and visualization. A participative methodology, which is (itself) a factor of knowledge construction and artistic expression, supports the creative process of MovieScape.","2020-02-13","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","1–7","","","","","","MovieScape","ARTECH 2019","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/4GPAIRB3/Hebling et al. - 2020 - MovieScape Audiovisual Landscapes for Silent Movi.pdf","","","Enactive Experience; Interactive Systems for Artistic Applications; Multimodal Installation; Silent Movies","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R85U79EU","conferencePaper","2009","Lombardo, Vincenzo; Valle, Andrea; Nunnari, Fabrizio","Tabula ex-cambio","Proceedings of the 17th ACM international conference on Multimedia","978-1-60558-608-3","","10.1145/1631272.1631511","https://dl.acm.org/doi/10.1145/1631272.1631511","Tabula-ex-cambio is an interactive installation that delivers visual and audio content. It works as a sort of perpetual billiard game, powered by the data originating from the trend of a stock exchange index. The visual content is a real-time animated 3D computer graphics, that represents a billiard game where each ball is associated with a stock in the index. The audio content is an electroacoustic music composition featuring as many layers of sounds as stocks in the index. All the balls in the billiard are musical sources that play an iterated sequence of sounds while moving on the billiard table; each sequence is altered in frequency by the trend of the related company index. The final delivered visual and audio contents depend on the interaction with the user, who can select a view/listening point on the billiard game. The physical installation consists of a deformed billiard structure connected to a stele through cables; the stele is a vertical display where the billiard game takes place. The visual display relies on the graphic engine Ogre, while the aural display is implemented in SuperCollider. The installation was exposed in Shanghai, Beijing, Birmingham, and Terni (Italy).","2009-10-19","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","1053–1062","","","","","","","MM '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/6UXVXZZH/Lombardo et al. - 2009 - Tabula ex-cambio.pdf","","","art installation; auralization; real-time computer graphics; stock index presentation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8MINHB4F","conferencePaper","2009","Sungjae, Hwang; Lee, Kibeom; Park, Daham; Yeo, Woon Seung","The Biolin: a current-based musical interface","Proceedings of the International Conference on Advances in Computer Entertainment Technology","978-1-60558-864-3","","10.1145/1690388.1690483","https://dl.acm.org/doi/10.1145/1690388.1690483","In this paper, we describe a media artwork that features the Biolin, a musical device that produces different sounds depending on the target object that it is being played on. Shaped like an ordinary violin bow, the Biolin analyzes the target using a weak electric current to produce a timbre that matches the target. The user can then perform by ""playing"" the target object using the Biolin. The Biolin was designed by modifying a violin bow in order to make it conductive and connected to a computer for the transmission of data, resulting in visual and auditory output. We showcased Biolin in front of a small audience, in which the users showed positive reactions to the new approach of making sounds from everyday objects and people around us.","2009-10-29","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","427–428","","","","","","The Biolin","ACE '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/QU8VC7P8/Sungjae et al. - 2009 - The Biolin a current-based musical interface.pdf","","","musical interface; interface design; media art","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FTU8S6X4","conferencePaper","2021","Robinson, Frederic Anthony; Velonaki, Mari; Bown, Oliver","Smooth Operator: Tuning Robot Perception Through Artificial Movement Sound","Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction","978-1-4503-8289-2","","10.1145/3434073.3444658","https://dl.acm.org/doi/10.1145/3434073.3444658","Can we influence how a robot is perceived by designing the sound of its movement? Drawing from practices in film sound, we overlaid a video depicting a robot's movement routine with three types of artificial movement sound. In a between-subject study design, participants saw either one of the three designs or a quiet control condition and rated the robot's movement quality, safety, capability, and attractiveness. We found that, compared to our control, the sound designs both increased and decreased perceived movement quality. Coupling the same robotic movement with different sounds lead to the motions being rated as more or less precise, elegant, jerky, or uncontrolled, among others. We further found that the sound conditions decreased perceived safety, and did not affect perceived capability and attractiveness. More unrealistic sound conditions led to larger differences in ratings, while the subtle addition of harmonic material was not rated differently to the control condition in any of the measures. Based on these findings, we discuss the challenges and opportunities regarding the use of artificial movement sound as an implicit channel of communication that may eventually be able to selectively target specific characteristics, helping designers in creating more refined and nuanced human-robot interactions.","2021-03-08","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","53–62","","","","","","Smooth Operator","HRI '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/A9GDJDAA/Robinson et al. - 2021 - Smooth Operator Tuning Robot Perception Through A.pdf","","","human-robot interaction; movement sonification; sonic interaction design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EQ46F84T","conferencePaper","2022","Tomé-Marques, Horácio","While my brain sounds my guitar imagines: A real time audiovisual performing act by the brain and the guitar","10th International Conference on Digital and Interactive Arts","978-1-4503-8420-9","","10.1145/3483529.3483763","https://dl.acm.org/doi/10.1145/3483529.3483763","This project is a multimedia art real time performing act that uses brain signals and guitar signals, both in the form of digital abstractions (i.e., data) to generate and control sound and imagery. The brain signals use, and are transduced by, the brain-computer interface technology, the electroencephalography method, and the open sound control protocol (OSC); the guitar signals use, and are transduced by, sensors, transducers, and the musical instrument digital interface protocol (MIDI). The brain is used to generate, and control, exclusively, all the sound constituents and the sounding narrative events (music component); but the guitar is used to generate, and control, exclusively, all the imagery constituents and the imaging narrative (visual component). One of the purposes is to problematize the roles that specific entities are usually supposed—in terms of general public perception—to play in selfcorrelated specific contexts. E.g., when one attends a concert by a guitarist, one could expect and accept that the guitar will produce sound, and the performer, besides playing that specific instrument, will imagine some imagery inside his/her brain, even if in aformal abstractions, correlated to the sound processing act. As artist(s), we also are impelled to problematize the institutionalized regimes and schemes on and about what the concept correlation is in itself. Also, what is imagination—sonic and/or visual—, and who/what (e.g., biological mechanism or technological mechanism, biological device or technological device, biological or technological extension/ prosthesis) is able to imagine and to process imagination. Not least, how reality correlates with imagination, what precedes what, what is the cause and what is the effect, what affects what, is imagery present in sound, is imagination of sound other thing than imagination as we, consensually, conceive it. The outcome of this performing act is manifested as audio visual narrative(s), that articulates real time generated sounds and real time generated images mainly correlated by the conceptual problematization framework.","2022-02-20","2023-07-06 04:50:11","2023-07-21 07:53:00","2023-07-05","1–4","","","","","","While my brain sounds my guitar imagines","ARTECH 2021","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/E3IDYPUN/Tomé-Marques - 2022 - While my brain sounds my guitar imagines A real t.pdf","","","Brain; EEG; Brain Sonification; Correlation Problematization; Guitar Imagination; Imaging; Role Problematization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H6T7QAT8","conferencePaper","2012","Grimshaw, Mark; Garner, Tom","The use of sound to represent data and concepts as a means to engender creative thought: some thoughts on implementation and a research agenda","Proceedings of the 7th Audio Mostly Conference: A Conference on Interaction with Sound","978-1-4503-1569-2","","10.1145/2371456.2371458","https://dl.acm.org/doi/10.1145/2371456.2371458","This paper poses the question: How can sound be used to function analogously to the function of the images in a graph in order to create the conditions for creative thought and insight to occur and thus to facilitate the synthesis of new knowledge? It uses this to develop further questions and a research agenda. In particular, it is concerned with the use of sonification of the non-audio data and concepts represented in a diagram or graph and the techniques that might be used to foster a creative research environment using sound. The ultimate goal of the research agenda is to go beyond sonification and to use sound pro-actively in a Virtual Research Environment in order to create the conditions for creative thinking and insight to occur with the hope that this may then lead to the synthesis of new knowledge.","2012-09-26","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","9–15","","","","","","The use of sound to represent data and concepts as a means to engender creative thought","AM '12","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/SPKSN6MQ/Grimshaw and Garner - 2012 - The use of sound to represent data and concepts as.pdf","","","emotion; sound; cognition; creativity; knowledge synthesis; meaning; virtual research environment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PPPZDNQW","conferencePaper","2016","Camurri, Antonio; Volpe, Gualtiero; Piana, Stefano; Mancini, Maurizio; Niewiadomski, Radoslaw; Ferrari, Nicola; Canepa, Corrado","The Dancer in the Eye: Towards a Multi-Layered Computational Framework of Qualities in Movement","Proceedings of the 3rd International Symposium on Movement and Computing","978-1-4503-4307-7","","10.1145/2948910.2948927","https://dl.acm.org/doi/10.1145/2948910.2948927","This paper presents a conceptual framework for the analysis of expressive qualities of movement. Our perspective is to model an observer of a dance performance. The conceptual framework is made of four layers, ranging from the physical signals that sensors capture to the qualities that movement communicate (e.g., in terms of emotions). The framework aims to provide a conceptual background the development of computational systems can build upon, with a particular reference to systems analyzing a vocabulary of expressive movement qualities, and translating them to other sensory channels, such as the auditory modality. Such systems enable their users to ""listen to a choreography"" or to ""feel a ballet"", in a new kind of cross-modal mediated experience.","2016-07-05","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","1–7","","","","","","The Dancer in the Eye","MOCO '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/93WM28QE/Camurri et al. - 2016 - The Dancer in the Eye Towards a Multi-Layered Com.pdf","","","Interactive sonification; Automated analysis of movement qualities; Cross-modal and multimodal interactive systems; Dance performance; Expressive movement","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9V3TVBEV","conferencePaper","2021","Papageorgopoulou, Penny; Delinikolas, Dimitris; Arsenopoulou, Natalia; Katsarou, Louiza; Rizopoulos, Charalampos; Psaltis, Antonios; Theona, Iouliani; Drymonitis, Alexandros; Korosidis, Antonios; Charitos, Dimitrios","Embedding an interactive art installation into a building for enhancing citizen's awareness on urban environmental conditions","Media Architecture Biennale 20","978-1-4503-9048-4","","10.1145/3469410.3469426","https://dl.acm.org/doi/10.1145/3469410.3469426","The paper presents an artwork, embedded in a public-use building complex, comprising four interactive, sight specific installations. The artwork employs ubiquitous computing technologies and a variety of other components (projector, LED matrix displays, physical and custom-made objects) to sense, collect and translate urban, environmental data and human input into evolving multisensory representations, affording hybrid spatial experiences. The artwork aims at highlighting the impact of human activity upon the environment and the nonhuman entities that inhabit it, shifting the focus to a post-anthropocentric view of the world.","2021-10-22","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","160–169","","","","","","","MAB20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/23L7RDHC/Papageorgopoulou et al. - 2021 - Embedding an interactive art installation into a b.pdf","","","data sonification; virtual reality; citizen sensing; data visualization; environmental sensing; urban data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7WEIW7NN","conferencePaper","2013","Batty, Joshua; Horn, Kipps; Greuter, Stefan","Audiovisual granular synthesis: micro relationships between sound and image","Proceedings of The 9th Australasian Conference on Interactive Entertainment: Matters of Life and Death","978-1-4503-2254-6","","10.1145/2513002.2513568","https://dl.acm.org/doi/10.1145/2513002.2513568","The real-time audio-visual instrument presented in this paper enables a unified environment for composition and performance through audio-visual granular synthesis. The instrument empowers the user to reconstruct grains into new experiences and make possible real-time improvisation of the material at the granular level. The user can form audio-visual relationships through interacting with and observing perceptual similarities between sound and image at the micro and macro level. These relationships provide the user with a unified parameter for audio-visual manipulation. Throughout the paper I discuss and illustrate the design of the instrument, its performance potential and its innovative application in the field of interactive entertainment, particularly in the performance work of DJs and VJs.","2013-09-30","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","1–7","","","","","","Audiovisual granular synthesis","IE '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/ASP7ANWP/Batty et al. - 2013 - Audiovisual granular synthesis micro relationship.pdf","","","visual music; audio-visual relationships; granular synthesis; interactive entertainment; music visualization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"93YLGEP6","conferencePaper","2009","McGookin, David; Brewster, Stephen","Eyes-free overviews for mobile map applications","Proceedings of the 11th International Conference on Human-Computer Interaction with Mobile Devices and Services","978-1-60558-281-8","","10.1145/1613858.1613959","https://dl.acm.org/doi/10.1145/1613858.1613959","We outline two new auditory interaction techniques which build upon existing visual techniques to display off-screen points of interest (POI) in map based mobile computing applications. SonicPie uses a pie menu and compass metaphor, allowing a user to scroll around the environment, hearing off-screen POIs in a spatialised auditory environment. EdgeTouch integrates with the Wedge technique of Gustafson et al. [2], sonifying the POIs as the user comes into contact with them when moving his or her finger around a ""sonification border"".","2009-09-15","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","1–2","","","","","","","MobileHCI '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/4UIEWJP7/McGookin and Brewster - 2009 - Eyes-free overviews for mobile map applications.pdf","","","auditory display; digital maps; off-screen data presentation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7DZPXW5E","conferencePaper","2012","Neidhardt, Annika; Rüppel, Anna","Multiplayer audio-only game: Pong on a massive multichannel loudspeaker system","Proceedings of the 7th Audio Mostly Conference: A Conference on Interaction with Sound","978-1-4503-1569-2","","10.1145/2371456.2371477","https://dl.acm.org/doi/10.1145/2371456.2371477","Interactive auditory displays are an interesting possibility presenting information in an alternative way. There have been lots of interesting works using binaural techniques. The use of a loudspeaker system has the advantage that more people can listen to the same data simultaneously. One application, where this is very important, is the audio gaming domain, as multiplayer games are usually more exciting. Additionally, the use of a loudspeaker system allows different dimensions of the game design. The main challenge in developing an interactive auditory display for a loudspeaker system is the design of the data sonification and the interaction for data exploration. In this paper we present an example implementation of such an interactive auditory display. The famous game Pong has been implemented using an audio-only loudspeaker display instead of a graphical. The goal of this investigation is to gather more experience in the perception of spatial audio-only representation of information.","2012-09-26","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","130–134","","","","","","Multiplayer audio-only game","AM '12","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/4NTRNBQH/Neidhardt and Rüppel - 2012 - Multiplayer audio-only game Pong on a massive mul.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U78XUAGI","conferencePaper","2013","Twardon, Lukas; Koesling, Hendrik; Finke, Andrea; Ritter, Helge","Gaze-contingent audio-visual substitution for the blind and visually impaired","Proceedings of the 7th International Conference on Pervasive Computing Technologies for Healthcare","978-1-936968-80-0","","10.4108/icst.pervasivehealth.2013.252018","https://dl.acm.org/doi/10.4108/icst.pervasivehealth.2013.252018","A lot of effort has been made to advance technologies that improve blind and partially sighted people's spatial perception. One common approach is to enhance or substitute vision by audition. Most sensory substitution systems, however, have not attached any importance to eye movements. But eye movements play an essential role in mental imagery even in the absence of visual input. Therefore, we propose a system for gaze-contingent auditory substitution of spatial vision. It is intended to be a mobile helper in everyday life of the visually impaired. The prototype we have developed combines eyetracking with depth measuring and sonification techniques. We carried out both a proof-of-concept study in complete darkness and an exploratory EEG study. Our findings indicate that gaze-contingent sensory substitution permits depth perception and leads to intermodal (audio-visual) processing in untrained subjects. Hence, as a result of neuroplasticity, the blind and visually impaired might learn to perceive gaze-dependent sound visually.","2013-05-05","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","129–136","","","","","","","PervasiveHealth '13","","","","ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)","Brussels, BEL","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/V85VYQII/Twardon et al. - 2013 - Gaze-contingent audio-visual substitution for the .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FJUVBYH4","conferencePaper","2020","Verano Merino, Mauricio; van der Storm, Tijs","Block-based syntax from context-free grammars","Proceedings of the 13th ACM SIGPLAN International Conference on Software Language Engineering","978-1-4503-8176-5","","10.1145/3426425.3426948","https://dl.acm.org/doi/10.1145/3426425.3426948","Block-based programming systems employ a jigsaw metaphor to write programs. They are popular in the domain of programming education (e.g., Scratch), but also used as a programming interface for end-users in other disciplines, such as arts, robotics, and configuration management. In particular, block-based environments promise a convenient interface for Domain-Specific Languages (DSLs) for domain experts who might lack a traditional programming education. However, building a block-based environment for a DSL from scratch requires significant effort. This paper presents an approach to engineer block-based language interfaces by reusing existing language artifacts. We present Kogi, a tool for deriving block-based environments from context-free grammars. We identify and define the abstract structure for describing block-based environments. Kogi transforms a context-free grammar into this structure, which then generates a block-based environment based on Google Blockly. The approach is illustrated with four case studies, a DSL for state machines, Sonification Blocks (a DSL for sound synthesis), Pico (a simple programming language), and QL (a DSL for questionnaires). The results show that usable block-based environments can be derived from context-free grammars, and with an order of magnitude reduction in effort.","2020-11-15","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","283–295","","","","","","","SLE 2020","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/SN79PS3Y/Verano Merino and van der Storm - 2020 - Block-based syntax from context-free grammars.pdf","","","block-based environments; Blockly; DSLs; grammars; language workbenches; Rascal; syntax; visual languages","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"826M5KGB","conferencePaper","2017","Correia, Nuno N.; Tanaka, Atau","AVUI: Designing a Toolkit for Audiovisual Interfaces","Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","978-1-4503-4655-9","","10.1145/3025453.3026042","https://dl.acm.org/doi/10.1145/3025453.3026042","The combined use of sound and image has a rich history, from audiovisual artworks to research exploring the potential of data visualization and sonification. However, we lack standard tools or guidelines for audiovisual (AV) interaction design, particularly for live performance. We propose the AVUI (AudioVisual User Interface), where sound and image are used together in a cohesive way in the interface; and an enabling technology, the ofxAVUI toolkit. AVUI guidelines and ofxAVUI were developed in a three-stage process, together with AV producers: 1) participatory design activities; 2) prototype development; 3) encapsulation of prototype as a plug-in, evaluation, and roll out. Best practices identified include: reconfigurable interfaces and mappings; object-oriented packaging of AV and UI; diverse sound visualization; flexible media manipulation and management. The toolkit and a mobile app developed using it have been released as open-source. Guidelines and toolkit demonstrate the potential of AVUI and offer designers a convenient framework for AV interaction design.","2017-05-02","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","1093–1104","","","","","","AVUI","CHI '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/CLF7R85L/Correia and Tanaka - 2017 - AVUI Designing a Toolkit for Audiovisual Interfac.pdf","","","participatory design; prototyping; audiovisual; crossmodal interaction; hackathons; interaction design; interface builder; toolkit; user interface","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V4ZZU4NZ","conferencePaper","2016","Hermann, Thomas; Yang, Jiajun; Nagai, Yukie","EmoSonics: Interactive Sound Interfaces for the Externalization of Emotions","Proceedings of the Audio Mostly 2016","978-1-4503-4822-5","","10.1145/2986416.2986437","https://dl.acm.org/doi/10.1145/2986416.2986437","This paper presents a novel approach for using sound to externalize emotional states so that they become an object for communication and reflection both for the users themselves and for interaction with other users such as peers, parents or therapists. We present an abstract, vocal, and physiology-based sound synthesis model whose sound space each covers various emotional associations. The key idea in our approach is to use an evolutionary optimization approach to enable users to find emotional prototypes which are then in turn fed into a kernel-regression-based mapping to allow users to navigate the sound space via a low-dimensional interface, which can be controlled in a playful way via tablet interactions. The method is intended to be used for supporting people with autism spectrum disorder.","2016-10-04","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","61–68","","","","","","EmoSonics","AM '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/56GI6TTB/Hermann et al. - 2016 - EmoSonics Interactive Sound Interfaces for the Ex.pdf","","","Sound; Auditory Display; Autism Spectrum Disorder (ASD); Emotions","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RIUE2QJD","conferencePaper","2015","Caramiaux, Baptiste; Altavilla, Alessandro; Pobiner, Scott G.; Tanaka, Atau","Form Follows Sound: Designing Interactions from Sonic Memories","Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems","978-1-4503-3145-6","","10.1145/2702123.2702515","https://dl.acm.org/doi/10.1145/2702123.2702515","Sonic interaction is the continuous relationship between user actions and sound, mediated by some technology. Because interaction with sound may be task oriented or experience-based it is important to understand the nature of action-sound relationships in order to design rich sonic interactions. We propose a participatory approach to sonic interaction design that first considers the affordances of sounds in order to imagine embodied interaction, and based on this, generates interaction models for interaction designers wishing to work with sound. We describe a series of workshops, called Form Follows Sound, where participants ideate imagined sonic interactions, and then realize working interactive sound prototypes. We introduce the Sonic Incident technique, as a way to recall memorable sound experiences. We identified three interaction models for sonic interaction design: conducting; manipulating; substituting. These three interaction models offer interaction designers and developers a framework on which they can build richer sonic interactions.","2015-04-18","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","3943–3952","","","","","","Form Follows Sound","CHI '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/CZCRLN5W/Caramiaux et al. - 2015 - Form Follows Sound Designing Interactions from So.pdf","","","sonic interaction design; sound; gesture; interaction design; methodology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KG53YUZ2","conferencePaper","2009","Hansen, Anne-Marie Skriver; Overholt, Dan; Burleson, Winslow; Jensen, Camilla Nørgaard","Pendaphonics: a tangible pendulum-based sonic interaction experience","Proceedings of the 3rd International Conference on Tangible and Embedded Interaction","978-1-60558-493-5","","10.1145/1517664.1517701","https://dl.acm.org/doi/10.1145/1517664.1517701","Pendaphonics is a tangible physical-digital-sonic environment and interactive system that engages users in individual, collaborative, group, and distributed interactive experiences. The development of this system, as an element of urban revitalization and as a transdisciplinary research endeavor, presents strategies for the design and evaluation of low-cost, flexible, and distributed tangible interaction architectures for public engagement, expression, and performance. Pendaphonics is installed in a public media arts space, where over 200 people experienced it during the environment's opening event. Internationally, interaction laboratories at five research universities are advancing explorations of Pendaphonics. This paper presents the development process and findings from observation and evaluation of people using Pendaphonics; diverse social interaction patterns among performers and the general public are discussed. In particular, we identify the repeated and sustained invitation to interact -- created by the cyclic motion of a pendulum's simple harmonic oscillation -- as a new tangible interaction modality for human computer interaction, in 3D physical-digital-sonic environments. An investigation of this and related elements of Pendaphonics' large-scale tangible interaction scenarios are articulated, along with descriptions of the system's broad potential as a compositional and choreographic tool; an educational exhibit and classroom manipulative; and as an interface that facilitates playful interaction, exploration, discovery and creativity.","2009-02-16","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","153–160","","","","","","Pendaphonics","TEI '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/3RZUW2GG/Hansen et al. - 2009 - Pendaphonics a tangible pendulum-based sonic inte.pdf","","","interaction design; sound and music experience; tangible user interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6R8KCNKH","conferencePaper","2017","Ferguson, Sam; Rowe, Anthony; Bown, Oliver; Birtles, Liam; Bennewith, Chris","Networked Pixels: Strategies for Building Visual and Auditory Images with Distributed Independent Devices","Proceedings of the 2017 ACM SIGCHI Conference on Creativity and Cognition","978-1-4503-4403-6","","10.1145/3059454.3059480","https://dl.acm.org/doi/10.1145/3059454.3059480","This paper describes the development of the hardware and software for Bloom, a light installation installed at Kew Gardens, London in December of 2016. The system is made up of a set of nearly 1000 distributed pixel devices each with LEDs, GPS sensor, and sound hardware, networked together with WiFi to form a display system. Media design for this system required consideration of the distributed nature of the devices. We outline the software and hardware designed for this system, and describe two approaches to the software and media design, one whereby we employ the distributed devices themselves for computation purposes (the approach we ultimately selected), and another whereby the devices are controlled from a central server that is performing most of the computation necessary. We then review these approaches and outline possibilities for future research.","2017-06-22","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","299–308","","","","","","Networked Pixels","C&amp;C '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/UXITQ7BF/Ferguson et al. - 2017 - Networked Pixels Strategies for Building Visual a.pdf","","","audio; display system; distributed; internet of things; pixel","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UXZ7EDGG","conferencePaper","2009","Hansen, Anne-Marie Skriver; Overholt, Dan; Burleson, Winslow; Jensen, Camilla Nørgaard; Lahey, Byron; Muldner, Kasia","Pendaphonics: an engaging tangible pendulum-based sonic interaction experience","Proceedings of the 8th International Conference on Interaction Design and Children","978-1-60558-395-2","","10.1145/1551788.1551859","https://dl.acm.org/doi/10.1145/1551788.1551859","Pendaphonics is a tangible physical-digital-sonic environment and interactive system that motivates children and adults to be physically active and explorative. The development of this system presents a strategy for the design and evaluation of a low-cost, flexible, large scale tangible system that is engaging for children and adults alike. Pendaphonics has been installed in a public new media arts space, where over 200 people have interacted with it. Here, we describe Pendaphonics tangible interaction scenarios, including the broad potential of this system as a compositional and choreographic tool, an educational exhibit, and as an interface that facilitates playful interaction, exploration, discovery and creativity.","2009-06-03","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","286–288","","","","","","Pendaphonics","IDC '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/AW5QUMAX/Hansen et al. - 2009 - Pendaphonics an engaging tangible pendulum-based .pdf","","","physical activity; interaction design; embodied experience; play","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FC6MQK3Y","conferencePaper","2010","Liljedahl, Mats; Fagerlönn, Johan","Methods for sound design: a review and implications for research and practice","Proceedings of the 5th Audio Mostly Conference: A Conference on Interaction with Sound","978-1-4503-0046-9","","10.1145/1859799.1859801","https://dl.acm.org/doi/10.1145/1859799.1859801","Sound design can be described as an inherently complex task, demanding the designer to understand, master and balance technology, human perception, aesthetics and semiotics. Given this complexity, there are surprisingly few tools available that meet the needs of the general designer or developer incorporating sound as design material. To attend to this situation, two software systems are being developed. The purpose with these is to inform and support general design projects where sound is one part. The first system is intended to inform early stages of sound design projects. The second system is intended to simulate the sounding dimension of physical environments. Together these tools can be used to support designers and developers when searching for, testing and evaluating sounds suitable for interfaces, products and environments. To further complement these systems, a number of methods and guidelines are being developed in tandem. Tests to verify the systems have been conducted with very promising results.","2010-09-15","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","1–8","","","","","","Methods for sound design","AM '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/J5K42L6C/Liljedahl and Fagerlönn - 2010 - Methods for sound design a review and implication.pdf","","","methods; sound design; guidelines; simulation; tool; user involvement","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"88QSJYML","conferencePaper","2012","Drossos, Konstantinos; Floros, Andreas; Kanellopoulos, Nikolaos-Grigorios","Affective acoustic ecology: towards emotionally enhanced sound events","Proceedings of the 7th Audio Mostly Conference: A Conference on Interaction with Sound","978-1-4503-1569-2","","10.1145/2371456.2371474","https://dl.acm.org/doi/10.1145/2371456.2371474","Sound events can carry multiple information, related to the sound source and to ambient environment. However, it is well-known that sound evokes emotions, a fact that is verified by works in the disciplines of Music Emotion Recognition and Music Information Retrieval that focused on the impact of music to emotions. In this work we introduce the concept of affective acoustic ecology that extends the above relation to the general concept of sound events. Towards this aim, we define sound event as a novel audio structure with multiple components. We further investigate the application of existing emotion models employed for music affective analysis to sonic, non-musical, content. The obtained results indicate that although such application is feasible, no significant trends and classification outcomes are observed that would allow the definition of an analytic relation between the technical characteristics of a sound event waveform and raised emotions.","2012-09-26","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","109–116","","","","","","Affective acoustic ecology","AM '12","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/UGTSMX5G/Drossos et al. - 2012 - Affective acoustic ecology towards emotionally en.pdf","","","emotion recognition; affective acoustic ecology; sound events","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZG7X4V7T","conferencePaper","2015","Morreale, Fabio; De Angeli, Antonella","Evaluating Visitor Experiences with Interactive Art","Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter","978-1-4503-3684-0","","10.1145/2808435.2808440","https://dl.acm.org/doi/10.1145/2808435.2808440","The Music Room is an interactive installation that allows visitor to compose classical music by moving throughout a space. The distance between them and their average speed maps the emotionality of music: in particular, distance influences the pleasantness of the music, while speed influences its intensity. This paper focuses on the evaluation of visitors' experience with The Music Room by examining log-data, video footages, interviews, and questionnaires, as collected in two public exhibitions of the installation. We examined this data to the identify the factors that fostered the engagement and to understand how players appropriated the original design idea. Reconsidering our design assumptions against behavioural data, we noticed a number of unexpected behaviours, which induced us to make some considerations on design and evaluation of interactive art.","2015-09-28","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","50–57","","","","","","","CHItaly 2015","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/SARWSFC6/Morreale and De Angeli - 2015 - Evaluating Visitor Experiences with Interactive Ar.pdf","","","audience engagement; Evaluation methods; interactive installations","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4DT54VK3","conferencePaper","2013","Salmon, Richard; Paine, Garth","Embodiment: auditory visual enhancement of interactive environments.","Proceedings of the 7th International Conference on Tangible, Embedded and Embodied Interaction","978-1-4503-1898-3","","10.1145/2460625.2460645","https://dl.acm.org/doi/10.1145/2460625.2460645","This paper reflects upon two experimental projects, implemented as auditory visual (AV) augmentation of the Articulated Head (AH) [9] which is an interactive robotic installation exhibited in the Power House Museum (PHM) [25], Ultimo, Sydney, Australia. Research participant Video Cued Recall (VCR) interviews and subsequent Interpretative Phenomenological Analysis (IPA) [28] indicate that the experimental projects did have some impact upon audience engagement. We discuss mediating considerations and constraints, which explicate confounding and compromising aspects of the experimental designs and their presentation to the interacting audience. Within the context of embodiment, the critical importance that dimensional layout and display have upon the effectiveness of audio visual aids and the strength of spatio-temporal contextualizing cues in relatively unconstrained interactive public exhibition spaces is considered. Conclusions contribute a refined experimental project design, aimed at expediting more encouraging participant reportage of the enhancement of engagement in Human Computer Interaction (HCI) with this, and similar types of interactive installation exhibits.","2013-02-10","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","129–136","","","","","","Embodiment","TEI '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/IW9HH7DC/Salmon and Paine - 2013 - Embodiment auditory visual enhancement of interac.pdf","","","articulated head (AH); embodied conversational agent (ECA); interpretative phenomenological analysis (IPA); video cued recall (VCR)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JU3DYIHY","conferencePaper","2017","Tsiros, Augoustinos; Leplâtre, Grégory","Utilising natural cross-modal mappings for visual control of feature-based sound synthesis","Proceedings of the 19th ACM International Conference on Multimodal Interaction","978-1-4503-5543-8","","10.1145/3136755.3136798","https://dl.acm.org/doi/10.1145/3136755.3136798","This paper presents the results of an investigation into audio-visual (AV) correspondences conducted as part of the development of Morpheme, a painting interface to control a corpus-based concatenative sound synthesis algorithm. Previous research has identified strong AV correspondences between dimensions such as pitch and vertical position or loudness and size. However, these correspondences are usually established empirically by only varying a single audio or visual parameter. Although it is recognised that the perception of AV correspondences is affected by the interaction between the parameters of auditory or visual stimuli when these are complex multidimensional objects, there has been little research into perceived AV correspondences when complex dynamic sounds are involved. We conducted an experiment in which two AV mapping strategies and three audio corpora were empirically evaluated. 110 participants were asked to rate the perceived similarity of six AV associations. The results confirmed that size/loudness, vertical position/pitch, colour brightness/spectral brightness are strongly associated. A weaker but significant association was found between texture granularity and sound dissonance, as well as colour complexity and sound dissonance. Harmonicity was found to have a moderating effect on the perceived strengths of these associations: the higher the harmonicity of the sounds, the stronger the perceived AV associations.","2017-11-03","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","128–136","","","","","","","ICMI '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/Q5KVG4XC/Tsiros and Leplâtre - 2017 - Utilising natural cross-modal mappings for visual .pdf","","","Concatenative Synthesis; Cross-domain mappings; Crossmodal Correspondence; Retrieval by sketch","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JEKT6G7X","conferencePaper","2022","Larrieux, Eric; Speziali, Stella","Augmented Objects as Portals into Virtual Worlds:Using Audio to Create Immersive Experiences in Extended Realities","Proceedings of the 17th International Audio Mostly Conference","978-1-4503-9701-8","","10.1145/3561212.3561243","https://dl.acm.org/doi/10.1145/3561212.3561243","Building on a long history of the integration of new technology into the arts, in this research we set out to develop and evaluate new methods for creating immersive, audio-centric artworks in a Mixed Reality (MR) context. We employ multiple levels of spatial audio, both within the room as well as the umbrellas that participants navigate the space with, in order to provide a 6 degree of freedom immersive experience. Furthermore, we use dynamic projection mapping, again both throughout the global environment, and on the umbrellas, to create immersive experiences. This allows the umbrellas to function as so-called Augmented Objects that facilitate the composition and navigation of Responsive Sonic Environments, thus encouraging participants to examine their own perceptions of reality while incorporating them into the larger artistic installation. Additionally, we explore the creation of Spatial Metacompositions, navigable environments that function as performance spaces for the participants who interact with them. Finally, we present the technical foundations necessary to create such systems and discuss techniques to effectively employ them in one’s artistic practice.","2022-10-10","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","44–51","","","","","","Augmented Objects as Portals into Virtual Worlds","AM '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/Q2STGNGA/Larrieux and Speziali - 2022 - Augmented Objects as Portals into Virtual WorldsU.pdf","","","spatial audio; mixed reality; ambisonics; Augmented objects; digital augmentation; embedded systems.; extended reality; immersion; interactive sonic art; projection mapping; responsive sonic environments; spatial augmented reality; spatial metacompositions","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GFGCD8DZ","conferencePaper","2017","Kuchera-Morin, JoAnn; Putnam, Lance; Peliti, Luca; Adderton, Dennis; Cabrera, Andres; Kim, Kon Hyong; Rincon, Gustavo; Tilbian, Joseph; Wolfe, Hannah; Wood, Tim; Youn, Keehong","PROBABLY/POSSIBLY? An Immersive Interactive Visual/Sonic Quantum Composition and Synthesizer","Proceedings of the 25th ACM international conference on Multimedia","978-1-4503-4906-2","","10.1145/3123266.3129324","https://dl.acm.org/doi/10.1145/3123266.3129324","This research project is based on 32 years of Kuchera-Morin's research and practice in spatio-temporal music composition and media arts. The project is an immersive interactive visual, sonic computational instrument presented as an installation, which includes the development of an open-source computational language, and Kuchera-Morin's immersive interactive visual/sonic composition PROBABLY/POSSIBLY? Using the mathematics of quantum mechanics, the immersive instrument and computational language facilitates the creation of new, unique visual/sonic art forms. This project allows the artist to drive scientific and technological research for creative expression. This same technology is giving physicists insight into higher dimensional representation. The immersive visual/sonic instrument and language is based on the time-dependent Schrödinger equation splitting a hydrogen-like atom's electron in superposition in various orbitals. The immersive media composition, PROBABLY/POSSIBLY? can be interactively performed using our multimodal computational platform and open source language. The instrument/installation can also be used to compose and perform a number of art works based on the time-dependent Schrödinger equation","2017-10-19","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","559–561","","","","","","PROBABLY/POSSIBLY?","MM '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/IIVGDSRE/Kuchera-Morin et al. - 2017 - PROBABLYPOSSIBLY An Immersive Interactive Visual.pdf","","","immersive multi-modal installation; immersive multi-modal multimedia system installation; interactive immersive multimedia art work","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BEPQCN59","conferencePaper","2020","Fraietta, Angelo","Transient Relics: Temporal Tangents to an Ancient Virtual Pilgrimage","Proceedings of the Fourteenth International Conference on Tangible, Embedded, and Embodied Interaction","978-1-4503-6107-1","","10.1145/3374920.3374923","https://dl.acm.org/doi/10.1145/3374920.3374923","This paper examines creating a temporary virtual relic through the creation of an interactive soundscape in the context of a religious pilgrimage known as theStations of The Cross. The paper examines the history of the rite and its transformation from a physical pilgrimage to a virtual one. It examines the phenomenon of iconic relics, which in some case have a reckoned value equivalent to that of the physical objects they represent. Also, it examines both the conceptual and legal implications of embodying sound into tangible objects, resulting in their treatment as protected relics. Finally, it describes the creation of an artwork, whereby religious pilgrims manipulate interactive sonic balls that communicate with other networked sonic devices in an attempt to correlate metaphors of human behaviours---such as play, humiliation, and mobs---into a sonic relic of the historical narrative of Christ taunted by Roman soldiers.","2020-02-09","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","377–391","","","","","","Transient Relics","TEI '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/PCR4XJPL/Fraietta - 2020 - Transient Relics Temporal Tangents to an Ancient .pdf","","","diads; happybrackets; networks; nime; pilgrimage; relics; rituals; tangible sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7Z4ILGP8","conferencePaper","2022","Bernard, Corentin; Monnoyer, Jocelyn; Ystad, Sølvi; Wiertlewski, Michael","Eyes-Off Your Fingers: Gradual Surface Haptic Feedback Improves Eyes-Free Touchscreen Interaction","Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems","978-1-4503-9157-3","","10.1145/3491102.3501872","https://dl.acm.org/doi/10.1145/3491102.3501872","Moving a slider to set the music volume or control the air conditioning is a familiar task that requires little attention. However, adjusting a virtual slider on a featureless touchscreen is much more demanding and can be dangerous in situations such as driving. Here, we study how a gradual tactile feedback, provided by a haptic touchscreen, can replace visual cues. As users adjust a setting with their finger, they feel a continuously changing texture, which spatial frequency correlates to the value of the setting. We demonstrate that, after training with visual and auditory feedback, users are able to adjust a setting on a haptic touchscreen without looking at the screen, thereby reducing visual distraction. Every learning strategy yielded similar performance, suggesting an amodal integration. This study shows that surface haptics can provide intuitive and precise tuning possibilities for tangible interfaces on touchscreens.","2022-04-29","2023-07-06 04:50:11","2023-07-06 04:50:11","2023-07-05","1–10","","","","","","Eyes-Off Your Fingers","CHI '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/Q6ZYXYWM/Bernard et al. - 2022 - Eyes-Off Your Fingers Gradual Surface Haptic Feed.pdf","","","multimodality; Haptic feedback; learning; surface haptics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7IZ7M43C","conferencePaper","2022","Rueben, Matthew; Horrocks, Matthew R; Martinez, Jennifer Eleanor; Cormier, Michelle V; LaLone, Nicolas; Fraune, Marlena; Toups Dugas, Z","“I See You!”: A Design Framework for Interface Cues about Agent Visual Perception from a Thematic Analysis of Videogames","Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems","978-1-4503-9157-3","","10.1145/3491102.3517699","https://dl.acm.org/doi/10.1145/3491102.3517699","As artificial agents proliferate, there will be more and more situations in which they must communicate their capabilities to humans, including what they can “see.” Artificial agents have existed for decades in the form of computer-controlled agents in videogames. We analyze videogames in order to not only inspire the design of better agents, but to stop agent designers from replicating research that has already been theorized, designed, and tested in-depth. We present a qualitative thematic analysis of sight cues in videogames and develop a framework to support human-agent interaction design. The framework identifies the different locations and stimulus types – both visualizations and sonifications – available to designers and the types of information they can convey as sight cues. Insights from several other cue properties are also presented. We close with suggestions for implementing such cues with existing technologies to improve the safety, privacy, and efficiency of human-agent interactions.","2022-04-29","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1–22","","","","","","“I See You!”","CHI '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/YI4GCGD6/Rueben et al. - 2022 - “I See You!” A Design Framework for Interface Cue.pdf","","","human-robot interaction; qualitative methodologies; sight cues; thematic analysis; videogames","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"25H9PSAU","conferencePaper","2022","Brown, Courtney Douglass; Greenberg, Ira; Brimhall, Brent","Skin Hunger: A Telematic Installation","Proceedings of the 8th International Conference on Movement and Computing","978-1-4503-8716-3","","10.1145/3537972.3538010","https://dl.acm.org/doi/10.1145/3537972.3538010","Skin Hunger is a web-based interactive system for telematic installation and performance that plays on the zoom-style video-chat that has become ubiquitous during the COVID-19 pandemic. Participants in the telematic installation can reach across the webcam screens to virtually ‘touch’ one another. By touching or moving together, participants create virtual organisms and sounds that emerge and evolve from participant relation and interaction, making the intangible connection tangible and giving it life. Skin Hunger explores a new way of enabling embodied joint music making and movement over a distance.","2022-06-30","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1–3","","","","","","Skin Hunger","MOCO '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/7YTWG9JT/Brown et al. - 2022 - Skin Hunger A Telematic Installation.pdf","","","Embodiment; HCI; Interactive Dance; NIME; Telematic Art","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XQRTUYG3","conferencePaper","2017","Beau, Edouard","Las Barricadas Misteriosas","Proceedings of the 25th ACM international conference on Multimedia","978-1-4503-4906-2","","10.1145/3123266.3129330","https://dl.acm.org/doi/10.1145/3123266.3129330","Innovative personal research and artistic project about the remains of the Spanish civil war and its memory. Theorising the use of documentary photography regarding conflict memories in correlation with other media and new technologies, particularly the open source Pure Data system.","2017-10-19","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","577–579","","","","","","","MM '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/JUWD3EQQ/Beau - 2017 - Las Barricadas Misteriosas.pdf","","","Sound and music computing; Performing arts; Art and humanities; Cross computing tools and techniques; Fine arts; General and reference; Media arts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I66P6X7J","conferencePaper","2009","Kim, Si-Jung; Thangjitham, Jennifer; Winchester, Woodrow","Assessing the efficacy of a mixed-modal auditory display system for enhancing auditory sensation","Proceedings of the 47th Annual Southeast Regional Conference","978-1-60558-421-8","","10.1145/1566445.1566520","https://dl.acm.org/doi/10.1145/1566445.1566520","This paper presents the design and evaluation of a new type of mixed-modal auditory display for enhancing auditory sensation. The purpose of this study is to determine whether or not a light apparatus in which LED lights are installed in front of a speaker panel and alternating lights based on sound frequency and intensity can support user awareness of the auditory source. Five different auditory sources were used, and a user study with 20 participants revealed that all auditory sources were better recognized when listened to with the light apparatus. The music and the emergency alert auditory source were best represented by the light apparatus. The experiment showed synchronized visual and audio representation enhanced the user's auditory sensation. Findings suggest that the light apparatus could be useful when auditory signal displays are not universally applicable because people who suffer from hearing loss are unable to use them effectively. It is expected that the result of this study could contribute to the design of hearing aids, emergency alarm device/mechanisms, communication trust or other assistive technologies that seek to provide context to received data.","2009-03-19","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1–2","","","","","","","ACM-SE 47","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/8EJ8X4ZX/Kim et al. - 2009 - Assessing the efficacy of a mixed-modal auditory d.pdf","","","auditory display; hearing aids; mixed modal interface; speaker","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DS6GI3V3","conferencePaper","2019","Ahmetovic, Dragan; Bernareggi, Cristian; Guerreiro, João; Mascetti, Sergio; Capietto, Anna","AudioFunctions.web: Multimodal Exploration of Mathematical Function Graphs","Proceedings of the 16th International Web for All Conference","978-1-4503-6716-5","","10.1145/3315002.3317560","https://dl.acm.org/doi/10.1145/3315002.3317560","We present AudioFunctions.web, a web app that uses sonification, earcons and speech synthesis to enable blind people to explore mathematical function graphs. The system is designed for personalized access through different interfaces (touchscreen, keyboard, touchpad and mouse) on both mobile and traditional devices, in order to better adapt to different user abilities and preferences. It is also publicly available as a web service and can be directly accessed from the teaching material through a hypertext link. An experimental evaluation with 13 visually impaired participants highlights that, while the usability of all the presented interaction modalities is high, users with different abilities prefer different interfaces to interact with the system. It is also shown that users with higher level of mathematical education are capable of better adapting to interaction modalities considered more difficult by others.","2019-05-13","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1–10","","","","","","AudioFunctions.web","W4A '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/8UDARSRH/Ahmetovic et al. - 2019 - AudioFunctions.web Multimodal Exploration of Math.pdf","","","Mathematics; Function graphs; Visual Impairments and Blindness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8C2I7FER","conferencePaper","2015","Cuykendall, Shannon; Junokas, Michael; Amanzadeh, Mohammad; Tcheng, David Kim; Wang, Yawen; Schiphorst, Thecla; Garnett, Guy; Pasquier, Philippe","Hearing movement: how taiko can inform automatic recognition of expressive movement qualities","Proceedings of the 2nd International Workshop on Movement and Computing","978-1-4503-3457-0","","10.1145/2790994.2791004","https://dl.acm.org/doi/10.1145/2790994.2791004","We describe the first stages of exploratory research undertaken to analyze expressive movement qualities of taiko performance, a Japaense artistic practice that combines stylized movement with drumming technique. The eventual goals of this research are to answer 1) Can expressive visual qualities of taiko be heard in the sound and 2) Can expressive sonic qualities of taiko be seen in the movement? We achieved high accuracy across multiple machine-learning algorithms in recognizing key sonic and visual qualities of taiko performance. In contrast to many current methods of studying expressive qualities of movement, we inform our data collection process and annotations with taiko technique. We seek to understand how the fundamentals of taiko create expression. More broadly, we suggest that codified artistic practices, like taiko, can inform automatic recognition and generation of expressive movement qualities that have been challenging to reliably classify, parse, and detect. In future work we propose ways to generalize expressive features of taiko so they can be recognized in other movement contexts.","2015-08-14","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","140–147","","","","","","Hearing movement","MOCO '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/HNW7Y62R/Cuykendall et al. - 2015 - Hearing movement how taiko can inform automatic r.pdf","","","machine learning; expressive movement; movement classification; musical gesture; sound-producing gesture; taiko","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AKEBU8N9","conferencePaper","2017","Schoemann, Sarah; Nitsche, Michael","Needle as Input: Exploring Practice and Materiality When Crafting Becomes Computing","Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction","978-1-4503-4676-4","","10.1145/3024969.3024999","https://dl.acm.org/doi/10.1145/3024969.3024999","A growing body of research combines craft and interaction design. The ""Stitch Sampler"" project, a sew-able musical instrument and craft platform, stands in this context. In particular, the project serves to underline the importance of two conceptual themes that have emerged in HCI over the last decade, specifically the ""material turn"" of research on computing and the ""practice"" or ""action-centric"" turn in HCI. We present that our prototype and its evolution process as an example of a third trend in HCI research that has developed closely along-side these shifts, with relation to research specifically on craft practice. We discuss the Stitch Sampler and related work that couple electronics and smart materials with craft practices. In that way the act of crafting has in some cases become a form of computation.","2017-03-20","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","299–308","","","","","","Needle as Input","TEI '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/6L9PV7D7/Schoemann and Nitsche - 2017 - Needle as Input Exploring Practice and Materialit.pdf","","","design; craft; materiality; practice; tangible computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3RDH75JS","conferencePaper","2007","Jacquemin, Christian; Ajaj, Rami; Cahen, Roland; Olivier, Yoan; Schwarz, Diemo","Plumage: design d'une interface 3D pour le parcours d'échantillons sonores granularisés","Proceedings of the 19th Conference on l'Interaction Homme-Machine","978-1-59593-791-9","","10.1145/1541436.1541450","https://dl.acm.org/doi/10.1145/1541436.1541450","Plumage is an interface for interactive 3D audio/graphic scene browsing and design. The interface relies on the notion of tape heads in a sonic and graphic 3D space made of feathers associated with sound micro-samples. The spatial layout of the feathers is defined by sound parameters of the associated samples. The musical play is the outcome of a continuous and interactive navigation in the sound space controlled by direct manipulation of tape head trajectories. This work is based on the design of interactions. A simple and efficient audio graphic navigation is designed through the combination and synchronization of elementer sound object triggerings.","2007-11-12","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","71–74","","","","","","Plumage","IHM '07","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/X6KPN3TS/Jacquemin et al. - 2007 - Plumage design d'une interface 3D pour le parcour.pdf","","","multimodality; audio-visual composition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5KGD2N9T","conferencePaper","2019","Fdili Alaoui, Sarah","Making an Interactive Dance Piece: Tensions in Integrating Technology in Art","Proceedings of the 2019 on Designing Interactive Systems Conference","978-1-4503-5850-7","","10.1145/3322276.3322289","https://dl.acm.org/doi/10.1145/3322276.3322289","I describe the research and creation journey of a choreographic dance piece called SKIN that I made with another choreographer, 3 dancers, 1 musician and 1 developer. The performance integrates interactive technologies mapping inner movement to sound and video on stage. We followed a research though practice method that includes iterative cycles of choreographic practice and interaction design. This generated a set of research questions that I address through experience explicitation interviews of both audience and creative team members. The interviews allow me to investigate the lived experience of making and attending the performance and the emergent relationships between dance, media and interaction as well as the tensions and negotiations that emerged from integrating technology in art. I discuss my approach as anti-solutionist and argue for more openness in HCI to allow artists to contribute to knowledge by embracing the messiness of their practice.","2019-06-18","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1195–1208","","","","","","Making an Interactive Dance Piece","DIS '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/4B3TB9R2/Fdili Alaoui - 2019 - Making an Interactive Dance Piece Tensions in Int.pdf","","","dance; embodied interaction; anti-solutionism; research through practice","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VISA9VKE","conferencePaper","2020","Seiça, Mariana; Roque, Licínio; Martins, Pedro; Cardoso, F. Amílcar","Contrasts and similarities between two audio research communities in evaluating auditory artefacts","Proceedings of the 15th International Audio Mostly Conference","978-1-4503-7563-4","","10.1145/3411109.3411146","https://dl.acm.org/doi/10.1145/3411109.3411146","The design of auditory artefacts has been establishing its practice as a scientific area for more than 20 years, with a crucial element in this process being how to properly evaluate acoustic outputs. In this paper, we sought to map the evaluation methods applied in a general search inside two main audio-focused conferences: Audio Mostly and the International Conference on Auditory Display (ICAD). Revisiting last year's editions, as well as a keyword-based search in the last ten years, we attempted to gather and classify each evaluation method according to the level of user involvement, their role, and the authors intentions in using each method. We propose an initial mapping for this gathering, in a framework of evaluation approaches which can reinforce and expand current practices in the creation of auditory artefacts.","2020-09-16","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","183–190","","","","","","","AM '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/LEHB8ZN4/Seiça et al. - 2020 - Contrasts and similarities between two audio resea.pdf","","","auditory display; evaluation method; interacting with audio; literature review","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9KBFCWM9","conferencePaper","2016","Ishizawa, Fumiko; Sakamoto, Mizuki; Nakajima, Tatsuo","Analyzing Two Case Studies for Enhancing the Meaning of the Real Space","Proceedings of the 9th Nordic Conference on Human-Computer Interaction","978-1-4503-4763-1","","10.1145/2971485.2996730","https://dl.acm.org/doi/10.1145/2971485.2996730","The paper extracts some insights from two case studies to enhance the meaning of the real space through incorporating virtuality. Our focus is examining how one can feel a sense of values in the enhanced real space through incorporating virtuality in the two case studies. The insights generated from the examination will be effective in guiding people's behavior in future digital social infrastructures that are crucial to addressing serious social problems, such as maintaining environmental sustainability or achieving human well-being. We hope that the research will help future researchers explore better theorization to enhance the real space.","2016-10-23","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1–6","","","","","","","NordiCHI '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/GPF8IAC7/Ishizawa et al. - 2016 - Analyzing Two Case Studies for Enhancing the Meani.pdf","","","Design analysis; Enhanced real space; Virtuality","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8IIBK46T","conferencePaper","2017","Feltham, Frank; Loke, Lian","Felt Sense through Auditory Display: A Design Case Study into Sound for Somatic Awareness while Walking","Proceedings of the 2017 ACM SIGCHI Conference on Creativity and Cognition","978-1-4503-4403-6","","10.1145/3059454.3059461","https://dl.acm.org/doi/10.1145/3059454.3059461","Walking is an everyday act that we, as humans, often take for granted. To walk requires the synergy of somatosensory, neurological and physiological processes for us to move at a regular pace by lifting and setting down each foot in turn. It can be argued that walking is also a source of creativity and exploration when conducted as an intentional act of somatic or self-awareness. This design case study aims to explore the kinds of somatic awareness and aesthetic engagement of walking apparent through the introduction of a pressure mediated sound generating surface with a group of Feldenkrais movement practitioners. These explorations reveal that there is an awareness of tempo and rhythm during the step cycle. This awareness takes on an internal focus as shifts in attention and bodily organization. Another key finding is that exploration and play are enabled due to the rich timbral qualities of the pressure mediated auditory feedback. The significance and contribution in this work is in the implications it has for the design of technologies that support kinaesthetic awareness through aesthetic and exploratory strategies.","2017-06-22","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","287–298","","","","","","Felt Sense through Auditory Display","C&amp;C '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/8492RUNM/Feltham and Loke - 2017 - Felt Sense through Auditory Display A Design Case.pdf","","","auditory feedback; walking; creativity; aesthetics of interaction; design case study.; kinaesthetic awareness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6R8SLYAX","conferencePaper","2016","Monastero, Beatrice; McGookin, David; Torre, Giuseppe","Wandertroper: supporting aesthetic engagement with everyday surroundings through soundscape augmentation","Proceedings of the 15th International Conference on Mobile and Ubiquitous Multimedia","978-1-4503-4860-7","","10.1145/3012709.3012725","https://dl.acm.org/doi/10.1145/3012709.3012725","In this paper we present the design and evaluation of Wandertroper, a mobile system designed to support reengagement with everyday surroundings during daily walks. Wandertroper generates real-time sound based on the spectromorphology of the inhabited soundscape, which is manipulated by how the user walks. Through an iterative participatory design process using semi-structured group discussions and 'in-the-wild' evaluations, we outline how design aspects, such as the degree of output abstraction and aesthetic personalisation of use, facilitated users' engagement with everyday surroundings. We discuss how Wandertroper turned daily walks into personally-meaningful aesthetic experiences.","2016-12-12","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","129–140","","","","","","Wandertroper","MUM '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/YBV27LRS/Monastero et al. - 2016 - Wandertroper supporting aesthetic engagement with.pdf","","","embodied interaction; aesthetic UX; rehabilitation of attention; soundscape spectromorphology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NZFPDXZA","conferencePaper","2015","Grimshaw, Mark; Walther-Hansen, Mads","The Sound of the Smell of my Shoes","Proceedings of the Audio Mostly 2015 on Interaction With Sound","978-1-4503-3896-7","","10.1145/2814895.2814900","https://dl.acm.org/doi/10.1145/2814895.2814900","Given the sensory poverty of virtual environments, such as those found in computer games that rely, in the main, solely on audio-visual interfaces, how best do we attain the experience of presence in those environments when presence requires the construction of a coherent (in the sense of realism) place in which to be and in which to act? The paper explores this question through an investigation of the senses of hearing and smell and suggests the possibility of introducing the experience of odours into such environments through the use of sound.","2015-10-07","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1–8","","","","","","","AM '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/TVFWJ5FR/Grimshaw and Walther-Hansen - 2015 - The Sound of the Smell of my Shoes.pdf","","","Virtual environments; Sound; Cross-modality; Hearing; Odour; Real virtuality; Smell; Sonic virtuality","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4SUS9FKV","conferencePaper","2010","McCullagh, P. J.; Ware, M. P.; Lightbody, G.","Brain Computer Interfaces for inclusion","Proceedings of the 1st Augmented Human International Conference","978-1-60558-825-4","","10.1145/1785455.1785461","https://dl.acm.org/doi/10.1145/1785455.1785461","In this paper, we describe an intelligent graphical user interface (IGUI) and a User Application Interface (UAI) tailored to Brain Computer Interface (BCI) interaction, designed for people with severe communication needs. The IGUI has three components; a two way interface for communication with BCI2000 concerning user events and event handling; an interface to user applications concerning the passing of user commands and associated device identifiers, and the receiving of notification of device status; and an interface to an extensible mark-up language (xml) file containing menu content definitions. The interface has achieved control of domotic applications. The architecture however permits control of more complex 'smart' environments and could be extended further for entertainment by interacting with media devices. Using components of the electroencephalogram (EEG) to mediate expression is also technically possible, but is much more speculative, and without proven efficacy. The IGUI-BCI approach described could potentially find wider use in the augmentation of the general population, to provide alternative computer interaction, an additional control channel and experimental leisure activities.","2010-04-02","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1–8","","","","","","","AH '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/VNB5445I/McCullagh et al. - 2010 - Brain Computer Interfaces for inclusion.pdf","","","user interface; brain computer interfaces; domotic control; entertainment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QC8DNMUX","conferencePaper","2019","Magnusson, C.; Ólafsdóttir, S. A.; Caltenco, H.; Rassmus-Gröhn, K.; Hafsteinsdottir, T.; Jónsdóttir, H.; Hjaltadóttir, I.; Rydeman, B.","Designing Motivating Interactive Balance and Walking Training for Stroke Survivors","Proceedings of the 13th EAI International Conference on Pervasive Computing Technologies for Healthcare","978-1-4503-6126-2","","10.1145/3329189.3329224","https://dl.acm.org/doi/10.1145/3329189.3329224","In the ActivAbles and STARR projects we are developing interactive training tools for stroke survivors. Our initial user studies pointed to balance being a key ability, therefore one of the developed tools is an interactive balance pad. Equipment exists for persons with good balance (eg. Wii), but most consumer games and exercises are less suited for many stroke survivors. The development process has been done in close collaboration with stroke survivors, and we have currently a prototype system that has been tested by 10 stroke survivors for a longer period in the home during a feasibility study. The system includes an interactive balance foam pad, feedback lamps and a step counting game app which all connect to a central server. The feedback is designed to be inclusive - designs are multimodal (visual and auditory), and the setup is flexible and can easily be adapted. In this paper we report and discuss the design of the system, pilot test results and the results from a feasibility study in the home.","2019-05-20","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","327–333","","","","","","","PervasiveHealth'19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/IGZ858CE/Magnusson et al. - 2019 - Designing Motivating Interactive Balance and Walki.pdf","","","Stroke; interactive; multimodal; rehabilitation; balance training; inclusive","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8GGEV9PW","conferencePaper","2021","Iber, Michael; Dumphart, Bernhard; de Jesus Oliveira, Victor-Adriel; Ferstl, Stefan; M. Reis, Joschua; Slijepčević, Djordje; Heller, Mario; Raberger, Anna-Maria; Horsak, Brian","Mind the Steps: Towards Auditory Feedback in Tele-Rehabilitation Based on Automated Gait Classification","Proceedings of the 16th International Audio Mostly Conference","978-1-4503-8569-5","","10.1145/3478384.3478398","https://dl.acm.org/doi/10.1145/3478384.3478398","We describe a proof-of-concept for the implementation of a mobile auditory biofeedback system based on automated classification of functional gait disorders. The classification is embedded in a sensor-instrumented insole and is based on ground reaction forces (GRFs). GRF data have been successfully used for the classification of gait patterns into clinically relevant classes and are frequently used in clinical practice to quantitatively describe human motion. A feed-forward neural network that was implemented on the firmware of the insole is used to estimate the GRFs using pressure and accelerator data. Compared to GRF measurements obtained from force plates, the estimated GRFs performed highly accurately. To distinguish between normal physiological gait and gait disorders, we trained and evaluated a support vector machine with labeled data from a publicly accessible database. The automated gait classification was sonified for auditory feedback. The high potential of the implemented auditory feedback for preventive and supportive applications in physical therapy, such as supervised therapy settings and tele-rehabilitation, was highlighted by a semi-structured interview with two experts.","2021-10-15","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","139–146","","","","","","Mind the Steps","AM '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/2DXYSU49/Iber et al. - 2021 - Mind the Steps Towards Auditory Feedback in Tele-.pdf","","","Auditory Feedback; Rehabilitation; Automated Gait Classification; Biomechanics; Physical Therapy","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8DPVUVM6","conferencePaper","2013","Ng, Patrick; Nesbitt, Keith","Informative sound design in video games","Proceedings of The 9th Australasian Conference on Interactive Entertainment: Matters of Life and Death","978-1-4503-2254-6","","10.1145/2513002.2513015","https://dl.acm.org/doi/10.1145/2513002.2513015","The importance of sound is quite well known in video games. Although frequently in the past sound was simply used to increase the immersion of the player. Now there is a growing interest in using sound as a means for providing the player with additional information. While the use of sound for displaying information has been a topic of research for a number of years, research into the area of informative sound design for games hasn't been widely investigated as such. As a result proper guidelines for game developers to follow when attempting to design informative sound for a game haven't been broadly established. In addition, there has been almost no work done in considering specific genres such as Real Time Strategy and First-Person Shooter games and how sounds can be best used to provide players with information in these types of games. In this work we review previous approaches to sound design including the use of patterns. We then review these approaches in relation to sound design for both FPS and RTS games. Finally we present some key design patterns in relation to these genres.","2013-09-30","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1–9","","","","","","","IE '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/VAZNUK8Z/Ng and Nesbitt - 2013 - Informative sound design in video games.pdf","","","auditory display; speech; auditory icons; earcons; sound design; computer games; design patterns; informative sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S5XF4BJU","conferencePaper","2012","McGookin, David; Brewster, Stephen","PULSE: the design and evaluation of an auditory display to provide a social vibe","Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","978-1-4503-1015-4","","10.1145/2207676.2208580","https://dl.acm.org/doi/10.1145/2207676.2208580","We present PULSE, a mobile application designed to allow users to gain a 'vibe', an intrinsic understanding of the people, places and activities around their current location, derived from messages on the Twitter social networking site. We compared two auditory presentations of the vibe. One presented message metadata implicitly through modification of spoken message attributes. The other presented the same metadata, but through additional auditory cues. We compared both techniques in a lab and real world study. Additional auditory cues were found to allow for smaller changes in metadata to be more accurately detected, but were least preferred when PULSE was used in context. Results also showed that PULSE enhanced and shaped user understanding, with audio presentation allowing a closer coupling of digital data to the physical world.","2012-05-05","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1263–1272","","","","","","PULSE","CHI '12","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/S3YNINVP/McGookin and Brewster - 2012 - PULSE the design and evaluation of an auditory di.pdf","","","auditory display; geo-social media; location-based services; twitter","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FQKUGKN5","conferencePaper","2019","Nordmoen, Charlotte; Armitage, Jack; Morreale, Fabio; Stewart, Rebecca; McPherson, Andrew","Making Sense of Sensors: Discovery Through Craft Practice With an Open-Ended Sensor Material","Proceedings of the 2019 on Designing Interactive Systems Conference","978-1-4503-5850-7","","10.1145/3322276.3322368","https://dl.acm.org/doi/10.1145/3322276.3322368","This paper explores the process by which designers come to terms with an unfamiliar and ambiguous sensor material. Drawing on craft practice and material-driven interaction design, we developed a simple yet flexible sensor technology based on the movement of conductive elements within a magnetic field. Variations in materials and structure give rise to objects which produce a complex time-varying signal in response to physical interaction. Sonifying the signal yields nuanced and intuitive action-sound correspondences which nonetheless defy easy categorisation in terms of conventional types of sensors. We reflect on a craft-based exploration of the material by one of the authors, then report on two workshops with groups of designers of varying background. Through examining the objects produced and the experience of the participants, we explore the tension between tacit and explicit understanding of unfamiliar materials and the ways that material thinking can create new design opportunities.","2019-06-18","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","135–146","","","","","","Making Sense of Sensors","DIS '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/86W6W6G4/Nordmoen et al. - 2019 - Making Sense of Sensors Discovery Through Craft P.pdf","","","embodiment; craft; material exploration; material improvisation; meaning making; sensor technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AVAD7323","conferencePaper","2020","Hong, Jiwoo; Yi, HyeonBeom; Pyun, Jaehoon; Lee, Woohun","SoundWear: Effect of Non-speech Sound Augmentation on the Outdoor Play Experience of Children","Proceedings of the 2020 ACM Designing Interactive Systems Conference","978-1-4503-6974-9","","10.1145/3357236.3395541","https://dl.acm.org/doi/10.1145/3357236.3395541","This study aims to clarify the effect of non-speech sound augmentation (i.e., everyday and instrumental sounds) on outdoor play for children, where has been lacking in empirical examination. In a within-subject observational study, sixteen children (ages 10-11) were divided into four equally sized groups and equipped with SoundWear, which is a wearable bracelet that allowed them to explore sounds, pick a desired sound, generate the sound with a swinging movement, and transfer the sound between multiple devices. Both the quantitative and qualitative results revealed that augmenting everyday sounds led to distinct play types with differences in physical, social, and imaginative behaviors, whereas instrumental sounds were naturally integrated into traditional games. Thus, sound augmentation with specific digital design features (e.g., transparent technology to provide new perspectives, margin for interpretation, and ownership through a sense of achievement) is significant for shaping distinctions in digitally enhanced play and requires considerable design attention.","2020-07-03","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","2201–2213","","","","","","SoundWear","DIS '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/VMYYN5YK/Hong et al. - 2020 - SoundWear Effect of Non-speech Sound Augmentation.pdf","","","social interaction; sound; design; children; imagination; interactions; open-ended play; outdoor play; physical activities; playful experiences; wearable","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D6P7WRCU","conferencePaper","2011","Houri, Naoyuki; Arita, Hiroyuki; Sakaguchi, Yutaka","Audiolizing body movement: its concept and application to motor skill learning","Proceedings of the 2nd Augmented Human International Conference","978-1-4503-0426-9","","10.1145/1959826.1959839","https://dl.acm.org/doi/10.1145/1959826.1959839","We propose a concept of ""audiolization of body movement,"" which transforms the posture/movement of the human body or human-controlled-tools into acoustic signals and feeds them back to the users in a real-time manner. It aims at helping people being aware of their body/tool states, and resultantly assisting their motor skill learning. The present paper describes features of the concepts and introduces some demonstrative applications.","2011-03-13","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1–4","","","","","","Audiolizing body movement","AH '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/CLPY46S4/Houri et al. - 2011 - Audiolizing body movement its concept and applica.pdf","","","body movement; body awareness; audiolization; motor skill learning; sensori-motor integration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TTEV85ZE","conferencePaper","2019","Holloway, Leona; Marriott, Kim; Butler, Matthew; Borning, Alan","Making Sense of Art: Access for Gallery Visitors with Vision Impairments","Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","978-1-4503-5970-2","","10.1145/3290605.3300250","https://dl.acm.org/doi/10.1145/3290605.3300250","While there is widespread recognition of the need to provide people with vision impairments (PVI) equitable access to cultural institutions such as art galleries, this is not easy. We present the results of a collaboration with a regional art gallery who wished to open their collection to PVIs in the local community. We describe a novel model that provides three different ways of accessing the gallery, depending upon visual acuity and mobility: virtual tours, self-guided tours and guided tours. As far as possible the model supports autonomous exploration by PVIs. It was informed by a value sensitive design exploration of the values and value conflicts of the primary stakeholders.","2019-05-02","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1–12","","","","","","Making Sense of Art","CHI '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/YGYB445L/Holloway et al. - 2019 - Making Sense of Art Access for Gallery Visitors w.pdf","","","accessibility; blindness; 3d printing; art; value sensitive design; vision impairment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VCWPJS8Z","conferencePaper","2014","Grimshaw, Mark; Garner, Tom","Imagining sound","Proceedings of the 9th Audio Mostly: A Conference on Interaction With Sound","978-1-4503-3032-9","","10.1145/2636879.2636881","https://dl.acm.org/doi/10.1145/2636879.2636881","We make the case in this essay that sound that is imagined is both a perception and as much a sound as that perceived through external stimulation. To argue this, we look at the evidence from auditory science, neuroscience, and philosophy, briefly present some new conceptual thinking on sound that accounts for this view, and then use this to look at what the future might hold in the context of imagining sound and developing technology.","2014-10-01","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1–8","","","","","","","AM '14","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/QHVB855Q/Grimshaw and Garner - 2014 - Imagining sound.pdf","","","auralization; audiation; auditory imagery; aural imagery; sonic imagination","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2C952DSM","conferencePaper","2010","McGookin, David; Robertson, Euan; Brewster, Stephen","Clutching at straws: using tangible interaction to provide non-visual access to graphs","Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","978-1-60558-929-9","","10.1145/1753326.1753583","https://dl.acm.org/doi/10.1145/1753326.1753583","We present a tangible user interface (TUI) called Tangible Graph Builder, that has been designed to allow visually impaired users to access graph and chart-based data. We describe the current paper-based materials used to allow independent graph construction and browsing, before discussing how researchers have applied virtual haptic and non-speech audio techniques to provide more flexible access. We discuss why, although these technologies overcome many of the problems of non-visual graph access, they also introduce new issues and why the application of TUIs is important. An evaluation of Tangible Graph Builder with 12 participants (8 sight deprived, 4 blind) revealed key design requirements for non-visual TUIs, including phicon design and handling marker detection failure. We finish by presenting future work and improvements to our system.","2010-04-10","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1715–1724","","","","","","Clutching at straws","CHI '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/X73Q7DDF/McGookin et al. - 2010 - Clutching at straws using tangible interaction to.pdf","","","visual impairment; graphs; haptic interaction; tangible user interface","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4NZFJST9","conferencePaper","2010","Morelli, Tony; Foley, John; Columna, Luis; Lieberman, Lauren; Folmer, Eelke","VI-Tennis: a vibrotactile/audio exergame for players who are visually impaired","Proceedings of the Fifth International Conference on the Foundations of Digital Games","978-1-60558-937-4","","10.1145/1822348.1822368","https://dl.acm.org/doi/10.1145/1822348.1822368","Lack of physical activity is a serious health concern for individuals who are visually impaired as they have fewer opportunities and incentives to engage in physical activities that provide the amounts and kinds of stimulation sufficient to maintain adequate fitness and to support a healthy standard of living. Exergames are video games that use physical activity as input and which have the potential to change sedentary lifestyles and associated health problems such as obesity. We identify that exergames have a number properties that could overcome the barriers to physical activity that individuals with visual impairments face. However, exergames rely upon being able to perceive visual cues that indicate to the player what input to provide. This paper presents VI Tennis, a modified version of a popular motion sensing exergame that explores the use of vibrotactile and audio cues. The effectiveness of providing multimodal (tactile/audio) versus unimodal (audio) cues was evaluated with a user study with 13 children who are blind. Children achieved moderate to vigorous levels of physical activity- the amount required to yield health benefits. No significant difference in active energy expenditure was found between both versions, though children scored significantly better with the tactile/audio version and also enjoyed playing this version more, which emphasizes the potential of tactile/audio feedback for engaging players for longer periods of time.","2010-06-19","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","147–154","","","","","","VI-Tennis","FDG '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/ZZ79W87V/Morelli et al. - 2010 - VI-Tennis a vibrotactileaudio exergame for playe.pdf","","","visually impaired; exergames; tactile; audio; health","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KAQAZDT7","conferencePaper","2017","Françoise, Jules; Candau, Yves; Fdili Alaoui, Sarah; Schiphorst, Thecla","Designing for Kinesthetic Awareness: Revealing User Experiences through Second-Person Inquiry","Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","978-1-4503-4655-9","","10.1145/3025453.3025714","https://dl.acm.org/doi/10.1145/3025453.3025714","We consider kinesthetic awareness, the perception of our own body position and movement in space, as a critical value for embodied design within third wave HCI. We designed an interactive sound installation that supports kinesthetic awareness of a participant's micro-movements. The installation's interaction design uses continuous auditory feedback and leverages an adaptive mapping strategy, refining its sensitivity to increase sonic resolution at lower levels of movement activity. The installation uses field recordings as rich source materials to generate a sound environment that attunes to a participant's micro-movements. Through a qualitative study using a second-person interview technique, we gained nuanced insights into the participants' subjective experiences of the installation. These reveal consistent temporal patterns, as participants build on a gradual process of integration to increase the complexity and capacity of their kinesthetic awareness during interaction.","2017-05-02","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","5171–5183","","","","","","Designing for Kinesthetic Awareness","CHI '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/265LHKUT/Françoise et al. - 2017 - Designing for Kinesthetic Awareness Revealing Use.pdf","","","auditory feedback; sound; interaction design; kinesthetic awareness; movement; qualitative methods; second person interviewing; user experience","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U8BGW5KB","conferencePaper","2021","van Rheden, Vincent; Harbour, Eric; Finkenzeller, Thomas; Burr, Lisa Anneke; Meschtscherjakov, Alexander; Tscheligi, Manfred","Run, Beep, Breathe: Exploring the Effects on Adherence and User Experience of 5 Breathing Instruction Sounds while Running","Proceedings of the 16th International Audio Mostly Conference","978-1-4503-8569-5","","10.1145/3478384.3478412","https://dl.acm.org/doi/10.1145/3478384.3478412","Running is a popular sport, especially during the recent pandemic. Breathing techniques can positively impact running, for example to prevent side-stitches or increase lung capacity. Sound instruction is a promising method to administer breathing techniques during running, as it is an established, low-friction method utilized in other contexts such as cycling and meditation. This paper describes an initial study (N=11) exploring the effects of five distinct breathing instruction sounds while running. Sounds were designed with varying information richness and tonality. The study focused on user adherence to the sound and subjective experience of running with the sound. Results show that all sounds were effective in stabilizing the breathing rate. Two-tone sounds were subjectively easier to follow; however, metronome sounds might be preferred for longer studies due to their simplicity and lower invasiveness.","2021-10-15","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","16–23","","","","","","Run, Beep, Breathe","AM '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/LSSAVIUX/van Rheden et al. - 2021 - Run, Beep, Breathe Exploring the Effects on Adher.pdf","","","human-computer interaction; sports; user experience; Auditory interface; sound-richness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F6QBSZYD","conferencePaper","2009","Strachan, Steven; Lefebvre, Grégoire; Zijp-Rouzier, Sophie","overView: physically-based vibrotactile feedback for temporal information browsing","Proceedings of the 11th International Conference on Human-Computer Interaction with Mobile Devices and Services","978-1-60558-281-8","","10.1145/1613858.1613910","https://dl.acm.org/doi/10.1145/1613858.1613910","An approach to providing tangible feedback to users of a mobile device in both highly visual touchscreen-based and eyes-free interaction scenarios and the transition between the two is presented. A rotational dynamical systems metaphor for the provision of feedback is proposed, which provides users with physically based feedback via the audio, tactile and visual senses. By using a consistent metaphor in this way it is possible to support the seamless movement between highly visual touch-based interaction and eyes-free gestural interaction.","2009-09-15","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1–4","","","","","","overView","MobileHCI '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/IUK7KWUZ/Strachan et al. - 2009 - overView physically-based vibrotactile feedback f.pdf","","","feedback; interaction; eyes-free; haptic; vibrotactile","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H2TYGF73","conferencePaper","2018","Rector, Kyle; Bartlett, Rachel; Mullan, Sean","Exploring Aural and Haptic Feedback for Visually Impaired People on a Track: A Wizard of Oz Study","Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility","978-1-4503-5650-3","","10.1145/3234695.3236345","https://dl.acm.org/doi/10.1145/3234695.3236345","Access to a variety of exercises is important for maintaining a healthy lifestyle. This variety includes physical activity in public spaces. A 400-meter jogging track is not accessible because it provides solely visual cues for people to remain in their lane. As a first step toward making exercise spaces accessible, we conducted an ecologically valid Wizard of Oz study to compare the accuracy and user experience of human guide, verbal, wrist vibration, and head beat feedback while people walked around the track. The technology conditions did not affect accuracy, but the order of preference was human guide, verbal, wrist vibration, and head beat. Participants had a difficult time perceiving vibrations when holding their cane or guide dog, and lower frequency sounds made it difficult to focus on their existing navigation strategies.","2018-10-08","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","295–306","","","","","","Exploring Aural and Haptic Feedback for Visually Impaired People on a Track","ASSETS '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/GQMG27JQ/Rector et al. - 2018 - Exploring Aural and Haptic Feedback for Visually I.pdf","","","accessibility; visual impairments; eyes-free; audio feedback; outdoor exercise; vibration feedback","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"93AWMV2E","conferencePaper","2022","Otterbein, Robin; Jochum, Elizabeth; Overholt, Daniel; Bai, Shaoping; Dalsgaard, Alex","Dance and Movement-Led Research for Designing and Evaluating Wearable Human-Computer Interfaces","Proceedings of the 8th International Conference on Movement and Computing","978-1-4503-8716-3","","10.1145/3537972.3537984","https://dl.acm.org/doi/10.1145/3537972.3537984","Movement research and practice in the context of wearable technologies and human-computer interaction (HCI) shifts the design paradigm to the lived body. Human movement is characterized by sense, intention and expressiveness. Designing HCI from this standpoint opens up new possibilities to make computational devices and applications more accessible and integrated. This work presents an iterative, collaborative, and cross-disciplinary approach using wearable sensor bands in an open-ended performative exploration in exchange with a professional dancer. The goal is to understand the benefits and challenges of using movement-centered tools originating from dance practice and movement research and how they might feed back into the design, development and evaluation process of embodied technologies to improve human-computer interactions. Movement analysis systems and motion computation models are reviewed and leveraged in an interactive audiovisual system, with focus on using force-sensing resistors for low-level motion descriptors and Laban Movement Analysis for higher-level movement features. The artistic methodology, which combines practice and research, results, discussion of the iterative and collaborative process, and the final system architecture are the main topics presented in the paper.","2022-06-30","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1–9","","","","","","","MOCO '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/ZYA64TUV/Otterbein et al. - 2022 - Dance and Movement-Led Research for Designing and .pdf","","","Interactive Dance; Audiovisual Feedback; Human-Computer-Interaction; Interactive Systems; Multimodal Wearable Sensors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DHGGMYRQ","conferencePaper","2015","Zou, Hong; Treviranus, Jutta","ChartMaster: A Tool for Interacting with Stock Market Charts using a Screen Reader","Proceedings of the 17th International ACM SIGACCESS Conference on Computers & Accessibility","978-1-4503-3400-6","","10.1145/2700648.2809862","https://dl.acm.org/doi/10.1145/2700648.2809862","Stock market charts guide investors in making financial decisions. Online stock market charts are largely interactive, driven by real-time financial data. However, these are not easily accessible via a screen reader. To enable screen reader users to query and effectively use interactive online stock market charts, we are developing a tool called ChartMaster. In this paper, we describe an early study conducted with sixteen visually impaired persons, most of whom were financial novices, for co-designing the interaction interface for C hartMaster. An inclusive design exercise was undertaken to discover alternative interfaces using non-visual modalities to interact with stock market charts. A user-centered process of co-design using HCI methods was carried out to iteratively evaluate and refine three input solutions: audio input, text input and dropdown menu. While the users ultimately declared the dropdown menu to be the most useful of the three solutions, they wanted all possible options to choose from based on task contexts and personal preferences. User feedback confirmed that a one-size-fits-all design is not ideal for accommodating diverse user needs within the widest possible range of contexts. It was also found that the ChartMaster tool with dropdown menu interface holds potential educational value for financial novices.","2015-10-26","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","107–116","","","","","","ChartMaster","ASSETS '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/N5X8EI3Q/Zou and Treviranus - 2015 - ChartMaster A Tool for Interacting with Stock Mar.pdf","","","accessibility; visual impairment; inclusive design; financial literacy; multi-modal; non-visual; screen reader; stock market charts; usability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W7WQJVWU","conferencePaper","2021","Verano Merino, Mauricio; Beckmann, Tom; van der Storm, Tijs; Hirschfeld, Robert; Vinju, Jurgen J.","Getting grammars into shape for block-based editors","Proceedings of the 14th ACM SIGPLAN International Conference on Software Language Engineering","978-1-4503-9111-5","","10.1145/3486608.3486908","https://dl.acm.org/doi/10.1145/3486608.3486908","Block-based environments are visual programming environments that allow users to program by interactively arranging visual jigsaw-like blocks. They have shown to be helpful in several domains but often require experienced developers for their creation. Previous research investigated the use of language workbenches to generate block-based editors based on grammars, but the generated block-based editors sometimes provided too many unnecessary blocks, leading to verbose environments and programs. To reduce the number of interactions, we propose a set of transformations to simplify the original grammar, yielding a reduction of the number of (useful) kinds of blocks available in the resulting editors. We show that our generated block-based editors are improved for a set of observed aesthetic criteria up to a certain complexity. As such, analyzing and simplifying grammars before generating block-based editors allows us to derive more compact and potentially more usable block-based editors, making reuse of existing grammars through automatic generation feasible.","2021-11-22","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","83–98","","","","","","","SLE 2021","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/8TPPYATK/Verano Merino et al. - 2021 - Getting grammars into shape for block-based editor.pdf","","","block-based environments; Blockly; DSLs; grammars; language workbenches; syntax; visual languages; Kogi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SPQWR6UP","conferencePaper","2012","Betella, Alberto; Carvalho, Rodrigo; Sanchez-Palencia, Jesus; Bernardet, Ulysses; Verschure, Paul F. M. J.","Embodied interaction with complex neuronal data in mixed-reality","Proceedings of the 2012 Virtual Reality International Conference","978-1-4503-1243-1","","10.1145/2331714.2331718","https://dl.acm.org/doi/10.1145/2331714.2331718","The study of natural and artificial phenomena generates massive amounts of data in many areas of research. This data is frequently left unused due to the lack of tools to effectively extract, analyze and understand it. Visual representation techniques can play a key role in helping to discover patterns and meaning within this data. Neuroscience is one of the scientific fields that generates the most extensive datasets. For this reason we built a 3D real-time visualization system to graphically represent the massive connectivity of neuronal network models in the eXperience Induction Machine (XIM). The XIM is an immersive space equipped with a number of sensors and effectors that we constructed to conduct experiments in mixed-reality. Using this infrastructure we developed an embodied interaction framework that allows the user to move freely in the space and navigate through the neuronal system. We conducted an empirical evaluation of the impact of different navigation mappings on the understanding of a neuronal dataset. Our results revealed that different navigation mappings affect the structural understanding of the system and the involvement with the data presented.","2012-03-28","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1–8","","","","","","","VRIC '12","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/BWGJGP2N/Betella et al. - 2012 - Embodied interaction with complex neuronal data in.pdf","","","visualization; embodied interaction; navigation; complex datasets; iqr; mixed-reality; neuronal networks; XIM","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MFFUSJS4","conferencePaper","2007","Frauenberger, C.; Stockman, T.; Bourguet, M. L.","Pattern design in the context space: paco - a methodological framework for designing auditory display with patterns","Proceedings of the 14th Conference on Pattern Languages of Programs","978-1-60558-411-9","","10.1145/1772070.1772091","https://dl.acm.org/doi/10.1145/1772070.1772091","This paper introduces a methodological framework for contextual design with patterns (paco). Its development was driven by the lack of guidance in designing audio in the user interface and by the need to communicate design knowledge within the community and to designers outside the field. The fundamental concepts presented in this paper, however, are generic and might be applicable similarly to other disciplines. The framework provides methods to create, apply and refine design patterns considering the particularities of small or pre-mature scientific disciplines which have less successful examples to draw upon - such as auditory display. After providing background on research in auditory display and current design practice, a set of requirements for the framework is developed, an appropriate format for design patterns is discussed and the context space is introduced as a key concept to facilitate the workflow within the framework. An example workflow shows the usage of the framework during the life-cycle of a design pattern and we elaborate on the next steps discussing an online design tool and the evaluation of the framework.","2007-09-05","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1–7","","","","","","Pattern design in the context space","PLOP '07","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/XPUNT7L7/Frauenberger et al. - 2007 - Pattern design in the context space paco - a meth.pdf","","","auditory display; design patterns; design methodology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8Y563QA3","conferencePaper","2014","Chia, Foong-Yi; Saakes, Daniel","Interactive training chopsticks to improve fine motor skills","Proceedings of the 11th Conference on Advances in Computer Entertainment Technology","978-1-4503-2945-3","","10.1145/2663806.2663816","https://dl.acm.org/doi/10.1145/2663806.2663816","Handling chopsticks requires fine motor skills that are challenging to master. We present interactive training chopsticks that help children develop the skills of eating with chopsticks. We discuss the design and implementation of two games that use the chopsticks as a controller for an augmented mirror application.","2014-11-11","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1–4","","","","","","","ACE '14","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/R2AH8XNQ/Chia and Saakes - 2014 - Interactive training chopsticks to improve fine mo.pdf","","","education; children; augmented mirror; chopsticks; gaming","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FXEQZQT9","conferencePaper","2022","van Koningsbruggen, Rosa; Waldschütz, Hannes; Hornecker, Eva","What is Data? - Exploring the Meaning of Data in Data Physicalisation Teaching","Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction","978-1-4503-9147-4","","10.1145/3490149.3501319","https://dl.acm.org/doi/10.1145/3490149.3501319","A growing body of work focuses on physicalisations based on personal, everyday data. Despite growing interest, little is known about how to educate people on their creation. We designed a teaching method of ’Data Diaries’, which consists of five representation assignments that move from visualising to physicalising personal data. The Data Diaries were used in a semester project, with the aim of creating an interactive physicalisation. We analysed the Data Diaries, written reports, and participant interviews. Our analysis shows that people need to overcome the challenge of using materiality to communicate data, which happens in four stages. Moreover, the materiality made participants realise that physicalisations do not focus on efficiency and accuracy, but on the story of the data, by referring to its origin, use of personal mappings, and reduction. As physicalisations blur the line between quantitative and qualitative, designing them engenders a change in our notion of ’data’.","2022-02-13","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1–21","","","","","","What is Data?","TEI '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/KSZ8ZQ4R/van Koningsbruggen et al. - 2022 - What is Data - Exploring the Meaning of Data in D.pdf","","","Data Diaries; Notion of Data; Physicalisation Literacy; Teaching","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T2ENJI9U","conferencePaper","2015","Doore, Karen; Vega, David; Fishwick, Paul","A Media-Rich Curriculum for Modeling and Simulation","Proceedings of the 3rd ACM SIGSIM Conference on Principles of Advanced Discrete Simulation","978-1-4503-3583-6","","10.1145/2769458.2769471","https://dl.acm.org/doi/10.1145/2769458.2769471","We discuss a novel approach for teaching Modeling and Simulation (M&S) using a rich, multi-media focus with an emphasis on student construction and creative representation of simulations for dynamic systems. The increased use of M&S to enhance understanding and analyze problems across an ever-widening range of disciplines means that the diversity and number of professionals who will work with simulations is also increasing. Therefore, it is important that expanded opportunities exist for a wide range of students to take M&S courses as part of their secondary or post-secondary education. In addition to introducing M&S to new audiences, it is important to consider how professionals from these diverse disciplines can enhance and improve the quality and effectiveness of simulations. Courses, which target non-traditional simulation students, can broaden the diversity of expertise within the M&S community. We introduce an M&S course that uses Max/MSP software, which is familiar for many multimedia students and faculty. The course targets a mixed class of graduate students from Art and Technology, Computer Science, and Engineering programs.","2015-06-10","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","23–34","","","","","","","SIGSIM PADS '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/9MEMIPEI/Doore et al. - 2015 - A Media-Rich Curriculum for Modeling and Simulatio.pdf","","","education; creativity; data flow; iot; max/msp; microcontrollers; modeling and simulation; petri nets","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KFWIHU5F","conferencePaper","2022","Derry, Lins; Kruguer, Jordan; Mueller, Maximilian; Schnapp, Jeffrey","Designing a Choreographic Interface During COVID-19","Proceedings of the 8th International Conference on Movement and Computing","978-1-4503-8716-3","","10.1145/3537972.3538020","https://dl.acm.org/doi/10.1145/3537972.3538020","In 2019, metaLAB (at) Harvard began work on Curatorial A(i)gents, a digital exhibition that was slated to premiere at the Harvard Art Museums’ Lightbox Gallery in 2020. Half of the projects would be interactive, using mouse and keyboard conventions. With the advent of Covid-19 and the postponement of the show, the authors set out to develop an interface solution that would enable visitors to interact with the works without having to touch any public devices like a tablet. Toward this end, we prototyped a “choreographic interface” that uses machine vision and machine learning to interpret a full-torso gestural vocabulary, which is then translated into interactions. To make the choreographic interface, we relied on open-source solutions, which have all come with equal limitations and opportunities. In 2022, Curatorial A(i)gents was presented in the Lightbox Gallery, where we had the opportunity to test and demonstrate the interface. This paper discusses our design journey in making a choreographic interface using open-source technologies during Covid-19.","2022-06-30","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1–7","","","","","","","MOCO '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/ASNYSQWY/Derry et al. - 2022 - Designing a Choreographic Interface During COVID-1.pdf","","","Machine learning; Embodied interaction; User interface; Design; Dance; Choreographic interfaces; Choreography; Covid-19","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N4UTZA5U","conferencePaper","2018","Ferguson, Jamie; Williamson, John; Brewster, Stephen","Evaluating mapping designs for conveying data through tactons","Proceedings of the 10th Nordic Conference on Human-Computer Interaction","978-1-4503-6437-9","","10.1145/3240167.3240175","https://dl.acm.org/doi/10.1145/3240167.3240175","Tactons are structured vibrotactile messages which can be used to transmit information solely through the cutaneous sense. These are particularly useful in situations where visual or auditory displays are unavailable or inappropriate. Most data:vibration mappings do not consider the user's perceptions of the mappings being used, which can lead to confusion and Tactons which are difficult to interpret. To explore this issue, we conducted a magnitude estimation experiment to map how a number of vibrotactile parameters such as duration and frequency relate to the perceived magnitude of data variables that may be used in a real-world context such as error and danger. Results from this study show that when tempo and duration are used to convey data, they are perceived in the same polarity, regardless of the type of data being conveyed. This study provides polarity and scaling values which may be directly utilised by Tacton designers when creating new sets of vibrotactile messages.","2018-09-29","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","215–223","","","","","","","NordiCHI '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/7WDSBLB7/Ferguson et al. - 2018 - Evaluating mapping designs for conveying data thro.pdf","","","vibrotactile; non-visual; analogy; mental model; tactons","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U5NKTZUJ","conferencePaper","2021","Spiel, Katta","The Bodies of TEI – Investigating Norms and Assumptions in the Design of Embodied Interaction","Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction","978-1-4503-8213-7","","10.1145/3430524.3440651","https://dl.acm.org/doi/10.1145/3430524.3440651","In the few decades since the first mainframe computers, computing technologies have grown smaller, and more pervasive, moving onto and even inside human bodies. Even as those bodies have received increased attention by scholars, designers, and technologists, the bodily expectations and understandings articulated by these technological artefacts have not been a focus of inquiry in the field. I conducted a feminist content analysis on select papers in the proceeding of the ACM International Conference on Tangible, Embedded and Embodied Interaction (TEI) since its inception in 2007. My analysis illustrates how artefacts are implicitly oriented on unmarked bodily norms, while technologies designed for non-normative bodies treat those as deviant and in need of correction. Subsequently, I derive a range of provocations focused on material bodies in embodied interaction which offer a point of reflection and identify potentials for future work in the field.","2021-02-14","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1–19","","","","","","","TEI '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/8PJR5LLQ/Spiel - 2021 - The Bodies of TEI – Investigating Norms and Assump.pdf","","","Embodiment; Somaesthetics; Design; Bodies; Embodied Computing; Feminist Content Analysis; Norms; Representation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P39P597B","conferencePaper","2022","Tajadura-Jimenez, Ana; Ley-Flores, Judith; Valdiviezo, Omar; Singh, Aneesha; Sanchez-Martin, Milagrosa; Diaz Duran, Joaquin; Márquez Segura, Elena","Exploring the Design Space for Body Transformation Wearables to Support Physical Activity through Sensitizing and Bodystorming","Proceedings of the 8th International Conference on Movement and Computing","978-1-4503-8716-3","","10.1145/3537972.3538001","https://dl.acm.org/doi/10.1145/3537972.3538001","Negative or disturbed body perceptions are often interwoven with people's physical inactivity. While wearables can support body perception changes (body transformation), the design space of body transformation wearables supporting physical activity remains narrow. To expand this design space, we conducted an embodied co-design workshop with users. Using conceptual and tangible sensitizing tools, we explored/reflected on bodily sensations at three moments of movement execution (before/during/after). Conceptual tools were used to evoke/reflect/capture past lived experiences, while tangible tools were used as ideation probes for sensory bodystorming. Two design concepts emerged, reflecting diverging approaches to body transformation wearables: one focused on reminders and movement correction; the other on sensory augmentation and facilitation. We reflect on how each facilitates useful representations of body sensations during movement, and present methodological recommendations for designing technology for sensory augmentation in this area. Finally, we propose a preliminary prototype based on our design concepts and discuss future steps.","2022-06-30","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1–9","","","","","","","MOCO '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/Z7WX3VXB/Tajadura-Jimenez et al. - 2022 - Exploring the Design Space for Body Transformation.pdf","","","Probes; Haptics; Sound; Body Awareness; Body Movement; Body-Perception; Bodystorming; Embodied Cognition; Embodied Design Methods; Health; Interactive Systems Design; Physical Activity; Psychological Factors; Self-Care Technology Design; Self-tracking; Technology Probes; Wearables","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K4ALXNZ5","conferencePaper","2014","Morrison, Cecily; Smyth, Neil; Corish, Robert; O'Hara, Kenton; Sellen, Abigail","Collaborating with computer vision systems: an exploration of audio feedback","Proceedings of the 2014 conference on Designing interactive systems","978-1-4503-2902-6","","10.1145/2598510.2598519","https://dl.acm.org/doi/10.1145/2598510.2598519","Computer visions (CV) systems are increasingly finding new roles in domains such as healthcare. These collaborative settings are a new challenge for CV systems, requiring the design of appropriate interaction paradigms. The provision of feedback, particularly of what the CV system can 'see,' is a key aspect, and may not always be possible to present visually. We explore the design space for audio feedback for a scenario of interest, the clinical assessment of Multiple Sclerosis using a CV system. We then present a mixed-methods experimental study aimed at providing some first insights into the challenges and opportunities of designing audio feedback of this kind. Specifically, we compare audio feedback that differentiates which body parts the CV system can see to audio feedback that is undifferentiated. The findings reveal that it is not enough to simply convey that something might be out of view of the camera as what the camera can 'see' depends on the specific configuration of participants and the peculiarities of the skeleton inference algorithms. The results highlight the importance of providing feedback which more naturally conveys spatial information in developing CV systems for collaborative use.","2014-06-21","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","229–238","","","","","","Collaborating with computer vision systems","DIS '14","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/ZW3CF9SS/Morrison et al. - 2014 - Collaborating with computer vision systems an exp.pdf","","","audio feedback; co-present interaction; computer vision technologies; kinect","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UEYLNG83","conferencePaper","2014","Guggenberger, Mario","Multimodal Alignment of Videos","Proceedings of the 22nd ACM international conference on Multimedia","978-1-4503-3063-3","","10.1145/2647868.2654862","https://dl.acm.org/doi/10.1145/2647868.2654862","Most multimedia synchronization methods developed in the past are unimodal and consider only the audio data or the video data. Just recently, methods started to emerge that embrace multimodality by utilizing both audio and video processing to improve synchronization results. Although promising, their results are still not sufficient for fully automatic synchronization of recordings from heterogeneous sources. Video processing is also often too expensive to be used on large corpora of recordings, e.g. as they are commonly produced by crowds at social events. In my doctoral thesis, I will try to develop synchronization methods further by (a) examining fundamental problems that are usually ignored by lab-developed methods and therefore compromising real-world applications, (b) creating a publicly available synchronization-method benchmarking dataset, and (c) developing a low-level video feature based synchronization method with a computational complexity not higher than current state of the art audio-based methods.","2014-11-03","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","667–670","","","","","","","MM '14","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/8S4VRRXL/Guggenberger - 2014 - Multimodal Alignment of Videos.pdf","","","multimodality; audio; video; crowd; features; synchronization; time drift","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NXT4TTSM","conferencePaper","2018","Muehlbradt, Annika; Atreya, Madhur; Guinness, Darren; Kane, Shaun K.","Exploring the Design of Audio-Kinetic Graphics for Education","Proceedings of the 20th ACM International Conference on Multimodal Interaction","978-1-4503-5692-3","","10.1145/3242969.3243004","https://dl.acm.org/doi/10.1145/3242969.3243004","Creating tactile representations of visual information, especially moving images, is difficult due to a lack of available tactile computing technology and a lack of tools for authoring tactile information. To address these limitations, we developed a software framework that enables educators and other subject experts to create graphical representations that combine audio descriptions with kinetic motion. These audio-kinetic graphics can be played back using off-the-shelf computer hardware. We report on a study in which 10 educators developed content using our framework, and in which 18 people with vision impairments viewed these graphics on our output device. Our findings provide insights on how to translate knowledge of visual information to non-visual formats.","2018-10-02","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","455–463","","","","","","","ICMI '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/X6LR7EXV/Muehlbradt et al. - 2018 - Exploring the Design of Audio-Kinetic Graphics for.pdf","","","accessibility; education; tangible interfaces; blindness; collaboration; non-visual interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q6MP6AJS","conferencePaper","2017","Thapa, Ratan Bahadur; Ferati, Mexhid; Giannoumis, G. Anthony","Using non-speech sounds to increase web image accessibility for screen-reader users","Proceedings of the 35th ACM International Conference on the Design of Communication","978-1-4503-5160-7","","10.1145/3121113.3121231","https://dl.acm.org/doi/10.1145/3121113.3121231","Screen-reader users access images on the Web using alternative text delivered via synthetic speech. However, research shows that this is a tedious and unsatisfying experience for blind users, because text-to-speech applications lack expressiveness. This paper, poses an alternative approach using an experiment that compares audemes, a type of non-speech sounds, with alternative text delivered using synthetic speech. In a pilot study with fourteen sighted users, findings show that audemes perform better across many areas. Specifically, audemes required lower mental and temporal demands and led to less effort and frustration and better task performance. Moreover, participants recognized audemes with higher accuracy and lower errors. Audemes were also perceived as more engaging compared to alternative text delivered using synthetic speech. Additionally, audemes were found to be richer in delivering information. This study suggests that non-speech sounds could substitute or complement alternative text when describing images on the Web.","2017-08-11","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","1–9","","","","","","","SIGDOC '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/8LQB6CQT/Thapa et al. - 2017 - Using non-speech sounds to increase web image acce.pdf","","","accessibility; alternative text; non-speech sounds; screen-reader; web image","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YI7IZKWQ","conferencePaper","2023","Sharif, Ather; Zhang, Andrew M.; Reinecke, Katharina; Wobbrock, Jacob O.","Understanding and Improving Drilled-Down Information Extraction from Online Data Visualizations for Screen-Reader Users","Proceedings of the 20th International Web for All Conference","9798400707483","","10.1145/3587281.3587284","https://dl.acm.org/doi/10.1145/3587281.3587284","Inaccessible online data visualizations can significantly disenfranchise screen-reader users from accessing critical online information. Current accessibility measures, such as adding alternative text to visualizations, only provide a high-level overview of data, limiting screen-reader users from exploring data visualizations in depth. In this work, we build on prior research to develop taxonomies of information sought by screen-reader users to interact with online data visualizations granularly through role-based and longitudinal studies with screen-reader users. Utilizing these taxonomies, we extended the functionality of VoxLens—an open-source multi-modal system that improves the accessibility of data visualizations—by supporting drilled-down information extraction. We assessed the performance of our VoxLens enhancements through task-based user studies with 10 screen-reader and 10 non-screen-reader users. Our enhancements “closed the gap” between the two groups by enabling screen-reader users to extract information with approximately the same accuracy as non-screen-reader users, reducing interaction time by 22% in the process.","2023-04-30","2023-07-06 04:54:44","2023-07-06 04:54:44","2023-07-05","18–31","","","","","","","W4A '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/IB2B4LHM/Sharif et al. - 2023 - Understanding and Improving Drilled-Down Informati.pdf","","","accessibility; blind; Data visualization; screen reader; voice assistant.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PG7VCS7M","conferencePaper","2020","Klefeker, Josephine; striegl, libi; Devendorf, Laura","What HCI Can Learn from ASMR: Becoming Enchanted with the Mundane","Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","978-1-4503-6708-0","","10.1145/3313831.3376741","https://dl.acm.org/doi/10.1145/3313831.3376741","In this paper we explore how the qualities of Autonomous Sensory Meridian Response (ASMR) media - its pairing of sonic and visual design, ability to subvert fast-paced technology for slow experiences, production of somatic responses, and attention to the everyday-might reveal new design possibilities for interactions with wearable technology. We recount our year-long design inquiry into the subject which began with an interview with a ""live"" ASMR creator and design probes, a series of first-person design exercises, and resulted in the creation of two interactive garments for attending, noticing, and becoming enchanted with our our everyday surroundings. We conclude by suggesting that these ASMR inspired designs cultivate personal, intimate, embodied, and felt practices of attention within our everyday, mundane, environments.","2020-04-23","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","1–12","","","","","","What HCI Can Learn from ASMR","CHI '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/VJYCI5R8/Klefeker et al. - 2020 - What HCI Can Learn from ASMR Becoming Enchanted w.pdf","","","wearable technology; asmr media; enchantment; smart textiles; sonic interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2MLDU42F","conferencePaper","2012","Stienstra, Jelle; Alonso, Miguel Bruns; Wensveen, Stephan; Kuenen, Stoffel","How to design for transformation of behavior through interactive materiality","Proceedings of the 7th Nordic Conference on Human-Computer Interaction: Making Sense Through Design","978-1-4503-1482-4","","10.1145/2399016.2399020","https://dl.acm.org/doi/10.1145/2399016.2399020","This paper presents a design approach tackling the transformation of behavior through 'interactive materiality' from a phenomenological perspective. It builds upon the Interaction Frogger framework that couples action to reaction for intuitive mapping in intelligent product interaction. Through the discussion of two research-through-design cases, the augmented speed-skate experience and affective pen, it highlights the opportunities for design of an action-perception loop. Consequently, an approach is suggested that defines three steps to be incorporated in the design process: affirming and appreciating current behavior; designing continuous mapping for transformation; and fine-tuning sensitivities in the interactive materiality. Thereby, it discusses how behavior transformation through interactive materiality derived from a theoretical level, can contribute to design knowledge on the implementation level. The aim of this paper is to inspire design-thinking to shift from the cognitive approach of persuasion, to a meaningful and embodied mechanism respecting all human skills, by providing practical insights for designers.","2012-10-14","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","21–30","","","","","","","NordiCHI '12","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/7QM3I7YI/Stienstra et al. - 2012 - How to design for transformation of behavior throu.pdf","","","interaction design; action-perception loops; continuous mapping; interaction Frogger framework; transforming behavior","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HGX849C7","conferencePaper","2023","Zorea, Marine; Kushi, Katsuhiko","Audible Imagery: Creative Contemplations on the Sounds of Home","Proceedings of the 15th Conference on Creativity and Cognition","9798400701801","","10.1145/3591196.3593366","https://dl.acm.org/doi/10.1145/3591196.3593366","This pictorial paper explores domestic sounds’ capacity to evoke imagery of everyday inhabited spaces. In recent decades, accelerated digitalization has been transforming our everyday sonic environment, and new interactive possibilities, such as multimodal and embodied interaction, draw attention to sound as a communicative medium. In that context, how can artificial product sounds integrate into the present ecology heard at home? This pictorial includes visual impressions of domestic sounds as documented in Japan through a sound diary experiment, and those drawn by their listeners during a participatory exhibition in Tel-Aviv. The drawings offer insight into the creative sense-making strategies that listeners employ to situate acousmatic sounds within the interrelated composition of their everyday auditory experience. Furthermore, they demonstrate drawing as a unique research tool for constructing and sharing meaning during the product sound design process. By contemplating drawing as a creative output and a reflective cognition process, this initial multimodal exploration highlights the participatory nature of domestic sound interactions and the opportunities posed by drawing for designing future home soundscapes.","2023-06-19","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","150–161","","","","","","Audible Imagery","C&amp;C '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/C59P825I/Zorea and Kushi - 2023 - Audible Imagery Creative Contemplations on the So.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8AUZEYNB","conferencePaper","2021","Jain, Mohit; Diwakar, Nirmalendu; Swaminathan, Manohar","Smartphone Usage by Expert Blind Users","Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems","978-1-4503-8096-6","","10.1145/3411764.3445074","https://dl.acm.org/doi/10.1145/3411764.3445074","People with vision impairments access smartphones with the help of screen reader apps such as TalkBack for Android and VoiceOver for iPhone. Prior research has mostly focused on understanding touchscreen phone adoption and typing performance of novice blind users by logging their real-world smartphone usage. Understanding smartphone usage pattern and practices of expert users can help in developing tools and tutorials for transitioning novice and intermediate users to expert users. In this work, we logged smartphone usage data of eight expert Android smartphone users with visual impairments for four weeks, and then interviewed them. This paper presents a detailed analysis that uncovered novel usage patterns, such as extensive usage of directional gestures, reliance on voice and external keyboard for text input, and repurposed explore by touch for single-tap. We conclude with design recommendations to inform the future of mobile accessibility, including hardware guidelines and rethinking accessible software design.","2021-05-07","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","1–15","","","","","","","CHI '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/LFH4D6RF/Jain et al. - 2021 - Smartphone Usage by Expert Blind Users.pdf","","","accessibility; gestures; touchscreen; vision impairment; app usage; blind users; expert users.; security; smartphone; TalkBack; text entry; TTS; usage pattern","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G2SI6CM4","conferencePaper","2022","Marentakis, Georgios","Movement interaction with a loudspeaker: an index of possibilities","Proceedings of the 14th Conference on Creativity and Cognition","978-1-4503-9327-0","","10.1145/3527927.3533026","https://dl.acm.org/doi/10.1145/3527927.3533026","Creating sound by manipulating the location of sound-producing objects using gestures is an interesting interaction paradigm. To understand it better, we analyzed videos of users interacting with ‘Random Access Lattice’. In this sound installation, users move a loudspeaker to explore sound laid out in space using a time to space mapping. We performed an inductive analysis of user movement in relation to the visitor intention and the sonic outcome. We identified several body, hand, and grip gestures which were performed with different movement qualities to manipulate the loudspeaker at variable speeds, orientations, and body areas. These were used to search and trace the sonic material in a goal-oriented fashion but also to interact creatively with sound by looping and modulating sounds. We provide a visual index of our findings which can be used when designing gesture interactions with sound.","2022-06-20","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","247–261","","","","","","Movement interaction with a loudspeaker","C&amp;C '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/6NPULDBT/Marentakis - 2022 - Movement interaction with a loudspeaker an index .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QHWAXLVK","conferencePaper","2017","Niforatos, Evangelos; Fedosov, Anton; Elhart, Ivan; Langheinrich, Marc","Augmenting skiers' peripheral perception","Proceedings of the 2017 ACM International Symposium on Wearable Computers","978-1-4503-5188-1","","10.1145/3123021.3123052","https://dl.acm.org/doi/10.1145/3123021.3123052","The growing popularity of winter sports, as well as the trend towards high speed carving skis, have increased the risk of accidents on today's ski slopes. While many skiers now wear ski helmets, their bulk might in turn lower skiers' ability to sense their surroundings, potentially leading to dangerous situations. In this paper, we describe our ""Smart"" Ski Helmet (S-SH) prototype. S-SH uses a set of laser range finders mounted on the back to detect skiers approaching from behind and warns the wearer about potential collisions using three LEDs mounted at the helmet's front edge, slightly above the wearer's eye level. In this work, we describe a controlled experiment with 20 ski and snowboarding enthusiasts and a follow-up on-slope deployment with 6 additional participants of varying levels of expertise. Our findings indicate that the S-SH can significantly increase skiers' peripheral perception on traverse trails.","2017-09-11","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","114–121","","","","","","","ISWC '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/42RPDEFT/Niforatos et al. - 2017 - Augmenting skiers' peripheral perception.pdf","","","distance tracking; LIDAR; peripheral perception; peripheral vision; ski safety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RUHDMUNS","conferencePaper","2017","Van Tendeloo, Yentl; Van Mierlo, Simon; Meyers, Bart; Vangheluwe, Hans","Concrete syntax: a multi-paradigm modelling approach","Proceedings of the 10th ACM SIGPLAN International Conference on Software Language Engineering","978-1-4503-5525-4","","10.1145/3136014.3136017","https://dl.acm.org/doi/10.1145/3136014.3136017","Domain-Specific Modelling Languages (DSLs) allow domain experts to create models using abstractions they are most familiar with. A DSL's syntax is specified in two parts: the abstract syntax defines the language's concepts and their allowed combinations, and the concrete syntax defines how those concepts are presented to the user (typically using a graphical or textual notation). However important concrete syntax is for the usability of the language, current modelling tools offer limited possibilities for defining the mapping between abstract and concrete syntax. Often, the language designer is restricted to defining a single icon representation of each concept, which is then rendered to the user in a (fixed) graphical interface. This paper presents a framework that explicitly models the bi-directional mapping between the abstract and concrete syntax, thereby making these restrictions easy to overcome. It is more flexible and allows, amongst others, for a model to be represented in multiple front-ends, using multiple representation formats, and multiple mappings. Our approach is evaluated with an implementation in our prototype tool, the Modelverse, and by applying it on an example language.","2017-10-23","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","182–193","","","","","","Concrete syntax","SLE 2017","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/U96JIXLX/Van Tendeloo et al. - 2017 - Concrete syntax a multi-paradigm modelling approa.pdf","","","Abstract Syntax; Concrete Syntax; Model Transformation; Plotting; Simulation; Visual","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YXCQKUJ3","conferencePaper","2019","Hussain, Aishah; Modekjaer, Camilla; Austad, Nicoline Warming; Dahl, Sofia; Erkut, Cumhur","Evaluating movement qualities with visual feedback for real-time motion capture","Proceedings of the 6th International Conference on Movement and Computing","978-1-4503-7654-9","","10.1145/3347122.3347123","https://dl.acm.org/doi/10.1145/3347122.3347123","The focus of this paper is to investigate how the design of visual feedback on full body movement affects the quality of the movements. Informed by the theory of embodiment in interaction design and media technology, as well as by the Laban theory of effort, a computer application was implemented in which users are able to project their movements onto two visuals ('Particle' and 'Metal') We investigated whether the visual designs influenced movers through an experiment where participants were randomly assigned to one of the visuals while performing a set of simple tasks. Qualitative analysis of participants' verbal movement descriptions as well as analysis of quantitative movement features combine several perspectives with respect to describing the differences and the change in the movement qualities. The qualitative data shows clear differences between the groups. The quantitative data indicates that all groups move differently when visual feedback is provided. Our results contribute to the design effort of visual modality in movement-focused design of extended realities.","2019-10-10","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","1–9","","","","","","","MOCO '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/SPSLGD4N/Hussain et al. - 2019 - Evaluating movement qualities with visual feedback.pdf","","","Embodied interaction; Motion capture; Visualization models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N69L3AL8","conferencePaper","2019","Bennett, Cynthia L.; Stangl, Abigale; Siu, Alexa F.; Miele, Joshua A.","Making Nonvisually: Lessons from the Field","Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility","978-1-4503-6676-2","","10.1145/3308561.3355619","https://dl.acm.org/doi/10.1145/3308561.3355619","The Maker movement promises access to activities from crafting to digital fabrication for anyone to invent and customize technology. But people with disabilities, who could benefit from Making, still encounter significant barriers to do so. In response, we share our personal experiences Making nonvisually and supporting its instruction. Specifically, we draw on examples from a series of workshops where we introduced Arduino to blind hobbyists and guided assembly of an accessible voltmeter prototype [24]. In so doing, we offer future directions for accessible Making research and application.","2019-10-24","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","279–285","","","","","","Making Nonvisually","ASSETS '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/6M5XT5A3/Bennett et al. - 2019 - Making Nonvisually Lessons from the Field.pdf","","","accessibility; blind; arduino; making","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H82KD3UK","conferencePaper","2011","Kostiainen, Juho; Erkut, Cumhur; Piella, Ferran Boix","Design of an audio-based mobile journey planner application","Proceedings of the 15th International Academic MindTrek Conference: Envisioning Future Media Environments","978-1-4503-0816-8","","10.1145/2181037.2181056","https://dl.acm.org/doi/10.1145/2181037.2181056","Using public transportation is a context in which awareness of time is important. Providing information related to both time and place by means of ambient media can not only remove the need to keep looking at the time in order to depart on time but also create the feel of being in control, by knowing the time available and the progress of the journey. In the mobile context, hands and eyes are often occupied, which significantly limits the amount of information that can be provided by typical applications based on textual and graphical interfaces. In this paper, we introduce a mobile journey planner application that utilizes audio in providing useful information as well as the user experience prototyping and design process behind it.","2011-09-28","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","107–113","","","","","","","MindTrek '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/HAV87SH3/Kostiainen et al. - 2011 - Design of an audio-based mobile journey planner ap.pdf","","","auditory display; sonic interaction design; experience and video prototyping; journey and route planning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JH83AAPB","conferencePaper","2016","Hajinejad, Nassrin; Grüter, Barbara; Roque, Licínio; Bogutzky, Simon","GangKlang: facilitating a movement-oriented walking experience through sonic interaction","Proceedings of the Audio Mostly 2016","978-1-4503-4822-5","","10.1145/2986416.2986447","https://dl.acm.org/doi/10.1145/2986416.2986447","Sedentary lifestyle and resulting health problems have increased interest in design strategies to supports people's motivation for physical activity. In our experience-driven approach, we focus on renewing walkers' sensibility to embodied aesthetic qualities of the processing activity itself. In this paper, we present our steps to design sonic interaction tailored towards re-emphasizing the bodily dimension within walker's experience. We designed and developed a mobile application that makes the user's walking movement audible. Using our mobile application, users' walking movements trigger sounds that evolve into a dynamic soundscape changing in real time. We discuss initial findings on how listening to a walking-generated soundscape influences walkers' experience in terms of attention focus.","2016-10-04","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","202–208","","","","","","GangKlang","AM '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/6JU8ADJH/Hajinejad et al. - 2016 - GangKlang facilitating a movement-oriented walkin.pdf","","","Sonic interaction design; body-routines; walking experience; walking-generated soundscape; work song","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BQ46XKNP","conferencePaper","2019","Urbanek, Michael; Güldenpfennig, Florian","Celebrating 20 Years of Computer-based Audio Gaming","Proceedings of the 14th International Audio Mostly Conference: A Journey in Sound","978-1-4503-7297-8","","10.1145/3356590.3356605","https://dl.acm.org/doi/10.1145/3356590.3356605","We look back on two decades of academic research on audio games. During this time, a substantial amount of research has explored many facets of this special genre of computer games. However, despite many publications, there is a lack of review papers, which help delineate this growing research field. For this reason, we take one step back and investigate 20 years of audio game research by synthesizing a literature review adopting grounded theory methods. The resulting research map provides an overview of efforts into audio games with a special focus on how to design for audio games. We observed three important trends or tensions in audio game research. Firstly, audio games research depended heavily on technological advancements during the last two decades. Secondly, most studies about audio games were conducted with novices to audio games in lab situations, that is, based on artificial situations and not on real gamers and their genuine experience. Thirdly, the audio game design process per se has been greatly neglected in the literature so far. We conclude the paper by discussing design or research implications.","2019-09-18","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","90–97","","","","","","","AM'19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/P3N7KSCY/Urbanek and Güldenpfennig - 2019 - Celebrating 20 Years of Computer-based Audio Gamin.pdf","","","Review; Audio Games; Game Design; Grounded TheoryMethods","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"58ZH8Q3A","conferencePaper","2023","Fernandez, Cassia; De Deus Lopes, Roseli; Blikstein, Paulo","Programming Representations: Uncovering the Process of Constructing Data Visualizations in a Block-based Programming Environment","Proceedings of the 2023 Symposium on Learning, Design and Technology","9798400707360","","10.1145/3594781.3594783","https://dl.acm.org/doi/10.1145/3594781.3594783","In this paper, we analyze how middle schoolers engaged in data visualization activities using PlayData, an educational tool designed to create representations for data by taking advantage of the flexibility and low entry point of block-based programming environments. Drawing on the analysis of artifacts and videos collected during a three-day workshop, we explore the types of visualizations created by participants and the process they engaged with to produce visualizations. Although the representational forms chosen by students were mainly traditional, our findings indicate that they were engaged in authentic data visualization practices throughout their programming process. These practices included translating ideas into programs, selecting parameters (such as color scheme and space between data points), inspecting the output, and adding annotations to provide context and better communicate the desired information. Moreover, our analysis pointed out opportunities for improving PlayData, mainly by the addition of new primitives for automating labeling and performing data transformations.","2023-06-23","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","11–20","","","","","","Programming Representations","LDT '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/DVN4E8ZW/Fernandez et al. - 2023 - Programming Representations Uncovering the Proces.pdf","","","data visualization; block-based programming environments; computational thinking; data science education","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E4RC72JE","conferencePaper","2010","Monache, Stefano Delle; Polotti, Pietro; Rocchesso, Davide","A toolkit for explorations in sonic interaction design","Proceedings of the 5th Audio Mostly Conference: A Conference on Interaction with Sound","978-1-4503-0046-9","","10.1145/1859799.1859800","https://dl.acm.org/doi/10.1145/1859799.1859800","Physics-based sound synthesis represents a promising paradigm for the design of a veridical and effective continuous feedback in augmented everyday contexts. In this paper, we introduce the Sound Design Toolkit (SDT), a software package available as a complete front-end application, providing a palette of virtual lutheries and foley pits, that can be exploited in sonic interaction design research and education. In particular, the package includes polyphonic features and connectivity to multiple external devices and sensors in order to facilitate the embedding of sonic attributes in interactive artifacts. The present release represents an initial version towards an effective and usable tool for sonic interaction designers.","2010-09-15","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","1–7","","","","","","","AM '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/M3UZX89M/Monache et al. - 2010 - A toolkit for explorations in sonic interaction de.pdf","","","sonic interaction design; user interfaces; physics-based sound synthesis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MGI3WIBP","conferencePaper","2009","Jeon, Myounghoon; Davison, Benjamin K.; Nees, Michael A.; Wilson, Jeff; Walker, Bruce N.","Enhanced auditory menu cues improve dual task performance and are preferred with in-vehicle technologies","Proceedings of the 1st International Conference on Automotive User Interfaces and Interactive Vehicular Applications","978-1-60558-571-0","","10.1145/1620509.1620528","https://dl.acm.org/doi/10.1145/1620509.1620528","Auditory display research for driving has mainly focused on collision warning signals, and recent studies on auditory in-vehicle information presentation have examined only a limited range of tasks (e.g., cell phone operation tasks or verbal tasks such as reading digit strings). The present study used a dual task paradigm to evaluate a plausible scenario in which users navigated a song list. We applied enhanced auditory menu navigation cues, including spearcons (i.e., compressed speech) and a spindex (i.e., a speech index that used brief audio cues to communicate the user's position in a long menu list). Twenty-four undergraduates navigated through an alphabetized song list of 150 song titles---rendered as an auditory menu---while they concurrently played a simple, perceptual-motor, ball-catching game. The menu was presented with text-to-speech (TTS) alone, TTS plus one of three types of enhanced auditory cues, or no sound at all. Both performance of the primary task (success rate of the game) and the secondary task (menu search time) were better with the auditory menus than with no sound. Subjective workload scores (NASA TLX) and user preferences favored the enhanced auditory cue types. Results are discussed in terms of multiple resources theory and practical IVT design applications.","2009-09-21","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","91–98","","","","","","","AutomotiveUI '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/6ULH26WY/Jeon et al. - 2009 - Enhanced auditory menu cues improve dual task perf.pdf","","","auditory display; auditory menus; dual task; infotainment; IVTs (in-vehicle technologies); multiple resources; spearcon; spindex; TTS (text-to-speech)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZZCGHG2E","conferencePaper","2011","Morelli, Tony; Folmer, Eelke","Real-time sensory substitution to enable players who are blind to play video games using whole body gestures","Proceedings of the 6th International Conference on Foundations of Digital Games","978-1-4503-0804-5","","10.1145/2159365.2159385","https://dl.acm.org/doi/10.1145/2159365.2159385","Gesture-based interaction adds a new level of immersion to video games, but players who are blind are unable to play them as games use visual cues to indicate what gesture to provide and when. Though visual cues can be substituted with audio or haptic cues, this often requires access to the source code, which is not attainable for commercial games. We present a solution that uses real-time video analysis to detect the presence of a particular visual cue, which is then substituted with a vibrotactile cue that is provided with an external controller. A user study with 28 sighted participants with a popular commercial gesture based game showed no significant difference between visual and vibro-tactile cues, which demonstrates the feasibility of real-time sensory substitution as a cost-effective approach for making gesture-based video games accessible to players who are blind.","2011-06-29","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","147–153","","","","","","","FDG '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/AE6UWA3M/Morelli and Folmer - 2011 - Real-time sensory substitution to enable players w.pdf","","","accessibility; blind; exergames; gestures; visual impairments; 3D interaction; haptics; natural interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q5NTL9W6","conferencePaper","2015","Françoise, Jules; Roby-Brami, Agnès; Riboud, Natasha; Bevilacqua, Frédéric","Movement sequence analysis using hidden Markov models: a case study in Tai Chi performance","Proceedings of the 2nd International Workshop on Movement and Computing","978-1-4503-3457-0","","10.1145/2790994.2791006","https://dl.acm.org/doi/10.1145/2790994.2791006","Movement sequences are essential to dance and expressive movement practice; yet, they remain underexplored in movement and computing research, where the focus on short gestures prevails. We propose a method for movement sequence analysis based on motion trajectory synthesis with Hidden Markov Models. The method uses Hidden Markov Regression for jointly synthesizing motion feature trajectories and their associated variances, that serves as basis for investigating performers' consistency across executions of a movement sequence. We illustrate the method with a use-case in Tai Chi performance, and we further extend the approach to cross-modal analysis of vocalized movements.","2015-08-14","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","29–36","","","","","","Movement sequence analysis using hidden Markov models","MOCO '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/64V9K3T2/Françoise et al. - 2015 - Movement sequence analysis using hidden Markov mod.pdf","","","movement analysis; cross-modal analysis; hidden Markov models; hidden Markov regression; Tai Chi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EH6M2RUP","conferencePaper","2014","Nylander, Stina; Tholander, Jakob","Designing for Movement: the Case of Sports","Proceedings of the 2014 International Workshop on Movement and Computing","978-1-4503-2814-2","","10.1145/2617995.2618018","https://dl.acm.org/doi/10.1145/2617995.2618018","We have identified six themes we identified as interesting for future work in movement based interaction design for sports: the central position of the subjective feeling, the core of sports is enough, feeling did not prevent injury, non-interpretive representations, the shortcomings of logging biodata, and temporality of feedback. The themes are grounded in technical explorations for golf and running and a set of interviews with athletes. Here, we outline findings from our work to illustrate these themes.","2014-06-16","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","130–135","","","","","","Designing for Movement","MOCO '14","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/MPUE6MT2/Nylander and Tholander - 2014 - Designing for Movement the Case of Sports.pdf","","","sports; Movement; design; interaction; accelerometer data; feeling; interviews; sensors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KWKGRF7Z","conferencePaper","2008","Nesbitt, Keith V.; Hoskens, Ian","Multi-sensory game interface improves player satisfaction but not performance","Proceedings of the ninth conference on Australasian user interface - Volume 76","978-1-920682-57-6","","","","Players of computer games tend to be discerning about game quality. So, to be successful, game designers need to ensure that players receive the best possible experience. A growing trend in the design of game interfaces is the use of multi-sensory (visual, auditory and haptic) interfaces to broaden the experience for players. The assumption is that, by displaying different information to different senses, it is possible to increase the amount of information available to players and so assist their performance. To test this assumption, the first-person shooter game, ""Quake 3: Arena"", was evaluated in four modes: with only visual cues; with both visual and auditory cues; with both visual and haptic cues; and with visual, auditory and haptic cues. Players reported improved 'immersion', 'confidence' and 'satisfaction' when additional sensory cues were included, the multisensory game interface seemed to improve the player's experience, but there was no statistically significant improvement in their performance. We suspect that a better design of the information being displayed for each sense may be required if multi-sensory displays are to significantly improve the player's performance on specific game tasks.","2008-01-01","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","13–18","","","","","","","AUIC '08","","","","Australian Computer Society, Inc.","AUS","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/MCFJYAR3/Nesbitt and Hoskens - 2008 - Multi-sensory game interface improves player satis.pdf","","","multimodal; computer games; multi-sensory; user-interface design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L4Y8TH69","conferencePaper","2022","Sharif, Ather; Wang, Olivia H.; Muongchan, Alida T.; Reinecke, Katharina; Wobbrock, Jacob O.","VoxLens: Making Online Data Visualizations Accessible with an Interactive JavaScript Plug-In","Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems","978-1-4503-9157-3","","10.1145/3491102.3517431","https://dl.acm.org/doi/10.1145/3491102.3517431","JavaScript visualization libraries are widely used to create online data visualizations but provide limited access to their information for screen-reader users. Building on prior findings about the experiences of screen-reader users with online data visualizations, we present VoxLens, an open-source JavaScript plug-in that—with a single line of code—improves the accessibility of online data visualizations for screen-reader users using a multi-modal approach. Specifically, VoxLens enables screen-reader users to obtain a holistic summary of presented information, play sonified versions of the data, and interact with visualizations in a “drill-down” manner using voice-activated commands. Through task-based experiments with 21 screen-reader users, we show that VoxLens improves the accuracy of information extraction and interaction time by 122% and 36%, respectively, over existing conventional interaction with online data visualizations. Our interviews with screen-reader users suggest that VoxLens is a “game-changer” in making online data visualizations accessible to screen-reader users, saving them time and effort.","2022-04-29","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","1–19","","","","","","VoxLens","CHI '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/MX6KCFGC/Sharif et al. - 2022 - VoxLens Making Online Data Visualizations Accessi.pdf","","","accessibility; blind; low-vision.; screen readers; Visualizations; voice-based interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SIN5PXWK","conferencePaper","2009","Abu Doush, Iyad; Pontelli, Enrico; Simon, Dominic; Son, Tran Cao; Ma, Ou","Making Microsoft Excel™: multimodal presentation of charts","Proceedings of the 11th international ACM SIGACCESS conference on Computers and accessibility","978-1-60558-558-1","","10.1145/1639642.1639669","https://dl.acm.org/doi/10.1145/1639642.1639669","Several solutions, based on aural and haptic feedback, have been developed to enable access to complex on-line information for people with visual impairments. Nevertheless, there are several components of widely used software applications that are still beyond the reach of screen readers and Braille displays. This paper investigates the non-visual accessibility issues associated with the graphing component of Microsoft Excel"". The goal is to provide flexible multi-modal navigation schemes which can help visually impaired users in comprehending Excel charts. The methodology identifies the need for 3 strategies used in interaction: exploratory, guided, and summarization. Switching between them supports the development of a mental model of a chart. Aural cues and commentaries are integrated in a haptic presentation to help understanding the presented chart. The methodology has been implemented using the Novint Falcon haptic device.","2009-10-25","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","147–154","","","","","","Making Microsoft Excel™","Assets '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/7XJM2IS6/Abu Doush et al. - 2009 - Making Microsoft Excel™ multimodal presentation o.pdf","","","assistive technology; haptic; accessible graphs","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3IISIQHD","conferencePaper","2015","Drossos, Konstantinos; Zormpas, Nikolaos; Giannakopoulos, George; Floros, Andreas","Accessible games for blind children, empowered by binaural sound","Proceedings of the 8th ACM International Conference on PErvasive Technologies Related to Assistive Environments","978-1-4503-3452-5","","10.1145/2769493.2769546","https://dl.acm.org/doi/10.1145/2769493.2769546","Accessible games have been researched and developed for many years, however, blind people still have very limited access and knowledge of them. This can pose a serious limitation, especially for blind children, since in recent years electronic games have become one of the most common and wide spread means of entertainment and socialization. For our implementation we use binaural technology which allows the player to hear and navigate the game space by adding localization information to the game sounds. With our implementation and user studies we provide insight on what constitutes an accessible game for blind people as well as a functional game engine for such games. The game engine developed allows the quick development of games for the visually impaired. Our work provides a good starting point for future developments on the field and, as the user studies show, was very well perceived by the visually impaired children that tried it.","2015-07-01","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","1–8","","","","","","","PETRA '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/AILZXNCN/Drossos et al. - 2015 - Accessible games for blind children, empowered by .pdf","","","audio only games; auditory interface; binaural processing; games for the visually impaired","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T654X9H7","conferencePaper","2014","Simões, Diogo; Cavaco, Sofia","An orientation game with 3D spatialized audio for visually impaired children","Proceedings of the 11th Conference on Advances in Computer Entertainment Technology","978-1-4503-2945-3","","10.1145/2663806.2663868","https://dl.acm.org/doi/10.1145/2663806.2663868","In this article, we propose an educational mobile game designed to help visually impaired children to develop orientation skills. These skills are usually trained at orientation and mobility classes for special needs children. The proposed game can be played on touch screen mobile devices and can be used in class or after classes. It uses a child appropriate theme and it aims at training children to perform accurate sound localization, while distinguishing concepts like front/back, left/right, close/far, etc. We have had very promising results from a preliminary test with blind and low vision students. Training these concepts on an entertaining environment can have very positive outcomes as it motivates children to spend more time training and at the same time allows children to forget that they need to train due to their special needs.","2014-11-11","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","1–4","","","","","","","ACE '14","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/BGSMYPTJ/Simões and Cavaco - 2014 - An orientation game with 3D spatialized audio for .pdf","","","visual impairment; android game; educational game; orientation and mobility","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NGFAJS3I","conferencePaper","2017","Çamcı, Anil; Lee, Kristine; Roberts, Cody J.; Forbes, Angus G.","INVISO: A Cross-platform User Interface for Creating Virtual Sonic Environments","Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology","978-1-4503-4981-9","","10.1145/3126594.3126644","https://dl.acm.org/doi/10.1145/3126594.3126644","The predominant interaction paradigm of current audio spatialization tools, which are primarily geared towards expert users, imposes a design process in which users are characterized as stationary, limiting the application domain of these tools. Navigable 3D sonic virtual realities, on the other hand, can support many applications ranging from soundscape prototyping to spatial data representation. Although modern game engines provide a limited set of audio features to create such sonic environments, the interaction methods are inherited from the graphical design features of such systems, and are not specific to the auditory modality. To address such limitations, we introduce INVISO, a novel web-based user interface for designing and experiencing rich and dynamic sonic virtual realities. Our interface enables both novice and expert users to construct complex immersive sonic environments with 3D dynamic sound components. INVISO is platform-independent and facilitates a variety of mixed reality applications, such as those where users can simultaneously experience and manipulate a virtual sonic environment. In this paper, we detail the interface design considerations for our audio-specific VR tool. To evaluate the usability of INVISO, we conduct two user studies: The first demonstrates that our visual interface effectively facilitates the generation of creative audio environments; the second demonstrates that both expert and non-expert users are able to use our software to accurately recreate complex 3D audio scenes.","2017-10-20","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","507–518","","","","","","INVISO","UIST '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/SZEWGM28/Çamcı et al. - 2017 - INVISO A Cross-platform User Interface for Creati.pdf","","","virtual reality; 3D audio; browser-based interface; design environment; virtual sonic environments","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RMVWQEQF","conferencePaper","2020","Moore, Dylan; Currano, Rebecca; Sirkin, David","Sound Decisions: How Synthetic Motor Sounds Improve Autonomous Vehicle-Pedestrian Interactions","12th International Conference on Automotive User Interfaces and Interactive Vehicular Applications","978-1-4503-8065-2","","10.1145/3409120.3410667","https://dl.acm.org/doi/10.1145/3409120.3410667","Electric vehicles’ (EVs) nearly silent operation has proved to be dangerous for bicyclists and pedestrians, who often use an internal combustion engine’s sound as one of many signals to locate nearby vehicles and predict their behavior. Inspired by regulations currently being implemented that will require EVs and hybrid vehicles (HVs) to play synthetic sound, we used a Wizard-of-Oz AV setup to explore how adding synthetic engine sound to a hybrid autonomous vehicle (AV) will influence how pedestrians interact with the AV in a naturalistic field study. Pedestrians reported increased interaction quality and clarity of intent of the vehicle to yield compared to a baseline condition without any added sound. These findings suggest that synthetic engine sound will not only be effective at helping pedestrians to hear EVs, but also may help AV developers implicitly signal to pedestrians when the vehicle will yield.","2020-09-20","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","94–103","","","","","","Sound Decisions","AutomotiveUI '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/UTYVLQQS/Moore et al. - 2020 - Sound Decisions How Synthetic Motor Sounds Improv.pdf","","","Autonomous vehicles; Driverless cars; External human-machine interfaces; Ghostdriver; Implicit interaction; Pedestrian interaction; Sound design; Wizard-of-Oz","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XSBE9YFL","conferencePaper","2010","Morelli, Tony; Foley, John; Folmer, Eelke","Vi-bowling: a tactile spatial exergame for individuals with visual impairments","Proceedings of the 12th international ACM SIGACCESS conference on Computers and accessibility","978-1-60558-881-0","","10.1145/1878803.1878836","https://dl.acm.org/doi/10.1145/1878803.1878836","Lack of sight forms a significant barrier to participate in physical activity. Consequently, individuals with visual impairments are at greater risk for developing serious health problems, such as obesity. Exergames are video games that provide physical exercise. For individuals with visual impairments, exergames have the potential to reduce health disparities as they may be safer to play and can be played without the help of others. This paper presents VI Bowling, a tactile/audio exergame that can be played using an inexpensive motion-sensing controller. VI Bowling explores tactile dowsing: a novel technique for performing spatial sensorimotor challenges, which can be used for motor learning. VI Bowling was evaluated with six blind adults. All players enjoyed VI Bowling and the challenge tactile dowsing provided. Players could throw their ball with an average error of 9.76 degrees using tactile dowsing. Participants achieved an average active energy expenditure of 4.61 kJ/Min while playing VI Bowling, which is comparable to walking.","2010-10-25","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","179–186","","","","","","Vi-bowling","ASSETS '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/YYQRPY4L/Morelli et al. - 2010 - Vi-bowling a tactile spatial exergame for individ.pdf","","","exergames; visual impairments; health; haptics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"743E8QBH","conferencePaper","2010","Dib, Lina; Petrelli, Daniela; Whittaker, Steve","Sonic souvenirs: exploring the paradoxes of recorded sound for family remembering","Proceedings of the 2010 ACM conference on Computer supported cooperative work","978-1-60558-795-0","","10.1145/1718918.1718985","https://dl.acm.org/doi/10.1145/1718918.1718985","Many studies have explored social processes and technologies associated with sharing photos. In contrast, we explore the role of sound as a medium for social reminiscing. We involved 10 families in recording 'sonic souvenirs' of their holidays. They shared and discussed their collections on their return. We compared these sounds with their photo taking activities and reminiscences. Both sounds and pictures triggered active collaborative reminiscing, and attempts to capture iconic representations of events. However sounds differed from photos in that they were more varied, familial and creative. Further, they often expressed the negative or mundane in order to be 'true to life', and were harder to interpret than photos. Finally we saw little use of pure explanatory narrative. We reflect on the relations between sound and family memory and propose new designs on the basis of our findings, to better support the sharing and manipulation of social sounds.","2010-02-06","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","391–400","","","","","","Sonic souvenirs","CSCW '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/B9C6GMZ5/Dib et al. - 2010 - Sonic souvenirs exploring the paradoxes of record.pdf","","","collaborative remembering; collective memory; families; fieldwork.; photos; sounds","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LFFUJE9I","conferencePaper","2008","Coleman, Graeme W.; Macaulay, Catriona; Newell, Alan F.","Sonic mapping: towards engaging the user in the design of sound for computerized artifacts","Proceedings of the 5th Nordic conference on Human-computer interaction: building bridges","978-1-59593-704-9","","10.1145/1463160.1463170","https://dl.acm.org/doi/10.1145/1463160.1463170","This paper argues for new approaches to the design of sound for contemporary interactive technologies. We begin by presenting what we feel to be the key challenges as yet unaddressed by conventional auditory display research. Subsequently, we propose a user-centered, acoustic ecology-informed, design method that we feel can be built upon to inform the design of sound for contemporary interactive technologies, thus tackling some of the challenges introduced. Our approach consists of three stages: firstly, encouraging designers and users to experience the original auditory environment, identifying the key sounds within that environment, and then summarizing this information into an 'earwitness account' that can be used as a prelude for informing the design of sonically enhanced technologies that may be used within similar environments. By trialing this method with undergraduate interactive media design students, we identify the methodological challenges involved in attempting to engage people, who are not necessarily 'sound professionals', with their existing auditory environments. We highlight the opportunities that arise and pitfalls that should be avoided when attempting to introduce such approaches within real-world design studies.","2008-10-20","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","83–92","","","","","","Sonic mapping","NordiCHI '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/9MFZ4QHH/Coleman et al. - 2008 - Sonic mapping towards engaging the user in the de.pdf","","","acoustic ecology; soundscapes; auditory environments; auditory interfaces; non-speech sound; user-centered design methods","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6X9CNPN7","conferencePaper","2017","Mueller, Florian 'Floyd'; Tan, Chek Tien; Byrne, Rich; Jones, Matt","13 Game Lenses for Designing Diverse Interactive Jogging Systems","Proceedings of the Annual Symposium on Computer-Human Interaction in Play","978-1-4503-4898-0","","10.1145/3116595.3116607","https://dl.acm.org/doi/10.1145/3116595.3116607","HCI is increasingly interested in designing technology for being physically active, and in many cases focuses on jogging. We find that many current approaches seem to view jogging only through a lens of athletic performance. However, jogging is multifaceted, yet there is so far no collated list of alternative lenses through which jogging could be viewed at by designers. In this paper, we draw on game design thinking to articulate 13 lenses through which designers can examine jogging. These 13 lenses are derived from related work and our combined experience of having designed and studied three different jogging systems. The lenses enable a structured articulation of key opportunities that interactive technology offers for jogging designers. With our work, we aim to support designers who want to create diverse interactive jogging systems so that more people can profit from the many benefits of jogging.","2017-10-15","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","43–56","","","","","","","CHI PLAY '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/WS9GPZJL/Mueller et al. - 2017 - 13 Game Lenses for Designing Diverse Interactive J.pdf","","","exertion; jogging; running; sport; whole-body interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G7WX6BJG","conferencePaper","2020","Zhang, Yichi; Hu, Junbo; Zhang, Yiting; Pardo, Bryan; Duan, Zhiyao","Vroom! A Search Engine for Sounds by Vocal Imitation Queries","Proceedings of the 2020 Conference on Human Information Interaction and Retrieval","978-1-4503-6892-6","","10.1145/3343413.3377963","https://dl.acm.org/doi/10.1145/3343413.3377963","Traditional search through collections of audio recordings compares a text-based query to text metadata associated with each audio file and does not address the actual content of the audio. Text descriptions do not describe all aspects of the audio content in detail. Query by vocal imitation (QBV) is a kind of query by example that lets users imitate the content of the audio they seek, providing an alternative search method to traditional text search. Prior work proposed several neural networks, such as TL-IMINET, for QBV, however, previous systems have not been deployed in an actual search engine nor evaluated by real users. We have developed a state-of-the-art QBV system (Vroom!) and a baseline query-by-text search engine (TextSearch). We deployed both systems in an experimental framework to perform user experiments with Amazon Mechanical Turk (AMT) workers. Results showed that Vroom! received significantly higher search satisfaction ratings than TextSearch did for sound categories that were difficult for subjects to describe by text. Results also showed a better overall ease-of-use rating for Vroom! than TextSearch on the sound library used in our experiments. These findings suggest that QBV, as a complimentary search approach to existing text-based search, can improve both search results and user experience.","2020-03-14","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","23–32","","","","","","","CHIIR '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/RLECYAUP/Zhang et al. - 2020 - Vroom! A Search Engine for Sounds by Vocal Imitati.pdf","","","siamese style convolutional recurrent neural networks; sound search; subjective evaluation; text description; vocal imitation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EXRAYXX7","conferencePaper","2018","Smith, Taliesin L.; Greenberg, Jesse; Reid, Sam; Moore, Emily B.","Parallel DOM Architecture for Accessible Interactive Simulations","Proceedings of the 15th International Web for All Conference","978-1-4503-5651-0","","10.1145/3192714.3192817","https://dl.acm.org/doi/10.1145/3192714.3192817","Interactive simulations are used in classrooms around the world to support student learning. Creating accessible interactive simulations is a complex challenge that pushes the boundaries of current accessibility approaches and standards. In this work, we present a new approach to addressing accessibility needs within complex interactives. Within a custom scene graph that utilizes a model-view-controller architectural pattern, we utilize a parallel document object model (PDOM) to create interactive simulations (PhET Interactive Simulations) accessible to students through alternative input devices and descriptions accessed with screen reader software. In this paper, we describe our accessibility goals, challenges, and approach to creating robust accessible interactive simulations, and provide examples from an accessible simulation we have developed and possibilities for future extensions.","2018-04-23","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","1–8","","","","","","","W4A '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/89VTC3GI/Smith et al. - 2018 - Parallel DOM Architecture for Accessible Interacti.pdf","","","accessibility; visual impairments; alternative input; Interactive simulations; software architectures","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NVPP9WI6","conferencePaper","2018","Morris, Meredith Ringel; Johnson, Jazette; Bennett, Cynthia L.; Cutrell, Edward","Rich Representations of Visual Content for Screen Reader Users","Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","978-1-4503-5620-6","","10.1145/3173574.3173633","https://dl.acm.org/doi/10.1145/3173574.3173633","Alt text (short for ""alternative text"") is descriptive text associated with an image in HTML and other document formats. Screen reader technologies speak the alt text aloud to people who are visually impaired. Introduced with HTML 2.0 in 1995, the alt attribute has not evolved despite significant changes in technology over the past two decades. In light of the expanding volume, purpose, and importance of digital imagery, we reflect on how alt text could be supplemented to offer a richer experience of visual content to screen reader users. Our contributions include articulating the design space of representations of visual content for screen reader users, prototypes illustrating several points within this design space, and evaluations of several of these new image representations with people who are blind. We close by discussing the implications of our taxonomy, prototypes, and user study findings.","2018-04-19","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","1–11","","","","","","","CHI '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/FTN6T94Y/Morris et al. - 2018 - Rich Representations of Visual Content for Screen .pdf","","","accessibility; visual impairment; blindness; alternative text; screen readers; alt text; captions","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L3DU75YD","conferencePaper","2018","Basman, Antranig; Tchernavskij, Philip; Bates, Simon; Beaudouin-Lafon, Michel","An anatomy of interaction: co-occurrences and entanglements","Companion Proceedings of the 2nd International Conference on the Art, Science, and Engineering of Programming","978-1-4503-5513-1","","10.1145/3191697.3214328","https://dl.acm.org/doi/10.1145/3191697.3214328","We present a new taxonomy for describing the conditions and implementation of interactions. Current mechanisms for embedding interaction in software promote a hard separation between the programmers who produce the software, and the communities who go on to use it. In order to support open ecologies of function and fabrication, where this separation is negotiated by communities of users and designers, we need to reconceive those mechanisms. We describe interaction in two phases: Co-occurrence, the prerequisite conditions for an interaction to take place; and entanglement, the temporary coupling and interplay between elements participating in the interaction. We then sketch a blueprint of a system where those phases and their adjacent mechanisms enable communities of users to build and use interactive software. There are many ways of conceiving this new design space, and we present three dominant metaphors which we have employed so far, based on chemical reactions, quantum physics and cooking. We exhibit different systems which we have implemented based on these metaphors, and sketch how future systems will further empower citizens to design and inhabit their own interactions, express ownership over them and share them with communities of interest.","2018-04-09","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","188–196","","","","","","An anatomy of interaction","Programming '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/URYZI3V2/Basman et al. - 2018 - An anatomy of interaction co-occurrences and enta.pdf","","","interaction architectures; Interaction models; interaction paradigms","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FY53A6EV","conferencePaper","2014","Östblad, Per Anders; Engström, Henrik; Brusk, Jenny; Backlund, Per; Wilhelmsson, Ulf","Inclusive game design: audio interface in a graphical adventure game","Proceedings of the 9th Audio Mostly: A Conference on Interaction With Sound","978-1-4503-3032-9","","10.1145/2636879.2636909","https://dl.acm.org/doi/10.1145/2636879.2636909","A lot of video games on the market are inaccessible to players with visual impairments because they rely heavily on use of graphical elements. This paper presents a project aimed at developing a point-and-click adventure game for smart phones and tablets that is equally functional and enjoyable by blind and sighted players. This will be achieved by utilizing audio to give blind players all necessary information and enjoyment without graphics. In addition to creating the game, the aim of the project is to identify design aspects that can be applied to more types of games to include more players. This paper also presents a pilot study that has been conducted on an early version of the game and the preliminary findings are discussed.","2014-10-01","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","1–8","","","","","","Inclusive game design","AM '14","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/CZT5DTVK/Östblad et al. - 2014 - Inclusive game design audio interface in a graphi.pdf","","","inclusive design; audiogame; mobile game","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F34349SZ","conferencePaper","2018","Gang, Nick; Sibi, Srinath; Michon, Romain; Mok, Brian; Chafe, Chris; Ju, Wendy","Don't Be Alarmed: Sonifying Autonomous Vehicle Perception to Increase Situation Awareness","Proceedings of the 10th International Conference on Automotive User Interfaces and Interactive Vehicular Applications","978-1-4503-5946-7","","10.1145/3239060.3265636","https://dl.acm.org/doi/10.1145/3239060.3265636","Lack of trust can arise when people do not know what autonomous vehicles perceive in the environment. To convey this information without causing alarm or compelling people to act, we designed and evaluated a way to sonify an autonomous vehicle's perception of salient driving events using abstract auditory icons, or ""earcons."" These are localized in space using an in-car quadraphonic speaker array to correspond with the direction of events. We describe the interaction design for these awareness cues and a validation experiment (N=28) examining the effects of sonified events on drivers' sense of situation awareness, comfort, and trust. Overall, this work suggests that our designed earcons do improve people's awareness of in-simulation events. The effect of the increased situational awareness on trust and comfort is inconclusive. However, post-study design feedback suggests that sounds should have low levels of intensity and dissonance, and a sense of belonging to a common family.","2018-09-23","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","237–246","","","","","","Don't Be Alarmed","AutomotiveUI '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/JHEBEFEW/Gang et al. - 2018 - Don't Be Alarmed Sonifying Autonomous Vehicle Per.pdf","","","situation awareness; spatial audio; interaction design; Sound design; autonomous vehicles; comfort; trust","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KG9RQ9ZW","conferencePaper","2020","Pohl, Henning; Dalsgaard, Tor-Salve; Krasniqi, Vesa; Hornbæk, Kasper","Body LayARs: A Toolkit for Body-Based Augmented Reality","Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology","978-1-4503-7619-8","","10.1145/3385956.3418946","https://dl.acm.org/doi/10.1145/3385956.3418946","Technological advances are enabling a new class of augmented reality (AR) applications that use bodies as substrates for input and output. In contrast to sensing and augmenting objects, body-based AR applications track people around the user and layer information on them. However, prototyping such applications is complex, time-consuming, and cumbersome, due to a lack of easily accessible tooling and infrastructure. We present Body LayARs, a toolkit for fast development of body-based AR prototypes. Instead of directly programming for a device, Body LayARs provides an extensible graphical programming environment with a device-independent runtime abstraction. We focus on face-based experiences for headset AR, and show how Body LayARs makes a range of body-based AR applications fast and easy to prototype.","2020-11-01","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","1–11","","","","","","Body LayARs","VRST '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/A97SCBY7/Pohl et al. - 2020 - Body LayARs A Toolkit for Body-Based Augmented Re.pdf","","","Augmented reality; toolkit; body-based augmentation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WCVJF57K","conferencePaper","2020","Masu, Raul; Correia, Nuno N.; Jurgens, Stephan; Druzetic, Ivana; Primett, William","How do Dancers Want to Use Interactive Technology? Appropriation and Layers of Meaning Beyond Traditional Movement Mapping","Proceedings of the 9th International Conference on Digital and Interactive Arts","978-1-4503-7250-3","","10.1145/3359852.3359869","https://dl.acm.org/doi/10.1145/3359852.3359869","There has been an increased interest in HCI research regarding the possibilities of interactive technology applied to the field of dance performance, particularly contemporary dance. This has produced numerous strategies to capture data from the moving bodies of the dancers and to map that data into different types of display formats. In this paper, we look at the role of interactive technology in dance performance from a broader perspective, aiming at understanding the needs of dancers and their relation with the audience. To this end, we ran a focus group with ten dancers with expertise in technology. We analysed the focus group using thematic analysis. We discuss the implications for design of our results by framing the role of technology in dance performance, proposing design guidelines related to the communication to the audience, use of technology, and mapping. Moreover, we propose different levels of ambiguity and appropriation related to the creators of the performance and the audience.","2020-02-13","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","1–9","","","","","","How do Dancers Want to Use Interactive Technology?","ARTECH 2019","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/UTUAFLJQ/Masu et al. - 2020 - How do Dancers Want to Use Interactive Technology.pdf","","","HCI; Dance performance; design guidelines; UCD","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y3AIN6NX","conferencePaper","2020","Dmitrenko, Dmitrijs; Maggioni, Emanuela; Brianza, Giada; Holthausen, Brittany E.; Walker, Bruce N.; Obrist, Marianna","CARoma Therapy: Pleasant Scents Promote Safer Driving, Better Mood, and Improved Well-Being in Angry Drivers","Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","978-1-4503-6708-0","","10.1145/3313831.3376176","https://dl.acm.org/doi/10.1145/3313831.3376176","Driving is a task that is often affected by emotions. The effect of emotions on driving has been extensively studied. Anger is an emotion that dominates in such investigations. Despite the knowledge on strong links between scents and emotions, few studies have explored the effect of olfactory stimulation in a context of driving. Such an outcome provides HCI practitioners very little knowledge on how to design for emotions using olfactory stimulation in the car. We carried out three studies to select scents of different valence and arousal levels (i.e. rose, peppermint, and civet) and anger eliciting stimuli (i.e. affective pictures and on-road events). We used this knowledge to conduct the fourth user study investigating how the selected scents change the emotional state, well-being, and driving behaviour of drivers in an induced angry state. Our findings enable better decisions on what scents to choose when designing interactions for angry drivers.","2020-04-23","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","1–13","","","","","","CARoma Therapy","CHI '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/DK4NTG4A/Dmitrenko et al. - 2020 - CARoma Therapy Pleasant Scents Promote Safer Driv.pdf","","","emotions; multimodal interfaces; perception; in-car user interfaces; notification systems; odour stimulation; smell","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HD6ZK886","conferencePaper","2017","Thieme, Anja; Morrison, Cecily; Villar, Nicolas; Grayson, Martin; Lindley, Siân","Enabling Collaboration in Learning Computer Programing Inclusive of Children with Vision Impairments","Proceedings of the 2017 Conference on Designing Interactive Systems","978-1-4503-4922-2","","10.1145/3064663.3064689","https://dl.acm.org/doi/10.1145/3064663.3064689","We investigate how technology can support collaborative learning by children with mixed-visual abilities. Responding to a growing need for tools inclusive of children with vision impairments (VI) for the teaching of computer programing to novice learners, we explore Torino -- a physical programing language for teaching programing constructs and computational thinking to children age 7-11. We draw insights from 12 learning sessions with Torino that involved five pairs of children with vision ranging from blindness to full-sight. Our findings show how sense-making of the technology, collaboration, and learning were enabled through an interplay of system design, programing tasks and social interactions, and how this differed between the pairs. The paper contributes insights on the role of touch, audio and visual representations in designs inclusive of people with VI, and discusses the importance and opportunities provided through the 'social' in negotiations of accessibility, for learning, and for self-perceptions of ability and self-esteem.","2017-06-10","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","739–752","","","","","","","DIS '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/CBK5LFJW/Thieme et al. - 2017 - Enabling Collaboration in Learning Computer Progra.pdf","","","accessibility; education; visual impairment; collaboration; design for children; tactility; tangibility; visual disability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QSCQ3ACD","conferencePaper","2013","Schofield, Tom; Dörk, Marian; Dade-Robertson, Martyn","Indexicality and visualization: exploring analogies with art, cinema and photography","Proceedings of the 9th ACM Conference on Creativity & Cognition","978-1-4503-2150-1","","10.1145/2466627.2466641","https://dl.acm.org/doi/10.1145/2466627.2466641","In this paper we offer a critical discussion of data visualization by adapting theories of indexicality as discussed in semiotics and art history. An indexical statement is broadly one whose meaning is dependent on context. We examine how indexicality has informed practices in cinema, photography, and contemporary art and make comparisons with data visualization. Specifically, we explore how these analogies can result in generative concepts that can inform the design and study of data visualization.","2013-06-17","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","175–184","","","","","","Indexicality and visualization","C&amp;C '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/BLNWDU3K/Schofield et al. - 2013 - Indexicality and visualization exploring analogie.pdf","","","data visualization; cinema; data; photography; semiotics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5BWB9IND","conferencePaper","2023","Das, Maitraye; Gergle, Darren; Piper, Anne Marie","Simphony: Enhancing Accessible Pattern Design Practices among Blind Weavers","Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","978-1-4503-9421-5","","10.1145/3544548.3581047","https://dl.acm.org/doi/10.1145/3544548.3581047","The maker movement has garnered significant attention as democratizing design; yet, recent work has called attention to the challenges disabled people encounter in making. Although researchers have built systems to improve accessibility of maker technologies, limited studies have centered disabled people’s engagement in traditional forms of making like fiber arts. We examine the practice of fabric pattern design among a community of blind weavers who create hand-woven products with sighted instructors. Grounded in seventeen interviews with blind weavers and sighted instructors, we built Simphony, an audio-tactile system that aims to support blind weavers in creating and perceiving patterns. Findings from eight design exploration sessions at the community studio reveal how blind weavers used Simphony to learn the process of pattern design and generate patterns with sighted instructors. We reflect on collaborative understanding of pattern design among blind and sighted individuals and discuss opportunities for integrating technological augmentations into traditional craftwork.","2023-04-19","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","1–19","","","","","","Simphony","CHI '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/DYB6Y43N/Das et al. - 2023 - Simphony Enhancing Accessible Pattern Design Prac.pdf","","","accessibility; blind; design; making; Disability; weaving","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TN4IYQLH","conferencePaper","2021","Cochrane, Karen; Loke, Lian; Leete, Matthew; Campbell, Andrew; Ahmadpour, Naseem","Understanding the First Person Experience of Walking Mindfulness Meditation Facilitated by EEG Modulated Interactive Soundscape","Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction","978-1-4503-8213-7","","10.1145/3430524.3440637","https://dl.acm.org/doi/10.1145/3430524.3440637","Walking meditation is a form of mindfulness training, where the act of walking provides a rhythmic meter for attentional focus. Whilst digital technologies to support sitting meditation and walking practices exist, less explored is the first person in-the-moment experience of technology-mediated walking meditation. We present a study of group walking meditation, with and without an interactive rhythmic soundscape modulated by one practitioner’s brainwave data. Six workshops were conducted with novice and advanced practitioners, involving a guided walking meditation with body scan, writing and drawing exercises and a group interview. The analysis yielded themes of shifting state, attention, self-regulation strategy, and immersion and reflection, and insights into how practitioners use sound to synchronize both walking and breathing. We contribute a method for eliciting, and a novel description of, the first person experience of walking meditation, as resources for the design of interactive technologies to support mindfulness practices of walking meditation.","2021-02-14","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","1–17","","","","","","","TEI '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/9U9F8ZEU/Cochrane et al. - 2021 - Understanding the First Person Experience of Walki.pdf","","","design; soundscape; first person; lived experience; mindfulness; phenomenology; walking meditation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KJX454YA","conferencePaper","2022","Oyallon-Koloski, Jenny; Junokas, Michael","Enhancing Film Choreography Through Digital Representation of Camera Movement and Agency","Proceedings of the 8th International Conference on Movement and Computing","978-1-4503-8716-3","","10.1145/3537972.3537990","https://dl.acm.org/doi/10.1145/3537972.3537990","Cinematography—aspect ratios, framing, and camera movement, especially—plays an essential role in film choreography. However, the camera’s capacity to expand the aesthetics of dance on film and contribute to the dynamism of figure movement is limited. In commercial (profit-oriented) cinematic practice, this limitation is bounded by the physical properties of the equipment, accessibility issues, limited time and financing for stylistic experimentation, and institutional memory loss of cinematic choreography techniques. The result in much of contemporary commercial dance on film is an emphasis on multi-camera coverage, tighter framings, and the use of editing to guide the dynamism of the figure movement in lieu of an emphasis on the relationship between the camera and the body. In this paper, we present theoretical frameworks and motion-capture driven utilities that empower the filmic choreographer beyond traditional, physical limits of the medium. We do so by providing digital representations and interactions using abstracted, artificial systems mimicking the live camera-dancer relationship that prioritize cinematic agency and movement as the principal subject material. This development parallels the growth of previsualization and camera motion control in the field of visual effects, linking us conceptually to existing industrial paradigms. Our initial foray into the expansion of this agency defines a progressive development of filmic choreography, escaping narrow limits of the traditional medium and provoking a more inclusive, accessible, and empowered form of creation through novel access of conventionally unmeasurable capability.","2022-06-30","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","1–5","","","","","","","MOCO '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/BTJK5KFY/Oyallon-Koloski and Junokas - 2022 - Enhancing Film Choreography Through Digital Repres.pdf","","","dance; cinema; agency; cinematography; Laban/Bartenieff Movement Studies; motion control; motion-capture","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"62LVJCCX","conferencePaper","2015","Mealin, Sean; Winters, Mike; Domínguez, Ignacio X.; Marrero-García, Michelle; Bozkurt, Alper; Sherman, Barbara L.; Roberts, David L.","Towards the non-visual monitoring of canine physiology in real-time by blind handlers","Proceedings of the 12th International Conference on Advances in Computer Entertainment Technology","978-1-4503-3852-3","","10.1145/2832932.2837018","https://dl.acm.org/doi/10.1145/2832932.2837018","One of the challenges to working with canines is that whereas humans are primarily vocal communicators, canines are primarily postural and behavioral communicators. It can take years to gain some level of proficiency at reading canine body language, even under the best of circumstances. In the case of guide dogs and visually-impaired handlers, this task is even more difficult. Luckily, new technology designed to help monitor canines may prove useful in helping handlers, especially those with visual impairments, to better understand and interpret what their working partners are feeling or saying. In prior work a light-weight, wearable, wireless physiological monitoring system was shown to be accurate for measuring canines' heart and respiratory rates [6]. In this paper, we consider the complementary problem of communicating physiological information to handlers. We introduce two non-visual interfaces for monitoring a canine's heart and respiratory rates, an audio interface and a vibrotactile interface. We also present the results of two initial studies to evaluate the efficacy of the interfaces. In the first study we found that many participants were more confident in detecting changes in heart and respiratory rate using the audio interface, however most of the time they were just as accurate with the vibrotactile interface with only a slight increase in detection latency.","2015-11-16","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","1–8","","","","","","","ACE '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/2XEBVK6G/Mealin et al. - 2015 - Towards the non-visual monitoring of canine physio.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VK7HHEHW","conferencePaper","2014","Oh, Seoug; Patrick, Veronica; Llach, Daniel Cardoso","Typologies of architectural interaction: a social dimension","Proceedings of the Symposium on Simulation for Architecture & Urban Design","","","","","Interactive architecture is concerned with exchanges between humans, environmental factors, and the built environment. These systems are commonly presented as instruments to a) maintain adequate levels of interior lighting and temperature adapting to occupants' needs, and b) reduce building energy consumption by autonomously regulating solar intake in response to environmental factors (Cardoso et al. 2011). While these approaches have yielded promising questions and applications, in this paper we are more interested in exploring interactive architecture's potential role as catalyst for social activity. First, we analyze a selection of contemporary, interactive architecture projects, proposing a set of typologies of architectural interaction driven by the kind of exchanges each project establishes with both occupants and the environment. Second, we test these typologies through a controlled experiment with a responsive artifact that we use as a platform to investigate different types of interactivity and their effects on social activity. In one study, for instance, our responsive artifact is programmed to respond solely to environmental factors. In the second study, it is programmed to respond exclusively to human input. By presenting the typologies, the prototypes, and our observations about the interactions they enable, this paper proposes a new way of thinking ""socially"" about interactive systems, expanding on a crucial ongoing discussion about the relationship between interactive buildings, humans, and the environment.","2014-04-13","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","1–8","","","","","","Typologies of architectural interaction","SimAUD '14","","","","Society for Computer Simulation International","San Diego, CA, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/JI79CREF/Oh et al. - 2014 - Typologies of architectural interaction a social .pdf","","","interaction design; human-centered design; responsive architecture; smart-skin","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2IU755PU","conferencePaper","2022","LC, RAY; Benayoun, Maurice; Lindborg, Permagnus; Xu, Hongshen; Chung Chan, Hin; Man Yip, Ka; Zhang, Tianyi","Power Chess: Robot-to-Robot Nonverbal Emotional Expression Applied to Competitive Play","10th International Conference on Digital and Interactive Arts","978-1-4503-8420-9","","10.1145/3483529.3483844","https://dl.acm.org/doi/10.1145/3483529.3483844","Human-machine communication has evolved from one-to-one to multi-agent systems where the interplay between machines themselves interacts with human perception and behavior, complicated by unconstrained emotion-based variables in social systems. To investigate Human-Robot and Robot-Robot-Human interaction while constraining the interaction variables in a rule-based system, we developed an artistic intervention using competitive game performance between robotic arms. Two robots play chess with each other while expressively making gestures like thinking, examining, hesitating, shows of satisfaction and bewilderment, breathing, etc. These nonverbal behaviors and evolving rules between games tell a narrative of power struggle between two robots of aggressive vs. reflective personalities. We used recorded videos to assay audience interpretations of individual and robot-to-robot expressions, finding that gestures like standing and confirming were perceived as aggressive, while head turns, deliberation, and audience alerts were seen as curious. Human perception of robot play-style and their own intended play strategies were influenced by robot-robot interactions, such as holding defensive strategies when the robot was deemed aggressive. Robotic movements caused audiences to attribute personality characteristics to them, modifying their intended strategy in patterns like pretending to be friendly first to lull the robot opponent. Our work uses artistic metaphors to study multi-agent environments that cannot be easily controlled for in scientific settings.","2022-02-20","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","1–11","","","","","","Power Chess","ARTECH 2021","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/UXZFQI9J/LC et al. - 2022 - Power Chess Robot-to-Robot Nonverbal Emotional Ex.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E4AHPY3G","conferencePaper","2014","Heller, Florian; Krämer, Aaron; Borchers, Jan","Simplifying orientation measurement for mobile audio augmented reality applications","Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","978-1-4503-2473-1","","10.1145/2556288.2557021","https://dl.acm.org/doi/10.1145/2556288.2557021","Audio augmented reality systems overlay the physical world with a virtual audio space. Today's smartphones provide enough processing power to create the impression of virtual sound sources being located in the real world. To achieve this, information about the user's location and orientation is necessary which requires additional hardware. In a real-world installation, however, we observed that instead of turning their head to localize sounds, users tend to turn their entire body. Therefore, we suggest to simply measure orientation of the user's body - or even just the mobile device she is holding - to generate the spatial audio. To verify this approach, we present two studies: Our first study in examines the user's head, body, and mobile device orientation when moving through an audio augmented reality system in a lab setting. Our second study analyzes the user experience in a real-world installation when using head, body, or device orientation to control the audio spatialization. We found that when navigating close to sound sources head tracking is necessary, but that it can potentially be replaced by device tracking in larger or more explorative usage scenarios. These findings help reduce the technical complexity of mobile audio augmented reality systems (MAARS), and enable their wider dissemination as mobile software-only apps.","2014-04-26","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","615–624","","","","","","","CHI '14","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/MDPXIJ43/Heller et al. - 2014 - Simplifying orientation measurement for mobile aud.pdf","","","spatial audio; mobile devices; orientation; audio augmented reality; binaural rendering; presence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HWPYDNXF","conferencePaper","2018","Boletsis, Costas; Chasanidou, Dimitra","Smart Tourism in Cities: Exploring Urban Destinations with Audio Augmented Reality","Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference","978-1-4503-6390-7","","10.1145/3197768.3201549","https://dl.acm.org/doi/10.1145/3197768.3201549","Audio augmented reality (AR) allows for the simultaneous perception of the real environment and a virtual audio overlay. This is especially important in a mobile use context, where users should be continuously aware of their surroundings, such as in the case of urban tourism, when tourists explore foreign cities and their tourist sights. In this work, we investigate the design and implementation of audio AR systems in urban tourism. Our prototype, called AudioNear, is designed to support tourists' exploration of open, urban environments while providing speech-based information about surrounding tourist sights, based on the users' location. At this stage, we present the design concept of AudioNear, its hardware implementation and the first usability feedback. Overall, the study indicated the promising potential of audio AR for providing informative tourist services and engaging experiences.","2018-06-26","2023-07-06 04:57:36","2023-07-06 04:57:36","2023-07-05","515–521","","","","","","Smart Tourism in Cities","PETRA '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/KADEBDRM/Boletsis and Chasanidou - 2018 - Smart Tourism in Cities Exploring Urban Destinati.pdf","","","exploration; Audio augmented reality; eyes-free interaction; tour guide; urban tourism","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6Y7H9V4U","conferencePaper","2017","Freeman, Euan; Wilson, Graham; Brewster, Stephen; Baud-Bovy, Gabriel; Magnusson, Charlotte; Caltenco, Hector","Audible Beacons and Wearables in Schools: Helping Young Visually Impaired Children Play and Move Independently","Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","978-1-4503-4655-9","","10.1145/3025453.3025518","https://dl.acm.org/doi/10.1145/3025453.3025518","Young children with visual impairments tend to engage less with their surroundings, limiting the benefits from activities at school. We investigated novel ways of using sound from a bracelet, such as speech or familiar noises, to tell children about nearby people, places and activities, to encourage them to engage more during play and help them move independently. We present a series of studies, the first two involving visual impairment educators, that give insight into challenges faced by visually impaired children at school and how sound might help them. We then present a focus group with visually impaired children that gives further insight into the effective use of sound. Our findings reveal novel ways of combining sounds from wearables with sounds from the environment, motivating audible beacons, devices for audio output and proximity estimation. We present scenarios, findings and a design space that show the novel ways such devices could be used alongside wearables to help visually impaired children at school.","2017-05-02","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","4146–4157","","","","","","Audible Beacons and Wearables in Schools","CHI '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/GD2CHXM4/Freeman et al. - 2017 - Audible Beacons and Wearables in Schools Helping .pdf","","","wearables; visual impairment; play; children; beacons","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DVQRAF92","conferencePaper","2019","Urbanek, Michael; Güldenpfennig, Florian","Unpacking the Audio Game Experience: Lessons Learned from Game Veterans","Proceedings of the Annual Symposium on Computer-Human Interaction in Play","978-1-4503-6688-5","","10.1145/3311350.3347182","https://dl.acm.org/doi/10.1145/3311350.3347182","People with or without visual impairments play and enjoy audio games. While this genre of computer games has attracted a strong fan base and some attention in HCI, little research has been dedicated to the people who actually play audio games in their daily life. There is a pressing need to capture the viewpoints of authentic or expert players, designers and developers to advance audio game design. Thus, we give voice to seven game veterans of sound-based gaming, i.e., people who each have more than a decade of profound experience in playing or designing audio games. We conducted a total of 14 interviews and employed grounded theory methods to unpack their experiences. We found that audio games enriched their life through creativity, play, and social exchange. Those core concepts were influenced by peripheral concepts like, inter alia, aesthetics & enjoyability, accessibility, or the availability of audio games. We show how they relate to each other and discuss design implications.","2019-10-17","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","253–264","","","","","","Unpacking the Audio Game Experience","CHI PLAY '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/4BK8AN6E/Urbanek and Güldenpfennig - 2019 - Unpacking the Audio Game Experience Lessons Learn.pdf","","","grounded theory; audio games; expert interviews","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GQGEK2HI","conferencePaper","2019","Guinness, Darren; Muehlbradt, Annika; Szafir, Daniel; Kane, Shaun K.","RoboGraphics: Dynamic Tactile Graphics Powered by Mobile Robots","Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility","978-1-4503-6676-2","","10.1145/3308561.3353804","https://dl.acm.org/doi/10.1145/3308561.3353804","Tactile graphics are a common way to present information to people with vision impairments. Tactile graphics can be used to explore a broad range of static visual content but aren't well suited to representing animation or interactivity. We introduce a new approach to creating dynamic tactile graphics that combines a touch screen tablet, static tactile overlays, and small mobile robots. We introduce a prototype system called RoboGraphics and several proof-of-concept applications. We evaluated our prototype with seven participants with varying levels of vision, comparing the RoboGraphics approach to a flat screen, audio-tactile interface. Our results show that dynamic tactile graphics can help visually impaired participants explore data quickly and accurately.","2019-10-24","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","318–328","","","","","","RoboGraphics","ASSETS '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/L5E4CI8U/Guinness et al. - 2019 - RoboGraphics Dynamic Tactile Graphics Powered by .pdf","","","accessibility; education; blindness; tactile; tangible user interfaces; robots","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R2D7QISX","conferencePaper","2015","Buzzi, Maria Claudia; Buzzi, Marina; Leporini, Barbara; Trujillo, Amaury","Exploring Visually Impaired People's Gesture Preferences for Smartphones","Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter","978-1-4503-3684-0","","10.1145/2808435.2808448","https://dl.acm.org/doi/10.1145/2808435.2808448","In this study we investigated how visually impaired people perform gestures on touch-screen smartphones. To this end, we recruited 36 visually impaired participants to explore differences and preferences in carrying out a set of gestures, selected according to certain characteristics (e.g., shape, number of fingers or strokes, etc.). For this purpose, we developed a system to collect gestures from several participants interacting with mobile smartphones at the same time. Results confirm previous research regarding the preference of visually impaired users for simple gestures, made with one finger and a single stroke. Moreover, rounded shapes were greatly preferred to angular ones.","2015-09-28","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","94–101","","","","","","","CHItaly 2015","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/NNY5Q5A6/Buzzi et al. - 2015 - Exploring Visually Impaired People's Gesture Prefe.pdf","","","multimodal interfaces; mobile devices; Accessibility; touch gestures; visually impairment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LRZVGMED","conferencePaper","2017","Niewiadomski, Radoslaw; Mancini, Maurizio; Piana, Stefano; Alborno, Paolo; Volpe, Gualtiero; Camurri, Antonio","Low-intrusive recognition of expressive movement qualities","Proceedings of the 19th ACM International Conference on Multimodal Interaction","978-1-4503-5543-8","","10.1145/3136755.3136757","https://dl.acm.org/doi/10.1145/3136755.3136757","In this paper we present a low-intrusive approach to the detection of expressive full-body movement qualities. We focus on two qualities: Lightness and Fragility and we detect them using the data captured by four wearable devices, two Inertial Movement Units (IMU) and two electromyographs (EMG), placed on the forearms. The work we present in the paper stems from a strict collaboration with expressive movement experts (e.g., contemporary dance choreographers) for defining a vocabulary of basic movement qualities. We recorded 13 dancers performing movements expressing the qualities under investigation. The recordings were next segmented and the perceived level of each quality for each segment was ranked by 5 experts using a 5-points Likert scale. We obtained a dataset of 150 segments of movement expressing Fragility and/or Lightness. In the second part of the paper, we define a set of features on IMU and EMG data and we extract them on the recorded corpus. We finally applied a set of supervised machine learning techniques to classify the segments. The best results for the whole dataset were obtained with a Naive Bayes classifier for Lightness (F-score 0.77), and with a Support Vector Machine classifier for Fragility (F-score 0.77). Our approach can be used in ecological contexts e.g., during artistic performances.","2017-11-03","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","230–237","","","","","","","ICMI '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/556CN2MC/Niewiadomski et al. - 2017 - Low-intrusive recognition of expressive movement q.pdf","","","HCI; dance; EMG; expressive qualities; IMU","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"24H5BW7G","conferencePaper","2017","Fdili Alaoui, Sarah; Françoise, Jules; Schiphorst, Thecla; Studd, Karen; Bevilacqua, Frederic","Seeing, Sensing and Recognizing Laban Movement Qualities","Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","978-1-4503-4655-9","","10.1145/3025453.3025530","https://dl.acm.org/doi/10.1145/3025453.3025530","Human movement has historically been approached as a functional component of interaction within human computer interaction. Yet movement is not only functional, it is also highly expressive. In our research, we explore how movement expertise as articulated in Laban Movement Analysis (LMA) can contribute to the design of computational models of movement's expressive qualities as defined in the framework of Laban Efforts. We include experts in LMA in our design process, in order to select a set of suitable multimodal sensors as well as to compute features that closely correlate to the definitions of Efforts in LMA. Evaluation of our model shows that multimodal data combining positional, dynamic and physiological information allows for a better characterization of Laban Efforts. We conclude with implications for design that illustrate how our methodology and our approach to multimodal capture and recognition of Effort qualities can be integrated to design interactive applications.","2017-05-02","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","4009–4020","","","","","","","CHI '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/6E86XJXC/Fdili Alaoui et al. - 2017 - Seeing, Sensing and Recognizing Laban Movement Qua.pdf","","","laban movement analysis; movement qualities; movement observation; movement recognition; movement-based interaction; multimodal measures","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JUV48TTB","conferencePaper","2021","Ahmetovic, Dragan; Kwon, Nahyun; Oh, Uran; Bernareggi, Cristian; Mascetti, Sergio","Touch Screen Exploration of Visual Artwork for Blind People","Proceedings of the Web Conference 2021","978-1-4503-8312-7","","10.1145/3442381.3449871","https://dl.acm.org/doi/10.1145/3442381.3449871","This paper investigates how touchscreen exploration and verbal feedback can be used to support blind people to access visual artwork. We present two artwork exploration modalities. The first one, attribute-based exploration, extends prior work on touchscreen image accessibility, and provides fine-grained segmentation of artwork visual elements; when the user touches an element, the associated attributes are read. The second one, hierarchical exploration, is designed with domain experts and provides multi-level segmentation of the artwork; the user initially accesses a general description of the entire artwork and then explores a coarse segmentation of the visual elements with the corresponding high-level descriptions; once selected, coarse segments are subdivided into fine-grained ones, which the user can access for more detailed descriptions. The two exploration modalities, implemented as a mobile web app, were evaluated through a user study with 10 blind participants. Both modalities were appreciated by the participants. Attribute-based exploration is perceived to be easier to access. Instead, the hierarchical exploration was considered more understandable, useful, interesting and captivating, and the participants remembered more details about the artwork with this modality. Participants commented that the two modalities work well together and therefore both should be made available.","2021-06-03","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","2781–2791","","","","","","","WWW '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/TCZ9BJ6E/Ahmetovic et al. - 2021 - Touch Screen Exploration of Visual Artwork for Bli.pdf","","","Touch screen; Art accessibility; Audio feedback.; Blindness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"THPTRIBJ","conferencePaper","2018","Metatla, Oussama; Bryan-Kinns, Nick; Stockman, Tony","“I Hear You”: Understanding Awareness Information Exchange in an Audio-only Workspace","Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","978-1-4503-5620-6","","10.1145/3173574.3174120","https://dl.acm.org/doi/10.1145/3173574.3174120","Graphical displays are a typical means for conveying awareness information in groupware systems to help users track joint activities, but are not ideal when vision is constrained. Understanding how people maintain awareness through non-visual means is crucial for designing effective alternatives for supporting awareness in such situations. We present a lab study simulating an extreme scenario where 32 pairs of participants use an audio-only tool to edit shared audio menus. Our aim is to characterise collaboration in this audio-only space in order to identify whether and how, by itself, audio can mediate collaboration. Our findings show that the means for audio delivery and choice of working styles in this space influence types and patterns of awareness information exchange. We thus highlight the need to accommodate different working styles when designing audio support for awareness, and extend previous research by identifying types of awareness information to convey in response to group work dynamics.","2018-04-21","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","1–13","","","","","","“I Hear You”","CHI '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/NI9G5ZSK/Metatla et al. - 2018 - “I Hear You” Understanding Awareness Information .pdf","","","collaboration; audio-only interaction; workspace awareness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NIL8D2KE","conferencePaper","2016","Mencarini, Eleonora; Leonardi, Chiara; De Angeli, Antonella; Zancanaro, Massimo","Design Opportunities for Wearable Devices in Learning to Climb","Proceedings of the 9th Nordic Conference on Human-Computer Interaction","978-1-4503-4763-1","","10.1145/2971485.2971509","https://dl.acm.org/doi/10.1145/2971485.2971509","In this paper, we present a field study on the learning of climbing aimed at defining the design space of wearable devices to support beginners. Three main findings have emerged from our study. First, climbing has a strong emotional impact on beginners; therefore, learning to climb requires mastering new motor patterns as well as negative emotions, such as stress and fear. Second, the feeling of danger that climbers often experience can be mitigated by trust in the climbing partner and the perception of her active presence. Finally, a big problem in climbing is the communication difficulty between the climbing partners and between climber and instructor. We conclude the paper presenting four design considerations for the design of wearable devices meant to support the learning of climbing by providing the actors involved with augmented communication. Such augmented communication should address both the physical and the emotional difficulties of this sport.","2016-10-23","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","1–10","","","","","","","NordiCHI '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/F5Z8VPCV/Mencarini et al. - 2016 - Design Opportunities for Wearable Devices in Learn.pdf","","","emotions; wearables; augmented communication; Climbing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EAS4N5KW","conferencePaper","2015","Baker, Catherine M.; Milne, Lauren R.; Ladner, Richard E.","StructJumper: A Tool to Help Blind Programmers Navigate and Understand the Structure of Code","Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems","978-1-4503-3145-6","","10.1145/2702123.2702589","https://dl.acm.org/doi/10.1145/2702123.2702589","It can be difficult for a blind developer to understand and navigate through a large amount of code quickly, as they are unable to skim as easily as their sighted counterparts. To help blind developers overcome this problem, we present StructJumper, an Eclipse plugin that creates a hierarchical tree based on the nesting structure of a Java class. The programmer can use the TreeView to get an overview of the code structure of the class (including all the methods and control flow statements) and can quickly switch between the TreeView and the Text Editor to get an idea of where they are within the nested structure. To evaluate StructJumper, we had seven blind programmers complete three tasks with and without our tool. We found that the users thought they would use StructJumper and there was a trend that they were faster completing the tasks with StructJumper.","2015-04-18","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","3043–3052","","","","","","StructJumper","CHI '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/RIRDJBMU/Baker et al. - 2015 - StructJumper A Tool to Help Blind Programmers Nav.pdf","","","accessibility; navigation; screen reader; blind programmers; code structure","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y7MJTZFF","conferencePaper","2019","Moore, Dylan; Dahl, Tobias; Varela, Paula; Ju, Wendy; Næs, Tormod; Berget, Ingunn","Unintended Consonances: Methods to Understand Robot Motor Sound Perception","Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","978-1-4503-5970-2","","10.1145/3290605.3300730","https://dl.acm.org/doi/10.1145/3290605.3300730","Recent research suggests that a robot's motors make sounds that can influence users' perception of the robot's characteristics. To more deeply understand users' associations with specific sonic characteristics, we adapted methods from sensory science including Check All That Apply (CATA) questions and Polarized Sensory Positioning (PSP) to tease out small differences in motor sounds in an online survey. These methods are straightforward for untrained people to do in an online setting, mathematically rigorous, and can explore a variety of subtle auditory and perceptual stimuli. We describe how to use these methods, interpret the results with several intuitive visual representations, and show that the results align with a previous study of the same dataset. We close by discussing benefits and limitations of applying these methods to study subtle phenomena in the HCI community.","2019-05-02","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","1–12","","","","","","Unintended Consonances","CHI '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/K4CUZZC9/Moore et al. - 2019 - Unintended Consonances Methods to Understand Robo.pdf","","","sonic interaction design; consequential sound; polarized sensory perception; sensory science","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3GUMLJ5L","conferencePaper","2021","Garbett, Andrew; Degutyte, Ziedune; Hodge, James; Astell, Arlene","Towards Understanding People’s Experiences of AI Computer Vision Fitness Instructor Apps","Designing Interactive Systems Conference 2021","978-1-4503-8476-6","","10.1145/3461778.3462094","https://dl.acm.org/doi/10.1145/3461778.3462094","The recent rise in on-device AI computer vision and dialogue systems has facilitated a growing number of AI fitness related instructional apps. However, these technologies have yet to be explored within the HCI community. To investigate this domain we recruited 12 participants and asked them to engage with five recently launched AI fitness instructor apps. We interviewed participants and thematically analysed transcripts to understand their experience and expectations of these technologies. Our qualitative analysis outlines five main themes focusing on; limitations of computer vision, visual feedback, dialogue with the AI, adapting to the user, and working out with the instructor. Based upon our findings we present five design considerations for designers that relate to three key areas: feedback and motivation, personalising the experience, and building a relationship with the AI. We contribute a first look into people’s initial experiences with on-device AI fitness instructor applications and we provide design considerations to guide future contextually-aware AI research in this domain.","2021-06-28","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","1619–1637","","","","","","","DIS '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/YQTSZB3U/Garbett et al. - 2021 - Towards Understanding People’s Experiences of AI C.pdf","","","mobile devices; qualitative methods; Computer vision; contextual AI; dialogue systems; fitness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YYB37MN8","conferencePaper","2022","Dodani, Arika; Van Koningsbruggen, Rosa; Hornecker, Eva","Birdbox: Exploring the User Experience of Crossmodal, Multisensory Data Representations","Proceedings of the 21st International Conference on Mobile and Ubiquitous Multimedia","978-1-4503-9820-6","","10.1145/3568444.3568455","https://dl.acm.org/doi/10.1145/3568444.3568455","We contribute to an improved understanding of how physical multisensory data representations are experienced and how specific modalities affect the user experience (UX). We investigate how people make sense of Birdbox, a crossmodal data representation that employs combined haptic-audio, audio-visual, or visual-haptic output for data about birds. Findings indicate that participants preferred haptic output for the bodily experience it triggered. Participants further created their own mappings between data and modality; haptic was mapped to aggression, and audio to speed. Especially with (soft) haptic output, Birdbox was experienced as a living entity. This can also be seen in participants’ bodily interactions, holding Birdbox as if it were a small bird. We contribute to a better understanding of the UX of different modalities in multisensory data representations, highlight strengths of the haptic modality, and of metaphorical understandings of modalities.","2022-12-29","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","12–21","","","","","","Birdbox","MUM '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/L23NFF4S/Dodani et al. - 2022 - Birdbox Exploring the User Experience of Crossmod.pdf","","","embodied interaction; haptification; InfoVis; metaphors; physicalisation; sensory modality; UX; VIS","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LHN9K7BW","conferencePaper","2019","Asakawa, Saki; Guerreiro, João; Sato, Daisuke; Takagi, Hironobu; Ahmetovic, Dragan; Gonzalez, Desi; Kitani, Kris M.; Asakawa, Chieko","An Independent and Interactive Museum Experience for Blind People","Proceedings of the 16th International Web for All Conference","978-1-4503-6716-5","","10.1145/3315002.3317557","https://dl.acm.org/doi/10.1145/3315002.3317557","Museums are gradually becoming more accessible to blind people, who have shown interest in visiting museums and in appreciating visual art. Yet, their ability to visit museums is still dependent on the assistance they get from their family and friends or from the museum personnel. Based on this observation and on prior research, we developed a solution to support an independent, interactive museum experience that uses the continuous tracking of the user's location and orientation to enable a seamless interaction between Navigation and Art Appreciation. Accurate localization and context-awareness allow for turn-by-turn guidance (Navigation Mode), as well as detailed audio content when facing an artwork within close proximity (Art Appreciation Mode). In order to evaluate our system, we installed it at The Andy Warhol Museum in Pittsburgh and conducted a user study where nine blind participants followed routes of interest while learning about the artworks. We found that all participants were able to follow the intended path, immediately grasped how to switch between Navigation and Art Appreciation modes, and valued listening to the audio content in front of each artwork. Also, they showed high satisfaction and an increased motivation to visit museums more often.","2019-05-13","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","1–9","","","","","","","W4A '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/2DIR3RRG/Asakawa et al. - 2019 - An Independent and Interactive Museum Experience f.pdf","","","visual impairments; art appreciation; indoor navigation; interactive space; Museum accessibility; non-visual interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DIWZDEHM","conferencePaper","2015","Goncu, Cagatay; Madugalla, Anuradha; Marinai, Simone; Marriott, Kim","Accessible On-Line Floor Plans","Proceedings of the 24th International Conference on World Wide Web","978-1-4503-3469-3","","10.1145/2736277.2741660","https://dl.acm.org/doi/10.1145/2736277.2741660","Better access to on-line information graphics is a pressing need for people who are blind or have severe vision impairment. We present a new model for accessible presentation of on-line information graphics and demonstrate its use for presenting floor plans. While floor plans are increasingly provided on-line, people who are blind are at best provided with only a high-level textual description. This makes it difficult for them to understand the spatial arrangement of the objects on the floor plan. Our new approach provides users with significantly better access to such plans. The users can automatically generate an accessible version of a floor plan from an on-line floor plan image quickly and independently by using a web service. This generates a simplified graphic showing the rooms, walls, doors and windows in the original floor plan as well as a textual overview. The accessible floor plan is presented on an iPad using audio feedback. As the users touch graphic elements on the screen, the element they are touching is described by speech and non-speech audio in order to help them navigate the graphic.","2015-05-18","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","388–398","","","","","","","WWW '15","","","","International World Wide Web Conferences Steering Committee","Republic and Canton of Geneva, CHE","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/PZBXK4GJ/Goncu et al. - 2015 - Accessible On-Line Floor Plans.pdf","","","accessibility; vision impairment; graphics; graphics recognition; transcription","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SHKEEUWR","conferencePaper","2020","Broscheit, Jessica","Embodied Atmospheres: A Symbiosis of Body and Environmental Information in the Form of Wearable Artifacts","Proceedings of the Fourteenth International Conference on Tangible, Embedded, and Embodied Interaction","978-1-4503-6107-1","","10.1145/3374920.3374958","https://dl.acm.org/doi/10.1145/3374920.3374958","This paper introduces a Ph.D. research project about wearable computer artifacts that intertwine digital information from both the human body and the environment. These artifacts use content-related sensors to make air constituents perceptible. In addition, the artifacts' system design is based on metaphorical representations to provide a conceptual system of thought and action for the user. Through the intuitive use of these wearable computer artifacts, participants are able to explore information about themselves and their surroundings with their enhanced body. To provide insights into these human-computer interactions, this Ph.D. research project conducts a user experience study as a 'real-life' ethnographic enactment.","2020-02-09","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","899–903","","","","","","Embodied Atmospheres","TEI '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/GAVRRCAU/Broscheit - 2020 - Embodied Atmospheres A Symbiosis of Body and Envi.pdf","","","human-computer interaction; tangible interaction; environmental sensing; quantified self","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NTT32LGB","conferencePaper","2019","An, Pengcheng; Bakker, Saskia; Ordanovski, Sara; Taconis, Ruurd; Paffen, Chris L.E.; Eggen, Berry","Unobtrusively Enhancing Reflection-in-Action of Teachers through Spatially Distributed Ambient Information","Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","978-1-4503-5970-2","","10.1145/3290605.3300321","https://dl.acm.org/doi/10.1145/3290605.3300321","Reflecting on their performance during classroom-teaching is an important competence for teachers. Such reflection-in-action (RiA) enables them to optimize teaching on the spot. But RiA is also challenging, demanding extra thinking in teachers' already intensive routines. Little is known on how HCI systems can facilitate teachers' RiA during classroom-teaching. To fill in this gap, we evaluate ClassBeacons, a system that uses spatially distributed lamps to depict teachers' ongoing performance on how they have divided their time and attention over students in the classroom. Empirical qualitative data from eleven teachers in 22 class periods show that this ambient information facilitated teachers' RiA without burdening teaching in progress. Based on our theoretical grounding and field evaluation, we contribute empirical knowledge about how an HCI system enhanced teachers' process of RiA as well as a set of design principles for unobtrusively supporting RiA.","2019-05-02","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","1–14","","","","","","","CHI '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/VG3AYEG8/An et al. - 2019 - Unobtrusively Enhancing Reflection-in-Action of Te.pdf","","","ambient information system; distributed cognition; periphery of attention; reflection-in-action; reflective practitioner; teacher","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"375V8FQT","conferencePaper","2013","El-Glaly, Yasmine N.; Quek, Francis; Smith-Jackson, Tonya; Dhillon, Gurjot","Touch-screens are not tangible: fusing tangible interaction with touch glass in readers for the blind","Proceedings of the 7th International Conference on Tangible, Embedded and Embodied Interaction","978-1-4503-1898-3","","10.1145/2460625.2460665","https://dl.acm.org/doi/10.1145/2460625.2460665","In this paper we introduce the idea of making touch surfaces of mobile devices (e.g. touch phones and tablets) truly tangible for Individuals with Blindness or Severe Visual Impairment (IBSVI). We investigate how to enable IBSVI to fuse tangible landmark patterns with layout of page and location of lexical elements -- words, phrases, and sentences. We designed a tactile overlay that gives tangible feedback to IBSVI when using touch devices for reading. The overlay was tested in a usability study, and the results showed the role of tangibility in leveraging accessibility of touch devices and supporting reading for IBSVI.","2013-02-10","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","245–253","","","","","","Touch-screens are not tangible","TEI '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/FYRTP834/El-Glaly et al. - 2013 - Touch-screens are not tangible fusing tangible in.pdf","","","touch screen; blindness; tactile; overlay; reader","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JYP57I8M","conferencePaper","2021","Elvitigala, Don Samitha; Huber, Jochen; Nanayakkara, Suranga","Augmented Foot: A Comprehensive Survey of Augmented Foot Interfaces","Proceedings of the Augmented Humans International Conference 2021","978-1-4503-8428-5","","10.1145/3458709.3458958","https://dl.acm.org/doi/10.1145/3458709.3458958","Augmented foot interfaces have been studied since the beginning of wearable computers. The worlds’ first wearable computer was an instrumented shoe that consisted of a toe operated switch with a wireless module. Since then, academic research and commercial products on augmented foot interfaces are booming with novel interfaces every year. This paper surveys the body of work on augmented foot interfaces and shows the current trends and guidelines for future augmented foot interfaces. We contribute a classification of over 100 academic papers and commercially-available products. We discuss the integration of augmented foot interfaces, interaction schemes and application domains. Finally, we contribute a set of design considerations to scaffold future research of augmented foot interfaces based on the classification and inspired by the surveyed work.","2021-07-11","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","228–239","","","","","","Augmented Foot","AHs '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/NDUV2RN8/Elvitigala et al. - 2021 - Augmented Foot A Comprehensive Survey of Augmente.pdf","","","Augmented Foot; Augmented Human; Foot Augmentation; Foot Interactions; Human Augmentation; Survey","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K4K7DAZP","conferencePaper","2015","Tholander, Jakob; Nylander, Stina","Snot, Sweat, Pain, Mud, and Snow: Performance and Experience in the Use of Sports Watches","Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems","978-1-4503-3145-6","","10.1145/2702123.2702482","https://dl.acm.org/doi/10.1145/2702123.2702482","We have conducted interviews with ten elite and recreational athletes to understand their experiences and engagement with endurance sport and personal and wearable sports technology. The athletes emphasized the experiential aspects of doing sports and the notion of feeling was repeatedly used to talk about their activities. Technology played both an instrumental role in measuring performance and feeding bio-data back to them, and an experiential role in supporting and enhancing the sport experience. To guide further interaction design research in the sports domain, we suggest two interrelated ways of looking at sports performances and experiences, firstly through the notion of a measured sense of performance, and secondly as a lived-sense of performance.","2015-04-18","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","2913–2922","","","","","","Snot, Sweat, Pain, Mud, and Snow","CHI '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/YLC8UZDA/Tholander and Nylander - 2015 - Snot, Sweat, Pain, Mud, and Snow Performance and .pdf","","","performance; sports; feeling; experience; heart rate monitors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IG4UY2V6","conferencePaper","2008","Yalla, Pavani; Walker, Bruce N.","Advanced auditory menus: design and evaluation of auditory scroll bars","Proceedings of the 10th international ACM SIGACCESS conference on Computers and accessibility","978-1-59593-976-0","","10.1145/1414471.1414492","https://dl.acm.org/doi/10.1145/1414471.1414492","Auditory menus have the potential to make devices that use visual menus accessible to a wide range of users. Visually impaired users could especially benefit from the auditory feedback received during menu navigation. However, auditory menus are a relatively new concept, and there are very few guidelines that describe how to design them. This paper details how visual menu concepts may be applied to auditory menus in order to help develop design guidelines. Specifically, this set of studies examined possible ways of designing an auditory scrollbar for an auditory menu. The following different auditory scrollbar designs were evaluated: single-tone, double-tone, alphabetical grouping, and proportional grouping. Three different evaluations were conducted to determine the best design. The first two evaluations were conducted with sighted users, and the last evaluation was conducted with visually impaired users. The results suggest that pitch polarity does not matter, and proportional grouping is the best of the auditory scrollbar designs evaluated here.","2008-10-13","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","105–112","","","","","","Advanced auditory menus","Assets '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/ILZI9XAN/Yalla and Walker - 2008 - Advanced auditory menus design and evaluation of .pdf","","","non-speech sounds; auditory menus; auditory scrollbar; universal design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3HX5A4L5","conferencePaper","2013","Gousie, Michael B.; Teresco, James D.","Helping students understand the datapath with simulators and crazy models","Proceeding of the 44th ACM technical symposium on Computer science education","978-1-4503-1868-6","","10.1145/2445196.2445295","https://dl.acm.org/doi/10.1145/2445196.2445295","Undergraduate computer science programs at many small colleges often include only one course focused on hardware. Many important concepts are covered in such a course, including the basics of computer architecture. By the end of such a course, students should have a good understanding of how a binary machine instruction is executed in hardware. Unfortunately, even a simplified diagram of a datapath is often difficult for students to master. We present two approaches that use lab exercises to help to address this problem. In one, students build a working model of the datapath out of ordinary materials; in the other, a software simulator is designed and implemented. These approaches are described and their merits discussed.","2013-03-06","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","329–334","","","","","","","SIGCSE '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/J4MQLTVK/Gousie and Teresco - 2013 - Helping students understand the datapath with simu.pdf","","","computer organization pedagogy; laboratory assignments; modeling computer architecture; simulating computer architecture","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K5B43A2H","conferencePaper","2022","Blanchard, Jeremiah; Hott, John R.; Berry, Vincent; Carroll, Rebecca; Edmison, Bob; Glassey, Richard; Karnalim, Oscar; Plancher, Brian; Russell, Seán","Stop Reinventing the Wheel! Promoting Community Software in Computing Education","Proceedings of the 2022 Working Group Reports on Innovation and Technology in Computer Science Education","9798400700101","","10.1145/3571785.3574129","https://dl.acm.org/doi/10.1145/3571785.3574129","Historically, computing instructors and researchers have developed a wide variety of tools to support teaching and educational research, including exam and code testing suites and data collection solutions. However, these tools often find limited adoption beyond their creators. As a result, it is common for many of the same functionalities to be re-implemented by different instructional groups within the Computing Education community. We hypothesise that this is due in part to discoverability, availability, and adaptability challenges. Further, instructors often face institutional barriers to deployment, which can include hesitance of institutions to rely on community developed solutions that often lack a centralised authority and may be community or individually maintained. To this end, our working group explored what solutions are currently available, what instructors needed, and the reasons behind the above-mentioned phenomenon. To do so, we reviewed existing literature and surveyed the community to identify the tools that have been developed by the community; the solutions that are currently available and in use by instructors; what features are needed moving forward for classroom and research use; what support for extensions is needed to support further Computing Education research; and what institutional challenges instructors and researchers are currently facing or have faced in using community software solutions. Finally, the working group identified factors that limited adoption of solutions. This work proposes ways to integrate and improve the availability, discoverability, and dissemination of existing community projects, as well as ways to manage and overcome institutional challenges.","2022-12-29","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","261–292","","","","","","","ITiCSE-WGR '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/BZSMWJQZ/Blanchard et al. - 2022 - Stop Reinventing the Wheel! Promoting Community So.pdf","","","open source software; community software; computing education; computing education research; educational tools","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZQJY48VZ","conferencePaper","2011","Encelle, Benoît; Ollagnier-Beldame, Magali; Pouchot, Stéphanie; Prié, Yannick","Annotation-based video enrichment for blind people: a pilot study on the use of earcons and speech synthesis","The proceedings of the 13th international ACM SIGACCESS conference on Computers and accessibility","978-1-4503-0920-2","","10.1145/2049536.2049560","https://dl.acm.org/doi/10.1145/2049536.2049560","Our approach to address the question of online video accessibility for people with sensory disabilities is based on video annotations that are rendered as video enrichments during the playing of the video. We present an exploratory work that focuses on video accessibility for blind people with audio enrichments composed of speech synthesis and earcons (i.e. nonverbal audio messages). Our main results are that earcons can be used together with speech synthesis to enhance understanding of videos; that earcons should be accompanied with explanations; and that a potential side effect of earcons is related to video rhythm perception.","2011-10-24","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","123–130","","","","","","Annotation-based video enrichment for blind people","ASSETS '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/6ABYA57Z/Encelle et al. - 2011 - Annotation-based video enrichment for blind people.pdf","","","accessibility for blind people; audio notification; video accessibility; video annotation; video enrichment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7DJYTPWY","conferencePaper","2016","Metatla, Oussama; Correia, Nuno N.; Martin, Fiore; Bryan-Kinns, Nick; Stockman, Tony","Tap the ShapeTones: Exploring the Effects of Crossmodal Congruence in an Audio-Visual Interface","Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems","978-1-4503-3362-7","","10.1145/2858036.2858456","https://dl.acm.org/doi/10.1145/2858036.2858456","There is growing interest in the application of crossmodal perception to interface design. However, most research has focused on task performance measures and often ignored user experience and engagement. We present an examination of crossmodal congruence in terms of performance and engagement in the context of a memory task of audio, visual, and audio-visual stimuli. Participants in a first study showed improved performance when using a visual congruent mapping that was cancelled by the addition of audio to the baseline conditions, and a subjective preference for the audio-visual stimulus that was not reflected in the objective data. Based on these findings, we designed an audio-visual memory game to examine the effects of crossmodal congruence on user experience and engagement. Results showed higher engagement levels with congruent displays with some reported preference for potential challenge and enjoyment that an incongruent display may support, particularly for increased task complexity.","2016-05-07","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","1055–1066","","","","","","Tap the ShapeTones","CHI '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/ZW7WS9J5/Metatla et al. - 2016 - Tap the ShapeTones Exploring the Effects of Cross.pdf","","","games; user experience; audio-visual display; crossmodal congruence; spatial mappings; user engagement","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MRAQ5SVV","conferencePaper","2021","Delle Monache, Stefano; MISDARIIS, Nicolas; Ozcan, Elif","Conceptualising sound-driven design: an exploratory discourse analysis","Proceedings of the 13th Conference on Creativity and Cognition","978-1-4503-8376-9","","10.1145/3450741.3465258","https://dl.acm.org/doi/10.1145/3450741.3465258","Sound-driven design is an emerging, human-centered design practice informed by technology and listening in the multisensory dimension of interaction. In this paper we present a discourse analysis approach aimed at qualitatively understanding the constituent concepts of such a practice, by means of semi-structured interviews with sound designers, design researchers, engineers and expert users in the context of critical care. Preliminary results show that sound-driven design is inherently embodied, situated, and participatory, that the four categories of interviewees equally contribute to the definition of the design problem, and yet that a clear, shared arena is still missing.","2021-06-22","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","1–8","","","","","","Conceptualising sound-driven design","C&amp;C '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/L42Q7HHC/Delle Monache et al. - 2021 - Conceptualising sound-driven design an explorator.pdf","","","design methods; design research; discourse analysis; Sound-driven design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4YW2H49C","conferencePaper","2015","Polys, Nicholas F.; Knapp, Benjamin; Bock, Matthew; Lidwin, Christina; Webster, Dane; Waggoner, Nathan; Bukvic, Ivica","Fusality: an open framework for cross-platform mirror world installations","Proceedings of the 20th International Conference on 3D Web Technology","978-1-4503-3647-5","","10.1145/2775292.2775317","https://dl.acm.org/doi/10.1145/2775292.2775317","As computing and displays become more pervasive and wireless networks are increasing the connections between people and things, humans inhabit both digital and physical realities. In this paper we describe our prototype Mirror Worlds framework, which is designed to fuse these realities: Fusality. Our goal for Fusality is to support innovative research and exhibitions in presence and collaboration, sensors and smart buildings and mixed reality in applications from engineering to art. By fusing live sensor data from the building and its occupants with online 3D environments and participants, we demonstrate a first-principles approach to online multi-entity messaging communication. This demonstration shows how the variety of Mirror Worlds clients can be supported through the open Web architecture. These technologies enable new possibilities for collaboration as well as directions for interoperability. Finally, we layout out our research agenda for the framework and discuss its transformative applications.","2015-06-18","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","171–179","","","","","","Fusality","Web3D '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/QA9YECS2/Polys et al. - 2015 - Fusality an open framework for cross-platform mir.pdf","","","mixed reality; X3D; ubiquitous computing; unity; X3DOM","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZZHCT495","conferencePaper","2011","Hug, Daniel; Misdariis, Nicolas","Towards a conceptual framework to integrate designerly and scientific sound design methods","Proceedings of the 6th Audio Mostly Conference: A Conference on Interaction with Sound","978-1-4503-1081-9","","10.1145/2095667.2095671","https://dl.acm.org/doi/10.1145/2095667.2095671","Sound design for interactive products is rapidly evolving to become a relevant topic in industry. Scientific research from the domains of Auditory Display (AD) and Sonic Interaction Design (SID) can play a central role in this development, but in order to make its way to market oriented applications, several issues still need to be addressed. Building on the sound design process employed at the Sound Perception and Design (SPD) team at Ircam, and the information gathered from interviews with professional sound designers, this paper focuses on revealing typical issues encountered in the design process of both science and design oriented communities, in particular the development of a valid and revisable, yet innovative, design hypothesis. A second aim is to improve the communication between sound and interaction designers. In order to address these challenges, a conceptual framework, which has been developed using both scientific and designerly methods, was presented and evaluated using expert reviews.","2011-09-07","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","23–30","","","","","","","AM '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/8KGNAXRL/Hug and Misdariis - 2011 - Towards a conceptual framework to integrate design.pdf","","","sonic interaction design; sound design; interactive commodities","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T9VDFGBB","conferencePaper","2016","Michalewicz, Marek T.; Lian, Tan Geok; Seng, Lim; Low, Jonathan; Southwell, David; Gunthorpe, Jason; Noaje, Gabriel; Chien, Dominic; Poppe, Yves; Chrzęszczyk, Jakub; Howard, Andrew; Wee, Tan Tin; Sing-Wu, Liou","InfiniCortex: present and future invited paper","Proceedings of the ACM International Conference on Computing Frontiers","978-1-4503-4128-8","","10.1145/2903150.2912887","https://dl.acm.org/doi/10.1145/2903150.2912887","Commencing in June 2014, A*STAR Computational Resource Centre (A*CRC) team in Singapore, together with dozens of partners world-wide, have been building the InfiniCortex. Four concepts are integrated together to realise InfiniCortex: i) High bandwidth (~ 10 to 100Gbps) intercontinental connectivity between four continents: Asia, North America, Australia and Europe; ii) InfiniBand extension technology supporting transcontinental distances using Obsidian's Longbow range extenders; iii) Connecting separate InfiniBand sub-nets with different net topologies to create a single computational resource: Galaxy of Supercomputers [10] iv) Running workflows and applications on such a distributed computational infrastructure. We have successfully demonstrated InfiniCortex prototypes at SC14 and SC15 conferences. The infrastructure comprised of computing resources residing at multiple locations in Singapore, Japan, Australia, USA, Canada, France and Poland. Various concurrent applications, including workflows, I/O heavy applications enabled with ADIOS system, Extempore real-time interactive applications, and in-situ realtime visualisations were demonstrated. In this paper we briefly report on basic ideas behind Infini-Cortex construct, our recent successes and some ideas about further growth and extension of this project.","2016-05-16","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","267–273","","","","","","InfiniCortex","CF '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/FYZIMVD4/Michalewicz et al. - 2016 - InfiniCortex present and future invited paper.pdf","","","ADIOS; data transfer; exascale computing; extempore; globally distributed concurrent supercomputer; in-situ visualisation; InfiniBand; InfiniCloud; InfiniCortex; RDMA; SKA; square kilometer array; workflows","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U32HQ87B","conferencePaper","2012","Stevens, Nicholas; Giannareas, Ana Rosa; Kern, Vanessa; Viesca, Adrian; Fortino-Mullen, Margaret; King, Andrew; Lee, Insup","Smart alarms: multivariate medical alarm integration for post CABG surgery patients","Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium","978-1-4503-0781-9","","10.1145/2110363.2110423","https://dl.acm.org/doi/10.1145/2110363.2110423","In order to monitor patients in the Intensive Care Unit, healthcare practitioners set threshold alarms on each of many individual vital sign monitors. The current alarm algorithms elicit numerous false positive alarms producing an inefficient healthcare system, where nurses habitually ignore low level alarms due to their overabundance. In this paper, we describe an algorithm that considers multiple vital signs when monitoring a post coronary artery bypass graft (post-CABG) surgery patient. The algorithm employs a Fuzzy Expert System to mimic the decision processes of nurses. In addition, it includes a Clinical Decision Support tool that uses Bayesian theory to display the possible CABG-related complications the patient might be undergoing at any point in time, as well as the most relevant risk factors. As a result, this multivariate approach decreases clinical alarms by an average of 59% with a standard deviation of 17% (Sample of 32 patients, 1,451 hours of vital sign data). Interviews comparing our proposed system with the approach currently used in hospitals have also confirmed the potential efficiency gains from this approach.","2012-01-28","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","533–542","","","","","","Smart alarms","IHI '12","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/EA442N3N/Stevens et al. - 2012 - Smart alarms multivariate medical alarm integrati.pdf","","","bayesian theory; clinical data integration; clinical decision support; fuzzy logic; vital sign monitor","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2H9NDX6Z","conferencePaper","2018","Glatz, Christiane; Krupenia, Stas S.; Bülthoff, Heinrich H.; Chuang, Lewis L.","Use the Right Sound for the Right Job: Verbal Commands and Auditory Icons for a Task-Management System Favor Different Information Processes in the Brain","Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","978-1-4503-5620-6","","10.1145/3173574.3174046","https://dl.acm.org/doi/10.1145/3173574.3174046","Design recommendations for notifications are typically based on user performance and subjective feedback. In comparison, there has been surprisingly little research on how designed notifications might be processed by the brain for the information they convey. The current study uses EEG/ERP methods to evaluate auditory notifications that were designed to cue long-distance truck drivers for task-management and driving conditions, particularly for automated driving scenarios. Two experiments separately evaluated naive students and professional truck drivers for their behavioral and brain responses to auditory notifications, which were either auditory icons or verbal commands. Our EEG/ERP results suggest that verbal commands were more readily recognized by the brain as relevant targets, but that auditory icons were more likely to update contextual working memory. Both classes of notifications did not differ on behavioral measures. This suggests that auditory icons ought to be employed for communicating contextual information and verbal commands, for urgent requests.","2018-04-21","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","1–13","","","","","","Use the Right Sound for the Right Job","CHI '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/8UX7ZIEU/Glatz et al. - 2018 - Use the Right Sound for the Right Job Verbal Comm.pdf","","","auditory displays; autonomous vehicles; electroencephalography; in-vehicle interfaces; notifications","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"46HX3JMD","conferencePaper","2009","Folmer, Eelke; Yuan, Bei; Carr, Dave; Sapre, Manjari","TextSL: a command-based virtual world interface for the visually impaired","Proceedings of the 11th international ACM SIGACCESS conference on Computers and accessibility","978-1-60558-558-1","","10.1145/1639642.1639654","https://dl.acm.org/doi/10.1145/1639642.1639654","The immersive graphics, large amount of user-generated content, and social interaction opportunities offered by popular virtual worlds, such as Second Life, could eventually make for a more interactive and informative World Wide Web. Unfortunately, virtual worlds are currently not accessible to users who are visually impaired. This paper presents the work on developing TextSL, a client for Second life that can be accessed with a screen reader. Users interact with TextSL using a command-based interface, which allows for performing a plethora of different actions on large numbers of objects and avatars; characterizing features of such virtual worlds. User studies confirm that a command-based interface is a feasible approach towards making virtual worlds accessible, as it allows screen reader users to explore Second Life, communicate with other avatars, and interact with objects as well as sighted users. Command-based exploration and object interaction is significantly slower, but communication can be performed with the same efficiency as in the Second Life viewer. We further identify that at least 31% of the objects in Second Life lack a descriptive name, which is a significant barrier towards making virtual worlds accessible to users who are visually impaired.","2009-10-25","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","59–66","","","","","","TextSL","Assets '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/4WZS3VV9/Folmer et al. - 2009 - TextSL a command-based virtual world interface fo.pdf","","","games; visual impairments; screen reader; virtual worlds","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WTDKATJS","conferencePaper","2022","Rosén, Anton Poikolainen; Normark, Maria; Wiberg, Mikael","Noticing the Environment – A Design Ethnography of Urban Farming","Nordic Human-Computer Interaction Conference","978-1-4503-9699-8","","10.1145/3546155.3546659","https://dl.acm.org/doi/10.1145/3546155.3546659","Sustainable HCI attempts to shift focus beyond humans, to care for both ourselves and our environment. In this paper, we build on this growing interest and contribute with a design ethnography of urban farming. We focus on practices of observing and gathering data about the environment which we frame as ‘noticing’. In our analysis, three approaches to noticing the environment were identified, and design suggestions were developed for each approach: Green Thumbs (control-oriented), Dirty Nails (sensibility-oriented) and BeeNoculars (appreciation-oriented). The design suggestions, presented as posters, focus on ways to improve the alignment of the acquisition and display of data with the identified approaches. We discuss two themes: the noticing and balancing of systemic relations and needs, and sensory-rich experiences of the environment. The paper contributes to a broader discussion in HCI of how technologies could create a different understanding of and relationship to the environment.","2022-10-08","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","1–13","","","","","","","NordiCHI '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/CBU7FBMR/Rosén et al. - 2022 - Noticing the Environment – A Design Ethnography of.pdf","","","Environmental Sensing; Ethnography; Noticing; Urban farming","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9D6YTUPT","conferencePaper","2015","Tajadura-Jiménez, Ana; Basia, Maria; Deroy, Ophelia; Fairhurst, Merle; Marquardt, Nicolai; Bianchi-Berthouze, Nadia","As Light as your Footsteps: Altering Walking Sounds to Change Perceived Body Weight, Emotional State and Gait","Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems","978-1-4503-3145-6","","10.1145/2702123.2702374","https://dl.acm.org/doi/10.1145/2702123.2702374","An ever more sedentary lifestyle is a serious problem in our society. Enhancing people's exercise adherence through technology remains an important research challenge. We propose a novel approach for a system supporting walking that draws from basic findings in neuroscience research. Our shoe-based prototype senses a person's footsteps and alters in real-time the frequency spectra of the sound they produce while walking. The resulting sounds are consistent with those produced by either a lighter or heavier body. Our user study showed that modified walking sounds change one's own perceived body weight and lead to a related gait pattern. In particular, augmenting the high frequencies of the sound leads to the perception of having a thinner body and enhances the motivation for physical activity inducing a more dynamic swing and a shorter heel strike. We here discuss the opportunities and the questions our findings open.","2015-04-18","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","2943–2952","","","","","","As Light as your Footsteps","CHI '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/ARCE785K/Tajadura-Jiménez et al. - 2015 - As Light as your Footsteps Altering Walking Sound.pdf","","","emotion; auditory body perception; interaction styles; multimodal interfaces; evaluation method; sonifica-tion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SP83IS36","conferencePaper","2020","Metatla, Oussama; Bardot, Sandra; Cullen, Clare; Serrano, Marcos; Jouffrais, Christophe","Robots for Inclusive Play: Co-designing an Educational Game With Visually Impaired and sighted Children","Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","978-1-4503-6708-0","","10.1145/3313831.3376270","https://dl.acm.org/doi/10.1145/3313831.3376270","Despite being included in mainstream schools, visually impaired children still face barriers to social engagement and participation. Games could potentially help, but games that cater for both visually impaired and sighted players are scarce. We used a co-design approach to design and evaluate a robot-based educational game that could be inclusive of both visually impaired and sighted children in the context of mainstream education. We ran a focus group discussion with visual impairment educators to understand barriers to inclusive play. And then a series of co-design workshops to engage visually impaired and sighted children and educators in learning about robot technology and exploring its potential to support inclusive play experiences. We present design guidelines and an evaluation workshop of a game prototype, demonstrating group dynamics conducive to collaborative learning experiences, including shared goal setting/execution, closely coupled division of labour, and interaction symmetry.","2020-04-23","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","1–13","","","","","","Robots for Inclusive Play","CHI '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/QGAPZDCQ/Metatla et al. - 2020 - Robots for Inclusive Play Co-designing an Educati.pdf","","","education; visual impairment; inclusion; games; co-design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NMEGHXX2","conferencePaper","2011","McGregor, Iain; Larsson, Pontus; Turner, Phil","Evaluating a vehicle auditory display: comparing a designer's expectations with listeners' experiences","Proceedings of the 29th Annual European Conference on Cognitive Ergonomics","978-1-4503-1029-1","","10.1145/2074712.2074731","https://dl.acm.org/doi/10.1145/2074712.2074731","This paper illustrates a method for the early evaluation of auditory displays in context. A designer was questioned about his expectations of an auditory display for Heavy Goods Vehicles, and the results were compared to the experiences of 10 listeners. Sound design is essentially an isolated practice and by involving listeners the process can become collaborative. A review of the level of agreement allowed the identification of attributes that might be meaningful for the design of future auditory displays. Results suggest that traditional auditory display design guidelines that focus on the acoustical properties of sound might not be suitable.","2011-08-24","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","89–92","","","","","","Evaluating a vehicle auditory display","ECCE '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/5FZP8QXD/McGregor et al. - 2011 - Evaluating a vehicle auditory display comparing a.pdf","","","auditory display; designer's expectations; evaluation; listeners' experiences; vehicle","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BZ3IQVQT","conferencePaper","2016","Wilson, Graham; Brewster, Stephen A.","Using Dynamic Audio Feedback to Support Peripersonal Reaching in Young Visually Impaired People","Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility","978-1-4503-4124-0","","10.1145/2982142.2982160","https://dl.acm.org/doi/10.1145/2982142.2982160","Blind children engage with their immediate environment much less than sighted children, particularly through self-initiated movement or exploration. Research has suggested that providing dynamic feedback about the environment and the child's actions within/against it may help to encourage reaching activity and support spatial cognitive learning. This paper investigated whether the accuracy of peripersonal reaching (space within arm's reach) can be improved by the use of dynamic sound from both the objects to reach for and the reaching hand itself (via a worn speaker). We ran two studies that tested the efficacy of static and dynamic audio feedback designs with blind and visually impaired young people, to identify optimal feedback designs. Study 1 was with young adults aged 18 to 22 and Study 2 involved children aged 12 to 17. The results showed that dynamic audio feedback helps to build spatial connections between the objects and the reaching hand and participants were able to reach more accurately, compared to unchanging feedback.","2016-10-23","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","209–218","","","","","","","ASSETS '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/5J3U4Q87/Wilson and Brewster - 2016 - Using Dynamic Audio Feedback to Support Periperson.pdf","","","visual impairment; reaching; sound perception","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XIJ8H7TX","conferencePaper","2023","Hoggenmueller, Marius; Lupetti, Maria Luce; van der Maden, Willem; Grace, Kazjon","Creative AI for HRI Design Explorations","Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction","978-1-4503-9970-8","","10.1145/3568294.3580035","https://dl.acm.org/doi/10.1145/3568294.3580035","Design fixation, a phenomenon describing designers' adherence to pre-existing ideas or concepts that constrain design outcomes, is particularly prevalent in human-robot interaction (HRI), for example, due to collectively held and stabilised imaginations of what a robot should look like or behave. In this paper, we explore the contribution of creative AI tools to overcome design fixation and enhance creative processes in HRI design. In a four weeks long design exploration, we used generative text-to-image models to ideate and visualise robotic artefacts and robot sociotechnical imaginaries. We exchanged results along with reflections through a digital postcard format. We demonstrate the usefulness of our approach to imagining novel robot concepts, surfacing existing assumptions and robot stereotypes, and situating robotic artefacts in context. We discuss the contribution to designerly HRI practices and conclude with lessons learnt for using creative AI tools as an emerging design practice in HRI research and beyond.","2023-03-13","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","40–50","","","","","","","HRI '23","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/FBE342N9/Hoggenmueller et al. - 2023 - Creative AI for HRI Design Explorations.pdf","","","human-robot interaction; design research; creative ai; generative ai; ideation; sociotechnical imaginaries; text-to-image models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HAPKBLS7","conferencePaper","2018","Didakis, Stavros","Computationally-Enhanced Ecologies, Organisms, and Parasites. Speculative Explorations of Symbiotic Oscillations","Proceedings of the 4th Media Architecture Biennale Conference","978-1-4503-6478-2","","10.1145/3284389.3284496","https://dl.acm.org/doi/10.1145/3284389.3284496","This paper presents theoretical and practical research related to computationally-enhanced environments, analysing speculative explorations of media technologies that extend our understanding on alternative modes of architectural realities. An analysis is made concerning this transformation of interior spaces that realise immaterial dimensions and hidden layers of information, demonstrating strategies for computationally-enhanced ecologies that become an essential organism of the functions and aesthetics of space. The case studies presented in this work attempt to focus on how technological development and cutting-edge media practices are utilized to create objects and systems within the physical space for augmenting and amplifying creative processes, and exploring not only new and novel applications, but rather redefine behavior, thought-process, context, and symbiosis.","2018-11-13","2023-07-06 05:00:07","2023-07-06 05:00:07","2023-07-05","84–94","","","","","","","MAB18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/TNLUEMSR/Didakis - 2018 - Computationally-Enhanced Ecologies, Organisms, and.pdf","","","Architecture; Artificial Ecologies; Computational Media; Interactive Design; Speculation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GZFDZ9AD","conferencePaper","2018","Lorenzoni, Valerio; Maes, Pieter-Jan; Van den Berghe, Pieter; De Clercq, Dirk; de Bie, Tijl; Leman, Marc","A biofeedback music-sonification system for gait retraining","Proceedings of the 5th International Conference on Movement and Computing","978-1-4503-6504-8","","10.1145/3212721.3212843","https://dl.acm.org/doi/10.1145/3212721.3212843","Auditory feedbacks are becoming increasingly popular in sports providing opportunities for monitoring and gait (re)training in ecological environments. We present the design process of a sonification strategy for modification of running parameters. The sonification provides real-time feedback of the performance through introduction of distortion of a baseline music track. The music BPM is continuously matched to the runners' cadence. The noise-based continuous feedback was able to significantly alter the mean running cadence in a non-instructed and non-disturbing way and performed better than standard verbal instructions. Although some of the participants did not respond effectively to the feedback, a large majority of the participants positively rated the feedback system in terms of pleasantness and motivation.","2018-06-28","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","1–5","","","","","","","MOCO '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/ZR9EFXTH/Lorenzoni et al. - 2018 - A biofeedback music-sonification system for gait r.pdf","","","Reinforcement learning; Perception; Musical feedback; Running gait; SPM","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HYB7CRR6","conferencePaper","2016","Volioti, Christina; Hadjidimitriou, Stelios; Manitsaris, Sotiris; Hadjileontiadis, Leontios; Charisis, Vasileios; Manitsaris, Athanasios","On mapping emotional states and implicit gestures to sonification output from the 'Intangible Musical Instrument'","Proceedings of the 3rd International Symposium on Movement and Computing","978-1-4503-4307-7","","10.1145/2948910.2948950","https://dl.acm.org/doi/10.1145/2948910.2948950","Sonification is an interdisciplinary field of research, aiming at generating sound from data based on systematic, objective and reproducible transformations. Towards this direction, expressive gestures play an important role in music performances facilitating the artistic perception by the audience. Moreover, emotions are linked with music, as sound has the ability to evoke emotions. In this vein, a combinatory approach which aims at gesture and emotion sonification in the context of music composition and performance is presented here. The added value of the proposed system is that both gesture and emotion are able to continuously manipulate the reproduced sound in real-time.","2016-07-05","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","1–5","","","","","","","MOCO '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/SEZ4HP3H/Volioti et al. - 2016 - On mapping emotional states and implicit gestures .pdf","","","sonification; emotional status; expressive gesture","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"344TKV34","conferencePaper","2019","Ronnberg, Niklas","Musical Elements in Sonification Support Visual Perception","Proceedings of the 31st European Conference on Cognitive Ergonomics","978-1-4503-7166-7","","10.1145/3335082.3335097","https://dl.acm.org/doi/10.1145/3335082.3335097","Visual representations of data are commonly used to communicate research results. However, such representations might introduce several possible challenges for the human visual perception system, for example in perceiving brightness levels. Sonification, adding sound to the visual representation, might be used to overcome these challenges. As sonification provides additional information, sonification could be useful in supporting interpretations of a visual perception. In the present study, usefulness in terms of accuracy of sonification was investigated with an interactive sonification test. In the experiment, participants were asked to identify the highest brightness level in a monochrome visual representation. The task was performed in four conditions, one with no sonification and three with different sonification settings. The results show that sonification is useful, as measured by higher task accuracy.","2019-09-10","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","114–117","","","","","","","ECCE '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/SAJ546VQ/Ronnberg - 2019 - Musical Elements in Sonification Support Visual Pe.pdf","","","interactive sonification; visualization; musical elements","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T2LBJGSK","conferencePaper","2021","Schuller, Björn W.; Virtanen, Tuomas; Riveiro, Maria; Rizos, Georgios; Han, Jing; Mesaros, Annamaria; Drossos, Konstantinos","Towards Sonification in Multimodal and User-friendlyExplainable Artificial Intelligence","Proceedings of the 2021 International Conference on Multimodal Interaction","978-1-4503-8481-0","","10.1145/3462244.3479879","https://dl.acm.org/doi/10.1145/3462244.3479879","We are largely used to hearing explanations. For example, if someone thinks you are sad today, they might reply to your “why?” with “because you were so Hmmmmm-mmm-mmm”. Today’s Artificial Intelligence (AI), however, is – if at all – largely providing explanations of decisions in a visual or textual manner. While such approaches are good for communication via visual media such as in research papers or screens of intelligent devices, they may not always be the best way to explain; especially when the end user is not an expert. In particular, when the AI’s task is about Audio Intelligence, visual explanations appear less intuitive than audible, sonified ones. Sonification has also great potential for explainable AI (XAI) in systems that deal with non-audio data – for example, because it does not require visual contact or active attention of a user. Hence, sonified explanations of AI decisions face a challenging, yet highly promising and pioneering task. That involves incorporating innovative XAI algorithms to allow pointing back at the learning data responsible for decisions made by an AI, and to include decomposition of the data to identify salient aspects. It further aims to identify the components of the preprocessing, feature representation, and learnt attention patterns that are responsible for the decisions. Finally, it targets decision-making at the model-level, to provide a holistic explanation of the chain of processing in typical pattern recognition problems from end-to-end. Sonified AI explanations will need to unite methods for sonification of the identified aspects that benefit decisions, decomposition and recomposition of audio to sonify which parts in the audio were responsible for the decision, and rendering attention patterns and salient feature representations audible. Benchmarking sonified XAI is challenging, as it will require a comparison against a backdrop of existing, state-of-the-art visual and textual alternatives, as well as synergistic complementation of all modalities in user evaluations. Sonified AI explanations will need to target different user groups to allow personalisation of the sonification experience for different user needs, to lead to a major breakthrough in comprehensibility of AI via hearing how decisions are made, hence supporting tomorrow’s humane AI’s trustability. Here, we introduce and motivate the general idea, and provide accompanying considerations including milestones of realisation of sonifed XAI and foreseeable risks.","2021-10-18","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","788–792","","","","","","","ICMI '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/VMK8TG9S/Schuller et al. - 2021 - Towards Sonification in Multimodal and User-friend.pdf","","","sonification; multimodality; Explainable artificial intelligence; human computer interaction; trustworthy artificial intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MBV3ZIQQ","conferencePaper","2017","Kreković, G.; Vican, I.","Towards a Parallel Computing Framework for Direct Sonification of Multivariate Chronological Data","Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences","978-1-4503-5373-1","","10.1145/3123514.3123551","https://dl.acm.org/doi/10.1145/3123514.3123551","This paper presents a generic and scalable framework for direct sonification of large multivariate data sets with an explicit time dimension. As digitalization and the process of data collection gathers momentum in many fields of human activity, such large data sets with many dimensions of different data types are common. The specificity of our framework is uniformness of the synthesis technique on different temporal scales achieved by using direct sonification of particular data rows in corresponding sound grains. This way, both distinctiveness of individual data rows and patterns on the higher scale should become perceivable in the synthesized audio content. In order to attain scalability, the implementation relies on parallel computing.","2017-08-23","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","1–4","","","","","","","AM '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/9S94CBIF/Kreković and Vican - 2017 - Towards a Parallel Computing Framework for Direct .pdf","","","sonification; auditory display; sound synthesis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E7X33KIP","conferencePaper","2021","Emsley, Iain; Chamberlain, Alan","Sounding out the System: Multidisciplinary Web Science Platforms for Creative Sonification","Companion Publication of the 13th ACM Web Science Conference 2021","978-1-4503-8525-1","","10.1145/3462741.3466667","https://dl.acm.org/doi/10.1145/3462741.3466667","In this paper, we present our initial findings in using digital methods to consider the way that different devices can connect to the same object. We take a more experimental view of the ways in which network data might be used in compositions to help us to move beyond traditional sonification techniques into more musical territories which enables us to start to understand the ways in which archival data and tools might be used as a creative response to the data and provide a more human way of engaging with the data archive. Such approaches can inform the ways in which future research platforms for Web Science can be developed in a truly multidisciplinary way which matches the needs of the wider research community and supports public engagement.","2021-06-21","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","50–52","","","","","","Sounding out the System","WebSci '21 Companion","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/NQ6G4V64/Emsley and Chamberlain - 2021 - Sounding out the System Multidisciplinary Web Sci.pdf","","","Sonification; Networks; Mobile Devices; Tool Criticism; Web Science","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GSJC9FDD","conferencePaper","2017","Burloiu, Grigore; Damian, Ştefan; Golumbeanu, Bogdan; Mihai, Valentin","Structured interaction in the SoundThimble real-time gesture sonification framework","Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences","978-1-4503-5373-1","","10.1145/3123514.3123543","https://dl.acm.org/doi/10.1145/3123514.3123543","We introduce SoundThimble, a design platform for layered sonic interaction based on the relationship between human motion and virtual objects in 3D space. A Vicon motion capture system and custom software are used to track, interpret and sonify the movement and gestures of a performer relative to a virtual object. We define three possible interaction dynamics, centred around object search, manipulation and arrangement. We explore the resulting possibilities for layered structures and extended perception and expression. The software developed is open source and portable to similar hardware systems, leaving room for further extension of the interaction mechanics.","2017-08-23","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","1–4","","","","","","","AM '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/LQ9UY4H6/Burloiu et al. - 2017 - Structured interaction in the SoundThimble real-ti.pdf","","","Sonification; motion capture; interactive sound installation; interaction design; gesture spotting","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7XX8EPQN","conferencePaper","2016","Kolykhalova, Ksenia; Alborno, Paolo; Camurri, Antonio; Volpe, Gualtiero","A serious games platform for validating sonification of human full-body movement qualities","Proceedings of the 3rd International Symposium on Movement and Computing","978-1-4503-4307-7","","10.1145/2948910.2948962","https://dl.acm.org/doi/10.1145/2948910.2948962","In this paper we describe a serious games platfrom for validating sonification of human full-body movement qualities. This platform supports the design and development of serious games aiming at validating (i) our techniques to measure expressive movement qualities, and (ii) the mapping strategies to translate such qualities in the auditory domain, by means of interactive sonification and active music experience. The platform is a part of a more general framework developed in the context of the EU ICT H2020 DANCE ""Dancing in the dark"" Project n.645553 that aims at enabling the perception of nonverbal artistic whole-body experiences to visual impaired people.","2016-07-05","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","1–5","","","","","","","MOCO '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/8FEKEY4K/Kolykhalova et al. - 2016 - A serious games platform for validating sonificati.pdf","","","sonification; movement qualities; movement analysis; Serious game","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RF5WNMZM","conferencePaper","2021","Dal Rì, Francesco; Masu, Raul","Zugzwang: Chess Representation Combining Sonification and Interactive Performance","Proceedings of the 16th International Audio Mostly Conference","978-1-4503-8569-5","","10.1145/3478384.3478394","https://dl.acm.org/doi/10.1145/3478384.3478394","In this paper we present the design process and the preliminary evaluations of Zugzwang. The system implemented a mixed approach to represent a chess game by combining direct sonification of the moves with instrumental improvisation. The improvisation is guided by a score displayed in real-time that represent the thinking process of the player based on the duration of the moves. To design the system we involved a professional chess player to provide suggestions. We also present a preliminary evaluation of the system with feedback from two chess players.","2021-10-15","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","89–92","","","","","","Zugzwang","AM '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/29GTPRTN/Dal Rì and Masu - 2021 - Zugzwang Chess Representation Combining Sonificat.pdf","","","Sonification; Chess; Mixed Media Performance; Screen Scores","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UGDMYDIV","conferencePaper","2019","Turchet, Luca","Interactive sonification and the IoT: the case of smart sonic shoes for clinical applications","Proceedings of the 14th International Audio Mostly Conference: A Journey in Sound","978-1-4503-7297-8","","10.1145/3356590.3356631","https://dl.acm.org/doi/10.1145/3356590.3356631","To date, little attention has been devoted by the research community to applications of the Internet of Things (IoT) paradigm to the field of interactive sonification. The IoT has the potential to facilitate the emergence of novel forms of interactive sonifications that are the result of shared control of the sonification system by both the user performing the gestures locally to the system itself, and one or more remote users. This can for instance impact therapies based on auditory feedback where the control of the sound generation may be shared by patients and doctors remotely connected. This paper describes a prototype of connected shoes for interactive sonification that can be remotely controlled and can collect data about the gait of a walker. The system targets primarily clinical applications where sound stimuli are utilized to help guide and improve walking actions of patients with motor impairments.","2019-09-18","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","252–255","","","","","","Interactive sonification and the IoT","AM'19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/NN6LZYWP/Turchet - 2019 - Interactive sonification and the IoT the case of .pdf","","","Interactive sonification; Internet of Things","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N2W9JCA7","conferencePaper","2019","Emsley, Iain; Roure, David de; Willcox, Pip; Chamberlain, Alan","Performing Shakespeare: From Symbolic Notation to Sonification","Proceedings of the 14th International Audio Mostly Conference: A Journey in Sound","978-1-4503-7297-8","","10.1145/3356590.3356614","https://dl.acm.org/doi/10.1145/3356590.3356614","We present an ongoing project using Joshua Steele's symbolic notation to represent prosody in Eighteenth Century dramatic performances. We discuss the sonification of the original notation to simulate the work and how it can be used to support other experiments. Drawing on two experimental models and their different methodologies, we consider how the digitised version relates to the historical work and the challenges that they bring. The framework and challenges for marking up the current work using semantic web technology, such as the PROV ontology, we demonstrate a notebook tool that links user annotations to the generated audio model, to store revisions and edits for re-use. This project demonstrates sonification's use as an experimental Humanities tool and as a way of thinking about historical prosody models.","2019-09-18","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","154–159","","","","","","Performing Shakespeare","AM'19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/3AM2ZEAI/Emsley et al. - 2019 - Performing Shakespeare From Symbolic Notation to .pdf","","","Sonification; Digital Humanities; Shakespeare","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RD7RH7D9","conferencePaper","2013","Françoise, Jules","Gesture--sound mapping by demonstration in interactive music systems","Proceedings of the 21st ACM international conference on Multimedia","978-1-4503-2404-5","","10.1145/2502081.2502214","https://dl.acm.org/doi/10.1145/2502081.2502214","In this paper we address the issue of mapping between gesture and sound in interactive music systems. Our approach, we call mapping by demonstration, aims at learning the mapping from examples provided by users while interacting with the system. We propose a general framework for modeling gesture--sound sequences based on a probabilistic, multimodal and hierarchical model. Two orthogonal modeling aspects are detailed and we describe planned research directions to improve and evaluate the proposed models.","2013-10-21","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","1051–1054","","","","","","","MM '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/LK8W37AM/Françoise - 2013 - Gesture--sound mapping by demonstration in interac.pdf","","","multimodal; mapping; sound synthesis; gesture; hierarchical modeling; hmm; music performance","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8TB8JPYJ","conferencePaper","2017","Sasaki, Daisuke; Nakajima, Musashi; Kanno, Yoshihiro","AQUBE: an interactive music reproduction system for aquariums","Proceedings of the 19th ACM International Conference on Multimodal Interaction","978-1-4503-5543-8","","10.1145/3136755.3143030","https://dl.acm.org/doi/10.1145/3136755.3143030","Aqube is an interactive music reproduction system aimed to enhance communication between visitors of aquariums. We propose a musical experience in the aquarium using image processing for visual information of marine organisms and sound reproductions which is related to state of the aquarium. Visitors set three color cubes in front of the aquarium and system produce musical feedback when marine organisms swim over the cube. Visitors will be able to participate producing music of exhibition space collaboratively with Aqube.","2017-11-03","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","512–513","","","","","","AQUBE","ICMI '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/LM5ILS4K/Sasaki et al. - 2017 - AQUBE an interactive music reproduction system fo.pdf","","","Music Spatial Presentation Real-time Reproduction Optical-flow Color-tracking Interaction Aquarium","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PTH2UVAY","conferencePaper","2017","Blanco, Andrea Lorena Aldana; Grautoff, Steffen; Hermann, Thomas","CardioSounds: Real-time Auditory Assistance for Supporting Cardiac Diagnostic and Monitoring","Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences","978-1-4503-5373-1","","10.1145/3123514.3123542","https://dl.acm.org/doi/10.1145/3123514.3123542","This paper presents a real-time sonification system for Electrocardiography (ECG) monitoring and diagnostic. We introduce two novel sonification designs: (a) Auditory magnification loupe, a method to sonify important beat-to-beat variations when doing sports activities, and (b) ST-segment water ambience sonification, which aims to assist clinicians in the diagnostic process by building a soundscape that exhibits ECG signal abnormalities as the analysed signal deviates from a healthy ECG. The proposed methods were designed to assist users to unobtrusively monitor their own (or their patients') heart signal in situations when a visual-only representation is not convenient for the proper fulfilment of a given task. Using CardioSounds users receive auditory feedback in order to monitor important heart rhythm disturbances (e.g. Arrhythmia) or pathologies due to a blocking of the heart's vessels.","2017-08-23","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","1–4","","","","","","CardioSounds","AM '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/6WRMQ2XL/Blanco et al. - 2017 - CardioSounds Real-time Auditory Assistance for Su.pdf","","","Sonification; Biofeedback; Electrocardiogram; Process Monitoring; Real-time System","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IUB4S2GI","conferencePaper","2008","Knees, Peter; Pohle, Tim; Widmer, Gerhard","Sound/tracks: real-time synaesthetic sonification of train journeys","Proceedings of the 16th ACM international conference on Multimedia","978-1-60558-303-7","","10.1145/1459359.1459592","https://dl.acm.org/doi/10.1145/1459359.1459592","Travelling on a train and looking out of the window at the moving scenery reveals a composition of ""visual music"" with its own tempo and rhythm, its own colours and harmonies. The project sound/tracks aims at capturing these visual impressions and translates them into a musical composition in real-time - producing an immediate and unique soundtrack to the train journey based on the passing landscape. To this end, the outside impressions are captured with a camera and translated into instantaneously played back piano music. The immediately added sound dimension allows for reflection of the visual impression and deepening of the state of contemplation. For the resulting compositions, the passing scenery can be considered the score. ""Re-transcription"" of this score to an image gives a panoramic overview over the complete journey and exhibits some interesting effects caused by the movement of the train, such as compression and stretching of passing objects. In addition to intensifying the experience of a train journey, sound/tracks permits to persistently capture and archive the fleeting impressions of journey and composition and allows for re-experiencing the trip both visually and acoustically at a later point.","2008-10-26","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","1117–1118","","","","","","Sound/tracks","MM '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/S3MF4I9V/Knees et al. - 2008 - Soundtracks real-time synaesthetic sonification .pdf","","","mobile music generation; train journey; real-time sonification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RSDKPG3A","conferencePaper","2017","Yang, Jiajun; Hermann, Thomas","Mode Explorer: Using Model-based Sonification to Investigate Basins of Attraction","Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences","978-1-4503-5373-1","","10.1145/3123514.3123525","https://dl.acm.org/doi/10.1145/3123514.3123525","This paper presents a novel interactive auditory data exploration method to investigate features of high-dimensional data distributions. The Mode Explorer couples a scratching-interaction on a 2D scatter plot of high-dimensional data to real-time dynamical processes, excited in data space at the nearest mode in the probability density function (pdf) obtained by kernel-density estimation. Specifically, the sign-inverted pdf is used as a potential function in which test particles perform oscillations at low friction, yielding signals that can directly be played back as sound. This Model-based sonification approach is used to interactively search the distribution for different modes, learn about their details, i.e. the Hessian matrix at the mode, and thus enable a non-parametric parameter selection for appropriate bandwidth.","2017-08-23","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","1–4","","","","","","Mode Explorer","AM '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/VYVXBFPK/Yang and Hermann - 2017 - Mode Explorer Using Model-based Sonification to I.pdf","","","Exploratory Data Analysis; Kernel-density estimation; Model-based Sonification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TRRLVZ2R","conferencePaper","2016","Chernyshov, George; Tag, Benjamin; Chen, Jiajun; Noriyasu, Vontin; Lukowicz, Paul; Kunze, Kai","Wearable ambient sound display: embedding information in personal music","Proceedings of the 2016 ACM International Symposium on Wearable Computers","978-1-4503-4460-9","","10.1145/2971763.2971789","https://dl.acm.org/doi/10.1145/2971763.2971789","In this paper we explore how to embed information into users' music playlists while limiting the obtrusiveness to the user. We focus on continuous over time information rather than discrete information (e.g. ""monitoring traffic flow"" versus ""new email received""). We are presenting results for a traffic monitoring task (4 levels) for 10 users that are engaged in either a cognitive (Sudoku solving) or one of two physical tasks (running on a treadmill or playing table tennis), with up to a 98 % detection rate.","2016-09-12","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","58–59","","","","","","Wearable ambient sound display","ISWC '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/GMRURNVD/Chernyshov et al. - 2016 - Wearable ambient sound display embedding informat.pdf","","","sonification; process monitoring; distraction free; frequency filtering; wearable audio display","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3D5L5QFG","conferencePaper","2020","Granzow, John; Vilaplana, Matias; Çamcı, Anıl","Capturing kinetic wave demonstrations for sound control","Proceedings of the 15th International Audio Mostly Conference","978-1-4503-7563-4","","10.1145/3411109.3411150","https://dl.acm.org/doi/10.1145/3411109.3411150","In musical acoustics, wave propagation, reflection, phase inversion, and boundary conditions can be hard to conceptualize. Physical kinetic wave demonstrations offer visible and tangible experiences of wave behavior and facilitate active learning. We implement such kinetic demonstrations, a long spring and a Shive machine, using contemporary fabrication techniques. Furthermore, we employ motion capture (MoCap) technology to transform these kinetic assemblies into audio controllers. Time-varying coordinates of Mo-Cap markers integrated into the assemblies are mapped to audio parameters, closing a multi-sensory loop where visual analogues of acoustic phenomena are in turn used to control digital audio. The project leads to a pedagogical practice where fabrication and sensing technologies are used to reconstitute demonstrations for the eye as controllers for the ear.","2020-09-16","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","273–276","","","","","","","AM '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/UQD35HYM/Granzow et al. - 2020 - Capturing kinetic wave demonstrations for sound co.pdf","","","sonification; motion capture; sonic interaction design; augmented reality; digital fabrication; music education; musical acoustics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JXME2UDT","conferencePaper","2022","Paquete, Hugo; Bernardino Bastos, Paulo; Fernandes-Marcos, Adérito","Orbital Eccentricity. Sound performance, using commercial and military satellites with real time tracking data","10th International Conference on Digital and Interactive Arts","978-1-4503-8420-9","","10.1145/3483529.3483748","https://dl.acm.org/doi/10.1145/3483529.3483748","This music performance starts with an inquiring about the possibility to generate sound and music elements using commercial and military satellites, established in a process of acquirement and conversion of satellite movement data sonified in real time, merged to midi-data language. Used to control hardware and software musical instruments. It`s importance, reflects on the autonomy of the satellites as objectual performers, actants that generate sonic content in an ecology of casual movements and programmed computational music rules. The routes and trajectories are mediated elements to think about composition in a performative dynamic environmental system, manipulated in real time by the performer in direct dialog with the external technological body. The satellite as an actant suspended in the edge of the human perceptive border that articulate a direct relation with the planet Earth as a place with external telematic objects. It represents the human activity in the boundaries of the universe limits. This performance starts with the production of hardware and software that captures the movement of public and military satellites. In technical collaboration and partnership with Christopher Zlaket (1992) from the Arizona State University who specializes in interface design and David Stingley (1993) of MIT who specializes in computer science. The sonic qualities are dependent of improvisational approaches developed in real time, pointing to aesthetic elements about dynamics, granulation, noise, and drone. Pointing to post-digital and micro sound aesthetics traditions and proposing ruptures.","2022-02-20","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","1–4","","","","","","","ARTECH 2021","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/R975XDFH/Paquete et al. - 2022 - Orbital Eccentricity. Sound performance, using com.pdf","","","Sonification; Sound art; Performance; Micro Sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CGCIE5FY","conferencePaper","2010","Henthorne, Cody; Tilevich, Eli","Sonifying performance data to facilitate tuning of complex systems: performance tuning: music to my ears","Proceedings of the ACM international conference companion on Object oriented programming systems languages and applications companion","978-1-4503-0240-1","","10.1145/1869542.1869548","https://dl.acm.org/doi/10.1145/1869542.1869548","In the modern computing landscape, the challenge of tuning software systems is exacerbated by the necessity to accommodate multiple divergent execution environments and stakeholders. Achieving optimal performance requires a different configuration for every combination of hardware setups and business requirements. In addition, the state of the art in system tuning can involve complex statistical models, which require deep expertise not commonly possessed by the average software developer. This paper presents a novel approach to tuning complex software systems by leveraging sound to convey performance information during execution. We conducted a scientific survey to determine which sound characteristics (e.g., loudness, panning, pitch, tempo, etc.) are most accurate to express information to the average programmer. As determined by the survey, the characteristics that scored the highest across all the participants were used to create a proof-of-concept demonstration. The demonstration showed that a programmer who is not an expert in either software tuning or enterprise computing can configure the parameters of a real world enterprise application server, so that its resulting performance surpasses that exhibited under the standard configuration. Our results indicate that sound-based tuning approaches can provide valuable solutions to the challenges of configuring complex computer systems.","2010-10-17","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","35–42","","","","","","Sonifying performance data to facilitate tuning of complex systems","OOPSLA '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/5UPSBV2E/Henthorne and Tilevich - 2010 - Sonifying performance data to facilitate tuning of.pdf","","","sonification; empirical studies; enterprise application servers; J2EE; performance tuning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G7Z2FNBG","conferencePaper","2020","Groß-Vogt, Katharina","The drinking reminder: prototype of a smart jar","Proceedings of the 15th International Audio Mostly Conference","978-1-4503-7563-4","","10.1145/3411109.3411130","https://dl.acm.org/doi/10.1145/3411109.3411130","The drinking reminder is a smart jar that reminds and motivates its user to drink a certain amount of water over time. The prototype is based on Arduino soft- and hardware and has been used as an exploration platform for sonic interaction design. The presented implementation follows a peripheral approach, using bird sounds.","2020-09-16","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","257–260","","","","","","The drinking reminder","AM '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/3SNVNLQS/Groß-Vogt - 2020 - The drinking reminder prototype of a smart jar.pdf","","","sonification; sonic interaction design; augmented reality","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FSEUXENW","conferencePaper","2016","Katan, Simon","Using Interactive Machine Learning to Sonify Visually Impaired Dancers' Movement","Proceedings of the 3rd International Symposium on Movement and Computing","978-1-4503-4307-7","","10.1145/2948910.2948960","https://dl.acm.org/doi/10.1145/2948910.2948960","This preliminary research investigates the application of Interactive Machine Learning (IML) to sonify the movements of visually impaired dancers. Using custom wearable devices with localized sound, our observations demonstrate how sonification enables the communication of time-based information about movements such as phrase length and periodicity, and nuanced information such as magnitudes and accelerations. The work raises a number challenges regarding the application of IML to this domain. In particular we identify a need for ensuring even rates of change in regression models when performing sonification and a need for consideration of how to convey machine learning approaches to end users.","2016-07-05","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","1–4","","","","","","","MOCO '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/4IBQ3JTF/Katan - 2016 - Using Interactive Machine Learning to Sonify Visua.pdf","","","Sonification; Dance; Accessible Interfaces; Interactive Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R8FZ7WMA","conferencePaper","2017","Nanayakkara, Suranga; Schroepfer, Thomas; Wyse, Lonce; Lian, Aloysius; Withana, Anusha","SonicSG: from floating to sounding pixels","Proceedings of the 8th Augmented Human International Conference","978-1-4503-4835-5","","10.1145/3041164.3041190","https://dl.acm.org/doi/10.1145/3041164.3041190","SonicSG aimed at fostering a holistic understanding of the ways in which technology is changing our thinking about design in high-density urban city and how its creative use can reflect a sense of place. The project consisted of a large-scale interactive light installation that consisted on 1,800 floating LED lights in the shape of the island nation. These lights were individually addressable through the network and used to generate light and sound effects. The field of light was extended with ""sonified personal pixels"" that were created by the audience through personal mobile devices. These personal pixels generated a light and sound ""texture"" that connected visitors to the light field in the river and to each other. In this paper, we describe the design concept, prototyping and, implementation of as well as the user reactions to this interactive public light installation.","2017-03-16","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","1–5","","","","","","SonicSG","AH '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/4XQ83X3W/Nanayakkara et al. - 2017 - SonicSG from floating to sounding pixels.pdf","","","sonification; experience design; interactive public installation; placemaking; smart city; urban lighting","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TNELJ5VG","conferencePaper","2018","Volta, Erica; Volpe, Gualtiero","Exploiting multimodal integration in adaptive interactive systems and game-based learning interfaces","Proceedings of the 5th International Conference on Movement and Computing","978-1-4503-6504-8","","10.1145/3212721.3212849","https://dl.acm.org/doi/10.1145/3212721.3212849","The main purpose of my work is to investigate multisensory and multimodal integration in the design and development of adaptive systems and interfaces for game-based learning applications in the areas of education and rehabilitation. To this aim, I contributed to the creation of a multimodal dataset of violin performances, integrating motion capture, video, audio, and on-body sensors (accelerometers and EMG), and I worked closely with psychophysicists and educators on the design of paradigms and technologies for multisensory and embodied learning of mathematics in primary school children. Main theoretical foundations of my research are multisensory processing and integration, psychophysics analysis, embodied cognition theories, computational models of non-verbal and emotion communication in full-body movement, and human-computer interaction models for adaptive interfaces and serious-games.","2018-06-28","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","1–4","","","","","","","MOCO '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/2FPE9B7S/Volta and Volpe - 2018 - Exploiting multimodal integration in adaptive inte.pdf","","","Sonification; Movement Analysis; Multimodal Interactive Systems; Multimodal Technologies; Multisensory learning; Multisensory perception; Music Learning; Perceptual learning; Serious games","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UMC2A75J","conferencePaper","2020","Winters, R. Michael; Koziej, Stephanie","An auditory interface for realtime brainwave similarity in dyads","Proceedings of the 15th International Audio Mostly Conference","978-1-4503-7563-4","","10.1145/3411109.3411147","https://dl.acm.org/doi/10.1145/3411109.3411147","We present a case-study in the development of a""hyperscanning"" auditory interface that transforms realtime brainwave-similarity between interacting dyads into music. Our instrument extends reality in face-to-face communication with a musical stream reflecting an invisible socio-neurophysiological signal. This instrument contributes to the historical context of brain-computer interfaces (BCIs) applied to art and music, but is unique because it is contingent on the correlation between the brainwaves of the dyad, and because it conveys this information using entirely auditory feedback. We designed the instrument to be i) easy to understand, ii) relatable and iii) pleasant for members of the general public in an exhibition context. We present how this context and user group led to our choice of EEG hardware, inter-brain similarity metric, and our auditory mapping strategy. We discuss our experience following four public exhibitions, as well as future improvements to the instrument design and user experience.","2020-09-16","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","261–264","","","","","","","AM '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/GXPIE5V5/Winters and Koziej - 2020 - An auditory interface for realtime brainwave simil.pdf","","","sonification; augmented reality; sound art; brain-computer interfaces; audio; neuroscience; social; sound interaction design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BJBYMBAH","conferencePaper","2019","Mathiesen, Signe Lund; Byrne, Derek Victor; Wang, Qian Janice","Sonic Mug: A Sonic Seasoning System","Proceedings of the 14th International Audio Mostly Conference: A Journey in Sound","978-1-4503-7297-8","","10.1145/3356590.3356634","https://dl.acm.org/doi/10.1145/3356590.3356634","This paper outlines the development of an in-progress prototype system that explores the interplay between sonic interaction and eating activities. The music-playing mug prototype is designed as a physical interface which aligns the user's senses with the act of drinking. Drinking from the mug involves multiple senses, including tactile interaction with the mug, gustatory stimuli from the beverage, and by engaging with the sonic mug, the user becomes attentive towards the onset of the sound when drinking, thereby involving the sense of hearing as well. The system is being developed as an experiential piece which allows the user to explore the nature of multisensory perception and to experience how what we taste can be influenced by what we listen to. An initial pilot study was carried out, revealing a relationship between sound liking and taste evaluation, in addition to certain design challenges to be addressed in subsequent iterations. In this paper, we discuss these issues and propose new directions for the development of the prototype.","2019-09-18","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","264–267","","","","","","Sonic Mug","AM'19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/CMMH2UKN/Mathiesen et al. - 2019 - Sonic Mug A Sonic Seasoning System.pdf","","","sonification; HCI; musicology; sound design; interaction design; crossmodal correspondences; flavour; multisensory; taste","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GTI457E6","conferencePaper","2017","Martin, Charles P.; Ellefsen, Kai Olav; Torresen, Jim","Deep Models for Ensemble Touch-Screen Improvisation","Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences","978-1-4503-5373-1","","10.1145/3123514.3123556","https://dl.acm.org/doi/10.1145/3123514.3123556","For many, the pursuit and enjoyment of musical performance goes hand-in-hand with collaborative creativity, whether in a choir, jazz combo, orchestra, or rock band. However, few musical interfaces use the affordances of computers to create or enhance ensemble musical experiences. One possibility for such a system would be to use an artificial neural network (ANN) to model the way other musicians respond to a single performer. Some forms of music have well-understood rules for interaction; however, this is not the case for free improvisation with new touch-screen instruments where styles of interaction may be discovered in each new performance. This paper describes an ANN model of ensemble interactions trained on a corpus of such ensemble touch-screen improvisations. The results show realistic ensemble interactions and the model has been used to implement a live performance system where a performer is accompanied by the predicted and sonified touch gestures of three virtual players.","2017-08-23","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","1–4","","","","","","","AM '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/TINLTC33/Martin et al. - 2017 - Deep Models for Ensemble Touch-Screen Improvisatio.pdf","","","deep learning; ensemble interaction; mobile music; RNN; touch screen performance","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XFWDS77F","conferencePaper","2015","Encinas, Enrique; Koulidou, Konstantia; Mitchell, Robb","The kraftwork and the knittstruments: augmenting knitting with sound","Proceedings of the 6th Augmented Human International Conference","978-1-4503-3349-8","","10.1145/2735711.2735833","https://dl.acm.org/doi/10.1145/2735711.2735833","This paper presents a novel example of technological augmentation of a craft practice. By translating the skilled, embodied knowledge of knitting practice into the language of sound, our study explores how audio augmentation of routinized motion patterns affects an individual's awareness of her bodily movements and alters conventional practice. Four different instruments (The Knittstruments: The ThereKnitt, The KnittHat, The Knittomic, and The KraftWork) were designed and tested in four different locations. This research entails cycles of data collection and analysis based on the action and grounded theory methods of noting, coding and memoing. Analysis of the data collected suggests substantial alterations in the knitters performance due to audio feedback at both an individual and group level and improvisation in the process of making. We argue that the usage of Knittstruments can have relevant consequences in the fields of interface design, wearable computing or artistic and musical creation in general and hope to provide a new inspiring venue for designers, artists and knitters to explore.","2015-03-09","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","49–52","","","","","","The kraftwork and the knittstruments","AH '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/EXRN6C4E/Encinas et al. - 2015 - The kraftwork and the knittstruments augmenting k.pdf","","","sonification; improvisation; embodied interaction; craft; instrument; knitting; skilled practice","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QPLM2TVQ","conferencePaper","2022","Sharma, Gulshan","Physiological Sensing for Media Perception & Activity Recognition","Proceedings of the 2022 International Conference on Multimodal Interaction","978-1-4503-9390-4","","10.1145/3536221.3557026","https://dl.acm.org/doi/10.1145/3536221.3557026","Wearable sensors have the intriguing potential to continuously evaluate human physiological characteristics in real-time without being obtrusive. This thesis aims to incorporate physiological sensors data to investigate the Media Perception and Activity Recognition. Our primary research goals include (a) neural encoding-based psycho-acoustic attribute analysis for data sonification, (b) empirical evidence for perceptual subjectivity in neural encoding during human-media interactions, the impact of incorporating behavioral ratings, and (c) the efficacy of attention-based transformer models on physiological data on human activity recognition problems.","2022-11-07","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","696–700","","","","","","","ICMI '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/23UG9PYG/Sharma - 2022 - Physiological Sensing for Media Perception & Activ.pdf","","","EEG; Data Sonification; Human Activity Recognition; Music Entrainment; Physiological Sensing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YPFP92VN","conferencePaper","2017","Ghisio, Simone; Volta, Erica; Alborno, Paolo; Gori, Monica; Volpe, Gualtiero","An open platform for full-body multisensory serious-games to teach geometry in primary school","Proceedings of the 1st ACM SIGCHI International Workshop on Multimodal Interaction for Education","978-1-4503-5557-5","","10.1145/3139513.3139523","https://dl.acm.org/doi/10.1145/3139513.3139523","Recent results from psychophysics and developmental psychology show that children have a preferential sensory channel to learn specific concepts. In this work, we explore the possibility of developing and evaluating novel multisensory technologies for deeper learning of arithmetic and geometry. The main novelty of such new technologies comes from the renewed understanding of the role of communication between sensory modalities during development that is that specific sensory systems have specific roles for learning specific concepts. Such understanding suggests that it is possible to open a new teaching/learning channel, personalized for each student based on the child’s sensory skills. Multisensory interactive technologies exploiting full-body movement interaction and including a hardware and software platform to support this approach will be presented and discussed. The platform is part of a more general framework developed in the context of the EU-ICT-H2020 weDRAW Project that aims to develop new multimodal technologies for multisensory serious-games to teach mathematics concepts in the primary school.","2017-11-13","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","49–52","","","","","","","MIE 2017","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/R2IA4YD7/Ghisio et al. - 2017 - An open platform for full-body multisensory seriou.pdf","","","sonification; Multisensory learning; automated full-body movement analysis; multimodal technologies; serious games","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZDI8I2XV","conferencePaper","2010","Breinbjerg, Morten; Riis, Morten S.; Ebsen, Tobias; Lunding, Rasmus B.","Experiencing the non-sensuous: on measurement, representation and conception in urban art installations","Proceedings of the 6th Nordic Conference on Human-Computer Interaction: Extending Boundaries","978-1-60558-934-3","","10.1145/1868914.1868986","https://dl.acm.org/doi/10.1145/1868914.1868986","In this paper we discuss the conflict between a scientific and an artistic approach to interface design in an urban experience-oriented installation, we designed for the Hopenhagen LIVE activities in Copenhagen during the COP15 climate summit meeting in December 2009. The installation called ""Atmosphere -- the sound and sight of CO2"" converted data from CO2 measurements to sound and visuals presented through headphones and on a 2-meter high, quadrant sculpture that functioned as a transparent, low resolution LED screen. Hereby a normally non-sensuous phenomenon became visible and audible giving the public sensuous access to the symbolic villain of climate change: Carbon dioxide. What the sound and visuals actually represented and how it was conceived is a rather complex question that is fundamental to the artistic concept and of epistemological concern for this paper.","2010-10-16","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","611–614","","","","","","Experiencing the non-sensuous","NordiCHI '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/FMVQ3PBK/Breinbjerg et al. - 2010 - Experiencing the non-sensuous on measurement, rep.pdf","","","sonification; visualization; interface design; measurement and epistemology; representation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J2UKZU8K","conferencePaper","2015","Brown, Courtney; Paine, Garth","Interactive Tango Milonga: designing internal experience","Proceedings of the 2nd International Workshop on Movement and Computing","978-1-4503-3457-0","","10.1145/2790994.2791013","https://dl.acm.org/doi/10.1145/2790994.2791013","The Argentine tango concept of connection refers to the experience of complete synchronicity between self, partner, and music. This paper presents Interactive Tango Milonga, an interactive system giving tango dancers agency over music in order to increase this sense of relation between both partners and music. Like an improvising musician in an ensemble, each dancer receives musical feedback from both her movements and her partner's. Thus, each dancer can respond to the music, driving musical feedback, thereby heightening awareness and agency in both the sound and her partner's movements. Via presentation of this system, this paper illustrates methods for developing interactive systems engaging with distinct musical, movement, and social traditions as well for composing sound-movement relationships leading to specific internal experiences within these social contexts.","2015-08-14","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","17–20","","","","","","Interactive Tango Milonga","MOCO '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/PRS482TM/Brown and Paine - 2015 - Interactive Tango Milonga designing internal expe.pdf","","","motion capture; interactive dance; Argentine tango; mapping strategies; social dance","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GDZQYPW2","conferencePaper","2018","Dahl, Luke; Visi, Federico","Modosc: A Library of Real-Time Movement Descriptors for Marker-Based Motion Capture","Proceedings of the 5th International Conference on Movement and Computing","978-1-4503-6504-8","","10.1145/3212721.3212842","https://dl.acm.org/doi/10.1145/3212721.3212842","Marker-based motion capture systems that stream precise movement data in real-time afford interaction scenarios that can be subtle, detailed, and immediate. However, challenges to effectively utilizing this data include having to build bespoke processing systems which may not scale well, and a need for higher-level representations of movement and movement qualities. We present modosc, a set of Max abstractions for computing motion descriptors from raw motion capture data in real time. Modosc is designed to address the data handling and synchronization issues that arise when working with complex marker sets, and to structure data streams in a meaningful and easily accessible manner. This is achieved by adopting a multiparadigm programming approach using o.dot and Open Sound Control. We describe an initial set of motion descriptors, the addressing system employed, and design decisions and challenges.","2018-06-28","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","1–4","","","","","","Modosc","MOCO '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/CT8BWI8A/Dahl and Visi - 2018 - Modosc A Library of Real-Time Movement Descriptor.pdf","","","interaction design; expressive movement; Motion capture; Max; modosc; motion analysis; motion descriptors; Open Sound Control","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"43LE6ECX","conferencePaper","2022","Bergsland, Andreas","Dance phrase onsets and endings in an interactive dance study","Proceedings of the 17th International Audio Mostly Conference","978-1-4503-9701-8","","10.1145/3561212.3561242","https://dl.acm.org/doi/10.1145/3561212.3561242","The paper describes a work-in-progress exploring the expressive and creative potential of dance phrase onsets and endings in interactive dance, using an artistic research approach. It briefly delineates the context of the presented work, before describing the technical setup applied, both in terms of hardware and software. The main part of the paper is concerned with the specific mappings of three different sections in the performance that the project resulted in. Subsequently, the process and performance are evaluated, including both the dancer’s feedback and observations by the author. The points from the evaluation are then discussed with reference to relevant research literature. Findings include that the dancer experienced an increased awareness of beginnings and endings in different sections of the performance, and that postural adjustments were necessary to make the interaction more robust.","2022-10-10","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","199–202","","","","","","","AM '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/PJCC68DL/Bergsland - 2022 - Dance phrase onsets and endings in an interactive .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GHVRQGE3","conferencePaper","2018","Johnson, Garrett Laroy; Peterson, Britta Joy; Ingalls, Todd; Wei, Sha Xin","Lanterns: An Enacted and Material Approach to Ensemble Group Activity with Responsive Media","Proceedings of the 5th International Conference on Movement and Computing","978-1-4503-6504-8","","10.1145/3212721.3212848","https://dl.acm.org/doi/10.1145/3212721.3212848","This paper takes an empirical and processual approach to the study of coordinated group activity through enacted and experimental movement research with responsive media. We focus on the dynamic emergence of what we will refer to as ensemble. Informed by readings from process philosophy and new materialism, we take into account the vitality of matter in investigating a notion of subjectivity unhinged from compartmentalized conceptions of the human. Methodologically we suspend ontological assumptions which occlude, mask, or ignore emergent relations and unprestatable events. These concerns underline the design tactics of an immersive media environment and apparatus for research creation. The Lanterns digital-physical system employs a design ethos of semantically shallow computation towards an experientially responsive environment. We give an overview of the motivating questions for this line of enacted inquiry and describe our experimental process and outcomes with the Lanterns.","2018-06-28","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","1–4","","","","","","Lanterns","MOCO '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/Z8FTIR6M/Johnson et al. - 2018 - Lanterns An Enacted and Material Approach to Ense.pdf","","","digital-physical hybrid systems; group activity and ensemble; material computing; movement research; research-creation; responsive media","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7JATVEYA","conferencePaper","2020","Donato, Balandino Di; Dewey, Christopher; Michailidis, Tychonas","Human-Sound Interaction: Towards a Human-Centred Sonic Interaction Design approach","Proceedings of the 7th International Conference on Movement and Computing","978-1-4503-7505-4","","10.1145/3401956.3404233","https://dl.acm.org/doi/10.1145/3401956.3404233","In this paper, we explore human-centered interaction design aspects that determine the realisation and appreciation of musical works (installations, composition and performance), interfaces for sound design and musical expression, augmented instruments, sonic aspects of virtual environments and interactive audiovisual performances. In this first work, with the human at the centre of the design, we started sketching modes of interaction with sound that could result direct, engaging, natural and embodied in a collaborative, interactive, inclusive and diverse music environment. We define this as Human-Sound Interaction (HSI). To facilitate the exploration of HSIs, we prototyped SoundSculpt, a cross-modal audio, holographic projection and mid-air haptic feedback system. During an informal half-day workshop, we observed that HSIs through SoundSculpt have the potential to foster new ways of interaction with sound and to make them accessible to diverse musicians, sound artists and audience.","2020-07-15","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","1–4","","","","","","Human-Sound Interaction","MOCO '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/RJR2H6SR/Donato et al. - 2020 - Human-Sound Interaction Towards a Human-Centred S.pdf","","","sound affordances; holographic projection; Human-Centere Interaction Design; Human-Sound Interaction; mid-air haptic feedback","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VXCSQRFI","conferencePaper","2021","Villa, Steeven; Niess, Jasmin; Eska, Bettina; Schmidt, Albrecht; Machulla, Tonja-Katrin","Assisting Motor Skill Transfer for Dance StudentsUsing Wearable Feedback","Proceedings of the 2021 ACM International Symposium on Wearable Computers","978-1-4503-8462-9","","10.1145/3460421.3478817","https://dl.acm.org/doi/10.1145/3460421.3478817","Dance plays a crucial role in human well-being and expression. To learn dance, transferring motor knowledge across humans is relevant. Several technologies have been proposed to support such knowledge transfer from teacher to student. However, most of such systems applied a pragmatic approach focused on the feedback and the quality of the feedback system and not necessarily on the human mechanisms behind the dance learning process. In contrast, we inquire about the teacher-to-student motor knowledge transfer from the neural perspective to design motor learning wearable systems. We conducted interviews with dance students and teachers using vignettes based on motor learning theory as a discussion base. We derived insights about dance learning and identified a series of requirements for motor skill transfer-focused wearable devices. Based on our results, we present a prototype that reflects the minimum functional setup for effectively supporting motor learning.","2021-09-21","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","38–42","","","","","","","ISWC '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/DHPRF94P/Villa et al. - 2021 - Assisting Motor Skill Transfer for Dance StudentsU.pdf","","","Wearable systems; Dance teaching; Motor Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2DHRHKGV","conferencePaper","2016","Mironcika, Svetlana; Pek, Joanne; Franse, Jochem; Shu, Ya","Whoosh Gloves: Interactive Tool to Form a Dialog Between Dancer and Choreographer","Proceedings of the TEI '16: Tenth International Conference on Tangible, Embedded, and Embodied Interaction","978-1-4503-3582-9","","10.1145/2839462.2872958","https://dl.acm.org/doi/10.1145/2839462.2872958","Whoosh Gloves is a tool for modern dance choreographers and dancers to iteratively develop choreography by involving creativity of both parties. With the help of Whoosh Gloves, the choreographer records sound of his/her movements which then can be freely interpreted by dancers. In this way the gloves facilitate a co-creative reflective process of dance creation where choreographers and dancers can be inspired by each other and build a choreography upon their explorations and findings.","2016-02-14","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","729–732","","","","","","Whoosh Gloves","TEI '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/Q2395BVZ/Mironcika et al. - 2016 - Whoosh Gloves Interactive Tool to Form a Dialog B.pdf","","","Interaction Design; Co-creative Choreography; Creative Process; Creative Tools; Physical Digital Hybrid; Tangible Interaction; Whoosh Gloves","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VFP3PHRR","conferencePaper","2015","Miller, A. Bill","Experimenting with noise in markerless motion capture","Proceedings of the 2nd International Workshop on Movement and Computing","978-1-4503-3457-0","","10.1145/2790994.2791019","https://dl.acm.org/doi/10.1145/2790994.2791019","Visual culture has embraced the visual glitch as just one of many aesthetics associated with digital media. A glitch is often associated with noise in a technological system. Some motion capture systems experience noise and glitches as they process human movement. Under normal conditions, a glitch is undesirable because it decreases the usability of the capture. This short paper and demonstration introduce our research into the non-traditional use of motion capture data for the generation of artistic animated works.","2015-08-14","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","128–131","","","","","","","MOCO '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/C3H33VPE/Miller - 2015 - Experimenting with noise in markerless motion capt.pdf","","","animation; glitch; noise","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NNIVFIMF","conferencePaper","2017","Ghisio, Simone; Alborno, Paolo; Volta, Erica; Gori, Monica; Volpe, Gualtiero","A multimodal serious-game to teach fractions in primary school","Proceedings of the 1st ACM SIGCHI International Workshop on Multimodal Interaction for Education","978-1-4503-5557-5","","10.1145/3139513.3139524","https://dl.acm.org/doi/10.1145/3139513.3139524","Multisensory learning is considered a relevant pedagogical framework for education since a very long time and several authors support the use of a multisensory and kinesthetic approach in children learning. Moreover, results from psychophysics and developmental psychology show that children have a preferential sensory channel to learn specific concepts (spatial and/or temporal), hence a further evidence for the need of a multisensory approach. In this work, we present an example of serious game for learning a particularly complicated mathematical concept: fractions. The main novelty of our proposal comes from the role covered by the communication between sensory modalities in particular, movement, vision, and sound. The game has been developed in the context of the EU-ICT-H2020 weDRAW Project aiming at developing new multimodal technologies for multisensory serious-games on mathematical concepts for primary school children.","2017-11-13","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","67–70","","","","","","","MIE 2017","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/S5VZWZJX/Ghisio et al. - 2017 - A multimodal serious-game to teach fractions in pr.pdf","","","Multisensory learning; multimodal technologies; serious games","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RELTEXQ9","conferencePaper","2022","Temor, Lucas; Husain, Zainab; Coppin, Peter","A cross-modal UX design pedagogy for industrial design","Proceedings of the 17th International Audio Mostly Conference","978-1-4503-9701-8","","10.1145/3561212.3561241","https://dl.acm.org/doi/10.1145/3561212.3561241","Everyday experience is multi-sensory, and user experience (UX) design aims to extend this to interactions with products, services, and designed worlds. However, tools and pedagogies for UX are overwhelmingly visual, whereas human-rights-based accessibility legislation mandates the inclusion of diverse peoples, including blind and partially sighted individuals. Coupling auditory and haptic UX techniques from human-computer interaction with industrial design’s (ID) cross-modal tradition of prototyping physical products fostered our novel cross-modal UX course for second-year ID undergraduates. Affordance-based theories of perception-action and Gestalt principles of perceptual organization were used to inform design in auditory, tactile, and visual sensory modalities situated in a novel pedagogical framework. Each week theoretical models were presented alongside hands-on workshops using the BBC micro:bit, developing computational literacy through cross-modal physical prototyping. Student projects demonstrate an understanding of theory and practice and include auditory and tactile interfaces.","2022-10-10","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","195–198","","","","","","","AM '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/L399AQHL/Temor et al. - 2022 - A cross-modal UX design pedagogy for industrial de.pdf","","","user experience; cross-sensory; design education; industrial design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2IFDYWHP","conferencePaper","2022","Leporini, Barbara; Rosellini, Michele; Forgione, Nicola","Haptic Wearable System to Assist Visually-Impaired People in Obstacle Detection","Proceedings of the 15th International Conference on PErvasive Technologies Related to Assistive Environments","978-1-4503-9631-8","","10.1145/3529190.3529217","https://dl.acm.org/doi/10.1145/3529190.3529217","One of the main difficulties encountered every day by visually-impaired people concerns moving and orienting themselves independently and safely in indoor and especially outdoor environments. Although several studies have been carried out to propose electronic aids to support orientation and mobility tasks, problems continue to exist. In this work, a wearable ultrasonic-based obstacle detector is proposed to give a further contribution to the field. The prototype is designed by paying particular attention to (1) the updating of the components and (2) the use by the blind users. It can be mounted on the user's preferred model of eyeglasses and can be used with or without the traditional white cane.","2022-07-11","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","269–272","","","","","","","PETRA '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/ZCG6QRDJ/Leporini et al. - 2022 - Haptic Wearable System to Assist Visually-Impaired.pdf","","","blind; haptic; glasses; mobility support; obstacle detection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XUQF82KF","conferencePaper","2010","Anlauff, Jan; Großhauser, Tobias; Hermann, Thomas","tacTiles: a low-cost modular tactile sensing system for floor interactions","Proceedings of the 6th Nordic Conference on Human-Computer Interaction: Extending Boundaries","978-1-60558-934-3","","10.1145/1868914.1868981","https://dl.acm.org/doi/10.1145/1868914.1868981","In this paper, we present a prototype of a spatially resolved force sensing floor surface. The force sensors are based on conductive paper and grouped into modules called tacTiles. Due to the cheap and widely available materials used for tacTiles, the approach is suitable as a low-cost alternative for spatially resolved tactile sensing. The necessary techniques are shared as an open source and open hardware project to provide an affordable tactile sensing for smart environments. As an interactive application of these tacTiles, we present a detection of step direction algorithm used to count steps into and out of a room.","2010-10-16","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","591–594","","","","","","tacTiles","NordiCHI '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/TTMAMFST/Anlauff et al. - 2010 - tacTiles a low-cost modular tactile sensing syste.pdf","","","HCI; force sensing; modular systems; open hardware; open source; paper FSR; tactile floor sensing; tacTiles","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QJVA4IBH","conferencePaper","2008","Jagdish, Deepak; Sawhney, Rahul; Gupta, Mohit; Nangia, Shreyas","Sonic Grid: an auditory interface for the visually impaired to navigate GUI-based environments","Proceedings of the 13th international conference on Intelligent user interfaces","978-1-59593-987-6","","10.1145/1378773.1378824","https://dl.acm.org/doi/10.1145/1378773.1378824","This paper explores the prototype design of an auditory interface enhancement called the Sonic Grid that helps visually impaired users navigate GUI-based environments. The Sonic Grid provides an auditory representation of GUI elements embedded in a two-dimensional interface, giving a 'global' spatial context for use of auditory icons, ear-cons and speech feedback. This paper introduces the Sonic Grid, discusses insights gained through participatory design with members of the visually impaired community, and suggests various applications of the technique, including its use to ease the learning curve for using computers by the visually impaired.","2008-01-13","2023-07-06 05:07:01","2023-07-06 05:07:01","2023-07-05","337–340","","","","","","Sonic Grid","IUI '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/2EKCEEVD/Jagdish et al. - 2008 - Sonic Grid an auditory interface for the visually.pdf","","","accessibility; auditory interface; touch based devices","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SVU2PQJW","journalArticle","2023","Turmo Vidal, Laia; Márquez Segura, Elena; Waern, Annika","Intercorporeal Biofeedback for Movement Learning","ACM Transactions on Computer-Human Interaction","","1073-0516","10.1145/3582428","https://dl.acm.org/doi/10.1145/3582428","Technology-supported movement learning has received increased attention in HCI. Previous design research has mostly focused on individual experiences, even though the social and situated context is essential to movement learning practices. Based on the experiences from two design projects in the fitness domain featuring open-ended biofeedback artefacts, we propose Intercorporeal Biofeedback as a strong concept to support the design and use of biofeedback in such practices. We ground the concept in situated movement learning theory, phenomenology of social cognition, and HCI work on biofeedback. We articulate four key characteristics of intercorporeal biofeedback: it provides participants with a shared frame of reference, upon which they engage in fluid meaning allocation and use it to guide attention and action, becoming an interactional resource. Intercorporeal biofeedback can serve to guide future design work for situated, social movement practices.","2023-06-10","2023-07-06 05:17:21","2023-07-06 05:17:21","2023-07-06 05:16:20","43:1–43:40","","3","30","","ACM Trans. Comput.-Hum. Interact.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/VMPVXZYW/Turmo Vidal et al. - 2023 - Intercorporeal Biofeedback for Movement Learning.pdf","","","wearables; biofeedback; research through design; augmented feedback; movement learning; movement teaching; open-ended design; Strong concepts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V89YJI3W","journalArticle","2014","Caramiaux, Baptiste; Montecchio, Nicola; Tanaka, Atau; Bevilacqua, Frédéric","Adaptive Gesture Recognition with Variation Estimation for Interactive Systems","ACM Transactions on Interactive Intelligent Systems","","2160-6455","10.1145/2643204","https://dl.acm.org/doi/10.1145/2643204","This article presents a gesture recognition/adaptation system for human--computer interaction applications that goes beyond activity classification and that, as a complement to gesture labeling, characterizes the movement execution. We describe a template-based recognition method that simultaneously aligns the input gesture to the templates using a Sequential Monte Carlo inference technique. Contrary to standard template-based methods based on dynamic programming, such as Dynamic Time Warping, the algorithm has an adaptation process that tracks gesture variation in real time. The method continuously updates, during execution of the gesture, the estimated parameters and recognition results, which offers key advantages for continuous human--machine interaction. The technique is evaluated in several different ways: Recognition and early recognition are evaluated on 2D onscreen pen gestures; adaptation is assessed on synthetic data; and both early recognition and adaptation are evaluated in a user study involving 3D free-space gestures. The method is robust to noise, and successfully adapts to parameter variation. Moreover, it performs recognition as well as or better than nonadapting offline template-based methods.","2014-12-19","2023-07-06 05:17:21","2023-07-06 05:17:21","2023-07-06 05:16:23","18:1–18:34","","4","4","","ACM Trans. Interact. Intell. Syst.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/A68FBDCQ/Caramiaux et al. - 2014 - Adaptive Gesture Recognition with Variation Estima.pdf","","","Gesture recognition; adaptive decoding; continuous gesture modeling; gesture analysis; particle filtering; real time","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CGZ439WS","journalArticle","2020","May, Keenan R.; Tomlinson, Brianna J.; Ma, Xiaomeng; Roberts, Phillip; Walker, Bruce N.","Spotlights and Soundscapes: On the Design of Mixed Reality Auditory Environments for Persons with Visual Impairment","ACM Transactions on Accessible Computing","","1936-7228","10.1145/3378576","https://dl.acm.org/doi/10.1145/3378576","For persons with visual impairment, forming cognitive maps of unfamiliar interior spaces can be challenging. Various technical developments have converged to make it feasible, without specialized equipment, to represent a variety of useful landmark objects via spatial audio, rather than solely dispensing route information. Although such systems could be key to facilitating cognitive map formation, high-density auditory environments must be crafted carefully to avoid overloading the listener. This article recounts a set of research exercises with potential users, in which the optimization of such systems was explored. In Experiment 1, a virtual reality environment was used to rapidly prototype and adjust the auditory environment in response to participant comments. In Experiment 2, three variants of the system were evaluated in terms of their effectiveness in a real-world building. This methodology revealed a variety of optimization approaches and recommendations for designing dense mixed-reality auditory environments aimed at supporting cognitive map formation by visually impaired persons.","2020-04-25","2023-07-06 05:17:21","2023-07-06 05:17:21","2023-07-06 05:16:26","8:1–8:47","","2","13","","ACM Trans. Access. Comput.","Spotlights and Soundscapes","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/9ZSG5FC5/May et al. - 2020 - Spotlights and Soundscapes On the Design of Mixed.pdf","","","auditory displays; Navigation; cognitive maps","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MKNCYJVG","journalArticle","2015","Caramiaux, Baptiste; Donnarumma, Marco; Tanaka, Atau","Understanding Gesture Expressivity through Muscle Sensing","ACM Transactions on Computer-Human Interaction","","1073-0516","10.1145/2687922","https://dl.acm.org/doi/10.1145/2687922","Expressivity is a visceral capacity of the human body. To understand what makes a gesture expressive, we need to consider not only its spatial placement and orientation but also its dynamics and the mechanisms enacting them. We start by defining gesture and gesture expressivity, and then we present fundamental aspects of muscle activity and ways to capture information through electromyography and mechanomyography. We present pilot studies that inspect the ability of users to control spatial and temporal variations of 2D shapes and that use muscle sensing to assess expressive information in gesture execution beyond space and time. This leads us to the design of a study that explores the notion of gesture power in terms of control and sensing. Results give insights to interaction designers to go beyond simplistic gestural interaction, towards the design of interactions that draw on nuances of expressive gesture.","2015-01-14","2023-07-06 05:17:21","2023-07-06 05:17:21","2023-07-06 05:16:28","31:1–31:26","","6","21","","ACM Trans. Comput.-Hum. Interact.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/FZB2ERVN/Caramiaux et al. - 2015 - Understanding Gesture Expressivity through Muscle .pdf","","","Gesture; feature extraction; electromyogram; experimental study; expressivity; mechanomyogram; muscle sensing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2HQR8ZTE","journalArticle","2022","Aziz, Nida; Stockman, Tony; Stewart, Rebecca","Planning Your Journey in Audio: Design and Evaluation of Auditory Route Overviews","ACM Transactions on Accessible Computing","","1936-7228","10.1145/3531529","https://dl.acm.org/doi/10.1145/3531529","Auditory overviews of routes can provide routing and map information to blind users enabling them to preview route maps before embarking on a journey. This article investigates the usefulness of a system designed to do this through a Preliminary Survey, followed by a Design Study to gather the design requirements, development of a prototype and evaluation through a Usability Study. The design is drawn in two stages with eight audio designers and eight potential blind users. The auditory route overview is sequential and automatically generated as integrated audio. It comprises auditory icons to represent points of interest, earcons for auditory brackets encapsulating repeating points of interest, and speech for directions. A prototype based on this design is developed and evaluated with 22 sighted and eight blind participants. The software architecture of the prototype including the route information retrieval and mapping onto audio has been included. The findings show that both groups perform well in route reconstruction and recognition tasks. Moreover, the functional route information and auditory icons are effectively designed and useful in forming a mental model of the route, which improves over time. However, the design of auditory brackets needs further improvement and testing. At all stages of the system development, input has been acquired from the end-user population and the design is adapted accordingly.","2022-10-22","2023-07-06 05:17:21","2023-07-06 05:17:21","2023-07-06 05:16:33","28:1–28:48","","4","15","","ACM Trans. Access. Comput.","Planning Your Journey in Audio","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/3BZ3XNQ2/Aziz et al. - 2022 - Planning Your Journey in Audio Design and Evaluat.pdf","","","auditory display design; auditory route overviews; Blind navigation; user-led design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QIWRZVIB","journalArticle","2021","Raheb, Katerina El; Stergiou, Marina; Katifori, Akrivi; Ioannidis, Yannis","Moving in the Cube: A Motion-based Playful Experience for Introducing Labanotation to Beginners","Journal on Computing and Cultural Heritage","","1556-4673","10.1145/3427379","https://dl.acm.org/doi/10.1145/3427379","Labanotation is one of the most used systems for notating, analysing, and preserving movement and dance, an important part of Intangible Cultural Heritage. Labanotation consists of a powerful expressive symbolic language for documenting movement with a long history in dance research, history, and anthropology since its introduction by Rudolf von Laban in the beginning of the 20th century. A number of valuable scores in this language are curated in both physical and digital archives throughout the world, describing both traditional dances and works of historical choreographers. Nevertheless, while Labanotation is considered the official language of dance scholars, it is not at all popular among dance educators, students, practitioners, and choreographers. In fact, few people of the dance community are familiar with it. One of the reasons is that it is considered a quite difficult symbolic system with a long learning curve, and practitioners are not easily motivated to learn it. Together with dance experts, we co-designed a movement-based experience in Kinect, based on the principles of playful design, to introduce dance and non-dance experts to Labanotation introductory concepts and symbols. We evaluate the experience with both people that have experience in dance or other movement practices, as well as participants with no expertise in movement or dance. The results show promising findings toward changing the attitude of the participants toward Labanotation, and all participants seemed to memorize or start learning the logic of this symbolic language for movement. We discuss the results of the evaluation on the whole experience and the potential of this symbolic language in the digital environment, as well as the potential and challenges that arise from this experiment based on the background of the participants, the limitation of the applied technology and interaction, as well as feedback on the introduced symbolic language.","2021-05-29","2023-07-06 05:17:21","2023-07-06 05:17:21","2023-07-06 05:16:35","14:1–14:26","","2","14","","J. Comput. Cult. Herit.","Moving in the Cube","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/X3TBZYYJ/Raheb et al. - 2021 - Moving in the Cube A Motion-based Playful Experie.pdf","","","Embodied interaction; learning; dance notation; intangible cultural heritage; labanotation; playful interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NPJNDA9L","journalArticle","2020","Baldwin, Mark S.; Mankoff, Jennifer; Nardi, Bonnie; Hayes, Gillian","An Activity Centered Approach to Nonvisual Computer Interaction","ACM Transactions on Computer-Human Interaction","","1073-0516","10.1145/3374211","https://dl.acm.org/doi/10.1145/3374211","In this work, we apply an activity theory lens to analyze nonvisual computing for blind and low-vision computer users. Our analysis indicates major challenges for users in translating the activities they are working towards into specific tasks to be completed in a system comprehensible manner. Specifically, blind and low-vision students learning to use accessible technologies struggled with organizing their activities, tracking the history and status of their operations, and understanding how the system was acting underneath these interactions. We discuss how activity-centered design can be applied to nonvisual interfaces to better match user behavior in a computational system.","2020-03-20","2023-07-06 05:17:21","2023-07-06 05:17:21","2023-07-06 05:16:37","12:1–12:27","","2","27","","ACM Trans. Comput.-Hum. Interact.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/A3EXHKVW/Baldwin et al. - 2020 - An Activity Centered Approach to Nonvisual Compute.pdf","","","visual impairment; assistive technology; blindness; Accessibility; activity based computing; activity theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A7GH4YTV","journalArticle","2021","Höök, Kristina; Benford, Steve; Tennent, Paul; Tsaknaki, Vasiliki; Alfaras, Miquel; Avila, Juan Martinez; Li, Christine; Marshall, Joseph; Roquet, Claudia Daudén; Sanches, Pedro; Ståhl, Anna; Umair, Muhammad; Windlin, Charles; Zhou, Feng","Unpacking Non-Dualistic Design: The Soma Design Case","ACM Transactions on Computer-Human Interaction","","1073-0516","10.1145/3462448","https://dl.acm.org/doi/10.1145/3462448","We report on a somaesthetic design workshop and the subsequent analytical work aiming to demystify what is entailed in a non-dualistic design stance on embodied interaction and why a first-person engagement is crucial to its unfoldings. However, as we will uncover through a detailed account of our process, these first-person engagements are deeply entangled with second- and third-person perspectives, sometimes even overlapping. The analysis furthermore reveals some strategies for bridging the body-mind divide by attending to our inner universe and dissolving or traversing dichotomies between inside and outside; individual and social; body and technology. By detailing the creative process, we show how soma design becomes a process of designing with and through kinesthetic experience, in turn letting us confront several dualisms that run like fault lines through HCI’s engagement with embodied interaction.","2021-11-15","2023-07-06 05:17:21","2023-07-06 05:17:21","2023-07-06 05:16:40","40:1–40:36","","6","28","","ACM Trans. Comput.-Hum. Interact.","Unpacking Non-Dualistic Design","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/N9MX42RZ/Höök et al. - 2021 - Unpacking Non-Dualistic Design The Soma Design Ca.pdf","","","first person; Soma design; somaesthetics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SR5BEFCB","journalArticle","2013","Gamper, Hannes; Dicke, Christina; Billinghurst, Mark; Puolamäki, Kai","Sound sample detection and numerosity estimation using auditory display","ACM Transactions on Applied Perception","","1544-3558","10.1145/2422105.2422109","https://dl.acm.org/doi/10.1145/2422105.2422109","This article investigates the effect of various design parameters of auditory information display on user performance in two basic information retrieval tasks. We conducted a user test with 22 participants in which sets of sound samples were presented. In the first task, the test participants were asked to detect a given sample among a set of samples. In the second task, the test participants were asked to estimate the relative number of instances of a given sample in two sets of samples. We found that the stimulus onset asynchrony (SOA) of the sound samples had a significant effect on user performance in both tasks. For the sample detection task, the average error rate was about 10% with an SOA of 100 ms. For the numerosity estimation task, an SOA of at least 200 ms was necessary to yield average error rates lower than 30%. Other parameters, including the samples' sound type (synthesized speech or earcons) and spatial quality (multichannel loudspeaker or diotic headphone playback), had no substantial effect on user performance. These results suggest that diotic, or indeed monophonic, playback with appropriately chosen SOA may be sufficient in practical applications for users to perform the given information retrieval tasks, if information about the sample location is not relevant. If location information was provided through spatial playback of the samples, test subjects were able to simultaneously detect and localize a sample with reasonable accuracy.","2013-03-04","2023-07-06 05:17:21","2023-07-06 05:17:21","2023-07-06 05:16:42","4:1–4:18","","1","10","","ACM Trans. Appl. Percept.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/XEELQSDU/Gamper et al. - 2013 - Sound sample detection and numerosity estimation u.pdf","","","earcons; diotic headphone playback; SOA; Spatial sound; speech synthesis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CL97U97A","journalArticle","2016","Stearns, Lee; Du, Ruofei; Oh, Uran; Jou, Catherine; Findlater, Leah; Ross, David A.; Froehlich, Jon E.","Evaluating Haptic and Auditory Directional Guidance to Assist Blind People in Reading Printed Text Using Finger-Mounted Cameras","ACM Transactions on Accessible Computing","","1936-7228","10.1145/2914793","https://dl.acm.org/doi/10.1145/2914793","The recent miniaturization of cameras has enabled finger-based reading approaches that provide blind and visually impaired readers with access to printed materials. Compared to handheld text scanners such as mobile phone applications, mounting a tiny camera on the user's own finger has the potential to mitigate camera framing issues, enable a blind reader to better understand the spatial layout of a document, and provide better control over reading pace. A finger-based approach, however, also introduces the need to guide the reader in physically navigating a document, such as tracing along lines of text. While previous work has proposed audio and haptic directional finger guidance for this purpose, user studies of finger-based reading have not provided an in-depth performance analysis of the finger-based reading process. To further investigate the effectiveness of finger-based sensing and feedback for reading printed text, we conducted a controlled laboratory experiment with 19 blind participants, comparing audio and haptic directional finger guidance within an iPad-based testbed. As a small follow-up, we asked four of those participants to return and provide feedback on a preliminary wearable prototype called HandSight. Findings from the controlled experiment show similar performance between haptic and audio directional guidance, although audio may offer an accuracy advantage for tracing lines of text. Subjective feedback also highlights trade-offs between the two types of guidance, such as the interference of audio guidance with speech output and the potential for desensitization to haptic guidance. While several participants appreciated the direct access to layout information provided by finger-based exploration, important concerns also arose about ease of use and the amount of concentration required. We close with a discussion on the effectiveness of finger-based reading for blind users and potential design improvements to the HandSight prototype.","2016-10-21","2023-07-06 05:17:21","2023-07-06 05:17:21","2023-07-06 05:16:45","1:1–1:38","","1","9","","ACM Trans. Access. Comput.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/RQ6RN2ZS/Stearns et al. - 2016 - Evaluating Haptic and Auditory Directional Guidanc.pdf","","","wearables; visual impairments; Accessibility; real-time OCR","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8YNKWXCT","journalArticle","2019","Niksirat, Kavous Salehzadeh; Silpasuwanchai, Chaklam; Cheng, Peng; Ren, Xiangshi","Attention Regulation Framework: Designing Self-Regulated Mindfulness Technologies","ACM Transactions on Computer-Human Interaction","","1073-0516","10.1145/3359593","https://dl.acm.org/doi/10.1145/3359593","Mindfulness practices are well-known for their benefits to mental and physical well-being. Given the prevalence of smartphones, mindfulness applications have attracted growing global interest. However, the majority of existing applications use guided meditation that is not adaptable to each user's unique needs or pace. This article proposes a novel framework called Attention Regulation Framework (ARF), which studies how more flexible and adaptable mindfulness applications could be designed, beyond guided meditation and toward self-regulated meditation. ARF proposes mindfulness interaction design guidelines and interfaces whereby practitioners naturally and constantly bring their attention back to the present moment and develop non-judgmental awareness. This is achieved by the performance of subtle movements, which are supported by non-intrusive detection-feedback mechanisms. We used two design cases to demonstrate ARF in static and kinetic meditation conditions. We conducted four user evaluation studies in unique situations where ARF is particularly effective, vis-à-vis mindfulness practice in busy environments and mindfulness interfaces that adapt to the pace of the user. The studies show that the design cases, compared with guided meditation applications, are more effective in improving attention, mindfulness, mood, well-being, and physical balance. Our work contributes to the development of self-regulated mindfulness technologies.","2019-11-02","2023-07-06 05:17:21","2023-07-06 05:17:21","2023-07-06 05:16:48","39:1–39:44","","6","26","","ACM Trans. Comput.-Hum. Interact.","Attention Regulation Framework","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/KAZT8Y9G/Niksirat et al. - 2019 - Attention Regulation Framework Designing Self-Reg.pdf","","","attention; attention-regulation; framework; MBMA; meditation; Mindfulness; mobile applications; self-regulation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZTSFTSX2","journalArticle","2018","Ducasse, Julie; Macé, Marc; Oriola, Bernard; Jouffrais, Christophe","BotMap: Non-Visual Panning and Zooming with an Actuated Tabletop Tangible Interface","ACM Transactions on Computer-Human Interaction","","1073-0516","10.1145/3204460","https://dl.acm.org/doi/10.1145/3204460","The development of novel shape-changing or actuated tabletop tangible interfaces opens new perspectives for the design of physical and dynamic maps, especially for visually impaired (VI) users. Such maps would allow non-visual haptic exploration with advanced functions, such as panning and zooming. In this study, we designed an actuated tangible tabletop interface, called BotMap, allowing the exploration of geographic data through non-visual panning and zooming. In BotMap, small robots represent landmarks and move to their correct position whenever the map is refreshed. Users can interact with the robots to retrieve the names of the landmarks they represent. We designed two interfaces, named Keyboard and Sliders, which enable users to pan and zoom. Two evaluations were conducted with, respectively, ten blindfolded and eight VI participants. Results show that both interfaces were usable, with a slight advantage for the Keyboard interface in terms of navigation performance and map comprehension, and that, even when many panning and zooming operations were required, VI participants were able to understand the maps. Most participants managed to accurately reconstruct maps after exploration. Finally, we observed three VI people using the system and performing a classical task consisting in finding the more appropriate itinerary for a journey.","2018-09-07","2023-07-06 05:17:21","2023-07-06 05:17:21","2023-07-06 05:16:50","24:1–24:42","","4","25","","ACM Trans. Comput.-Hum. Interact.","BotMap","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/3BHVSXXI/Ducasse et al. - 2018 - BotMap Non-Visual Panning and Zooming with an Act.pdf","","","tangible interaction; tangible user interface; non-visual interaction; actuated interface; interactive map; pan; tactile map; Visual impairment; zoom","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S7FR6JE3","journalArticle","2022","Rateau, Hanae; Lank, Edward; Liu, Zhe","Leveraging Smartwatch and Earbuds Gesture Capture to Support Wearable Interaction","Proceedings of the ACM on Human-Computer Interaction","","","10.1145/3567710","https://dl.acm.org/doi/10.1145/3567710","Due to the proliferation of smart wearables, it is now the case that designers can explore novel ways that devices can be used in combination by end-users. In this paper, we explore the gestural input enabled by the combination of smart earbuds coupled with a proximal smartwatch. We identify a consensus set of gestures and a taxonomy of the types of gestures participants create through an elicitation study. In a follow-on study conducted on Amazon's Mechanical Turk, we explore the social acceptability of gestures enabled by watch+earbud gesture capture. While elicited gestures continue to be simple, discrete, in-context actions, we find that elicited input is frequently abstract, varies in size and duration, and is split almost equally between on-body, proximal, and more distant actions. Together, our results provide guidelines for on-body, near-ear, and in-air input using earbuds and a smartwatch to support gesture capture.","2022-11-14","2023-07-06 05:17:21","2023-07-06 05:17:21","2023-07-06 05:16:53","557:31–557:50","","ISS","6","","Proc. ACM Hum.-Comput. Interact.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/EXZLKL8Z/Rateau et al. - 2022 - Leveraging Smartwatch and Earbuds Gesture Capture .pdf","","","wearables; earbuds; elicitation study; smartwatch","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XXQV66ZQ","journalArticle","2019","Shi, Lei; Tomlinson, Brianna J.; Tang, John; Cutrell, Edward; McDuff, Daniel; Venolia, Gina; Johns, Paul; Rowan, Kael","Accessible Video Calling: Enabling Nonvisual Perception of Visual Conversation Cues","Proceedings of the ACM on Human-Computer Interaction","","","10.1145/3359233","https://dl.acm.org/doi/10.1145/3359233","Nonvisually Accessible Video Calling (NAVC) is a prototype that detects visual conversation cues in a video call and uses audio cues to convey them to a user who is blind or low-vision. NAVC uses audio cues inspired by movie soundtracks to convey Attention, Agreement, Disagreement, Happiness, Thinking, and Surprise. When designing NAVC, we partnered with people who are blind or low-vision through a user-centered design process that included need-finding interviews and design reviews. To evaluate NAVC, we conducted a user study with 16 participants. The study provided feedback on the NAVC prototype and showed that the participants could easily discern some cues, like Attention and Agreement, but had trouble distinguishing others. The accuracy of the prototype in detecting conversation cues emerged as a key concern, especially in avoiding false positives and in detecting negative emotions, which tend to be masked in social conversations. This research identified challenges and design opportunities in using AI models to enable accessible video calling.","2019-11-07","2023-07-06 05:17:21","2023-07-06 05:17:21","2023-07-06 05:16:55","131:1–131:22","","CSCW","3","","Proc. ACM Hum.-Comput. Interact.","Accessible Video Calling","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/AXGD4KG2/Shi et al. - 2019 - Accessible Video Calling Enabling Nonvisual Perce.pdf","","","assistive technology; computer-mediated communication; audio cues; blind or low vision; video calling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NB7YCEKG","journalArticle","2023","Zhao, Yiran; Tao, Yujie; Le, Grace; Maki, Rui; Adams, Alexander; Lopes, Pedro; Choudhury, Tanzeem","Affective Touch as Immediate and Passive Wearable Intervention","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","","","10.1145/3569484","https://dl.acm.org/doi/10.1145/3569484","We investigated affective touch as a new pathway to passively mitigate in-the-moment anxiety. While existing mobile interventions offer great promises for health and well-being, they typically focus on achieving long-term effects such as shifting behaviors. As such, most mobile interventions are not applicable to provide immediate help in acute conditions -- when a user experiences a high anxiety level during ongoing events (e.g., completing high-stake tasks or mitigating interpersonal conflicts). A few works have developed passive interventions that are effective in-the-moment by leveraging breathing regulations and biofeedback. In this paper, we drew on neuroscientific findings on affective touch, the slow stroking on hairy skin that can elicit innate pleasantness and evaluated affective touch as a mobile health intervention. To induce affective touch, we first engineered a wearable device that renders a soft stroking sensation on the user's forearm. Then, we conducted a between-group experiment, in which participants underwent high-stress situations with/without receiving affective touch and post-experiment interviews, with 24 participants. Our results showed that participants who received affective touch experienced lower state anxiety and the same physiological stress response level compared to the control group participants. We also found that affective touch facilitated emotion regulation by rendering pleasantness, providing emotional support, and shifting attention. Finally, we discussed the immediate effect of affective touch on anxiety and physiological stress, the benefits of affective touch as a passive intervention, and the implementation considerations to use affective touch in just-in-time systems.","2023-01-11","2023-07-06 05:17:21","2023-07-06 05:17:21","2023-07-06 05:17:01","200:1–200:23","","4","6","","Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/I34TQSU6/Zhao et al. - 2023 - Affective Touch as Immediate and Passive Wearable .pdf","","","wearable; haptics; affective touch; anxiety; behavioral health; health intervention; mental health; passive intervention","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X7X7G867","journalArticle","2008","Crossan, Andrew; Brewster, Stephen","Multimodal Trajectory Playback for Teaching Shape Information and Trajectories to Visually Impaired Computer Users","ACM Transactions on Accessible Computing","","1936-7228","10.1145/1408760.1408766","https://dl.acm.org/doi/10.1145/1408760.1408766","There are difficulties in presenting nontextual or dynamic information to blind or visually impaired users through computers. This article examines the potential of haptic and auditory trajectory playback as a method of teaching shapes and gestures to visually impaired people. Two studies are described which test the success of teaching simple shapes. The first study examines haptic trajectory playback alone, played through a force-feedback device, and compares performance of visually impaired users with sighted users. It demonstrates that the task is significantly harder for visually impaired users. The second study builds on these results, combining force-feedback with audio to teach visually impaired users to recreate shapes. The results suggest that users performed significantly better when presented with multimodal haptic and audio playback of the shape, rather than haptic only. Finally, an initial test of these ideas in an application context is described, with sighted participants describing drawings to visually impaired participants through touch and sound. This study demonstrates in what situations trajectory playback can prove a useful role in a collaborative setting.","2008-10-01","2023-07-06 05:17:21","2023-07-06 05:17:21","2023-07-06 05:17:03","12:1–12:34","","2","1","","ACM Trans. Access. Comput.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/LGPZWQ8Q/Crossan and Brewster - 2008 - Multimodal Trajectory Playback for Teaching Shape .pdf","","","multimodal; Accessibility; evaluation; trajectory playback","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HBCQQAK6","journalArticle","2018","Reichinger, Andreas; Carrizosa, Helena Garcia; Wood, Joanna; Schröder, Svenja; Löw, Christian; Luidolt, Laura Rosalia; Schimkowitsch, Maria; Fuhrmann, Anton; Maierhofer, Stefan; Purgathofer, Werner","Pictures in Your Mind: Using Interactive Gesture-Controlled Reliefs to Explore Art","ACM Transactions on Accessible Computing","","1936-7228","10.1145/3155286","https://dl.acm.org/doi/10.1145/3155286","Tactile reliefs offer many benefits over the more classic raised line drawings or tactile diagrams, as depth, 3D shape, and surface textures are directly perceivable. Although often created for blind and visually impaired (BVI) people, a wider range of people may benefit from such multimodal material. However, some reliefs are still difficult to understand without proper guidance or accompanying verbal descriptions, hindering autonomous exploration. In this work, we present a gesture-controlled interactive audio guide (IAG) based on recent low-cost depth cameras that can be operated directly with the hands on relief surfaces during tactile exploration. The interactively explorable, location-dependent verbal and captioned descriptions promise rapid tactile accessibility to 2.5D spatial information in a home or education setting, to online resources, or as a kiosk installation at public places. We present a working prototype, discuss design decisions, and present the results of two evaluation studies: the first with 13 BVI test users and the second follow-up study with 14 test users across a wide range of people with differences and difficulties associated with perception, memory, cognition, and communication. The participant-led research method of this latter study prompted new, significant and innovative developments.","2018-03-22","2023-07-06 05:17:21","2023-07-06 05:17:21","2023-07-06 05:17:06","2:1–2:39","","1","11","","ACM Trans. Access. Comput.","Pictures in Your Mind","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/WKCWIIVQ/Reichinger et al. - 2018 - Pictures in Your Mind Using Interactive Gesture-C.pdf","","","Blind; multimodal interaction; low vision; gestures; auditory interface; cognitive disability; design for all; learning disability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HBRFRS28","journalArticle","2011","Jeon, Myounghoon; Walker, Bruce N.","Spindex (Speech Index) Improves Auditory Menu Acceptance and Navigation Performance","ACM Transactions on Accessible Computing","","1936-7228","10.1145/1952383.1952385","https://dl.acm.org/doi/10.1145/1952383.1952385","Users interact with mobile devices through menus, which can include many items. Auditory menus have the potential to make those devices more accessible to a wide range of users. However, auditory menus are a relatively new concept, and there are few guidelines that describe how to design them. In this paper, we detail how visual menu concepts may be applied to auditory menus in order to help develop design guidelines. Specifically, we examine how to optimize the designs of a new contextual cue, called “spindex” (i.e., speech index). We developed and evaluated various design alternatives for spindex and iteratively refined the design with sighted users and visually impaired users. As a result, the “attenuated” spindex was the best in terms of preference as well as performance, across user groups. Nevertheless, sighted and visually impaired participants showed slightly different responses and feedback. Results are discussed in terms of acoustical theory, practical display design, and assistive technology design.","2011-04-01","2023-07-06 05:17:21","2023-07-06 05:17:21","2023-07-06 05:17:08","10:1–10:26","","3","3","","ACM Trans. Access. Comput.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/IDU3EXDY/Jeon and Walker - 2011 - Spindex (Speech Index) Improves Auditory Menu Acce.pdf","","","assistive technology; spindex; Auditory menus","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HUG2ZWVH","journalArticle","2021","Preum, Sarah Masud; Munir, Sirajum; Ma, Meiyi; Yasar, Mohammad Samin; Stone, David J.; Williams, Ronald; Alemzadeh, Homa; Stankovic, John A.","A Review of Cognitive Assistants for Healthcare: Trends, Prospects, and Future Directions","ACM Computing Surveys","","0360-0300","10.1145/3419368","https://dl.acm.org/doi/10.1145/3419368","Healthcare cognitive assistants (HCAs) are intelligent systems or agents that interact with users in a context-aware and adaptive manner to improve their health outcomes by augmenting their cognitive abilities or complementing a cognitive impairment. They assist a wide variety of users ranging from patients to their healthcare providers (e.g., general practitioner, specialist, surgeon) in several situations (e.g., remote patient monitoring, emergency response, robotic surgery). While HCAs are critical to ensure personalized, scalable, and efficient healthcare, there exists a knowledge gap in finding the emerging trends, key challenges, design guidelines, and state-of-the-art technologies suitable for developing HCAs. This survey aims to bridge this gap for researchers from multiple domains, including but not limited to cyber-physical systems, artificial intelligence, human-computer interaction, robotics, and smart health. It provides a comprehensive definition of HCAs and outlines a novel, practical categorization of existing HCAs according to their target user role and the underlying application goals. This survey summarizes and assorts existing HCAs based on their characteristic features (i.e., interactive, context-aware, and adaptive) and enabling technological aspects (i.e., sensing, actuation, control, and computation). Finally, it identifies critical research questions and design recommendations to accelerate the development of the next generation of cognitive assistants for healthcare.","2021-02-02","2023-07-06 05:17:21","2023-07-06 05:17:21","2023-07-06 05:17:10","130:1–130:37","","6","53","","ACM Comput. Surv.","A Review of Cognitive Assistants for Healthcare","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/GTM4QMEB/Preum et al. - 2021 - A Review of Cognitive Assistants for Healthcare T.pdf","","","agent based systems for healthcare; Cognitive assistant; healthcare application; intelligent agent; intelligent assistant; personal assistant; smart health; virtual agent; virtual assistant","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QVNWQ7Y7","journalArticle","2011","Plimmer, Beryl; Reid, Peter; Blagojevic, Rachel; Crossan, Andrew; Brewster, Stephen","Signing on the tactile line: A multimodal system for teaching handwriting to blind children","ACM Transactions on Computer-Human Interaction","","1073-0516","10.1145/1993060.1993067","https://dl.acm.org/doi/10.1145/1993060.1993067","We present McSig, a multimodal system for teaching blind children cursive handwriting so that they can create a personal signature. For blind people handwriting is very difficult to learn as it is a near-zero feedback activity that is needed only occasionally, yet in important situations; for example, to make an attractive and repeatable signature for legal contracts. McSig aids the teaching of signatures by translating digital ink from the teacher's stylus gestures into three non-visual forms: (1) audio pan and pitch represents the x and y movement of the stylus; (2) kinaesthetic information is provided to the student through a force-feedback haptic pen that mimics the teacher's stylus movement; and (3) a physical tactile line on the writing sheet is created by the haptic pen. McSig has been developed over two major iterations of design, usability testing and evaluation. The final step of the first iteration was a short evaluation with eight visually impaired children. The results suggested that McSig had the highest potential benefit for congenitally and totally blind children and also indicated some areas where McSig could be enhanced. The second prototype incorporated significant modifications to the system, improving the audio, tactile and force-feedback. We then ran a detailed, longitudinal evaluation over 14 weeks with three of the congenitally blind children to assess McSig's effectiveness in teaching the creation of signatures. Results demonstrated the effectiveness of McSig—they all made considerable progress in learning to create a recognizable signature. By the end of ten lessons, two of the children could form a complete, repeatable signature unaided, the third could do so with a little verbal prompting. Furthermore, during this project, we have learnt valuable lessons about providing consistent feedback between different communications channels (by manual interactions, haptic device, pen correction) that will be of interest to others developing multimodal systems.","2011-08-08","2023-07-06 05:17:21","2023-07-06 05:17:21","2023-07-06 05:17:12","17:1–17:29","","3","18","","ACM Trans. Comput.-Hum. Interact.","Signing on the tactile line","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/K66HPDGN/Plimmer et al. - 2011 - Signing on the tactile line A multimodal system f.pdf","","","handwriting; children; audio and haptic feedback; Multimodal interaction; visually impaired users","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XBQRPPRA","journalArticle","2020","Madugalla, Anuradha; Marriott, Kim; Marinai, Simone; Capobianco, Samuele; Goncu, Cagatay","Creating Accessible Online Floor Plans for Visually Impaired Readers","ACM Transactions on Accessible Computing","","1936-7228","10.1145/3410446","https://dl.acm.org/doi/10.1145/3410446","We present a generic model for providing blind and severely vision-impaired readers with access to online information graphics. The model supports fully and semi-automatic transcription and allows the reader a choice of presentation mediums. We evaluate the model through a case study: online house floor plans. To do so, we conducted a formative user study with severely vision impaired users to determine what information they would like from an online floor plan and how to present the floor plan as a text-only description, tactile graphic, and on a touchscreen with audio feedback. We then built an automatic transcription tool using specialized graphics recognition algorithms. Finally, we measured the quality of system recognition as well as conducted a second user study to evaluate the usefulness of the accessible graphics produced by the tool for each of the three formats. The results generally support the design of the generic model and the usefulness of the tool we have produced. However, they also reveal the inability of current graphics recognition algorithms to handle unforeseen graphical conventions. This highlights the need for automatic transcription systems to return a level of confidence in the recognized components and to present this to the end-user so they can have an appropriate level of trust.","2020-10-15","2023-07-06 05:17:21","2023-07-06 05:17:21","2023-07-06 05:17:15","15:1–15:37","","4","13","","ACM Trans. Access. Comput.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/CC7QDG4I/Madugalla et al. - 2020 - Creating Accessible Online Floor Plans for Visuall.pdf","","","visual impairment; navigation; trust; Floor plans","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W7RGUNQR","journalArticle","2022","Putze, Felix; Putze, Susanne; Sagehorn, Merle; Micek, Christopher; Solovey, Erin T.","Understanding HCI Practices and Challenges of Experiment Reporting with Brain Signals: Towards Reproducibility and Reuse","ACM Transactions on Computer-Human Interaction","","1073-0516","10.1145/3490554","https://dl.acm.org/doi/10.1145/3490554","In human-computer interaction (HCI), there has been a push towards open science, but to date, this has not happened consistently for HCI research utilizing brain signals due to unclear guidelines to support reuse and reproduction. To understand existing practices in the field, this paper examines 110 publications, exploring domains, applications, modalities, mental states and processes, and more. This analysis reveals variance in how authors report experiments, which creates challenges to understand, reproduce, and build on that research. It then describes an overarching experiment model that provides a formal structure for reporting HCI research with brain signals, including definitions, terminology, categories, and examples for each aspect. Multiple distinct reporting styles were identified through factor analysis and tied to different types of research. The paper concludes with recommendations and discusses future challenges. This creates actionable items from the abstract model and empirical observations to make HCI research with brain signals more reproducible and reusable.","2022-03-31","2023-07-06 05:17:21","2023-07-06 05:17:21","2023-07-06 05:17:18","31:1–31:43","","4","29","","ACM Trans. Comput.-Hum. Interact.","Understanding HCI Practices and Challenges of Experiment Reporting with Brain Signals","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/5BMFWTTM/Putze et al. - 2022 - Understanding HCI Practices and Challenges of Expe.pdf","","","EEG; electroencephalography; Brain sensing; experiment model; fNIRS; functional near-infrared spectroscopy; reproducibility","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RGL2AQCD","journalArticle","2023","Sun, Yanke; Greaves, Dwaynica A.; Orgs, Guido; de C. Hamilton, Antonia F.; Day, Sally; Ward, Jamie A.","Using Wearable Sensors to Measure Interpersonal Synchrony in Actors and Audience Members During a Live Theatre Performance","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","","","10.1145/3580781","https://dl.acm.org/doi/10.1145/3580781","Studying social interaction in real-world settings is of increasing importance to social cognitive researchers. Theatre provides an ideal opportunity to study rich face-to-face interactions in a controlled, yet natural setting. Here we collaborated with Flute Theatre to investigate interpersonal synchrony between actors-actors, actors-audience and audience-audience within a live theatrical setting. Our 28 participants consisted of 6 actors and 22 audience members, with 5 of these audience members being audience participants in the show. The performance was a compilation of acting, popular science talks and demonstrations, and an audience participation period. Interpersonal synchrony was measured using inertial measurement unit (IMU) wearable accelerometers worn on the heads of participants, whilst audio-visual data recorded everything that occurred on the stage. Participants also completed post-show self-report questionnaires on their engagement with the overall scientists and actors performance. Cross Wavelet Transform (XWT) and Wavelet Coherence Transform (WCT) analysis were conducted to extract synchrony at different frequencies, pairing with audio-visual data. Findings revealed that XWT and WCT analysis are useful methods in extracting the multiple types of synchronous activity that occurs when people perform or watch a live performance together. We also found that audience members with higher ratings on questionnaire items such as the strength of their emotional response to the performance, or how empowered they felt by the performance, showed a high degree of interpersonal synchrony with actors during the acting segments of performance. We further found that audience members rated the scientists performance higher than the actors performance on questions related to their emotional response to the performance as well as, how uplifted, empowered, and connected to social issues they felt. This shows the types of potent connections audience members can have with live performances. Additionally, our findings highlight the importance of the performance context for audience engagement, in our case a theatre performance as part of public engagement with science rather than a stand-alone theatre performance. In sum we conclude that interdisciplinary real-world paradigms are an important and understudied route to understanding in-person social interactions.","2023-03-28","2023-07-06 05:17:21","2023-07-06 05:17:21","2023-07-06 05:17:21","27:1–27:29","","1","7","","Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/V7AIWNHW/Sun et al. - 2023 - Using Wearable Sensors to Measure Interpersonal Sy.pdf","","","audiences; face-to-face interaction; interpersonal synchrony; live performance; theatre neuroscience; wearable sensors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GBXWXQDI","journalArticle","2022","Liu, Wanyu; Magalhaes, Michelle Agnes; Mackay, Wendy E.; Beaudouin-Lafon, Michel; Bevilacqua, Frédéric","Motor Variability in Complex Gesture Learning: Effects of Movement Sonification and Musical Background","ACM Transactions on Applied Perception","","1544-3558","10.1145/3482967","https://dl.acm.org/doi/10.1145/3482967","With the increasing interest in movement sonification and expressive gesture-based interaction, it is important to understand which factors contribute to movement learning and how. We explore the effects of movement sonification and users’ musical background on motor variability in complex gesture learning. We contribute an empirical study in which musicians and non-musicians learn two gesture sequences over three days, with and without movement sonification. Results show the interlaced interaction effects of these factors and how they unfold in the three-day learning process. For gesture 1, which is fast and dynamic with a direct “action-sound” sonification, movement sonification induces higher variability for both musicians and non-musicians on day 1. While musicians reduce this variability to a similar level as no auditory feedback condition on day 2 and day 3, non-musicians remain to have significantly higher variability. Across three days, musicians also have significantly lower variability than non-musicians. For gesture 2, which is slow and smooth with an “action-music” metaphor, there are virtually no effects. Based on these findings, we recommend future studies to take into account participants’ musical background, consider longitudinal study to examine these effects on complex gestures, and use awareness when interpreting the results given a specific design of gesture and sound.","2022-01-06","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:24:43","2:1–2:21","","1","19","","ACM Trans. Appl. Percept.","Motor Variability in Complex Gesture Learning","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/4FCIE6WV/Liu et al. - 2022 - Motor Variability in Complex Gesture Learning Eff.pdf","","","auditory feedback; movement sonification; complex gesture learning; Motor variability; musical background","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PDIQCAEJ","journalArticle","2022","Cantrell, Stanley J.; Winters, R. Michael; Kaini, Prakriti; Walker, Bruce N.","Sonification of Emotion in Social Media: Affect and Accessibility in Facebook Reactions","Proceedings of the ACM on Human-Computer Interaction","","","10.1145/3512966","https://dl.acm.org/doi/10.1145/3512966","Facebook Reactions are a collection of animated icons that enable users to share and express their emotions when interacting with Facebook content. The current design of Facebook Reactions utilizes visual stimuli (animated graphics and text) to convey affective information, which presents usability and accessibility barriers for visually-impaired Facebook users. In this paper, we investigate the use of sonification as a universally-accessible modality to aid in the conveyance of affect for blind and sighted social media users. We discuss the design and evaluation of 48 sonifications, leveraging Facebook Reactions as a conceptual framework. We conducted an online sound-matching study with 75 participants (11 blind, 64 sighted) to evaluate the performance of these sonifications. We found that sonification is an effective tool for conveying emotion for blind and sighted participants, and we highlight sonification design strategies that contribute to improved efficacy. Finally, we contextualize these findings and discuss the implications of this research with respect to HCI and the accessibility of online communities and platforms.","2022-04-07","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:24:45","119:1–119:26","","CSCW1","6","","Proc. ACM Hum.-Comput. Interact.","Sonification of Emotion in Social Media","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/XM8Z5H4V/Cantrell et al. - 2022 - Sonification of Emotion in Social Media Affect an.pdf","","","sonification; accessibility; music; emotion; affective computing; universal design; computer-mediated communication; design and evaluation; social media","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TTZTFRXN","journalArticle","2023","Latupeirissa, Adrian B.; Panariello, Claudio; Bresin, Roberto","Probing Aesthetics Strategies for Robot Sound: Complexity and Materiality in Movement Sonification","ACM Transactions on Human-Robot Interaction","","","10.1145/3585277","https://dl.acm.org/doi/10.1145/3585277","This paper presents three studies where we probe aesthetics strategies of sound produced by movement sonification of a Pepper robot by mapping its movements to sound models. We developed two sets of sound models. The first set was made by two sound models, a sawtooth-based one and another based on feedback chains, for investigating how the perception of synthesized robot sounds would depend on their design complexity. We implemented the second set of sound models for probing the “materiality” of sound made by a robot in motion. This set consisted of a sound synthesis based on an engine highlighting the robot’s internal mechanisms, a metallic sound synthesis highlighting the robot’s typical appearance, and a whoosh sound synthesis highlighting the movement. We conducted three studies. The first study explores how the first set of sound models can influence the perception of expressive gestures of a Pepper robot through an online survey. In the second study, we carried out an experiment in a museum installation with a Pepper robot presented in two scenarios: (1) while welcoming patrons into a restaurant and (2) while providing information to visitors in a shopping center. Finally, in the third study, we conducted an online survey with stimuli similar to those used in the second study. Our findings suggest that participants preferred more complex sound models for the sonification of robot movements. Concerning the materiality, participants liked better subtle sounds that blend well with the ambient sound (i.e., less distracting) and soundscapes in which sound sources can be identified. Also, sound preferences varied depending on the context in which participants experienced the robot-generated sounds (e.g., as a live museum installation vs. an online display).","2023-03-17","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:24:47","","","","","","J. Hum.-Robot Interact.","Probing Aesthetics Strategies for Robot Sound","","","","","","","","","","","","ACM Digital Library","","Just Accepted","","/Users/minsik/Zotero/storage/5NTXDWU8/Latupeirissa et al. - 2023 - Probing Aesthetics Strategies for Robot Sound Com.pdf","","","sonification; sound design; HRI; robot sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CQS3AHQT","journalArticle","2012","Alonso-Arevalo, Miguel A.; Shelley, Simon; Hermes, Dik; Hollowood, Jacqueline; Pettitt, Michael; Sharples, Sarah; Kohlrausch, Armin","Curve shape and curvature perception through interactive sonification","ACM Transactions on Applied Perception","","1544-3558","10.1145/2355598.2355600","https://dl.acm.org/doi/10.1145/2355598.2355600","In this article we present an approach that uses sound to communicate geometrical data related to a virtual object. This has been developed in the framework of a multimodal interface for product design. The interface allows a designer to evaluate the quality of a 3-D shape using touch, vision, and sound. Two important considerations addressed in this article are the nature of the data that is sonified and the haptic interaction between the user and the interface, which in fact triggers the sound and influences its characteristics. Based on these considerations, we present a number of sonification strategies that are designed to map the geometrical data of interest into sound. The fundamental frequency of various sounds was used to convey the curve shape or the curvature to the listeners. Two evaluation experiments are described, one involves partipants with a varied background, the other involved the intended users, i.e. participants with a background in industrial design. The results show that independent of the sonification method used and independent of whether the curve shape or the curvature were sonified, the sonification was quite successful. In the first experiment participants had a success rate of about 80% in a multiple choice task, in the second experiment it took the participants on average less than 20 seconds to find the maximum, minimum or inflection points of the curvature of a test curve.","2012-10-22","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:24:49","17:1–17:19","","4","9","","ACM Trans. Appl. Percept.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/TEPD5PJ3/Alonso-Arevalo et al. - 2012 - Curve shape and curvature perception through inter.pdf","","","Sonification; sound synthesis; haptics; modal synthesis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9JPGEBT9","journalArticle","2021","Wirfs-Brock, Jordan; Fam, Alli; Devendorf, Laura; Keegan, Brian","Examining Narrative Sonification: Using First-Person Retrospection Methods to Translate Radio Production to Interaction Design","ACM Transactions on Computer-Human Interaction","","1073-0516","10.1145/3461762","https://dl.acm.org/doi/10.1145/3461762","We present a first-person, retrospective exploration of two radio sonification pieces that employ narrative scaffolding to teach audiences how to listen to data. To decelerate and articulate design processes that occurred at the rapid pace of radio production, the sound designer and producer wrote retrospective design accounts. We then revisited the radio pieces through principles drawn from guidance design, data storytelling, visualization literacy, and sound studies. Finally, we speculated how these principles might be applied through interactive, voice-based technologies. First-person methods enabled us to access the implicit knowledge embedded in radio production and translate it to technologies of interest to the human–computer-interaction community, such as voice user interfaces that rely on auditory display. Traditionally, sonification practitioners have focused more on generating sounds than on teaching people how to listen; our process, however, treated sound and narrative as a holistic, sonic-narrative experience. Our first-person retrospection illuminated the role of narrative in designing to support people as they learn to listen to data.","2021-11-15","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:24:51","41:1–41:34","","6","28","","ACM Trans. Comput.-Hum. Interact.","Examining Narrative Sonification","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/BPHRMY3I/Wirfs-Brock et al. - 2021 - Examining Narrative Sonification Using First-Pers.pdf","","","sonification; sound; interaction design; data; voice user interfaces; narrative; radio; Retrospection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VUB3Z4WX","journalArticle","2022","Gao, Zihan; Wang, Huiqiang; Feng, Guangsheng; Lv, Hongwu","Exploring Sonification Mapping Strategies for Spatial Auditory Guidance in Immersive Virtual Environments","ACM Transactions on Applied Perception","","1544-3558","10.1145/3528171","https://dl.acm.org/doi/10.1145/3528171","Spatial auditory cues are important for many tasks in immersive virtual environments, especially guidance tasks. However, due to the limited fidelity of spatial sounds rendered by generic Head-Related Transfer Functions (HRTFs), sound localization usually has a limited accuracy, especially in elevation, which can potentially impact the effectiveness of auditory guidance. To address this issue, we explored whether integrating sonification with spatial audio can enhance the perceptions of auditory guidance cues so user performance in auditory guidance tasks can be improved. Specifically, we investigated the effects of sonification mapping strategy using a controlled experiment that compared four elevation sonification mapping strategies: absolute elevation mapping, unsigned relative elevation mapping, signed relative elevation mapping, and binary relative elevation mapping. In addition, we examined whether azimuth sonification mapping can further benefit the perception of spatial sounds. The results demonstrate that spatial auditory cues can be effectively enhanced by integrating elevation and azimuth sonification, where the accuracy and speed of guidance tasks can be significantly improved. In particular, the overall results suggest that binary relative elevation mapping is generally the most effective strategy among four elevation sonification mapping strategies, which indicates that auditory cues with clear directional information are key to efficient auditory guidance.","2022-09-02","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:24:54","9:1–9:21","","3","19","","ACM Trans. Appl. Percept.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/PURZH2CX/Gao et al. - 2022 - Exploring Sonification Mapping Strategies for Spat.pdf","","","Auditory user interface; interactive sonification; auditory feedback; 3D sound; spatial audio; non-visual guidance","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5JML5TUN","journalArticle","2023","Clark, Matthew; Doryab, Afsaneh","Sounds of Health: Using Personalized Sonification Models to Communicate Health Information","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","","","10.1145/3570346","https://dl.acm.org/doi/10.1145/3570346","This paper explores the feasibility of using sonification in delivering and communicating health and wellness status on personal devices. Ambient displays have proven to inform users of their health and wellness and help them to make healthier decisions, yet, little technology provides health assessments through sounds, which can be even more pervasive than visual displays. We developed a method to generate music from user preferences and evaluated it in a two-step user study. In the first step, we acquired general healthiness impressions from each user. In the second step, we generated customized melodies from music preferences in the first step to capture participants' perceived healthiness of those melodies. We deployed our surveys for 55 participants to complete on their own over 31 days. We analyzed the data to understand commonalities and differences in users' perceptions of music as an expression of health. Our findings show the existence of clear associations between perceived healthiness and different music features. We provide useful insights into how different musical features impact the perceived healthiness of music, how perceptions of healthiness vary between users, what trends exist between users' impressions, and what influences (or does not influence) a user's perception of healthiness in a melody. Overall, our results indicate validity in presenting health data through personalized music models. The findings can inform the design of behavior management applications on personal and ubiquitous devices.","2023-01-11","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:24:56","206:1–206:31","","4","6","","Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.","Sounds of Health","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/STDYRN22/Clark and Doryab - 2023 - Sounds of Health Using Personalized Sonification .pdf","","","Sonification; music; ambient displays; behavior change; mobile health; personalized healthcare","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HNJI2IXD","journalArticle","2017","Mascetti, Sergio; Gerino, Andrea; Bernareggi, Cristian; Picinali, Lorenzo","On the Evaluation of Novel Sonification Techniques for Non-Visual Shape Exploration","ACM Transactions on Accessible Computing","","1936-7228","10.1145/3046789","https://dl.acm.org/doi/10.1145/3046789","There are several situations in which a person with visual impairment or blindness needs to extract information from an image. For example, graphical representations are often used in education, in particular, in STEM (science, technology, engineering, and mathematics) subjects. In this contribution, we propose a set of six sonification techniques to support individuals with visual impairment or blindness in recognizing shapes on touchscreen devices. These techniques are compared among themselves and with two other sonification techniques already proposed in the literature. Using Invisible Puzzle, a mobile application which allows one to conduct non-supervised evaluation sessions, we conducted tests with 49 subjects with visual impairment and blindness, and 178 sighted subjects. All subjects involved in the process successfully completed the evaluation session, showing a high level of engagement, demonstrating, therefore, the effectiveness of the evaluation procedure. Results give interesting insights into the differences among the sonification techniques and, most importantly, show that after a short training, subjects are able to successfully identify several different shapes.","2017-04-04","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:24:58","13:1–13:28","","4","9","","ACM Trans. Access. Comput.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/NX8RWKBF/Mascetti et al. - 2017 - On the Evaluation of Novel Sonification Techniques.pdf","","","Image sonification; touch screen; visual impairments; evaluation techniques","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AF8EP3NP","journalArticle","2022","de Berardinis, Jacopo; Cangelosi, Angelo; Coutinho, Eduardo","Measuring the Structural Complexity of Music: From Structural Segmentations to the Automatic Evaluation of Models for Music Generation","IEEE/ACM Transactions on Audio, Speech and Language Processing","","2329-9290","10.1109/TASLP.2022.3178203","https://dl.acm.org/doi/10.1109/TASLP.2022.3178203","Composing musical ideas longer than motifs or figures is still rare in music generated by machine learning methods, a problem that is commonly referred to as the lack of long-term structure in the generated sequences. In addition, the evaluation of the structural complexity of artificial compositions is still a manual task, requiring expert knowledge, time and involving subjectivity which is inherent in the perception of musical structure. Based on recent advancements in music structure analysis, we automate the evaluation process by introducing a collection of metrics that can objectively describe structural properties of the music signal. This is done by segmenting music hierarchically, and computing our metrics on the resulting hierarchies to characterise the decomposition process of music into its structural components. We tested our method on a dataset collecting music with different degrees of structural complexity, from random and computer-generated pieces to real compositions of different genres and formats. Results indicate that our method can discriminate between these classes of complexity and identify further non-trivial subdivisions according to their structural properties. Our work contributes a simple yet effective framework for the evaluation of music generation models in regard to their ability to create structurally meaningful compositions.","2022-06-02","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:24:59","1963–1976","","","30","","IEEE/ACM Trans. Audio, Speech and Lang. Proc.","Measuring the Structural Complexity of Music","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/9SHKDRCB/de Berardinis et al. - 2022 - Measuring the Structural Complexity of Music From.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D5RNUBHU","journalArticle","2008","Zhao, Haixia; Plaisant, Catherine; Shneiderman, Ben; Lazar, Jonathan","Data Sonification for Users with Visual Impairment: A Case Study with Georeferenced Data","ACM Transactions on Computer-Human Interaction","","1073-0516","10.1145/1352782.1352786","https://dl.acm.org/doi/10.1145/1352782.1352786","We describe the development and evaluation of a tool, iSonic, to assist users with visual impairment in exploring georeferenced data using coordinated maps and tables, augmented with nontextual sounds and speech output. Our in-depth case studies with 7 blind users during 42 hours of data collection, showed that iSonic enabled them to find facts and discover trends in georeferenced data, even in unfamiliar geographical contexts, without special devices. Our design was guided by an Action-by-Design-Component (ADC) framework, which was also applied to scatterplots to demonstrate its generalizability. Video and download is available at www.cs.umd.edu/hcil/iSonic/.","2008-05-01","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:01","4:1–4:28","","1","15","","ACM Trans. Comput.-Hum. Interact.","Data Sonification for Users with Visual Impairment","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/EA5XRNX6/Zhao et al. - 2008 - Data Sonification for Users with Visual Impairment.pdf","","","Interactive sonification; auditory user interfaces; information seeking; universal usability; users with visual impairment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W2SVZPXK","journalArticle","2021","Presti, Giorgio; Ahmetovic, Dragan; Ducci, Mattia; Bernareggi, Cristian; Ludovico, Luca A.; Baratè, Adriano; Avanzini, Federico; Mascetti, Sergio","Iterative Design of Sonification Techniques to Support People with Visual Impairments in Obstacle Avoidance","ACM Transactions on Accessible Computing","","1936-7228","10.1145/3470649","https://dl.acm.org/doi/10.1145/3470649","Obstacle avoidance is a major challenge during independent mobility for blind or visually impaired (BVI) people. Typically, BVI people can only perceive obstacles at a short distance (about 1 m, in case they are using the white cane), and some obstacles are hard to detect (e.g., those elevated from the ground), or should not be hit by the white cane (e.g., a standing person). A solution to these problems can be found in recent computer-vision techniques that can run on mobile and wearable devices to detect obstacles at a distance. However, in addition to detecting obstacles, it is also necessary to convey information about them in real time. This contribution presents WatchOut, a sonification technique for conveying real-time information about the main properties of an obstacle to a BVI person, who can then use this additional feedback to safely navigate in the environment. WatchOut was designed with a user-centered approach, involving four iterations of online listening tests with BVI participants in order to define, improve and evaluate the sonification technique, eventually obtaining an almost perfect recognition accuracy. WatchOut was also implemented and tested as a module of a mobile app that detects obstacles using state-of-the-art computer vision technology. Results show that the system is considered usable and can guide the users to avoid more than 85% of the obstacles.","2021-10-15","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:04","19:1–19:27","","4","14","","ACM Trans. Access. Comput.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/IQSIJ7TI/Presti et al. - 2021 - Iterative Design of Sonification Techniques to Sup.pdf","","","navigation assistance; orientation & mobility; Turn-by-turn navigation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BZN64LDX","journalArticle","2008","Andersen, Tue Haste; Zhai, Shumin","“Writing with music”: Exploring the use of auditory feedback in gesture interfaces","ACM Transactions on Applied Perception","","1544-3558","10.1145/1773965.1773968","https://dl.acm.org/doi/10.1145/1773965.1773968","We investigate the use of auditory feedback in pen-gesture interfaces in a series of informal and formal experiments. Initial iterative exploration showed that gaining performance advantage with auditory feedback was possible using absolute cues and state feedback after the gesture was produced and recognized. However, gaining learning or performance advantage from auditory feedback tightly coupled with the pen-gesture articulation and recognition process was more difficult. To establish a systematic baseline, Experiment 1 formally evaluated gesture production accuracy as a function of auditory and visual feedback. Size of gestures and the aperture of the closed gestures were influenced by the visual or auditory feedback, while other measures such as shape distance and directional difference were not, supporting the theory that feedback is too slow to strongly influence the production of pen stroke gestures. Experiment 2 focused on the subjective aspects of auditory feedback in pen-gesture interfaces. Participants' rating on the dimensions of being wonderful and stimulating was significantly higher with musical auditory feedback. Several lessons regarding pen gestures and auditory feedback are drawn from our exploration: a few simple functions such as indicating the pen-gesture recognition results can be achieved, gaining performance and learning advantage through tightly coupled process-based auditory feedback is difficult, pen-gesture sets and their recognizers can be designed to minimize visual dependence, and people's subjective experience of gesture interaction can be influenced using musical auditory feedback. These lessons may serve as references and stepping stones toward future research and development in pen-gesture interfaces with auditory feedback.","2008-06-18","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:05","17:1–17:24","","3","7","","ACM Trans. Appl. Percept.","“Writing with music”","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/7Y4AECDA/Andersen and Zhai - 2008 - “Writing with music” Exploring the use of auditor.pdf","","","music; sound; gesture; feedback; Audio; auditory interface; pen; text input","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HS94RWBS","journalArticle","2017","Abdallah, Samer; Benetos, Emmanouil; Gold, Nicolas; Hargreaves, Steven; Weyde, Tillman; Wolff, Daniel","The Digital Music Lab: A Big Data Infrastructure for Digital Musicology","Journal on Computing and Cultural Heritage","","1556-4673","10.1145/2983918","https://dl.acm.org/doi/10.1145/2983918","In musicology and music research generally, the increasing availability of digital music, storage capacities, and computing power enable and require new and intelligent systems. In the transition from traditional to digital musicology, many techniques and tools have been developed for the analysis of individual pieces of music, but large-scale music data that are increasingly becoming available require research methods and systems that work on the collection-level and at scale. Although many relevant algorithms have been developed during the past 15 years of research in Music Information Retrieval, an integrated system that supports large-scale digital musicology research has so far been lacking. In the Digital Music Lab (DML) project, a collaboration among music librarians, musicologists, computer scientists, and human-computer interface specialists, the DML software system has been developed that fills this gap by providing intelligent large-scale music analysis with a user-friendly interactive interface supporting musicologists in their exploration and enquiry. The DML system empowers musicologists by addressing several challenges: distributed processing of audio and other music data, management of the data analysis process and results, remote analysis of data under copyright, logical inference on the extracted information and metadata, and visual web-based interfaces for exploring and querying the music collections. The DML system is scalable and based on Semantic Web technology and integrates into Linked Data with the vision of a distributed system that enables music research across archives, libraries, and other providers of music data. A first DML system prototype has been set up in collaboration with the British Library and I Like Music Ltd. This system has been used to analyse a diverse corpus of currently 250,000 music tracks. In this article, we describe the DML system requirements, design, architecture, components, and available data sources, explaining their interaction. We report use cases and applications with initial evaluations of the proposed system.","2017-01-02","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:07","2:1–2:21","","1","10","","J. Comput. Cult. Herit.","The Digital Music Lab","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/DZYVUJZT/Abdallah et al. - 2017 - The Digital Music Lab A Big Data Infrastructure f.pdf","","","music information retrieval; big data; Digital musicology; semantic web","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2JCBIU66","journalArticle","2018","Ananthabhotla, Ishwarya; Paradiso, Joseph A.","SoundSignaling: Realtime, Stylistic Modification of a Personal Music Corpus for Information Delivery","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","","","10.1145/3287032","https://dl.acm.org/doi/10.1145/3287032","Drawing inspiration from the notion of cognitive incongruence associated with Stroop's famous experiment, from musical principles, and from the observation that music consumption on an individual basis is becoming increasingly ubiquitous, we present the SoundSignaling system -- a software platform designed to make real-time, stylistically relevant modifications to a personal corpus of music as a means of conveying information or notifications. In this work, we discuss in detail the system's technical implementation and its motivation from a musical perspective, and validate these design choices through a crowd-sourced signal identification experiment consisting of 200 independent tasks performed by 50 online participants. We then qualitatively discuss the potential implications of such a system from the standpoint of switch cost, cognitive load, and listening behavior by considering the anecdotal outcomes of a small-scale, in-the-wild experiment consisting of over 180 hours of usage from 6 participants. Through this work, we suggest a re-evaluation of the age-old paradigm of binary audio notifications in favor of a system designed to operate upon the relatively unexplored medium of a user's musical preferences.","2018-12-27","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:09","154:1–154:23","","4","2","","Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.","SoundSignaling","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/K4DQGWH7/Ananthabhotla and Paradiso - 2018 - SoundSignaling Realtime, Stylistic Modification o.pdf","","","sonification; music; signal processing; audio; notifications; attention; cognitive load; genre; modification; SoundSignaling; switch cost","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BFDBD5ZK","journalArticle","2018","Tulilaulu, Aurora; Nelimarkka, Matti; Paalasmaa, Joonas; Johnson, Daniel; Ventura, Dan; Myllys, Petri; Toivonen, Hannu","Data Musicalization","ACM Transactions on Multimedia Computing, Communications, and Applications","","1551-6857","10.1145/3184742","https://dl.acm.org/doi/10.1145/3184742","Data musicalization is the process of automatically composing music based on given data as an approach to perceptualizing information artistically. The aim of data musicalization is to evoke subjective experiences in relation to the information rather than merely to convey unemotional information objectively. This article is written as a tutorial for readers interested in data musicalization. We start by providing a systematic characterization of musicalization approaches, based on their inputs, methods, and outputs. We then illustrate data musicalization techniques with examples from several applications: one that perceptualizes physical sleep data as music, several that artistically compose music inspired by the sleep data, one that musicalizes on-line chat conversations to provide a perceptualization of liveliness of a discussion, and one that uses musicalization in a gamelike mobile application that allows its users to produce music. We additionally provide a number of electronic samples of music produced by the different musicalization applications.","2018-04-25","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:11","47:1–47:27","","2","14","","ACM Trans. Multimedia Comput. Commun. Appl.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/4QSIZAG4/Tulilaulu et al. - 2018 - Data Musicalization.pdf","","","sonification; music; data analysis; automated composition; computational creativity; Data musicalization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GY453YQD","journalArticle","2021","Zalkow, Frank; Müller, Meinard","CTC-Based Learning of Chroma Features for Score&#x2013;Audio Music Retrieval","IEEE/ACM Transactions on Audio, Speech and Language Processing","","2329-9290","10.1109/TASLP.2021.3110137","https://dl.acm.org/doi/10.1109/TASLP.2021.3110137","This paper deals with a score&#x2013;audio music retrieval task where the aim is to find relevant audio recordings of Western classical music, given a short monophonic musical theme in symbolic notation as a query. Strategies for comparing score and audio data are often based on a common mid-level representation, such as chroma features, which capture melodic and harmonic properties. Recent studies demonstrated the effectiveness of neural networks that learn task-specific mid-level representations. Usually, such supervised learning approaches require score&#x2013;audio pairs where the score&#x0027;s individual note events are aligned to the corresponding time positions of the audio excerpt. However, in practice, it is tedious to generate such strongly aligned training pairs. As one contribution, we show how to apply the Connectionist Temporal Classification (CTC) loss in the training procedure, which only uses weakly aligned training pairs. In such a pair, only the time positions of the beginning and end of a theme occurrence are annotated in an audio recording, rather than requiring local alignment annotations. We evaluate the resulting features in our theme retrieval scenario and show that they improve the state of the art for this task. As a main result, we demonstrate that with the CTC-based training procedure using weakly annotated data, we can achieve results almost as good as with strongly annotated data. Furthermore, we assess our chroma features in depth by inspecting their temporal smoothness or granularity as an important property and by analyzing the impact of different degrees of musical complexity on the features.","2021-09-08","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:13","2957–2971","","","29","","IEEE/ACM Trans. Audio, Speech and Lang. Proc.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/8449E36X/Zalkow and Müller - 2021 - CTC-Based Learning of Chroma Features for Score&#x.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FZKM2YJA","journalArticle","2018","Volioti, Christina; Manitsaris, Sotiris; Hemery, Edgar; Hadjidimitriou, Stelios; Charisis, Vasileios; Hadjileontiadis, Leontios; Katsouli, Eleni; Moutarde, Fabien; Manitsaris, Athanasios","A Natural User Interface for Gestural Expression and Emotional Elicitation to Access the Musical Intangible Cultural Heritage","Journal on Computing and Cultural Heritage","","1556-4673","10.1145/3127324","https://dl.acm.org/doi/10.1145/3127324","This article describes a prototype natural user interface, named the Intangible Musical Instrument, which aims to facilitate access to knowledge of performers that constitutes musical Intangible Cultural Heritage using off-the-shelf motion capturing that is easily accessed by the public at large. This prototype is able to capture, model, and recognize musical gestures (upper body including fingers) as well as to sonify them. The emotional status of the performer affects the sound parameters at the synthesis level. Intangible Musical Instrument is able to support both learning and performing/composing by providing to the user not only intuitive gesture control but also a unique user experience. In addition, the first evaluation of the Intangible Musical Instrument is presented, in which all the functionalities of the system are assessed. Overall, the results with respect to this evaluation were very promising.","2018-04-12","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:15","10:1–10:20","","2","11","","J. Comput. Cult. Herit.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/RFS5DLQ3/Volioti et al. - 2018 - A Natural User Interface for Gestural Expression a.pdf","","","sonification; evaluation; emotional status; Gesture recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GA7GTS6F","journalArticle","2008","Nees, Michael A.; Walker, Bruce N.","Data density and trend reversals in auditory graphs: Effects on point-estimation and trend-identification tasks","ACM Transactions on Applied Perception","","1544-3558","10.1145/1402236.1402237","https://dl.acm.org/doi/10.1145/1402236.1402237","Auditory graphs—displays that represent quantitative information with sound—have the potential to make data (and therefore science) more accessible for diverse user populations. No research to date, however, has systematically addressed the attributes of data that contribute to the complexity (the ease or difficulty of comprehension) of auditory graphs. A pair of studies examined the role of data density (i.e., the number of discrete data points presented per second) and the number of trend reversals for both point-estimation and trend-identification tasks with auditory graphs. For the point-estimation task, more trend reversals led to performance decrements. For the trend-identification task, a large main effect was again observed for trend reversals, but an interaction suggested that the effect of the number of trend reversals was different across lower data densities (i.e., as density increased from 1 to 2 data points per second). Results are discussed in terms of data sonification applications and rhythmic theories of auditory pattern perception.","2008-09-12","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:17","13:1–13:24","","3","5","","ACM Trans. Appl. Percept.","Data density and trend reversals in auditory graphs","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/6MCHUG38/Nees and Walker - 2008 - Data density and trend reversals in auditory graph.pdf","","","sonification; auditory display; Auditory graphs","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FVTE9UQC","journalArticle","2015","Oh, Uran; Branham, Stacy; Findlater, Leah; Kane, Shaun K.","Audio-Based Feedback Techniques for Teaching Touchscreen Gestures","ACM Transactions on Accessible Computing","","1936-7228","10.1145/2764917","https://dl.acm.org/doi/10.1145/2764917","While sighted users may learn to perform touchscreen gestures through observation (e.g., of other users or video tutorials), such mechanisms are inaccessible for users with visual impairments. As a result, learning to perform gestures without visual feedback can be challenging. We propose and evaluate two techniques to teach touchscreen gestures to users with visual impairments: (1) gesture sonification to generate sound based on finger touches, creating an audio representation of a gesture; and (2) corrective verbal feedback that combined automatic analysis of the user's drawn gesture with speech feedback. To refine and evaluate the techniques, we conducted three controlled laboratory studies. The first study, with 12 sighted participants, compared parameters for sonifying gestures in an eyes-free scenario. We identified pitch+stereo panning as the best combination. In the second study, ten blind and low-vision participants completed gesture replication tasks for single-stroke, multistroke, and multitouch gestures using the gesture sonification feedback. We found that multistroke gestures were more difficult to understand in sonification, but that playing each finger sound serially may improve understanding. In the third study, six blind and low-vision participants completed gesture replication tasks with both the sonification and corrective verbal feedback techniques. Subjective data and preliminary performance findings indicated that the techniques offer complementary advantages: although verbal feedback was preferred overall primarily due to the precision of its instructions, almost all participants appreciated the sonification for certain situations (e.g., to convey speed). This article extends our previous publication on gesture sonification by extending these techniques to multistroke and multitouch gestures. These findings provide a foundation for nonvisual training systems for touchscreen gestures.","2015-11-14","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:20","9:1–9:29","","3","7","","ACM Trans. Access. Comput.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/GGKS3TT3/Oh et al. - 2015 - Audio-Based Feedback Techniques for Teaching Touch.pdf","","","sonification; gestures; touchscreen; visual impairments; Blindness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NZ6FTNVY","journalArticle","2021","Groppe, Sven; Klinckenberg, Rico; Warnke, Benjamin","Sound of databases: sonification of a semantic web database engine","Proceedings of the VLDB Endowment","","2150-8097","10.14778/3476311.3476322","https://dl.acm.org/doi/10.14778/3476311.3476322","Sonifications map data to auditory dimensions and offer a new audible experience to their listeners. We propose a sonification of query processing paired with a corresponding visualization both integrated in a web application. In this demonstration we show that the sonification of different types of relational operators generates different sound patterns, which can be recognized and identified by listeners increasing their understanding of the operators' functionality and supports easy remembering of requirements like merge joins work on sorted input. Furthermore, new ways of analyzing query processing are possible with the sonification approach.","2021-07-01","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:22","2695–2698","","12","14","","Proc. VLDB Endow.","Sound of databases","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/9GQSSI56/Groppe et al. - 2021 - Sound of databases sonification of a semantic web.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NKGKSABK","journalArticle","2017","Berman, Lewis; Gallagher, Keith; Kozaitis, Suzanne","Evaluating the Use of Sound in Static Program Comprehension","ACM Transactions on Applied Perception","","1544-3558","10.1145/3129456","https://dl.acm.org/doi/10.1145/3129456","Comprehension of computer programs is daunting, due in part to clutter in the software developer's visual environment and the need for frequent visual context changes. Previous research has shown that nonspeech sound can be useful in understanding the runtime behavior of a program. We explore the viability and advantages of using nonspeech sound in an ecological framework to help understand the static structure of software. We describe a novel concept for auditory display of program elements in which sounds indicate characteristics and relationships among a Java program's classes, interfaces, and methods. An empirical study employing this concept was used to evaluate 24 sighted software professionals and students performing maintenance-oriented tasks using a 2×2 crossover. Viability is strong for differentiation and characterization of software entities, less so for identification. The results suggest that sonification can be advantageous under certain conditions, though they do not indicate the overall advantage of using sound in terms of task duration at a 5% level of significance. The results uncover other findings such as differences in comprehension strategy based on the available tool environment. The participants reported enthusiasm for the idea of software sonification, mitigated by lack of familiarity with the concept and the brittleness of the tool. Limitations of the present research include restriction to particular types of comprehension tasks, a single sound mapping, a single programming language, and limited training time, but the use of sound in program comprehension shows sufficient promise for continued research.","2017-10-06","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:23","7:1–7:20","","1","15","","ACM Trans. Appl. Percept.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/U8AJW3SZ/Berman et al. - 2017 - Evaluating the Use of Sound in Static Program Comp.pdf","","","Sonification; interactive sonification; auditory display; program comprehension; sound design; applied sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C5AA47XH","journalArticle","2023","Mannone, Maria; Seidita, Valeria; Chella, Antonio","The Sound of Swarm. Auditory Description of Swarm Robotic Movements","ACM Transactions on Human-Robot Interaction","","","10.1145/3596203","https://dl.acm.org/doi/10.1145/3596203","Movements of robots in a swarm can be mapped to sounds, highlighting the group behavior through the coordinated and simultaneous variations of musical parameters across time. The vice versa is also possible: sound parameters can be mapped to robotic motion parameters, giving instructions through sound. In this article, we first develop a theoretical framework to relate musical parameters such as pitch, timbre, loudness, and articulation (for each time) with robotic parameters such as position, identity, motor status, and sensor status. We propose a definition of musical spaces as Hilbert spaces, and musical paths between parameters as elements of bigroupoids, generalizing existing conceptions of musical spaces. The use of Hilbert spaces allows us to build up quantum representations of musical states, inheriting quantum computing resources, already used for robotic swarms. We present the theoretical framework and then some case studies as toy examples. In particular, we discuss a 2D video and matrix simulation with two robo-caterpillars; a 2D simulation of 10 robo-ants with Webots; a 3D simulation of three robo-fish in an underwater search&rescue mission.","2023-05-04","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:25","","","","","","J. Hum.-Robot Interact.","","","","","","","","","","","","","ACM Digital Library","","Just Accepted","","/Users/minsik/Zotero/storage/AVHXCRAG/Mannone et al. - 2023 - The Sound of Swarm. Auditory Description of Swarm .pdf","","","sonification; musical spaces; nature-inspired swarms; quantum computing; robotic spaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8HH9JNNB","journalArticle","2010","Walker, B. N.; Mauney, L. M.","Universal Design of Auditory Graphs: A Comparison of Sonification Mappings for Visually Impaired and Sighted Listeners","ACM Transactions on Accessible Computing","","1936-7228","10.1145/1714458.1714459","https://dl.acm.org/doi/10.1145/1714458.1714459","Determining patterns in data is an important and often difficult task for scientists and students. Unfortunately, graphing and analysis software typically is largely inaccessible to users with vision impairment. Using sound to represent data (i.e., sonification or auditory graphs) can make data analysis more accessible; however, there are few guidelines for designing such displays for maximum effectiveness. One crucial yet understudied design issue is exactly how changes in data (e.g., temperature) are mapped onto changes in sound (e.g., pitch), and how this may depend on the specific user. In this study, magnitude estimation was used to determine preferred data-to-display mappings, polarities, and psychophysical scaling functions relating data values to underlying acoustic parameters (frequency, tempo, or modulation index) for blind and visually impaired listeners. The resulting polarities and scaling functions are compared to previous results with sighted participants. There was general agreement about polarities obtained with the two listener populations, with some notable exceptions. There was also evidence for strong similarities regarding the magnitudes of the slopes of the scaling functions, again with some notable differences. For maximum effectiveness, sonification software designers will need to consider carefully their intended users’ vision abilities. Practical implications and limitations are discussed.","2010-03-01","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:27","12:1–12:16","","3","2","","ACM Trans. Access. Comput.","Universal Design of Auditory Graphs","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/72UIRX5C/Walker and Mauney - 2010 - Universal Design of Auditory Graphs A Comparison .pdf","","","auditory display; visually impaired; Magnitude estimation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JHPHTD36","journalArticle","2013","Csapó, Ádám; Wersényi, György","Overview of auditory representations in human-machine interfaces","ACM Computing Surveys","","0360-0300","10.1145/2543581.2543586","https://dl.acm.org/doi/10.1145/2543581.2543586","In recent years, a large number of research projects have focused on the use of auditory representations in a broadened scope of application scenarios. Results in such projects have shown that auditory elements can effectively complement other modalities not only in the traditional desktop computer environment but also in virtual and augmented reality, mobile platforms, and other kinds of novel computing environments. The successful use of auditory representations in this growing number of application scenarios has in turn prompted researchers to rediscover the more basic auditory representations and extend them in various directions. The goal of this article is to survey both classical auditory representations (e.g., auditory icons and earcons) and those auditory representations that have been created as extensions to earlier approaches, including speech-based sounds (e.g., spearcons and spindex representations), emotionally grounded sounds (e.g., auditory emoticons and spemoticons), and various other sound types used to provide sonifications in practical scenarios. The article concludes by outlining the latest trends in auditory interface design and providing examples of these trends.","2013-12-27","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:29","19:1–19:23","","2","46","","ACM Comput. Surv.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/8C8TKCG6/Csapó and Wersényi - 2013 - Overview of auditory representations in human-mach.pdf","","","sonification; Auditory icon; earcon","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8DMRCIUN","journalArticle","2018","Françoise, Jules; Bevilacqua, Frédéric","Motion-Sound Mapping through Interaction: An Approach to User-Centered Design of Auditory Feedback Using Machine Learning","ACM Transactions on Interactive Intelligent Systems","","2160-6455","10.1145/3211826","https://dl.acm.org/doi/10.1145/3211826","Technologies for sensing movement are expanding toward everyday use in virtual reality, gaming, and artistic practices. In this context, there is a need for methodologies to help designers and users create meaningful movement experiences. This article discusses a user-centered approach for the design of interactive auditory feedback using interactive machine learning. We discuss Mapping through Interaction, a method for crafting sonic interactions from corporeal demonstrations of embodied associations between motion and sound. It uses an interactive machine learning approach to build the mapping from user demonstrations, emphasizing an iterative design process that integrates acted and interactive experiences of the relationships between movement and sound. We examine Gaussian Mixture Regression and Hidden Markov Regression for continuous movement recognition and real-time sound parameter generation. We illustrate and evaluate this approach through an application in which novice users can create interactive sound feedback based on coproduced gestures and vocalizations. Results indicate that Gaussian Mixture Regression and Hidden Markov Regression can efficiently learn complex motion-sound mappings from few examples.","2018-06-13","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:31","16:1–16:30","","2","8","","ACM Trans. Interact. Intell. Syst.","Motion-Sound Mapping through Interaction","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/AQWSBSKB/Françoise and Bevilacqua - 2018 - Motion-Sound Mapping through Interaction An Appro.pdf","","","sonification; sound and music computing; movement; user-centered design; Interactive machine learning; programming-by-demonstration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GESTFA69","journalArticle","2016","Cincuegrani, S. Mealla; Jordà, S.; Väljamäe, A.","Physiopucks: Increasing User Motivation by Combining Tangible and Implicit Physiological Interaction","ACM Transactions on Computer-Human Interaction","","1073-0516","10.1145/2838732","https://dl.acm.org/doi/10.1145/2838732","In this article, we evaluate b-Reactable, a digital music instrument that combines implicit physiology-based interaction through EEG and ECG, and explicit gestural interaction for sound generation and control. This multimodality is embodied in tangible objects named physiopucks, which are driven by biosignals. We hypothesize that multimodality increases users’ motivation in a musical task, compared to the use of a gesture-only tabletop system (the Reactable). We compared motivational aspects in dyads collaborating in three experimental groups (N = 56): the Physio group (one physiology- and one gesture-based user), the Sham group (one prerecorded physiology- and one gesture-based user), and the Control group (two gesture users). Between-group comparisons showed that motivation dimensions of Confidence and Satisfaction were higher in b-Reactable than in the gesture-only tangible interface, and that fake physiology-based feedback significantly reduced these effects. Our study also shows the potential of combined implicit and explicit interaction modes in multiuser HCI scenarios.","2016-02-20","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:32","4:1–4:22","","1","23","","ACM Trans. Comput.-Hum. Interact.","Physiopucks","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/MLVPXH6N/Cincuegrani et al. - 2016 - Physiopucks Increasing User Motivation by Combini.pdf","","","human-computer interaction; multimodal interfaces; implicit interaction; Brain computer interfaces; Brain tangible user interface; computer-supported cooperative work; musical collaboration; physiological computing; physiopucks; tabletops","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R44HW372","journalArticle","2021","Kang, Laewoo; Jackson, Steven","Tech-Art-Theory: Improvisational Methods for HCI Learning and Teaching","Proceedings of the ACM on Human-Computer Interaction","","","10.1145/3449156","https://dl.acm.org/doi/10.1145/3449156","This paper explores the nature and potential of improvisation as a method for learning and teaching in CSCW and HCI. It starts by reviewing concepts of improvisational learning in classic and more recent work in educational theory, art and music, and HCI that emphasize the reconstructive, materially-driven, error-engaged, transgressive, and collaborative nature of human learning processes. It then describes three pedagogical interventions of our own in which improvisational techniques were deployed as methods of teaching and learning. From this integrated study, we report specific pedagogical conditions (socio-material evaluations, multi-sensory practices, and making safe spaces for error) that can support improvisational learning, and three common challenges of HCI pedagogy relevance, assessment, and inclusion that improvisational methods can help to address.","2021-04-22","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:34","82:1–82:25","","CSCW1","5","","Proc. ACM Hum.-Comput. Interact.","Tech-Art-Theory","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/GM45EA5W/Kang and Jackson - 2021 - Tech-Art-Theory Improvisational Methods for HCI L.pdf","","","music; performance; improvisation; learning; art; ethnography; pedagogy","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MFQB8RHW","journalArticle","2013","Merer, Adrien; Aramaki, Mitsuko; Ystad, Sølvi; Kronland-Martinet, Richard","Perceptual characterization of motion evoked by sounds for synthesis control purposes","ACM Transactions on Applied Perception","","1544-3558","10.1145/2422105.2422106","https://dl.acm.org/doi/10.1145/2422105.2422106","This article addresses the question of synthesis and control of sound attributes from a perceptual point of view. We focused on an attribute related to the general concept of motion evoked by sounds. To investigate this concept, we tested 40 monophonic abstract sounds on listeners via a questionnaire and drawings, using a parametrized custom interface. This original procedure, which was defined with synthesis and control perspectives in mind, provides an alternative means of determining intuitive control parameters for synthesizing sounds evoking motion. Results showed that three main shape categories (linear, with regular oscillations, and with circular oscillations) and three types of direction (rising, descending, and horizontal) were distinguished by the listeners. In addition, the subjects were able to perceive the low-frequency oscillations (below 8 Hz) quite accurately. Three size categories (small, medium, and large) and three levels of randomness (none, low amplitude irregularities, and high amplitude irregularities) and speed (constant speed and speeds showing medium and large variations) were also observed in our analyses of the participants' drawings. We further performed a perceptual test to confirm the relevance of the contribution of some variables with synthesized sounds combined with visual trajectories. Based on these results, a general typology of evoked motion was drawn up and an intuitive control strategy was designed, based on a symbolic representation of continuous trajectories (provided by devices such as motion capture systems, pen tablets, etc.). These generic tools could be used in a wide range of applications such as sound design, virtual reality, sonification, and music.","2013-03-04","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:36","1:1–1:24","","1","10","","ACM Trans. Appl. Percept.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/H2965E3Y/Merer et al. - 2013 - Perceptual characterization of motion evoked by so.pdf","","","mapping; motion; perception; sound perception; Description; drawing; synthesis control; trajectories","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PYN9NKW3","journalArticle","2016","Magerko, Brian; Freeman, Jason; Mcklin, Tom; Reilly, Mike; Livingston, Elise; Mccoid, Scott; Crews-Brown, Andrea","EarSketch: A STEAM-Based Approach for Underrepresented Populations in High School Computer Science Education","ACM Transactions on Computing Education","","","10.1145/2886418","https://dl.acm.org/doi/10.1145/2886418","This article presents EarSketch, a learning environment that combines computer programming with sample-based music production to create a computational remixing environment for learning introductory computing concepts. EarSketch has been employed in both formal and informal settings, yielding significant positive results in student content knowledge and attitudes toward computing as a discipline, especially in ethnic and gender populations that are currently underrepresented in computing fields. This article describes the rationale and components of EarSketch, the evaluation design, and lessons learned to apply to future environment design and development.","2016-09-29","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:38","14:1–14:25","","4","16","","ACM Trans. Comput. Educ.","EarSketch","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/9XDIGJRV/Magerko et al. - 2016 - EarSketch A STEAM-Based Approach for Underreprese.pdf","","","computer science principles; Music remixing; STEAM education","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SS2N42ZJ","journalArticle","2019","Fiebrink, Rebecca","Machine Learning Education for Artists, Musicians, and Other Creative Practitioners","ACM Transactions on Computing Education","","","10.1145/3294008","https://dl.acm.org/doi/10.1145/3294008","This article aims to lay a foundation for the research and practice of machine learning education for creative practitioners. It begins by arguing that it is important to teach machine learning to creative practitioners and to conduct research about this teaching, drawing on related work in creative machine learning, creative computing education, and machine learning education. It then draws on research about design processes in engineering and creative practice to motivate a set of learning objectives for students who wish to design new creative artifacts with machine learning. The article then draws on education research and knowledge of creative computing practices to propose a set of teaching strategies that can be used to support creative computing students in achieving these objectives. Explanations of these strategies are accompanied by concrete descriptions of how they have been employed to develop new lectures and activities, and to design new experiential learning and scaffolding technologies, for teaching some of the first courses in the world focused on teaching machine learning to creative practitioners. The article subsequently draws on data collected from these courses—an online course as well as undergraduate and masters-level courses taught at a university—to begin to understand how this curriculum supported student learning, to understand learners’ challenges and mistakes, and to inform future teaching and research.","2019-09-13","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:40","31:1–31:32","","4","19","","ACM Trans. Comput. Educ.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/656RUCKD/Fiebrink - 2019 - Machine Learning Education for Artists, Musicians,.pdf","","","creative computing; Machine learning education; STEAM","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MW6RLKQF","journalArticle","2022","Rosenzweig, Sebastian; Scherbaum, Frank; Müller, Meinard","Computer-assisted Analysis of Field Recordings: A Case Study of Georgian Funeral Songs","Journal on Computing and Cultural Heritage","","1556-4673","10.1145/3551645","https://dl.acm.org/doi/10.1145/3551645","Three-voiced funeral songs from Svaneti in North-West Georgia (also referred to as Zär) are believed to represent one of Georgia’s oldest preserved forms of collective music-making. Throughout a Zär performance, the singers often jointly and intentionally drift upwards in pitch. Furthermore, the singers tend to use pitch slides at the beginning and end of sung notes. Musicological studies on tonal analysis or transcription have to account for such musical peculiarities, e.g., by compensating for pitch drifts or identifying stable note events (located between pitch slides). These tasks typically require labor-intensive annotation processes with manual corrections executed by experts with domain knowledge. For instance, in the context of a previous musicological study on pitch inventories (or pitch-class histograms) of Zär performances, ethnomusicologists tediously annotated fundamental frequency (F0) trajectories, stable note events, and pitch drifts for a set of 11 multitrack field recordings. In this article, we study how musicological studies on field recordings can benefit from interactive computational tools that support such annotation processes. As one contribution of this article, we compile a dataset from the previously annotated audio material, which we release under an open-source license for research purposes. As a second contribution, we introduce two computational tools for removing pitch slides and compensating pitch drifts in performances. Our tools were developed in close collaboration with ethnomusicologists and allow for incorporating domain knowledge (e.g., on singing styles or musically relevant harmonic intervals) in the different processing steps. In a case study using our Zär dataset, we evaluate our tools by reproducing the pitch inventories from the original musicological study and subsequently discuss the potential of computer-assisted approaches for interdisciplinary research.","2022-12-24","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:41","13:1–13:16","","1","16","","J. Comput. Cult. Herit.","Computer-assisted Analysis of Field Recordings","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/T5DEQQ6A/Rosenzweig et al. - 2022 - Computer-assisted Analysis of Field Recordings A .pdf","","","Georgia; Interactive tools; pitch drift; pitch slides; tonal analysis; vocal music; Zär","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"83FXB6Z7","journalArticle","2021","Bressolette, Benjamin; Denjean, Sébastien; Roussarie, Vincent; Aramaki, Mitsuko; Ystad, Sølvi; Kronland-Martinet, Richard","MovEcho: A Gesture-Sound Interface Allowing Blind Manipulations in a Driving Context","ACM Transactions on Applied Perception","","1544-3558","10.1145/3464692","https://dl.acm.org/doi/10.1145/3464692","Most recent vehicles are equipped with touchscreens, which replace arrays of buttons that control secondary driving functions, such as temperature level, strength of ventilation, GPS, or choice of radio stations. While driving, manipulating such interfaces can be problematic in terms of safety, because they require the drivers’ sight. In this article, we develop an innovative interface, MovEcho, which is piloted with gestures and associated with sounds that are used as informational feedback. We compare this interface to a touchscreen in a perceptual experiment that took place in a driving simulator. The results show that MovEcho allows for a better visual task completion related to traffic and is preferred by the participants. These promising results in a simulator condition have to be confirmed in future studies, in a real vehicle with a comparable expertise for both interfaces.","2021-08-20","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:43","15:1–15:19","","3","18","","ACM Trans. Appl. Percept.","MovEcho","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/XDXNWLVZ/Bressolette et al. - 2021 - MovEcho A Gesture-Sound Interface Allowing Blind .pdf","","","virtual reality; Multisensory perception; cognitive load","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WR4ZG4QG","journalArticle","2016","Perrotin, Olivier; D’alessandro, Christophe","Seeing, Listening, Drawing: Interferences between Sensorimotor Modalities in the Use of a Tablet Musical Interface","ACM Transactions on Applied Perception","","1544-3558","10.1145/2990501","https://dl.acm.org/doi/10.1145/2990501","Audio, visual, and proprioceptive actions are involved when manipulating a graphic tablet musical interface. Previous works suggested a possible dominance of the visual over the auditory modality in this situation. The main goal of the present study is to examine the interferences between these modalities in visual, audio, and audio-visual target acquisition tasks. Experiments are based on a movement replication paradigm, where a subject controls a cursor on a screen or the pitch of a synthesized sound by changing the stylus position on a covered graphic tablet. The experiments consisted of the following tasks: (1) a target acquisition task that was aimed at a visual target (reaching a cue with the cursor displayed on a screen), an audio target (reaching a reference note by changing the pitch of the sound played in headsets), or an audio-visual target, and (2) the replication of the target acquisition movement in the opposite direction. In the return phase, visual and audio feedback were suppressed. Different gain factors perturbed the relationships among the stylus movements, visual cursor movements, and audio pitch movements. The deviations between acquisition and return movements were analyzed. The results showed that hand amplitudes varied in accordance with visual, audio, and audio-visual perturbed gains, showing a larger effect for the visual modality. This indicates that visual, audio, and audio-visual actions interfered with the motor modality and confirms the spatial representation of pitch reported in previous studies. In the audio-visual situation, vision dominated over audition, as the latter had no significant influence on motor movement. Consequently, visual feedback is helpful for musical targeting of pitch on a graphic tablet, at least during the learning phase of the instrument. This result is linked to the underlying spatial organization of pitch perception. Finally, this work brings a complementary approach to previous studies showing that audition may dominate over vision for other aspects of musical sound (e.g., timing, rhythm, and timbre).","2016-10-22","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:45","10:1–10:19","","2","14","","ACM Trans. Appl. Percept.","Seeing, Listening, Drawing","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/F8YDL38Y/Perrotin and D’alessandro - 2016 - Seeing, Listening, Drawing Interferences between .pdf","","","graphic tablet; Sensorimotor modalities; target acquisition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JLTV6H83","journalArticle","2018","Manaris, Bill","Computing in the arts: the algorithm is the medium","Journal of Computing Sciences in Colleges","","1937-4771","","","Algorithms have existed for at least 2,000 years (e.g., Euclid's algorithm). In music and art, algorithms appear as early as Guido d'Arezzo (ca. 1000 A.D.), and in compositions by Bach, Mozart, John Cage, Iannis Xenakis, among others. Modern examples include data sonification for scientific or aesthetic purposes, such as sonifying biosignals, images, orbits of planets, and human movement (e.g., dance), among others. This talk will focus on Computing in the Arts (CITA), an NSF-funded model curriculum, which combines creativity, problem solving, and computer programming to prepare students for graduate school and careers in technology and arts industries of the 21st century. CITA is part of the new movement to combine art and design with science, technology, engineering and math (STEM + Art = STEAM). Several examples will be presented, including: • SoundMorpheus (an innovative interface for positioning sounds via arm movements); • Diving into Infinity (a motion-based system which explores depictions of infinity in M.C. Escher's works); and • JythonMusic (a programming environment for developing interactive music experiences and systems). Bill Manaris is Professor of Computer Science, and Director of the Computing in the Arts program at the College of Charleston. His areas of expertise include computer music, human-computer interaction and artificial intelligence. He explores interaction design, modeling of aesthetics and creativity, sound spatialization, and telematics. As an undergraduate, he studied computer science and music at the University of New Orleans, and holds M.S. and Ph.D. degrees in Computer Science from the University of Louisiana. He also studied classical and jazz guitar. Recently, he published a textbook in Computer Music and Creative Programming. His research has been supported by the National Science Foundation, Google, IBM, the Louisiana Board of Regents, and the Stavros Niarchos Foundation.","2018-06-01","2023-07-06 05:26:17","2023-07-06 05:26:17","","5–6","","6","33","","J. Comput. Sci. Coll.","Computing in the arts","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/4TGQDNII/Manaris - 2018 - Computing in the arts the algorithm is the medium.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NDFLCS3U","journalArticle","2017","Rector, Kyle; Salmon, Keith; Thornton, Dan; Joshi, Neel; Morris, Meredith Ringel","Eyes-Free Art: Exploring Proxemic Audio Interfaces For Blind and Low Vision Art Engagement","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","","","10.1145/3130958","https://dl.acm.org/doi/10.1145/3130958","Engagement in the arts1 is an important component of participation in cultural activities, but remains a largely unaddressed challenge for people with sensory disabilities. Visual arts are generally inaccessible to people with visual impairments due to their inherently visual nature. To address this, we present Eyes-Free Art, a design probe to explore the use of proxemic audio for interactive sonic experiences with 2D art work. The proxemic audio interface allows a user to move closer and further away from a painting to experience background music, a novel sonification, sound effects, and a detailed verbal description. We conducted a lab study by creating interpretations of five paintings with 13 people with visual impairments and found that participants enjoyed interacting with the artwork. We then created a live installation with a visually impaired artist to iterate on this concept to account for multiple users and paintings. We learned that a proxemic audio interface allows for people to feel immersed in the artwork. Proxemic audio interfaces are similar to visual because they increase in detail with closer proximity, but are different because they need a descriptive verbal overview to give context. We present future research directions in the space of proxemic audio interactions.","2017-09-11","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:48","93:1–93:21","","3","1","","Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.","Eyes-Free Art","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/92NGC8G2/Rector et al. - 2017 - Eyes-Free Art Exploring Proxemic Audio Interfaces.pdf","","","blind; low vision; Accessibility; art; eyes-free; depth camera; proxemic interface","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"26MJ499P","journalArticle","2016","Vazquez-Alvarez, Yolanda; Aylett, Matthew P.; Brewster, Stephen A.; Jungenfeld, Rocio Von; Virolainen, Antti","Designing Interactions with Multilevel Auditory Displays in Mobile Audio-Augmented Reality","ACM Transactions on Computer-Human Interaction","","1073-0516","10.1145/2829944","https://dl.acm.org/doi/10.1145/2829944","Auditory interfaces offer a solution to the problem of effective eyes-free mobile interactions. In this article, we investigate the use of multilevel auditory displays to enable eyes-free mobile interaction with indoor location-based information in non-guided audio-augmented environments. A top-level exocentric sonification layer advertises information in a gallery-like space. A secondary interactive layer is used to evaluate three different conditions that varied in the presentation (sequential versus simultaneous) and spatialisation (non-spatialised versus egocentric/exocentric spatialisation) of multiple auditory sources. Our findings show that (1) participants spent significantly more time interacting with spatialised displays; (2) using the same design for primary and interactive secondary display (simultaneous exocentric) showed a negative impact on the user experience, an increase in workload and substantially increased participant movement; and (3) the other spatial interactive secondary display designs (simultaneous egocentric, sequential egocentric, and sequential exocentric) showed an increase in time spent stationary but no negative impact on the user experience, suggesting a more exploratory experience. A follow-up qualitative and quantitative analysis of user behaviour support these conclusions. These results provide practical guidelines for designing effective eyes-free interactions for far richer auditory soundscapes.","2016-12-31","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:50","3:1–3:30","","1","23","","ACM Trans. Comput.-Hum. Interact.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/VQNWSD9E/Vazquez-Alvarez et al. - 2016 - Designing Interactions with Multilevel Auditory Di.pdf","","","auditory displays; spatial audio; exploratory behaviour; Eyes-free interaction; mobile audio-augmented reality","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TQCV4KFE","journalArticle","2019","Zhang, Yichi; Pardo, Bryan; Duan, Zhiyao","Siamese Style Convolutional Neural Networks for Sound Search by Vocal Imitation","IEEE/ACM Transactions on Audio, Speech and Language Processing","","2329-9290","10.1109/TASLP.2018.2868428","https://dl.acm.org/doi/10.1109/TASLP.2018.2868428","Conventional methods for finding audio in databases typically search text labels, rather than the audio itself. This can be problematic as labels may be missing, irrelevant to the audio content, or not known by users. Query by vocal imitation lets users query using vocal imitations instead. To do so, appropriate audio feature representations and effective similarity measures of imitations and original sounds must be developed. In this paper, we build upon our preliminary work to propose Siamese style convolutional neural networks to learn feature representations and similarity measures in a unified end-to-end training framework. Our Siamese architecture uses two convolutional neural networks to extract features, one from vocal imitations and the other from original sounds. The encoded features are then concatenated and fed into a fully connected network to estimate their similarity. We propose two versions of the system: IMINET is symmetric where the two encoders have an identical structure and are trained from scratch, while TL-IMINET is asymmetric and adopts the transfer learning idea by pretraining the two encoders from other relevant tasks: spoken language recognition for the imitation encoder and environmental sound classification for the original sound encoder. Experimental results show that both versions of the proposed system outperform a state-of-the-art system for sound search by vocal imitation, and the performance can be further improved when they are fused with the state of the art system. Results also show that transfer learning significantly improves the retrieval performance. This paper also provides insights to the proposed networks by visualizing and sonifying input patterns that maximize the activation of certain neurons in different layers.","2019-02-01","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:52","429–441","","2","27","","IEEE/ACM Trans. Audio, Speech and Lang. Proc.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/2WYC6D8E/Zhang et al. - 2019 - Siamese Style Convolutional Neural Networks for So.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DKVT7BDB","journalArticle","2022","Albert, Iannis; Burkard, Nicole; Queck, Dirk; Herrlich, Marc","The Effect of Auditory-Motor Synchronization in Exergames on the Example of the VR Rhythm Game BeatSaber","Proceedings of the ACM on Human-Computer Interaction","","","10.1145/3549516","https://dl.acm.org/doi/10.1145/3549516","Physical inactivity and an increasingly sedentary lifestyle constitute a significant public health concern. Exergames try to tackle this problem by combining exercising with motivational gameplay. Another approach in sports science is the use of auditory-motor synchronization, the entrainment of movements to the rhythm of music. There are already commercially successful games making use of the combination of both, such as the popular VR rhythm game BeatSaber. However, unlike traditional exercise settings often relying on periodic movements that can be easily entrained to a rhythmic pulse, exergames typically offer an additional cognitive challenge through their gameplay and might be based more on reaction or memorization. That poses the question as to what extent the effects of auditory-motor synchronization can be transferred to exergames, and if the synchronization of music and gameplay facilitates the playing experience. We conducted a user study (N = 54) to investigate the effects of different degrees of synchronization between music and gameplay using the VR rhythm game BeatSaber. Results show significant effects on performance, perceived workload, and player experience between the synchronized and non-synchronized conditions, but the results seem to be strongly mediated by the ability of the participants to consciously perceive the synchronization differences.","2022-10-31","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:54","253:1–253:26","","CHI PLAY","6","","Proc. ACM Hum.-Comput. Interact.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/CPDPL2S3/Albert et al. - 2022 - The Effect of Auditory-Motor Synchronization in Ex.pdf","","","exergames; auditory-motor synchronization; rhythm games","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P98F96WT","journalArticle","2018","Sikora, Marjan; Russo, Mladen; Đerek, Jurica; Jurčević, Ante","Soundscape of an Archaeological Site Recreated with Audio Augmented Reality","ACM Transactions on Multimedia Computing, Communications, and Applications","","1551-6857","10.1145/3230652","https://dl.acm.org/doi/10.1145/3230652","This article investigates the use of an audio augmented reality (AAR) system to recreate the soundscape of a medieval archaeological site. The aim of our work was to explore whether it is possible to enhance a tourist's archaeological experience, which is often derived from only scarce remains. We developed a smartphone-based AAR system, which uses location and orientation sensors to synthesize the soundscape of a site and plays it to the user via headphones. We recreated the ancient soundscape of a medieval archaeological site in Croatia and tested it in situ on two groups of participants using the soundwalk method. One test group performed the soundwalk while listening to the recreated soundscape using the AAR system, while the second control group did not use the AAR equipment. We measured the experiences of the participants using two methods: the standard soundwalk questionnaire and affective computing equipment for detecting the emotional state of participants. The results of both test methods show that participants who were listening to the ancient soundscape using our AAR system experienced higher arousal than those visiting the site without AAR.","2018-07-24","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:55","74:1–74:22","","3","14","","ACM Trans. Multimedia Comput. Commun. Appl.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/CULXIAMY/Sikora et al. - 2018 - Soundscape of an Archaeological Site Recreated wit.pdf","","","Augmented reality; affective computing; auralization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MSNMCF7C","journalArticle","2008","Fontana, Federico; Rocchesso, Davide","Auditory distance perception in an acoustic pipe","ACM Transactions on Applied Perception","","1544-3558","10.1145/1402236.1402240","https://dl.acm.org/doi/10.1145/1402236.1402240","In a study of auditory distance perception, we investigated the effects of exaggeration the acoustic cue of reverberation where the intensity of sound did not vary noticeably. The set of stimuli was obtained by moving a sound source inside a 10.2-m long pipe having a 0.3-m diameter. Twelve subjects were asked to listen to a speech sound while keeping their head inside the pipe and then to estimate the egocentric distance from the sound source using a magnitude production procedure. The procedure was repeated eighteen times using six different positions of the sound source. Results show that the point at which perceived distance equals physical distance is located approximately 3.5 m away from the listening point, with an average range of distance estimates of approximately 3.3 m, i.e., 1.65 to 4.9 m. The absence of intensity cues makes the acoustic pipe a potentially interesting modeling paradigm for the design of auditory interfaces in which distance is rendered independently of loudness. The proposed acoustic environment also confirms the known unreliability of certain distance cues.","2008-09-12","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:57","16:1–16:15","","3","5","","ACM Trans. Appl. Percept.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/UZ995DBG/Fontana and Rocchesso - 2008 - Auditory distance perception in an acoustic pipe.pdf","","","auditory display; Acoustic pipe; distance perception","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9G8L5LHC","journalArticle","2022","Millán-Castillo, Roberto San; Martino, Luca; Morgado, Eduardo; Llorente, Fernando","An Exhaustive Variable Selection Study for Linear Models of Soundscape Emotions: Rankings and Gibbs Analysis","IEEE/ACM Transactions on Audio, Speech and Language Processing","","2329-9290","10.1109/TASLP.2022.3192664","https://dl.acm.org/doi/10.1109/TASLP.2022.3192664","In the last decade, soundscapes have become one of the most active topics in Acoustics, providing a holistic approach to the acoustic environment, which involves human perception and context. Soundscapes-elicited emotions are central and substantially subtle and unnoticed (compared to speech or music). Currently, soundscape emotion recognition is a very active topic in the literature. We provide an exhaustive variable selection study (i.e., a selection of the soundscapes indicators) to a well-known dataset (emo-soundscapes). We consider linear soundscape emotion models for two soundscapes descriptors: arousal and valence. Several ranking schemes and procedures for selecting the number of variables are applied. We have also performed an alternating optimization scheme for obtaining the best sequences keeping fixed a certain number of features. Furthermore, we have designed a novel technique based on Gibbs sampling, which provides a more complete and clear view of the relevance of each variable. Finally, we have also compared our results with the analysis obtained by the classical methods based on p-values. As a result of our study, we suggest two simple and parsimonious linear models of only 7 and 16 variables (within the 122 possible features) for the two outputs (arousal and valence), respectively. The suggested linear models provide very good and competitive performance, with <inline-formula><tex-math notation=""LaTeX"">$R^{2}&gt;0.86$</tex-math></inline-formula> and <inline-formula><tex-math notation=""LaTeX"">$R^{2}&gt;0.63$</tex-math></inline-formula> (values obtained after a cross-validation procedure), respectively.","2022-07-20","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:25:59","2460–2474","","","30","","IEEE/ACM Trans. Audio, Speech and Lang. Proc.","An Exhaustive Variable Selection Study for Linear Models of Soundscape Emotions","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/DNWAYI24/Millán-Castillo et al. - 2022 - An Exhaustive Variable Selection Study for Linear .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RMR2N9HD","journalArticle","2023","Fan, Danyang; Fay Siu, Alexa; Rao, Hrishikesh; Kim, Gene Sung-Ho; Vazquez, Xavier; Greco, Lucy; O'Modhrain, Sile; Follmer, Sean","The Accessibility of Data Visualizations on the Web for Screen Reader Users: Practices and Experiences During COVID-19","ACM Transactions on Accessible Computing","","1936-7228","10.1145/3557899","https://dl.acm.org/doi/10.1145/3557899","Data visualization has become an increasingly important means of effective data communication and has played a vital role in broadcasting the progression of COVID-19. Accessible data representations, however, have lagged behind, leaving areas of information out of reach for many blind and visually impaired (BVI) users. In this work, we sought to understand (1) the accessibility of current implementations of visualizations on the web; (2) BVI users’ preferences and current experiences when accessing data-driven media; (3) how accessible data representations on the web address these users’ access needs and help them navigate, interpret, and gain insights from the data; and (4) the practical challenges that limit BVI users’ access and use of data representations. To answer these questions, we conducted a mixed-methods study consisting of an accessibility audit of 87 data visualizations on the web to identify accessibility issues, an online survey of 127 screen reader users to understand lived experiences and preferences, and a remote contextual inquiry with 12 of the survey respondents to observe how they navigate, interpret, and gain insights from accessible data representations. Our observations during this critical period of time provide an understanding of the widespread accessibility issues encountered across online data visualizations, the impact that data accessibility inequities have on the BVI community, the ways screen reader users sought access to data-driven information and made use of online visualizations to form insights, and the pressing need to make larger strides towards improving data literacy, building confidence, and enriching methods of access. Based on our findings, we provide recommendations for researchers and practitioners to broaden data accessibility on the web.","2023-03-29","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:26:02","4:1–4:29","","1","16","","ACM Trans. Access. Comput.","The Accessibility of Data Visualizations on the Web for Screen Reader Users","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/7B7RFM39/Fan et al. - 2023 - The Accessibility of Data Visualizations on the We.pdf","","","blind; visually impaired; Accessibility; data visualization; user experience; accessible data visualization; audit; web accessibility","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CWHXK43D","journalArticle","2015","Baharin, Hanif; Viller, Stephen; Rintel, Sean","SonicAIR: Supporting Independent Living with Reciprocal Ambient Audio Awareness","ACM Transactions on Computer-Human Interaction","","1073-0516","10.1145/2754165","https://dl.acm.org/doi/10.1145/2754165","Sonic Atomic Interaction Radio (SonicAIR) is an ambient awareness technology probe designed to explore how connecting the soundscapes of friends or family members might reduce the isolation of seniors living independently. At its core, SonicAIR instruments kitchen activity sites to produce an always-on real-time aural representation of remote domestic rhythms. This article reports how users in two pilot SonicAIR deployments used the sounds as resources for recognizing comfortable narratives of sociability. Used alongside telecare monitoring, such technologized interaction might enable older people to engage in community-oriented soundscape narratives of shared social responsibility.","2015-07-09","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:26:04","18:1–18:23","","4","22","","ACM Trans. Comput.-Hum. Interact.","SonicAIR","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/9A9PULE6/Baharin et al. - 2015 - SonicAIR Supporting Independent Living with Recip.pdf","","","soundscapes; earcons; Ambient awareness; domestic; independent living; phatic technology; social presence; telecare","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FCJC2JW4","journalArticle","2014","Zamborlin, Bruno; Bevilacqua, Frederic; Gillies, Marco; D'inverno, Mark","Fluid gesture interaction design: Applications of continuous recognition for the design of modern gestural interfaces","ACM Transactions on Interactive Intelligent Systems","","2160-6455","10.1145/2543921","https://dl.acm.org/doi/10.1145/2543921","This article presents Gesture Interaction DEsigner (GIDE), an innovative application for gesture recognition. Instead of recognizing gestures only after they have been entirely completed, as happens in classic gesture recognition systems, GIDE exploits the full potential of gestural interaction by tracking gestures continuously and synchronously, allowing users to both control the target application moment to moment and also receive immediate and synchronous feedback about system recognition states. By this means, they quickly learn how to interact with the system in order to develop better performances. Furthermore, rather than learning the predefined gestures of others, GIDE allows users to design their own gestures, making interaction more natural and also allowing the applications to be tailored by users' specific needs. We describe our system that demonstrates these new qualities—that combine to provide fluid gesture interaction design—through evaluations with a range of performers and artists.","2014-01-01","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:26:05","22:1–22:30","","4","3","","ACM Trans. Interact. Intell. Syst.","Fluid gesture interaction design","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/VKP7RC3E/Zamborlin et al. - 2014 - Fluid gesture interaction design Applications of .pdf","","","continuous and synchronous control; design and application of gesture interaction systems; Gesture interaction; meaningful feedback; personalisation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UZXTHPY4","journalArticle","2019","Ahmetovic, Dragan; Mascetti, Sergio; Bernareggi, Cristian; Guerreiro, João; Oh, Uran; Asakawa, Chieko","Deep Learning Compensation of Rotation Errors During Navigation Assistance for People with Visual Impairments or Blindness","ACM Transactions on Accessible Computing","","1936-7228","10.1145/3349264","https://dl.acm.org/doi/10.1145/3349264","Navigation assistive technologies are designed to support people with visual impairments during mobility. In particular, turn-by-turn navigation is commonly used to provide walk and turn instructions, without requiring any prior knowledge about the traversed environment. To ensure safe and reliable guidance, many research efforts focus on improving the localization accuracy of such instruments. However, even when the localization is accurate, imprecision in conveying guidance instructions to the user and in following the instructions can still lead to unrecoverable navigation errors. Even slight errors during rotations, amplified by the following frontal movement, can result in the user taking an incorrect and possibly dangerous path. In this article, we analyze trajectories of indoor travels in four different environments, showing that rotation errors are frequent in state-of-art navigation assistance for people with visual impairments. Such errors, caused by the delay between the instruction to stop rotating and when the user actually stops, result in over-rotation. To compensate for over-rotation, we propose a technique to anticipate the stop instruction so that the user stops rotating closer to the target rotation. The technique predicts over-rotation using a deep learning model that takes into account the user’s current rotation speed, duration, and angle; the model is trained with a dataset of rotations performed by blind individuals. By analyzing existing datasets, we show that our approach outperforms a naive baseline that predicts over-rotation with a fixed value. Experiments with 11 blind participants also show that the proposed compensation method results in lower rotation errors (18.8° on average) compared to the non-compensated approach adopted in state-of-the-art solutions (30.1°).","2019-12-16","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:26:08","19:1–19:19","","4","12","","ACM Trans. Access. Comput.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/TNI6IP9D/Ahmetovic et al. - 2019 - Deep Learning Compensation of Rotation Errors Duri.pdf","","","navigation assistance; Turn-by-turn navigation; orientation 8 mobility","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VL7HIJFQ","journalArticle","2017","Grussenmeyer, William; Folmer, Eelke","Accessible Touchscreen Technology for People with Visual Impairments: A Survey","ACM Transactions on Accessible Computing","","1936-7228","10.1145/3022701","https://dl.acm.org/doi/10.1145/3022701","Touchscreens have become a de facto standard of input for mobile devices as they most optimally use the limited input and output space that is imposed by their form factor. In recent years, people who are blind and visually impaired have been increasing their usage of smartphones and touchscreens. Although basic access is available, there are still many accessibility issues left to deal with in order to bring full inclusion to this population. Many of the accessibility problems are complex; in the past decade, various solutions have been explored. This article provides a review of the current state of the art of touchscreen accessibility for people with visual impairments and identifies new directions for research.","2017-01-17","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:26:09","6:1–6:31","","2","9","","ACM Trans. Access. Comput.","Accessible Touchscreen Technology for People with Visual Impairments","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/7JQEWVVL/Grussenmeyer and Folmer - 2017 - Accessible Touchscreen Technology for People with .pdf","","","accessibility; blind; visually impaired; eyes-free; gestures.; mobile computing; nonvisual; Touchscreens","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZDLS86PN","journalArticle","2013","Ferres, Leo; Lindgaard, Gitte; Sumegi, Livia; Tsuji, Bruce","Evaluating a Tool for Improving Accessibility to Charts and Graphs","ACM Transactions on Computer-Human Interaction","","1073-0516","10.1145/2533682.2533683","https://dl.acm.org/doi/10.1145/2533682.2533683","This article reports a case study of the iterative design and evaluation of a natural language-driven assistive technology, iGraph-Lite, providing people who are blind access to line graphs. Two laboratory-based usability studies involving blind and sighted people are presented with a discussion of the ensuing implementation of changes. Blind participants were found to adopt different graph interrogation strategies than sighted participants. A small field study is then reported in which a blind user who works with graphs took part to determine the degree to which the iGraph-Lite commands would meet the needs of blind graph experts. The final study invited sighted graph experts and novices to visually inspect and explain a set of line graphs comparable to those used in the usability studies. It aimed to highlight the concepts and the range of words sighted people use, to ascertain the appropriateness of the iGraph-Lite lexicon. A set of preliminary guidelines is presented.","2013-11-01","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:26:11","28:1–28:32","","5","20","","ACM Trans. Comput.-Hum. Interact.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/5BLUKQTI/Ferres et al. - 2013 - Evaluating a Tool for Improving Accessibility to C.pdf","","","Accessibility (blind and visually impaired); natural language interaction; statistical graphs","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CIRLSHVU","journalArticle","2010","Abu Doush, Iyad; Pontelli, Enrico; Son, Tran Cao; Simon, Dominic; Ma, Ou","Multimodal Presentation of Two-Dimensional Charts: An Investigation Using Open Office XML and Microsoft Excel","ACM Transactions on Accessible Computing","","1936-7228","10.1145/1857920.1857925","https://dl.acm.org/doi/10.1145/1857920.1857925","Several solutions, based on aural and haptic feedback, have been developed to enable access to complex on-line and digital information contents for people with visual impairment. Nevertheless, there are several components of widely used software applications that are still beyond the reach of traditional screen readers and Braille displays. This article investigates the nonvisual accessibility issues associated with the graphing component of Microsoft Excel and proposes a novel approach and system. The goal is to provide flexible multi-modal presentation schemes which can help visually impaired users in comprehending the most commonly used two dimensional business charts, demonstrated within the familiar context of Excel charts. The methodology identifies the need for three distinct strategies used in the user interaction with a chart: exploratory, guided, and summarization. These methodologies have been implemented using a multimodal approach, which combines aural cues, speech commentaries, and 3-dimensional haptic feedback. The prototype implementation and the preliminary studies suggest that the multimodality can be effectively realized and users denote preferences in intertwining these methodologies to gain understanding of the content of charts. These methodologies have been implemented in a system, which makes use of the Novint Falcon haptic device and integrated as a plug-in in Microsoft Excel.","2010-11-01","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:26:13","8:1–8:50","","2","3","","ACM Trans. Access. Comput.","Multimodal Presentation of Two-Dimensional Charts","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/ZTL7FL9G/Abu Doush et al. - 2010 - Multimodal Presentation of Two-Dimensional Charts.pdf","","","assistive technology; accessible graphs; Haptic; nonvisual charts navigation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BRUMKIAL","journalArticle","2022","Jansen, Pascal; Colley, Mark; Rukzio, Enrico","A Design Space for Human Sensor and Actuator Focused In-Vehicle Interaction Based on a Systematic Literature Review","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","","","10.1145/3534617","https://dl.acm.org/doi/10.1145/3534617","Automotive user interfaces constantly change due to increasing automation, novel features, additional applications, and user demands. While in-vehicle interaction can utilize numerous promising modalities, no existing overview includes an extensive set of human sensors and actuators and interaction locations throughout the vehicle interior. We conducted a systematic literature review of 327 publications leading to a design space for in-vehicle interaction that outlines existing and lack of work regarding input and output modalities, locations, and multimodal interaction. To investigate user acceptance of possible modalities and locations inferred from existing work and gaps unveiled in our design space, we conducted an online study (N=48). The study revealed users' general acceptance of novel modalities (e.g., brain or thermal activity) and interaction with locations other than the front (e.g., seat or table). Our work helps practitioners evaluate key design decisions, exploit trends, and explore new areas in the domain of in-vehicle interaction.","2022-07-07","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:26:15","56:1–56:51","","2","6","","Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/7C4KEE4D/Jansen et al. - 2022 - A Design Space for Human Sensor and Actuator Focus.pdf","","","design space; human sensors and actuators; in-vehicle interaction; systematic literature review","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YXDSC5ND","journalArticle","2022","Zhao, Kaixing; Mulet, Julie; Sorita, Clara; Oriola, Bernard; Serrano, Marcos; Jouffrais, Christophe","Remote Graphic-Based Teaching for Pupils with Visual Impairments: Understanding Current Practices and Co-designing an Accessible Tool with Special Education Teachers","Proceedings of the ACM on Human-Computer Interaction","","","10.1145/3567733","https://dl.acm.org/doi/10.1145/3567733","The lockdown period related to the COVID-19 pandemic has had a strong impact on the educational system in general, but more particularly on the special education system. Indeed, in the case of people with visual impairments, the regular tools relying heavily on images and videos were no longer usable. This specific situation highlighted an urgent need to develop tools that are accessible and that can provide solutions for remote teaching with people with VI. However, there is little work on the difficulties that this population encounters when they learn remotely as well as on the current practices of special education teachers. Such a lack of understanding limits the development of remote teaching systems that are adapted. In this paper, we conducted an online survey regarding the practices of 59 professionals giving lessons to pupils with VI, followed by a series of focus groups with special education workers facing teaching issues during the lockdown period. We followed an iterative design process where we designed successive low-fidelity prototypes to drive successive focus groups. We contribute with an analysis of the issues faced by special education teachers in this situation, and a concept to drive the future development of a tool for remote graphic-based teaching with pupils with VI.","2022-11-14","2023-07-06 05:26:17","2023-07-06 05:26:17","2023-07-06 05:26:17","580:538–580:567","","ISS","6","","Proc. ACM Hum.-Comput. Interact.","Remote Graphic-Based Teaching for Pupils with Visual Impairments","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/YGGFNNIX/Zhao et al. - 2022 - Remote Graphic-Based Teaching for Pupils with Visu.pdf","","","Co-design; Graphic-based; Pupils with VI; Remote teaching","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JXTAXIZM","journalArticle","2012","Jeon, Myounghoon; Walker, Bruce N.; Srivastava, Abhishek","“Spindex” (Speech Index) Enhances Menus on Touch Screen Devices with Tapping, Wheeling, and Flicking","ACM Transactions on Computer-Human Interaction","","1073-0516","10.1145/2240156.2240162","https://dl.acm.org/doi/10.1145/2240156.2240162","Users interact with many electronic devices via menus such as auditory or visual menus. Auditory menus can either complement or replace visual menus. We investigated how advanced auditory cues enhance auditory menus on a smartphone, with tapping, wheeling, and flicking input gestures. The study evaluated a spindex (speech index), in which audio cues inform users where they are in a menu; 122 undergraduates navigated through a menu of 150 songs. Study variables included auditory cue type (text-to-speech alone or TTS plus spindex), visual display mode (on or off), and input gesture (tapping, wheeling, or flicking). Target search time and subjective workload were lower with spindex than without for all input gestures regardless of visual display mode. The spindex condition was rated subjectively higher than plain speech. The effects of input method and display mode on navigation behaviors were analyzed with the two-stage navigation strategy model. Results are discussed in relation to attention theories and in terms of practical applications.","2012-07-01","2023-07-06 05:41:48","2023-07-06 05:41:48","2023-07-06 05:41:41","14:1–14:27","","2","19","","ACM Trans. Comput.-Hum. Interact.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/IYSRJDHJ/Jeon et al. - 2012 - “Spindex” (Speech Index) Enhances Menus on Touch S.pdf","","","touch screen; spindex; Auditory menus; flicking; input gestures; tapping; wheeling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7SWQIU47","journalArticle","2013","Moll, Jonas; Pysander, Eva-Lotta Sallnäs","A Haptic Tool for Group Work on Geometrical Concepts Engaging Blind and Sighted Pupils","ACM Transactions on Accessible Computing","","1936-7228","10.1145/2493171.2493172","https://dl.acm.org/doi/10.1145/2493171.2493172","In the study presented here, two haptic and visual applications for learning geometrical concepts in group work in primary school have been designed and evaluated. The aim was to support collaborative learning among sighted and visually impaired pupils. The first application is a static flattened 3D environment that supports learning to distinguish between angles by means of a 3D haptic device providing touch feedback. The second application is a dynamic 3D environment that supports learning of spatial geometry. The scene is a room with a box containing geometrical objects, which pupils can pick up and move around. The applications were evaluated in four schools with groups of two sighted and one visually impaired pupil. The results showed the support for the visually impaired pupil and for the collaboration to be satisfying. A shared understanding of the workspace could be achieved, as long as the virtual environment did not contain movable objects. Verbal communication was crucial for the work process but haptic guiding to some extent substituted communication about direction. When it comes to joint action between visually impaired and sighted pupils a number of interesting problems were identified when the dynamic and static virtual environments were compared. These problems require further investigation. The study extends prior work in the areas of assistive technology and multimodal communication by evaluating functions for joint haptic manipulation in the unique setting of group work in primary school.","2013-07-01","2023-07-06 05:41:48","2023-07-06 05:41:48","2023-07-06 05:41:44","14:1–14:37","","4","4","","ACM Trans. Access. Comput.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/TGTJILCQ/Moll and Pysander - 2013 - A Haptic Tool for Group Work on Geometrical Concep.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VTISEJET","journalArticle","2021","Hirskyj-Douglas, Ilyena; Piitulainen, Roosa; Lucero, Andrés","Forming the Dog Internet: Prototyping a Dog-to-Human Video Call Device","Proceedings of the ACM on Human-Computer Interaction","","","10.1145/3488539","https://dl.acm.org/doi/10.1145/3488539","Over the past decade, many systems have been developed for humans to remotely connect to their pets at home. Yet little attention has been paid to how animals can control such systems and what the implications are of animals using internet systems. This paper explores the creation of a video call device to allow a dog to remotely call their human, giving the animal control and agency over technology in their home. After building and prototyping a novel interaction method over several weeks and iterations, we test our system with a dog and a human. Analysing our experience and data, we reflect on power relations, how to quantify an animal's user experience and what interactive internet systems look like with animal users. This paper builds upon Human-Computer Interaction methods for unconventional users, uncovering key questions that advance the creation of animal-to-human interfaces and animal internet devices.","2021-11-05","2023-07-06 05:41:48","2023-07-06 05:41:48","2023-07-06 05:41:46","494:1–494:20","","ISS","5","","Proc. ACM Hum.-Comput. Interact.","Forming the Dog Internet","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/LJRZDH5F/Hirskyj-Douglas et al. - 2021 - Forming the Dog Internet Prototyping a Dog-to-Hum.pdf","","","participatory design; animal-computer interaction; remote human-animal systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VED3FJPK","conferencePaper","2013","Misawa, Daichi","Transparent sculpture: an embodied auditory interface for sound sculpture","Proceedings of the 7th International Conference on Tangible, Embedded and Embodied Interaction","978-1-4503-1898-3","","10.1145/2460625.2460707","https://dl.acm.org/doi/10.1145/2460625.2460707","Toward ecologically distributed interactions of sound in the real world, this paper presents an embodied auditory interface for a sound sculpture; it is composed of orientations' structure of sounds from directional speakers and a pedestal to capture a certain real space.","2013-02-10","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","389–390","","","","","","Transparent sculpture","TEI '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/RIULZBK4/Misawa - 2013 - Transparent sculpture an embodied auditory interf.pdf","","","interaction; sound sculpture; transparent interface","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JL4GJLG8","conferencePaper","2016","Yu, Bin; Hu, Jun; Funk, Mathias; Feijs, Loe","A Study on User Acceptance of Different Auditory Content for Relaxation","Proceedings of the Audio Mostly 2016","978-1-4503-4822-5","","10.1145/2986416.2986418","https://dl.acm.org/doi/10.1145/2986416.2986418","The use of auditory interface at the relaxation-assisted interactive system is becoming increasingly popular. This study aims to investigate the effects of different types of auditory content on the subjective relaxation experience. The participants listened to fifteen sound samples from five categories: (a) nature white noise, (b) natural soundscape, (c) ambient music, (d) instrumental music, (e) instrumental music mixed with the natural soundscape. These auditory contents were selected or designed specifically for assisting relaxation. The study measured the subjective relaxation rating after listening to each sample and interviewed the listeners to understand what causes the differences in relaxation experience. The results indicate that the instrumental music and the combination of nature soundscape and music might be a better auditory content or audio form to induce relaxation compared to the ambient music, pure natural soundscape, and nature white noise. The findings of this study can be used in the design of musical and auditory display in many interactive systems for stress mitigation and relaxation exercises.","2016-10-04","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","69–76","","","","","","","AM '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/RTE3TZWK/Yu et al. - 2016 - A Study on User Acceptance of Different Auditory C.pdf","","","Music; Auditory interface; Nature sounds; Relaxation; User experience","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A89E3MJA","conferencePaper","2011","Sato, Daisuke; Zhu, Shaojian; Kobayashi, Masatomo; Takagi, Hironobu; Asakawa, Chieko","Sasayaki: augmented voice web browsing experience","Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","978-1-4503-0228-9","","10.1145/1978942.1979353","https://dl.acm.org/doi/10.1145/1978942.1979353","Auditory user interfaces have great Web-access potential for billions of people with visual impairments, with limited literacy, who are driving, or who are otherwise unable to use a visual interface. However a sequential speech-based representation can only convey a limited amount of information. In addition, typical auditory user interfaces lose the visual cues such as text styles and page structures, and lack effective feedback about the current focus. To address these limitations, we created Sasayaki (from whisper in Japanese), which augments the primary voice output with a secondary whisper of contextually relevant information, automatically or in response to user requests. It also offers new ways to jump to semantically meaningful locations. A prototype was implemented as a plug-in for an auditory Web browser. Our experimental results show that the Sasayaki can reduce the task completion times for finding elements in webpages and increase satisfaction and confidence.","2011-05-07","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","2769–2778","","","","","","Sasayaki","CHI '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/BK9LM3X8/Sato et al. - 2011 - Sasayaki augmented voice web browsing experience.pdf","","","auditory interface; web accessibility; multiple voices; Sasayaki; voice augmented browsing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MEY7N26Q","conferencePaper","2010","Wolf, Katrin; Dicke, Christina; Grasset, Raphael","Touching the void: gestures for auditory interfaces","Proceedings of the fifth international conference on Tangible, embedded, and embodied interaction","978-1-4503-0478-8","","10.1145/1935701.1935772","https://dl.acm.org/doi/10.1145/1935701.1935772","Nowadays, mobile devices provide new possibilities for gesture interaction due to the large range of embedded sensors they have and their physical form factor. In addition, auditory interfaces can now be more easily supported through advanced mobile computing capabilities. Although different types of gesture techniques have been proposed for handheld devices, there is still little knowledge about the acceptability and use of some of these techniques, especially in the context of an auditory interface. In this paper, we propose a novel approach to the problem by studying the design space of gestures proposed by end-users for a mobile auditory interface. We discuss the results of this explorative study, in terms of the scope of the gestures proposed, the tangible aspects, and the users' preferences. This study delivers some initial gestures recommendations for eyes-free auditory interfaces.","2010-01-22","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","305–308","","","","","","Touching the void","TEI '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/A48UE85J/Wolf et al. - 2010 - Touching the void gestures for auditory interface.pdf","","","auditory display; gestures; embodied interaction; mobile; eyes-free; participatory design.; tangible interface","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B2IYWEF7","conferencePaper","2012","Oki, Maho; Tsukada, Koji; Kurihara, Kazutaka; Siio, Itiro","HomeOrgel: interactive music box for the aural representation of home activities","Proceedings of the 10th asia pacific conference on Computer human interaction","978-1-4503-1496-1","","10.1145/2350046.2350083","https://dl.acm.org/doi/10.1145/2350046.2350083","We propose a music-box-type interface, ""HomeOrgel"", that can express various activities in the home using sound. Users can also control the volume and content using common methods for controlling a music box: opening the cover and winding the spring. Users can hear the sounds of past home activities, such as cooking and the opening/closing of doors with the background music (BGM) mechanism of the music box. We developed the HomeOrgel device and installed it in an actual house. We also verify the effectiveness of our system through evaluation and discussion.","2012-08-28","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","177–186","","","","","","HomeOrgel","APCHI '12","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/3JJEYCYN/Oki et al. - 2012 - HomeOrgel interactive music box for the aural rep.pdf","","","auditory display; smart home; ubiquitous computing; music box","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BCXHLM9Z","conferencePaper","2021","Boger, Tal; Ananthabhotla, Ishwarya; Paradiso, Joseph","Manipulating Causal Uncertainty in Sound Objects","Proceedings of the 16th International Audio Mostly Conference","978-1-4503-8569-5","","10.1145/3478384.3478405","https://dl.acm.org/doi/10.1145/3478384.3478405","Causal uncertainty – how sure we are in what produced a sound that we are listening to – is a fundamental aspect of auditory cognition. It is known to be a driver of affect perception, attention, and memory, among other processes. Here, we present an optimization pipeline that systematically manipulates a sound object’s intrinsic causal uncertainty by applying a set of acoustic transforms, such as scaling a sound’s pitch, amplitude, playback speed, etc. The optimization estimator attempts to produce parameter values for these transforms that modify a sound’s causal uncertainty (Hcu), as measured by the prediction confidence of an audio classification neural network, while minimizing changes to the resulting prediction labels and transform magnitudes. We then conduct a listening test with N=20 participants to confirm that the causal uncertainty changes resulting from our proposed procedure align with human perception. Though a simple approach, this work demonstrates a first step towards generative audio systems that operate along cognitive dimensions, with powerful implications for user experience design.","2021-10-15","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","9–15","","","","","","","AM '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/7RPB6YQG/Boger et al. - 2021 - Manipulating Causal Uncertainty in Sound Objects.pdf","","","auditory perception; auditory cognition; causal uncertainty; optimization; sound manipulation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U4GLNYAJ","conferencePaper","2011","Yuksel, Kamer Ali; Buyukbas, Sinan; Adali, Serdar Hasan","Designing mobile phones using silent speech input and auditory feedback","Proceedings of the 13th International Conference on Human Computer Interaction with Mobile Devices and Services","978-1-4503-0541-9","","10.1145/2037373.2037492","https://dl.acm.org/doi/10.1145/2037373.2037492","In this work, we have propose a novel design for a basic mobile phone, which is focused on the essence of mobile communication and connectivity, based on a silent speech interface and auditory feedback. This assistive interface takes the advantages of voice control systems while discarding its disadvantages such as the background noise, privacy and social acceptance. The proposed device utilizes low-cost and commercially available hardware components. Thus, it would be affordable and accessible by majority of users including disabled, elderly and illiterate people.","2011-08-30","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","711–713","","","","","","","MobileHCI '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/95PS46ZY/Yuksel et al. - 2011 - Designing mobile phones using silent speech input .pdf","","","mobile phone; silent speech interface; vibration sensors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5AF6QN2M","conferencePaper","2022","Zhang, Lotus; Shao, Jingyao; Liu, Augustina Ao; Jiang, Lucy; Stangl, Abigale; Fourney, Adam; Morris, Meredith Ringel; Findlater, Leah","Exploring Interactive Sound Design for Auditory Websites","Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems","978-1-4503-9157-3","","10.1145/3491102.3517695","https://dl.acm.org/doi/10.1145/3491102.3517695","Auditory interfaces increasingly support access to website content, through recent advances in voice interaction. Typically, however, these interfaces provide only limited audio styling, collapsing rich visual design into a static audio output style with a single synthesized voice. To explore the potential for more aesthetic and intuitive sound design for websites, we prompted 14 professional sound designers to create auditory website mockups and interviewed them about their designs and rationale. Our findings reveal their prioritized design considerations (aesthetics and emotion, user engagement, audio clarity, information dynamics, and interactivity), specific sound design ideas to support each consideration (e.g., replacing spoken labels with short, memorable audio expressions), and challenges with applying sound design practices to auditory websites. These findings provide promising direction for how to support designers in creating richer auditory website experiences.","2022-04-29","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","1–16","","","","","","","CHI '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/ZDNYAESS/Zhang et al. - 2022 - Exploring Interactive Sound Design for Auditory We.pdf","","","audio display; interaction design; voice interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VY94ZRU8","conferencePaper","2009","Kurdyukova, Ekaterina","Inspire, guide, and entertain: designing a mobile assistant for runners","Proceedings of the 11th International Conference on Human-Computer Interaction with Mobile Devices and Services","978-1-60558-281-8","","10.1145/1613858.1613947","https://dl.acm.org/doi/10.1145/1613858.1613947","The paper presents the design of a mobile assistant for runners. We propose visual and auditory user interface for a mobile assistant, called Mobota. The system supports navigation on a new track, provides competition against a virtual rival, monitors real time user performance, entertains and encourages runners. We introduce entertaining and inspiring community notes that convey emotional messages from other sportsmen who exercise on the same track. The design and evaluation of Mobota provides an insight into the specifics of visual and auditory design of running assistants.","2009-09-15","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","1–2","","","","","","Inspire, guide, and entertain","MobileHCI '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/SJ3QLCVZ/Kurdyukova - 2009 - Inspire, guide, and entertain designing a mobile .pdf","","","auditory design; mobile sports; mobile UI; training assistant","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7LFA2UEF","conferencePaper","2008","Dicke, Christina; Deo, Shaleen; Billinghurst, Mark; Adams, Nathan; Lehikoinen, Juha","Experiments in mobile spatial audio-conferencing: key-based and gesture-based interaction","Proceedings of the 10th international conference on Human computer interaction with mobile devices and services","978-1-59593-952-4","","10.1145/1409240.1409251","https://dl.acm.org/doi/10.1145/1409240.1409251","In this paper we describe an exploration into the usability of spatial sound and multimodal interaction techniques for a mobile phone conferencing application. We compared traditional keypad based-interaction to that of a newer approach using the phone itself as a device to navigate within a virtual spatial auditory environment. While the traditional keypad interaction proved to be more straightforward to use, there was no significant impact on task completion times or number of interaction movements made between the techniques. Overall, users felt that the spatial audio application supported group awareness while aiding peripheral task monitoring. They also felt it aided the feeling of social connectedness and offered enhanced support for communication.","2008-09-02","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","91–100","","","","","","Experiments in mobile spatial audio-conferencing","MobileHCI '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/CNRL4RNV/Dicke et al. - 2008 - Experiments in mobile spatial audio-conferencing .pdf","","","spatial audio; gesture interaction; mobile HCI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K5KYEIRC","conferencePaper","2013","Chittaro, Luca; Zuliani, Francesco","Exploring audio storytelling in mobile exergames to affect the perception of physical exercise","Proceedings of the 7th International Conference on Pervasive Computing Technologies for Healthcare","978-1-936968-80-0","","10.4108/icst.pervasivehealth.2013.252016","https://dl.acm.org/doi/10.4108/icst.pervasivehealth.2013.252016","Exergames (video games that combine exercise and play) can make the experience of physical activities more enjoyable. Mobile exergames are particularly interesting as they can be used for outdoors, open-air physical activities. Unfortunately, current mobile exergames tend to require the player to frequently or continuously look at the screen. This can be hard to do while exercising, and it also requires the player to considerably distract visual attention from the surrounding physical environment, introducing safety issues in activities such as outdoor running. In this paper, we focus on two main goals. First, we explore how to use audio storytelling techniques to make physical exercise more engaging and enjoyable by exploiting a soundscape that provides prompt feedback in response to players' activity and does not require the player to look at the screen during running. Second, we study if the exergame is fun for users and if it positively affects the perception of the running experience. We measure important variables such as level of physical activity in player's lifestyle and player's physical activity enjoyment through validated methods employed in the medical literature. The results of the study show that the use of audio storytelling techniques in mobile exergames is appreciated by users, and the exergame has positive effects on the perception of physical exercise.","2013-05-05","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","1–8","","","","","","","PervasiveHealth '13","","","","ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)","Brussels, BEL","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/U65W44CN/Chittaro and Zuliani - 2013 - Exploring audio storytelling in mobile exergames t.pdf","","","exergames; audio; mobile; health; enjoyment; pervasive technology; physical exercise; safety; storytelling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UGFA6EGL","conferencePaper","2010","Vazquez Alvarez, Yolanda; Brewster, Stephen A.","Designing spatial audio interfaces to support multiple audio streams","Proceedings of the 12th international conference on Human computer interaction with mobile devices and services","978-1-60558-835-3","","10.1145/1851600.1851642","https://dl.acm.org/doi/10.1145/1851600.1851642","Auditory interfaces offer a solution to the problem of effective eyes-free mobile interactions. However, a problem with audio, as opposed to visual displays, is dealing with multiple simultaneous outputs. Any audio interface needs to consider: 1) simultaneous versus sequential presentation of multiple audio streams, 2) 3D audio techniques to place sounds in different spatial locations versus a single point of presentation, 3) dynamic movement versus fixed locations of audio sources. We present an experiment using a divided-attention task where a continuous podcast and an audio menu compete for attention. A sequential presentation baseline assessed the impact of cognitive load, and as expected, dividing attention had a significant effect on overall performance. However, spatial audio still increased the users' ability to attend to two streams, while dynamic movement of streams led to higher perceived workload. These results will provide guidelines for designers when building eyes-free auditory interfaces for mobile applications.","2010-09-07","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","253–256","","","","","","","MobileHCI '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/XZ3RCAH6/Vazquez Alvarez and Brewster - 2010 - Designing spatial audio interfaces to support mult.pdf","","","spatial audio; auditory interfaces; divided-attention task; mobile systems; multiple audio streams","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U69BRXX9","conferencePaper","2007","Stahl, Christoph","The roaring navigator: a group guide for the zoo with shared auditory landmark display","Proceedings of the 9th international conference on Human computer interaction with mobile devices and services","978-1-59593-862-6","","10.1145/1377999.1378042","https://dl.acm.org/doi/10.1145/1377999.1378042","In this paper, we introduce a shared auditory landmark display which conveys spatial survey knowledge and navigational aid to multiple users. Our guide is situated in a zoo environment, so we use recordings of animal voices to indicate the location of the animal enclosures. Spatial audio manipulates the volume and stereo balance of the sound clips, so that the listener can identify their distance and direction. The system also proactively presents audio clips with detailed information about each animal. To avoid the typical effect of social isolation through audio guides, we use shared audio so that the same sounds will be presented to each user at the same time. We have conducted an initial user study of paired visitors in the zoo to evaluate the usability of the system with positive results. The participants reported that the system is easy to use and has a stimulating influence on the communication between the visitors. As a further result, the study indicates that 'lightweight' navigational aid can be sufficient for wayfinding tasks in certain environments, which provides only the linear distance and direction of the destination.","2007-09-09","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","383–386","","","","","","The roaring navigator","MobileHCI '07","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/4N8ZGSNS/Stahl - 2007 - The roaring navigator a group guide for the zoo w.pdf","","","spatial audio; audio guide; auditory landmark display; electronic guidebooks; pedestrian navigation; shared audio","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TE4Z9DPY","conferencePaper","2008","Wang, Lei; Roe, Paul; Pham, Binh; Tjondronegoro, Dian","An audio wiki supporting mobile collaboration","Proceedings of the 2008 ACM symposium on Applied computing","978-1-59593-753-7","","10.1145/1363686.1364145","https://dl.acm.org/doi/10.1145/1363686.1364145","Wikis have proved to be very effective collaboration and knowledge management tools in large variety of fields thanks to their simplicity and flexible nature. Another important development for the internet is the emergence of powerful mobile devices supported by fast and reliable wireless networks. The combination of these developments begs the question of how to extend wikis on mobile devices and how to leverage mobile devices' rich modalities to supplement current wikis. Realizing that composing and consuming through auditory channel is the most natural and efficient way for mobile device user, this paper explores the use of audio as the medium of wiki. Our work, as the first step towards this direction, creates a framework called Mobile Audio Wiki which facilitates asynchronous audio-mediated collaboration on the move. In this paper, we present the design of Mobile Audio Wiki. As a part of such design, we propose an innovative approach for a light-weight audio content annotation system for enabling group editing, versioning and cross-linking among audio clips. To elucidate the novel collaboration model introduced by Mobile Audio Wiki, its four usage modes are identified and presented in storyboard format. Finally, we describe the initial design for presentation and navigation of Mobile Audio Wiki.","2008-03-16","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","1889–1896","","","","","","","SAC '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/GWTR9Q2Z/Wang et al. - 2008 - An audio wiki supporting mobile collaboration.pdf","","","mobile; asynchronous audio-mediated collaboration; wiki","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5J8AH95H","conferencePaper","2010","Murphy, Emma; Bates, Enda; Fitzpatrick, Dónal","Designing auditory cues to enhance spoken mathematics for visually impaired users","Proceedings of the 12th international ACM SIGACCESS conference on Computers and accessibility","978-1-60558-881-0","","10.1145/1878803.1878819","https://dl.acm.org/doi/10.1145/1878803.1878819","Visual mathematic notation provides a succinct and unambiguous description of the structure of mathematical formulae in a manner that is difficult to replicate through the linear channels of synthesized speech and Braille. It is proposed that the use of auditory cues can enhance accessibility to mathematical material and reduce common ambiguities encountered through spoken mathematics. However, the use of additional complex hierarchies of non-speech sounds to represent the structure and scope of equations may be cognitively demanding to process. This can detract from the users' understanding of the mathematical content. In this paper, a new system is presented, which uses a mixture of non-speech auditory cues, modified speech (spearcons) and binaural spatialization to disambiguate the structure of mathematical formulae. A design study, involving an online survey with 56 users, was undertaken to evaluate an existing set of auditory cues and to brainstorm alternative ideas and solutions from users before implementing modified designs and conducting a separate controlled evaluation. It is proposed that by involving a wide number of users in the creative design process, intuitive auditory cues will be implemented with the potential to enhance spoken mathematics for visually impaired users.","2010-10-25","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","75–82","","","","","","","ASSETS '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/5L3CUKI3/Murphy et al. - 2010 - Designing auditory cues to enhance spoken mathemat.pdf","","","accessibility; spearcons; non-speech sound; visually impaired users; design methods for user interfaces; mathematics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I55ZRYXF","conferencePaper","2008","Garzonis, Stavros; Bevan, Chris; O'Neill, Eamonn","Mobile service audio notifications: intuitive semantics and noises","Proceedings of the 20th Australasian Conference on Computer-Human Interaction: Designing for Habitus and Habitat","978-0-9803063-4-7","","10.1145/1517744.1517793","https://dl.acm.org/doi/10.1145/1517744.1517793","It is hoped that context-aware systems will present users with an increasing number of relevant services in an increasingly wide range of contexts. With this expansion, numerous service notifications could overwhelm users. Therefore, careful design of the notification mechanism is needed. In this paper, we investigate how semantic richness of different types of audio stimuli can be utilised to shape the intuitiveness of mobile service notifications. In order to do so, we first develop a categorisation of mobile services so that clustered services can share the same notifications. Not surprisingly, it was found that overall speech performed better than non-speech sounds, and auditory icons performed overall better than earcons. However, exceptions were observed when richer semantics were utilised in the seemingly poorer medium. We argue that success and subjective preference of auditory mobile service notifications heavily depends on the success and level of directness of the metaphors used.","2008-12-08","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","156–163","","","","","","Mobile service audio notifications","OZCHI '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/LECD8C6W/Garzonis et al. - 2008 - Mobile service audio notifications intuitive sema.pdf","","","auditory icons; earcons; context awareness; intuitiveness of audio notifications; mobile audio notifications; mobile services categorisation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VHIKH8RM","conferencePaper","2022","Han, Ying; Zhang, Yuqing","Application of Efficient Emotional Arousal in the ""Internet +"" Smart-classroom Teaching Environment","Proceedings of the 2022 3rd International Conference on Education Development and Studies","978-1-4503-9627-1","","10.1145/3528137.3528158","https://dl.acm.org/doi/10.1145/3528137.3528158","In the context of artificial intelligence, the application of ""Internet +"" smart-classroom teaching environment is of great value to modern teaching practice. In education environment, teachers can control the teaching environment and adopt emotional awakening as the main penetration principle, which can effectively promote students' learning efficiency. This study is based on the ecological valence theory (EVT), Combining Thomas and Chess PTQ as well as the odor preference questionnaire, 339 Chinese children aged 3 – 7 were randomly selected as subjects, The results show that 1, olfactory sensitivity can be used as an important environmental science application element in the smart-classroom of children aged 3-7; 2, it can effectively improve Chinese children's mood and persistence that the intelligent ecosystem in the environment through the linkage application; 3, it contributes to the accurate dissemination of odor categories in smart classrooms that establishment of child trait big data analysis; 4, uses different sensing devices and man-machine interaction modules to effectively increase the control of the teaching space, enhance students' sense of self-efficacy.","2022-05-31","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","42–51","","","","","","","ICEDS '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/M4995XDN/Han and Zhang - 2022 - Application of Efficient Emotional Arousal in the .pdf","","","emotional arousal, temperament, olfactory sensitivity; Key words: Internet +; smart classroom","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EBIRI4QZ","conferencePaper","2015","Feng, Feng; Stockman, Tony; Bryan-Kinns, Nick; AI-Thani, Dena","An investigation into the comprehension of map information presented in audio","Proceedings of the XVI International Conference on Human Computer Interaction","978-1-4503-3463-1","","10.1145/2829875.2829896","https://dl.acm.org/doi/10.1145/2829875.2829896","The growth in mobile and multimodal Computing is leading to the consideration of alternative modes of information presentation, particularly in situations such as driving or walking in unfamiliar locations where the eyes are needed for primary navigation. We report the results of an experiment in which map information is presented to 10 normally sighted participants using an auditory display. Several measures of performance are reported, including the time to navigate a virtual route, keystroke errors and the ability to construct a visual representation of the route travelled based on audio instructions only. The results show significant variability in levels of performance between individuals, though most participants were able to make sense of the auditory display and produce a reasonable visual representation of the virtual route i.e. participants were able to comprehend the presented audio map.","2015-09-07","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","1–8","","","","","","","Interacci&#xf3;n '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/XXD8PJFG/Feng et al. - 2015 - An investigation into the comprehension of map inf.pdf","","","Auditory display; audio information understanding; audio map system; spatial information","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7JH8LKJ4","conferencePaper","2012","Yang, Tao; Ferati, Mexhid; Liu, Yikun; Rohani Ghahari, Romisa; Bolchini, Davide","Aural browsing on-the-go: listening-based back navigation in large web architectures","Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","978-1-4503-1015-4","","10.1145/2207676.2207715","https://dl.acm.org/doi/10.1145/2207676.2207715","Mobile web navigation requires highly-focused visual attention, which poses problems when it is inconvenient or distracting to continuously look at the screen (e.g., while walking). Aural interfaces support more eyes-free experiences, as users can primarily listen to the content and occasionally look at the device. Yet, designing aural information architectures remains a challenge. Specifically, back navigation is inefficient in the aural setting, as it forces users to listen to each previous page to retrieve the desired content. This paper introduces topic- and list-based back: two navigation strategies to enhance aural browsing. Both are manifest in Green-Savers Mobile (GSM), an aural mobile site. A study (N=29) compared both solutions to traditional back mechanisms. Our findings indicate that topic- and list-based back enable faster access to previous pages, improve the navigation experience and reduce perceived cognitive load. The proposed designs apply to a wide range of content-intensive, ubiquitous web systems.","2012-05-05","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","277–286","","","","","","Aural browsing on-the-go","CHI '12","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/J45R5LVC/Yang et al. - 2012 - Aural browsing on-the-go listening-based back nav.pdf","","","aural web; back navigation; info architecture; mobile web","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2FX8KJ3Y","conferencePaper","2018","Pradhan, Alisha; Mehta, Kanika; Findlater, Leah","""Accessibility Came by Accident"": Use of Voice-Controlled Intelligent Personal Assistants by People with Disabilities","Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","978-1-4503-5620-6","","10.1145/3173574.3174033","https://dl.acm.org/doi/10.1145/3173574.3174033","From an accessibility perspective, voice-controlled, home-based intelligent personal assistants (IPAs) have the potential to greatly expand speech interaction beyond dictation and screen reader output. To examine the accessibility of off-the-shelf IPAs (e.g., Amazon Echo) and to understand how users with disabilities are making use of these devices, we conducted two exploratory studies. The first, broader study is a content analysis of 346 Amazon Echo reviews that include users with disabilities, while the second study more specifically focuses on users with visual impairments, through interviews with 16 current users of home-based IPAs. Findings show that, although some accessibility challenges exist, users with a range of disabilities are using the Amazon Echo, including for unexpected cases such as speech therapy and support for caregivers. Richer voice-based applications and solutions to support discoverability would be particularly useful to users with visual impairments. These findings should inform future work on accessible voice-based IPAs.","2018-04-21","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","1–13","","","","","","""Accessibility Came by Accident""","CHI '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/6F9XHD9E/Pradhan et al. - 2018 - Accessibility Came by Accident Use of Voice-Con.pdf","","","accessibility; speech; conversational interfaces; disability; intelligent personal assistants","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PQISJYGD","conferencePaper","2019","Andrade, Ronny; Rogerson, Melissa J.; Waycott, Jenny; Baker, Steven; Vetere, Frank","Playing Blind: Revealing the World of Gamers with Visual Impairment","Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","978-1-4503-5970-2","","10.1145/3290605.3300346","https://dl.acm.org/doi/10.1145/3290605.3300346","Previous research on games for people with visual impairment (PVI) has focused on co-designing or evaluating specific games - mostly under controlled conditions. In this research, we follow a game-agnostic, ""in-the-wild"" approach, investigating the habits, opinions and concerns of PVI regarding digital games. To explore these issues, we conducted an online survey and follow-up interviews with gamers with VI (GVI). Dominant themes from our analysis include the particular appeal of digital games to GVI, the importance of social trajectories and histories of gameplay, the need to balance complexity and accessibility in both games targeted to PVI and mainstream games, opinions about the state of the gaming industry, and accessibility concerns around new and emerging technologies such as VR and AR. Our study gives voice to an underrepresented group in the gaming community. Understanding the practices, experiences and motivations of GVI provides a valuable foundation for informing development of more inclusive games.","2019-05-02","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","1–14","","","","","","Playing Blind","CHI '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/LIIX98A9/Andrade et al. - 2019 - Playing Blind Revealing the World of Gamers with .pdf","","","visual impairment; audiogames; digital games; empowerment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6WP8VR9U","conferencePaper","2022","Johansen, Stine S.; van Berkel, Niels; Fritsch, Jonas","Characterising Soundscape Research in Human-Computer Interaction","Designing Interactive Systems Conference","978-1-4503-9358-4","","10.1145/3532106.3533458","https://dl.acm.org/doi/10.1145/3532106.3533458","‘Soundscapes’ are an increasingly active topic in Human-Computer Interaction (HCI) and interaction design. From mapping acoustic environments through sound recordings to designing compositions as interventions, soundscapes appear as a recurring theme across a wide body of HCI research. Based on this growing interest, now is the time to explore the types of studies in which soundscapes provide a valuable lens to HCI research. In this paper, we review papers from conferences sponsored or co-sponsored by the ACM Special Interest Group on Computer-Human Interaction in which the term ’soundscape’ occurs. We analyse a total of 235 papers to understand the role of soundscapes as a research focus and identify untapped opportunities for soundscape research within HCI. We identify two common soundscape conceptualisations: (1) Acoustic environments and (2) Compositions, and describe what characterises studies into each concept and the hybrid forms that also occur. On the basis of this, we carve out a foundation for future soundscape research in HCI as a methodological anchor to form a common ground and support this growing research interest. Finally, we offer five recommendations for further research into soundscapes within HCI.","2022-06-13","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","1394–1417","","","","","","","DIS '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/6VDRWGZX/Johansen et al. - 2022 - Characterising Soundscape Research in Human-Comput.pdf","","","Soundscape; audio; literature review; sounds; theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7EZ6KGMB","conferencePaper","2016","Fritsch, Jonas; Grönvall, Erik; Breinbjerg, Morten","Analyzing the aesthetics of participation of media architecture","Proceedings of the 3rd Conference on Media Architecture Biennale","978-1-4503-4749-5","","10.1145/2946803.2946807","https://dl.acm.org/doi/10.1145/2946803.2946807","This paper presents a theoretical framework for analyzing the aesthetics of participation of media architecture. The framework is based on a close reading of French philosopher Jacques Rancière and provides four points of emphasis: modes of sense perception, forms of engagement, community and emancipation. The framework is put to use in the analysis of three experimental media architectural projects; Ekkomaten/Echoes from Møllevangen, the coMotion Bench and FeltRadio. We discuss the findings from this analysis and outline future perspectives on how to develop and use the framework prospectively in the design of media architectural projects and other interactive environments.","2016-06-01","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","1–10","","","","","","","MAB","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/KPE9VERV/Fritsch et al. - 2016 - Analyzing the aesthetics of participation of media.pdf","","","media architecture; aesthetics of participation; experience philosophy; politics of sensation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IMU58RA5","conferencePaper","2014","Fritsch, Jonas; Breinbjerg, Morten; Jensen, Tue S.","Designing interactive listening situations","Proceedings of the 26th Australian Computer-Human Interaction Conference on Designing Futures: the Future of Design","978-1-4503-0653-9","","10.1145/2686612.2686618","https://dl.acm.org/doi/10.1145/2686612.2686618","This article presents the interactive sound installation Ekkomaten, a machine originally designed to let people explore an 18th century soundscape as part of a historical festival in Aarhus, Denmark. We present the design of the installation focusing on three core concerns when designing interactive listening situations; the physical interface, the site-specific soundscape and the affectively engaging listening experience. We then provide a detailed video analysis of the richness of use of the installation, focusing on the interaction and ways of listening facilitated by the setup. Based on this, we highlight the ways in which Ekkomaten has provided an interactive listening situation engaging people affectively and intellectually both in the exploration of sonified stories about the 18th century as well as of the installation in itself as an interactive machine for listening. Further, we reflect on important insights when designing interactive listening situations, critically reflect on the data and evaluation and outline a future experiment with the Ekkomaten infrastructure.","2014-12-02","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","31–40","","","","","","","OzCHI '14","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/9SSQ9CCL/Fritsch et al. - 2014 - Designing interactive listening situations.pdf","","","affective engagement; audio design; cultural heritage; interactive environments and installations; interactive sound design; site-specific design; urban computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SW8FKX3V","conferencePaper","2020","Chuang, Yaliang","Designing the Expressivity of Multiple Smart Things for Intuitive and Unobtrusive Interactions","Proceedings of the 2020 ACM Designing Interactive Systems Conference","978-1-4503-6974-9","","10.1145/3357236.3395450","https://dl.acm.org/doi/10.1145/3357236.3395450","Connected products and systems are becoming popular, but they seldom provide direct and intuitive communication to the users. In this study, we applied Disney's animation principles to design the expressivity with LED lights and speakers commonly embedded in electronic devices. We explored the subtle transitions of brightness and controlled the timing to compose individual and system-level behaviors with multiple devices. The designs were evaluated and improved through three iterations. In the main study, we recruited 16 designer participants to investigate whether lights and sounds could be intuitively interpreted as what the system wanted to convey. The result shows that group light behaviors could evoke meanings that are highly similar to the intents of the system. When the acoustic accompaniments were provided, participants could better perceive the presence of devices. We concluded with six sets of light behaviors that are capable of expressing smart devices and systems' intents intuitively and unobtrusively.","2020-07-03","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","2007–2019","","","","","","","DIS '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/PTUJU4VX/Chuang - 2020 - Designing the Expressivity of Multiple Smart Thing.pdf","","","situation awareness; feedback; feedforward; internet of things; direct interaction; semantic expression","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CX8WHTM8","conferencePaper","2010","Dicke, Christina; Wolf, Katrin; Tal, Yaroslav","Foogue: eyes-free interaction for smartphones","Proceedings of the 12th international conference on Human computer interaction with mobile devices and services","978-1-60558-835-3","","10.1145/1851600.1851705","https://dl.acm.org/doi/10.1145/1851600.1851705","Graphical user interfaces for mobile devices have several drawbacks in mobile situations. In this paper, we present Foogue, an eyes-free interface that utilizes spatial audio and gesture input. Foogue does not require visual attention and hence does not divert visual attention from the task at hand. Foogue has two modes, which are designed to fit the usage patterns of mobile users. For user input we designed a gesture language build of a limited number of simple but also easy to differentiate gesture elements.","2010-09-07","2023-07-06 05:49:55","2023-07-06 05:49:55","2023-07-05","455–458","","","","","","Foogue","MobileHCI '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/USUVSVXK/Dicke et al. - 2010 - Foogue eyes-free interaction for smartphones.pdf","","","spatial audio; mobile; auditory interface; gesture interaction; 3D","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VV7XR2AL","conferencePaper","2013","Fagerlönn, Johan; Larsson, Stefan; Lindberg, Stefan","An auditory display that assist commercial drivers in lane changing situations","Proceedings of the 8th Audio Mostly Conference","978-1-4503-2659-9","","10.1145/2544114.2544120","https://dl.acm.org/doi/10.1145/2544114.2544120","This paper presents a simulator study that evaluates four auditory displays to assist commercial drivers in lane changing situations. More specifically, the displays warned the drivers about vehicles in the adjacent lane. Three displays utilized different variants of graded auditory warnings (early and late signals) while one display contained a single-stage warning (late signal). For all graded warnings, a manipulation of the turn indicator sound was utilized to alert the driver. The study investigated whether the graded warnings had different effects on safety and driver acceptance compared to a single stage warning. In addition, the study examined whether the idea of changing the turn indicator sound influenced traffic safety and initial acceptance. The results support that graded warnings are more effective compared to single stage warnings. Manipulating the turn indicator was effective and the acceptance for the solution was high. The implications for design based on the results are presented.","2013-09-18","2023-07-06 05:53:57","2023-07-06 05:53:57","2023-07-05","1–7","","","","","","","AM '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/YZI8K9DQ/Fagerlönn et al. - 2013 - An auditory display that assist commercial drivers.pdf","","","auditory display; safety; alarm; alert; auditory warning; driving","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BIBVLZIM","conferencePaper","2017","Fagerlönn, Johan; Hammarberg, Kristin; Lindberg, Stefan; Sirkka, Anna; Larsson, Sofia","Designing a Multimodal Warning Display for an Industrial Control Room","Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences","978-1-4503-5373-1","","10.1145/3123514.3123516","https://dl.acm.org/doi/10.1145/3123514.3123516","This paper presents the development of a multimodal warning display for a paper mill control room. In previous work, an informative auditory display for control room warnings was proposed. The proposed auditory solution conveys information about urgent events by using a combination of auditory icons and tonal components. The main aim of the present study was to investigate if a complementary visual display could increase the effectiveness and acceptance of the existing auditory solution. The visual display was designed in a user-driven design process with operators. An evaluation was conducted both before and after the implementation. Subjective ratings showed that operators found it easier to identify the alarming section using the multimodal display. These results can be useful for any designer intending to implement a multimodal display for warnings in an industrial context.","2017-08-23","2023-07-06 05:53:57","2023-07-06 05:53:57","2023-07-05","1–5","","","","","","","AM '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/NUR2GPTS/Fagerlönn et al. - 2017 - Designing a Multimodal Warning Display for an Indu.pdf","","","auditory display; multimodal interaction; alarms; process industry; Warnings","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LH3IJS2P","conferencePaper","2011","Weinberg, Garrett; Harsham, Bret; Medenica, Zeljko","Evaluating the usability of a head-up display for selection from choice lists in cars","Proceedings of the 3rd International Conference on Automotive User Interfaces and Interactive Vehicular Applications","978-1-4503-1231-8","","10.1145/2381416.2381423","https://dl.acm.org/doi/10.1145/2381416.2381423","It has been established that head-down displays (HDDs), such as those commonly placed in the dashboard of commercial automobiles, negatively affect drivers' visual attention [1]. This problem can be exacerbated when screens are ""busy"" with graphics or rich information. In this paper, which is an extension of a user-preference study [23], we present the results of a driving simulator experiment where we examined two potential alternatives to HDDs for presenting textual lists. Subjects conducted a series of street name finding tasks using each of three system variants: one with a head-down display (HDD), one with a head-up display (HUD), and one with only an auditory display. We found that the auditory display had the least impact on driving performance and mental load, but at the expense of task completion efficiency. The HUD variant had a low impact on mental load and scored highest in user satisfaction, and therefore appears to be the most viable target for future study.","2011-11-30","2023-07-06 05:53:57","2023-07-06 05:53:57","2023-07-05","39–46","","","","","","","AutomotiveUI '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/M36436EI/Weinberg et al. - 2011 - Evaluating the usability of a head-up display for .pdf","","","auditory display; driving simulation; HDD; head-down display; head-up display; HUD; speech recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PU7ELWGM","conferencePaper","2015","Dicke, Christina; Müller, Jörg","Evaluating Mid-air List Interaction for Spatial Audio Interfaces","Proceedings of the 3rd ACM Symposium on Spatial User Interaction","978-1-4503-3703-8","","10.1145/2788940.2788945","https://dl.acm.org/doi/10.1145/2788940.2788945","Selecting items from lists is a common task in many applications. For wearable devices where no display is available, list selection can be challenging. To explore potential solutions we present four user studies evaluating mid-air gestures to interact with lists in an eyes-free interface. We found that a spatialized audio list in the shape of a 110~degree arc angled towards the dominant hand was a comfortable and usable layout for most users. A selection takes less than 10.6 seconds on average and error rates are below 4% when users locate and select an item in an unknown, unordered list of 20 items. For lists of 10 items the mean selection time is 5.5 seconds or less, and error rates drop below 1.4%. We compared monophonic to binaural playback of feedback sounds (musicons) and found no statistical difference for task completion times or error rates between the conditions. We also implemented and evaluated a music player application to showcase spatial audio list selection in an applied scenario.","2015-08-08","2023-07-06 05:53:57","2023-07-06 05:53:57","2023-07-05","24–33","","","","","","","SUI '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/TIYNACKV/Dicke and Müller - 2015 - Evaluating Mid-air List Interaction for Spatial Au.pdf","","","auditory display; direct manipulation; list selection; mid-air gestures","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6TVGV43Z","conferencePaper","2019","Weger, Marian; Höldrich, Robert","A hear-through system for plausible auditory contrast enhancement","Proceedings of the 14th International Audio Mostly Conference: A Journey in Sound","978-1-4503-7297-8","","10.1145/3356590.3356593","https://dl.acm.org/doi/10.1145/3356590.3356593","In many of our everyday and professional routines, we rely on knowledge we gather from the auditory feedback of physical interactions. In an attempt to facilitate some of these listening practices (particularly percussion), we introduce a hear-through system for intra-stimulus Auditory Contrast Enhancement (ACE) in real time. Plausible spectral ACE is achieved by adopting the neural mechanism of lateral inhibition. Additional decay prolongation facilitates pitch perception. Perceptual plausibility of the augmented auditory feedback from the observer-perspective is investigated in an experiment with auditory-visual stimuli. Measured plausibility forms material-specific patterns depending on spectral dynamics and decay prolongation.","2019-09-18","2023-07-06 05:53:57","2023-07-06 05:53:57","2023-07-05","1–8","","","","","","","AM'19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/KL8WTAML/Weger and Höldrich - 2019 - A hear-through system for plausible auditory contr.pdf","","","auditory display; auditory augmentation; ace; auditory contrast enhancement; augmented auditory feedback; auscultation; cartoonification; percussion; spectral contrast","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8YH5WCPI","conferencePaper","2008","Stockholm, Jack; Pasquier, Philippe","Eavesdropping: audience interaction in networked audio performance","Proceedings of the 16th ACM international conference on Multimedia","978-1-60558-303-7","","10.1145/1459359.1459434","https://dl.acm.org/doi/10.1145/1459359.1459434","Eavesdropping is an internet-based, interactive audio system that explores network mediated, musical performance in shared public spaces. The project aims to develop an environment which increases audience interaction and connectedness in a localized, computer-controlled performance. The system is a client-server architecture made of three components: (1) an audio preparation interface, (2) an interactive performance interface, and (3) a machine learning-based conductor. An artificial conductor mixes an acoustic ecology based on mood data entered by participants while learning from their feedback. Technicalities and early evaluation are presented.","2008-10-26","2023-07-06 05:53:57","2023-07-06 05:53:57","2023-07-05","559–568","","","","","","Eavesdropping","MM '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/KNIZ9VPG/Stockholm and Pasquier - 2008 - Eavesdropping audience interaction in networked a.pdf","","","artificial intelligence; auditory display; acoustic ecology; computer music; net art; reinforcement learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WDH3EG5K","conferencePaper","2011","Delle Monache, Stefano; Rocchesso, Davide","The curse of the where-rabbit: research through design of auditory trajectories","Proceedings of the 9th ACM SIGCHI Italian Chapter International Conference on Computer-Human Interaction: Facing Complexity","978-1-4503-0876-2","","10.1145/2037296.2037318","https://dl.acm.org/doi/10.1145/2037296.2037318","Computation can be considered a fundamental dimension of design, together with other elements like materials, colour, sound, form and function. Positioned between acoustics, computer science and design, sonic interaction design is about shaping the sonic behaviour of artefacts, by designing relevant sonic interactions. In the light of Research through Design (RtD) method of inquiry, recently emerged in HCI, we tackled the design problem of distributing short sequences of sounds in space as well in time, by exploring a non-visual illusion called auditory saltation effect. Spatially distributing auditory displays can be important for many applications, including the signaling of hidden hot spots.","2011-09-13","2023-07-06 05:53:57","2023-07-06 05:53:57","2023-07-05","79–84","","","","","","The curse of the where-rabbit","CHItaly","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/38HAXFHR/Delle Monache and Rocchesso - 2011 - The curse of the where-rabbit research through de.pdf","","","auditory perception; auditory display; sonic interaction design; research through design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UL2KG3YH","conferencePaper","2010","Shahab, Qonita; Terken, Jacques; Eggen, Berry","Auditory messages for speed advice in advanced driver assistance systems","Proceedings of the 2nd International Conference on Automotive User Interfaces and Interactive Vehicular Applications","978-1-4503-0437-5","","10.1145/1969773.1969783","https://dl.acm.org/doi/10.1145/1969773.1969783","Simple tones in in-car systems are mostly used for status indication or warning and alerting purposes. We argue that simple tones can also be used for the purpose of advising drivers through an Advanced Driver Assistance System (ADAS). Our ADAS application is called Cooperative Speed Assistance (CSA), where drivers receive advice to slow down or speed up to coordinate their speed with the speed of other vehicles in the traffic. Two concepts of auditory messages are presented: Looping messages are played as long as the advice applies, while Toggle messages mark the beginning and the end of an advice. For each concept, two prototypes of simple-tone signals were designed based on existing guidelines about sound characteristics affecting urgency and evaluation by users. The temporal characteristics of the signals indicated how much or how fast drivers should adapt their speed. The concepts were evaluated by having users drive in a driving simulator. Objective measurements indicated that there was no difference in effectiveness between the two concepts. Subjective evaluation indicated that users preferred the Toggle concept.","2010-11-11","2023-07-06 05:53:57","2023-07-06 05:53:57","2023-07-05","50–56","","","","","","","AutomotiveUI '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/CRK53HUT/Shahab et al. - 2010 - Auditory messages for speed advice in advanced dri.pdf","","","auditory display; sound design; ADAS; automotive user interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QUD8QVM8","conferencePaper","2018","Studley, Thomas; Vella, Richard; Scott, Nathan; Nesbitt, Keith","A definition of creative-based music games","Proceedings of the Australasian Computer Science Week Multiconference","978-1-4503-5436-3","","10.1145/3167918.3167921","https://dl.acm.org/doi/10.1145/3167918.3167921","Growing interest in the study of video game music has led an increasing base of scholars to pursue a multi-faceted investigation of music-based games. Within this domain, a distinct subset of 'creative-based' music games are emerging as a fertile new ground for the examination of music in interactive gaming environments. This paper aims to analyse the nature of these 'creative-based music games'. We review the current state of game music literature and show that music traditionally occupies a 'supportive' role in games. The diverse genre of 'music-games' is then framed as a departure from this supportive paradigm, introducing a further review covering the existing classifications of 'music-games'. Discussion of the component elements in 'creative-based music games' provides a more formal definition of this class of games. To illustrate this definition further an early prototype for an original game design ('EvoMusic') is then described. The rules, mechanics, and underlying concept of EvoMusic are examined in detail and then discussed in relation to a comparable game from the field (Soundrop). We conclude that the classification of 'creative-based music games' suggests key design dimensions that help distinguish games like EvoMusic from other closely related modes of 'exploratory' musical interaction. Using this classification there is the potential to explore, develop and evaluate as yet untapped design features for musical games.","2018-01-29","2023-07-06 05:53:57","2023-07-06 05:53:57","2023-07-05","1–10","","","","","","","ACSW '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/2LB5D9WI/Studley et al. - 2018 - A definition of creative-based music games.pdf","","","ludomusicology; music games; video game music","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"43E7L6YG","conferencePaper","2017","Wang, MinJuan; Lyckvi, Sus Lundgren; Chen, Chenhui; Dahlstedt, Palle; Chen, Fang","Using Advisory 3D Sound Cues to Improve Drivers' Performance and Situation Awareness","Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","978-1-4503-4655-9","","10.1145/3025453.3025634","https://dl.acm.org/doi/10.1145/3025453.3025634","Within vehicle Human Machine Interface design, visual displays are predominant, taking up more and more of the visual channel for each new system added to the car, e.g. navigation systems, blind spot information and forward collision warnings. Sounds however, are mainly used to alert or warn drivers together with visual information. In this study we investigated the design of auditory displays for advisory information, by designing a 3D auditory advisory traffic information system (3DAATIS) which was evaluated in a drive simulator study with 30 participants. Our findings indicate that overall, drivers' performance and situation awareness improved when using this system. But, more importantly, the results also point towards the advantages and limitations of the use of advisory 3D-sounds in cars, e.g. attention capture vs. limited auditory resolution. These findings are discussed and expressed as design implications.","2017-05-02","2023-07-06 05:53:57","2023-07-06 05:53:57","2023-07-05","2814–2825","","","","","","","CHI '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/FX2TXXEQ/Wang et al. - 2017 - Using Advisory 3D Sound Cues to Improve Drivers' P.pdf","","","auditory display; 3d auditory advisory traffic information system; drive behavior; in-vehicle design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"75A2CZV3","conferencePaper","2020","Payne, William Christopher; Xu, Alex Yixuan; Ahmed, Fabiha; Ye, Lisa; Hurst, Amy","How Blind and Visually Impaired Composers, Producers, and Songwriters Leverage and Adapt Music Technology","Proceedings of the 22nd International ACM SIGACCESS Conference on Computers and Accessibility","978-1-4503-7103-2","","10.1145/3373625.3417002","https://dl.acm.org/doi/10.1145/3373625.3417002","Today, music creation software and hardware are central to the workflow of most professional composers, producers, and songwriters. Music is an aural art form, but it is notated graphically, and highly visual mainstream technologies pose significant accessibility barriers to blind and visually impaired users. Very few studies address the current state of accessibility in music technologies, and fewer propose alternative designs. To address a lack of understanding about the experiences of blind and visually impaired music technology users, we conducted an interview study with 11 music creators who, we demonstrate, find ingenious workarounds to bend inaccessible technologies to their needs, but still face persistent barriers including a lack of options, a limited but persistent need for sighted help, and accessibility features that fail to cover all use cases. We reflect on our findings and present opportunities and guidelines to promote more inclusive design of future music technologies.","2020-10-29","2023-07-06 05:53:57","2023-07-06 05:53:57","2023-07-05","1–12","","","","","","","ASSETS '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/UZVYF8JC/Payne et al. - 2020 - How Blind and Visually Impaired Composers, Produce.pdf","","","accessibility; music technology; blindness; visual impairments; design; music creation; music learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MGF9H8NW","conferencePaper","2016","Grønbæk, Jens Emil; Jakobsen, Kasper Buhl; Petersen, Marianne Graves; Rasmussen, Majken Kirkegård; Winge, Jakob; Stougaard, Jeppe","Designing for Children's Collective Music Making: How Spatial Orientation and Configuration Matter","Proceedings of the 9th Nordic Conference on Human-Computer Interaction","978-1-4503-4763-1","","10.1145/2971485.2971552","https://dl.acm.org/doi/10.1145/2971485.2971552","Hitmachine empowers children to make music through building physical, shared interactive instruments from Lego Mindstorms™ and playing them to a beat. The design rationale for Hitmachine draws upon the collective interaction model, theories of proxemics and F-formations, as well as a framework for social interaction. Hitmachine was evaluated during a 4-day workshop where 150 children aged 3-13 engaged with the system. Based on lessons from this workshop we point to key issues to consider when designing for collective music making. This includes designing for multiple access points and spatial orientation of these, designing for sense of impact as well as sense of control, and giving careful consideration to how the spatial configuration of technological artifacts and furniture can provide opportunities for social interaction.","2016-10-23","2023-07-06 05:53:57","2023-07-06 05:53:57","2023-07-05","1–10","","","","","","Designing for Children's Collective Music Making","NordiCHI '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/FWEX22T9/Grønbæk et al. - 2016 - Designing for Children's Collective Music Making .pdf","","","Collective interaction; Embodied constraints; F-formations; Proxemics; Social interaction; Tangible music","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WJLK6DNB","conferencePaper","2022","Zheng, Haiyun; Jiang, Zhengqing","Comparative Study of Music Visualization based on CiteSpace at China and the World","Proceedings of the 2021 5th International Conference on Computer Science and Artificial Intelligence","978-1-4503-8415-5","","10.1145/3507548.3507604","https://dl.acm.org/doi/10.1145/3507548.3507604","Music visualization is a visual art form for understanding, analyzing and comparing the internal structure and expressive features of music. It meets the aesthetic demand of the masses in the digital age. This paper reviews the development and research status of the music visualization literature in the past 20 years, comprehensively analyzes the research process and current hotspots of music visualization, and speculates the future development trend. We have used Web of Science (WoS) and China National Knowledge Infrastructure (CNKI) as data sources, used CiteSpace software to compare and analyze the year, country, subject distribution and hot keywords of music visualization literature at China and the world from 2000 to 2020 by the method of Mapping Knowledge Domain. The results show that the research on music visualization at China and other countries is showing an upward trend, and it presents the characteristics of multi-disciplinary integration. Different application scenarios, research methods and development stages lead to different research hotspots between different countries. The shortcomings of Chinese research in this field lies in that the research content needs to be deepened, the interdisciplinary content needs to be integrated, and applications of music visualization needs to be popularized.","2022-03-09","2023-07-06 05:53:57","2023-07-06 05:53:57","2023-07-05","365–371","","","","","","","CSAI '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/BDY7JQEJ/Zheng and Jiang - 2022 - Comparative Study of Music Visualization based on .pdf","","","CiteSpace; Mapping Knowledge Domain; Music Visualization; Visual Analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2PQ4CN89","conferencePaper","2010","Cohen, Michael","Under-explored dimensions in spatial sound","Proceedings of the 9th ACM SIGGRAPH Conference on Virtual-Reality Continuum and its Applications in Industry","978-1-4503-0459-7","","10.1145/1900179.1900199","https://dl.acm.org/doi/10.1145/1900179.1900199","An introduction to spatial sound in the context of hypermedia, interactive multimedia, and virtual reality is presented. Basic principals of relevant physics and psychophysics are reviewed (ITDs: interaural time differences, IIDs: interaural intensity differences, and frequency-dependent attenuation capturable by transfer functions). Modeling of sources and sinks (listeners) elaborates such models to include such as intensity, radiation, distance attenuation & filtering, and reflections & reverberation. Display systems---headphones and headsets, loudspeakers, nearphones, stereo, home theater and other surround systems, discrete speaker systems, speaker arrays, WFS (wave field synthesis), and spatially immersive displays---are described. Distributed applications are surveyed, including stereotelephony, chat-spaces, and massively multiplayer online role-playing games (MMORPGs), with references to immersive virtual environments.","2010-12-12","2023-07-06 05:53:57","2023-07-06 05:53:57","2023-07-05","95–102","","","","","","","VRCAI '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/NGWREIY8/Cohen - 2010 - Under-explored dimensions in spatial sound.pdf","","","ambient media; awareware; impulse response; narrowcasting; pervasive computing; transfer function; ubicomp (ubiquitous computing); virtual auditory display; wearware; whereware","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AS3LXJQ6","conferencePaper","2022","Xin, Xin; Wang, Yiji; Xiang, Guo; Yang, Wenmin; Liu, Wei","Effectiveness of Multimodal Display in Navigation Situation","The Ninth International Symposium of Chinese CHI","978-1-4503-8695-1","","10.1145/3490355.3490361","https://dl.acm.org/doi/10.1145/3490355.3490361","With the development of technology, the interactive experience in the car has become more abundant, drivers have to face a large amount of information. In the navigation situation, drivers need to receive navigation information to make correct driving behavior, but this may increase drivers’ workload. The current display includes visual display, auditory display, and haptic display, but they have limitations. In order to solve the above problems and provide more suggestions for driving safety, this research explored the effectiveness of multimodal display and its merits than unimodal display in the navigation situation, dependent variables are driving behavior performance and subjective mental workload, eye tracking behavior. We adopted interview to explore current navigation situations and classified lane changing and turning have high workload than straight driving. The simulated driving experiment conducted in the later stage of research, it is 2 × 5 mixed experiment, the between-subjects factors are navigation situations (high load, low load), and the within-subjects’ factors are information display methods (visual, auditory, haptic, multimodal, control). The experiment recruited 18 participants and randomly divided them into two groups to experience each information display in turn. It is found that the multimodal display is good than visual modality under high load situations, and the lateral speed control of the driver is more stable under the multimodal condition. Although the mental workload of drivers under multimodal conditions did not show a significant difference from other conditions, the scores were still lower than other conditions. The multimodal display has the potential to ensure driving safety, but it need further research to discuss.","2022-02-07","2023-07-06 05:56:29","2023-07-06 05:56:29","2023-07-05","50–62","","","","","","","Chinese CHI 2021","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/SUBRF6XL/Xin et al. - 2022 - Effectiveness of Multimodal Display in Navigation .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SR4NP7HL","conferencePaper","2014","Marentakis, Georgios; Liepins, Rudolfs","Evaluation of hear-through sound localization","Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","978-1-4503-2473-1","","10.1145/2556288.2557168","https://dl.acm.org/doi/10.1145/2556288.2557168","Listening and interacting with audio commonly relies on using earphones which limit the ability of users to perceive their auditory environment. Earphone sets that integrate miniature microphones on their exterior can, however, be used to hear-through the auditory environment. We present an evaluation study in which sound localization when wearing such a hear-through system is compared to normal earphones, open headphones and unblocked ears. Although localization performance is improved compared to open headphones, we find that it is compromised in comparison to listening without earphones because confusions of sound direction increase and localization judgment distributions are more dispersed and show a weaker correlation to the test directions. The implications of the results to human computer interaction and possible improvements to hear-through system design are discussed.","2014-04-26","2023-07-06 05:56:29","2023-07-06 05:56:29","2023-07-05","267–270","","","","","","","CHI '14","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/M96J4JTY/Marentakis and Liepins - 2014 - Evaluation of hear-through sound localization.pdf","","","auditory augmented reality; hear-through systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U4E92LTK","conferencePaper","2017","Sterkenburg, Jason; Landry, Steven; Jeon, Myounghoon","Eyes-free In-vehicle Gesture Controls: Auditory-only Displays Reduced Visual Distraction and Workload","Proceedings of the 9th International Conference on Automotive User Interfaces and Interactive Vehicular Applications Adjunct","978-1-4503-5151-5","","10.1145/3131726.3131747","https://dl.acm.org/doi/10.1145/3131726.3131747","Visual distractions increase crash risk while driving. Our research focuses on creating and evaluating an air gesture control system that is less visually demanding than current infotainment systems. We completed a within-subjects experiment with 24 participants, each of whom completed a simulated drive while using six different prototypes, in turn. The primary research questions were the influence of combinations of visual and auditory displays (visual, visual/auditory, auditory) and control orientation (vertical vs horizontal). We recorded lane departures, eye glance behavior, secondary task performance, and driver workload. Results demonstrated that for lane departures all prototypes performed comparably, with the auditory-only showing a strong tendency of improvements. A deeper look illustrated a tradeoff between eyes-on-road time and secondary task completion time for the auditory-only display -- the safest but slowest among the six prototypes. The auditory-only also reduced overall workload. Control orientation showed only small subjective effect in favor of vertical controls.","2017-09-24","2023-07-06 05:56:29","2023-07-06 05:56:29","2023-07-05","195–200","","","","","","Eyes-free In-vehicle Gesture Controls","AutomotiveUI '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/5JULRMTP/Sterkenburg et al. - 2017 - Eyes-free In-vehicle Gesture Controls Auditory-on.pdf","","","Auditory displays; driving simulation; compatibility; in-air gesture controls","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5ZIIUEIZ","conferencePaper","2014","Frisson, Christian; Dupont, Stéphane; Yvart, Willy; Riche, Nicolas; Siebert, Xavier; Dutoit, Thierry","AudioMetro: directing search for sound designers through content-based cues","Proceedings of the 9th Audio Mostly: A Conference on Interaction With Sound","978-1-4503-3032-9","","10.1145/2636879.2636880","https://dl.acm.org/doi/10.1145/2636879.2636880","Sound designers source sounds in massive collections, heavily tagged by themselves and sound librarians. For each query, once successive keywords attained a limit to filter down the results, hundreds of sounds are left to be reviewed. AudioMetro combines a new content-based information visualization technique with instant audio feedback to facilitate this part of their workflow. We show through user evaluations by known-item search in collections of textural sounds that a default grid layout ordered by filename unexpectedly outperforms content-based similarity layouts resulting from a recent dimension reduction technique (Student-t Stochastic Neighbor Embedding), even when complemented with content-based glyphs that emphasize local neighborhoods and cue perceptual features. We propose a solution borrowed from image browsing: a proximity grid, whose density we optimize for nearest neighborhood preservation among the closest cells. Not only does it remove overlap but we show through a subsequent user evaluation that it also helps to direct the search. We based our experiments on an open dataset (the OLPC sound library) for replicability.","2014-10-01","2023-07-06 05:56:29","2023-07-06 05:56:29","2023-07-05","1–8","","","","","","AudioMetro","AM '14","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/NS9QNWS4/Frisson et al. - 2014 - AudioMetro directing search for sound designers t.pdf","","","music information retrieval; content-based similarity; known-item search; media browsers; sound effects; visual variables","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7RHBLTES","conferencePaper","2020","Hug, Daniel","How do you sound design? an exploratory investigation of sound design process visualizations","Proceedings of the 15th International Audio Mostly Conference","978-1-4503-7563-4","","10.1145/3411109.3411144","https://dl.acm.org/doi/10.1145/3411109.3411144","Sound design is increasingly diversifying into many areas beyond its traditional domains in film, television, radio or theatre. This leads to sound designers being confronted with a multitude of design and development processes. The related methodologies have an impact on how problems are framed and what is considered an ideal path to achieve their solutions. From this a need for an educated discourse in sound design education and professional practice arises. This article investigates the creative process from the perspective of an emerging generation of sound designers. The first part of the paper outlines concepts and models of the design process in various fields of practice. The second part is devoted to an interpretive comparative analysis of sound design process visualizations created by sound design students with a professional background. Apart from gaining a better understanding of the creative process of the sound designers, the goal of this work is to contribute to a better integration of the sound design craft into contemporary design process methodologies, ultimately leading to an empowerment of the sound designer in complex, dynamic and interdisciplinary project settings.","2020-09-16","2023-07-06 05:56:29","2023-07-06 05:56:29","2023-07-05","114–121","","","","","","How do you sound design?","AM '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/ZM7ZVB97/Hug - 2020 - How do you sound design an exploratory investigat.pdf","","","sound design; methodology; design process; sound design education","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"78GHGWJC","conferencePaper","2007","Foale, Cameron; Vamplew, Peter","Portal-based sound propagation for first-person computer games","Proceedings of the 4th Australasian conference on Interactive entertainment","978-1-921166-87-7","","","","First-person computer games are a popular modern video game genre. A new method is proposed, the Directional Propagation Cache, that takes advantage of the very common portal spatial subdivision method to accelerate environmental acoustics simulation for first-person games, by caching sound propagation information between portals.","2007-12-03","2023-07-06 05:56:29","2023-07-06 05:56:29","2023-07-05","1–8","","","","","","","IE '07","","","","RMIT University","Melbourne, AUS","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/S6WMGT9A/Foale and Vamplew - 2007 - Portal-based sound propagation for first-person co.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G5R5HDRY","conferencePaper","2015","Erkut, Cumhur; Serafin, Stefania; Hoby, Michael; Sårde, Jonniy","Product Sound Design: Form, Function, and Experience","Proceedings of the Audio Mostly 2015 on Interaction With Sound","978-1-4503-3896-7","","10.1145/2814895.2814920","https://dl.acm.org/doi/10.1145/2814895.2814920","Current interactive products, services, and environments are appraised by their sensory attributes, in addition to their form and function. Sound is an important factor in these multisensory product appraisals. Integrating this sound opportunity into the design and development of interactive products, which are fit for real-world, yet constitute a strong brand identity, remains a challenge. We address this challenge by applying the research know-how of an academic institution and business practices of a sound agency SME within the core R&D and production process of the third industrial partner. Our approach has clear application scenarios in, e.g., extended wireless headsets, car audio appliances, and portable entertainment devices. We describe the prototypes developed during the project life span, and the activities and outcomes of a half-day workshop designed to disseminate the project results.","2015-10-07","2023-07-06 05:56:29","2023-07-06 05:56:29","2023-07-05","1–6","","","","","","Product Sound Design","AM '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/G29BV5DA/Erkut et al. - 2015 - Product Sound Design Form, Function, and Experien.pdf","","","Auditory Feedback; Pedagogy; Sonic Interaction Design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KTCMLCZN","conferencePaper","2016","Grani, Francesco; Nordahl, Rolf; Serafin, Stefania","Multimodal interactions, virtual reality and 360 movies: applications using Wavefield synthesis","Proceedings of the Audio Mostly 2016","978-1-4503-4822-5","","10.1145/2986416.2986430","https://dl.acm.org/doi/10.1145/2986416.2986430","We present a report covering our preliminary research on the use of wavefield synthesis WFS in a multimodal context. Traditionally, WFS has been used as a way to faithfully reproduce auditory experiences. To our knowledge, little research has tried to understand how to gesturally control WFS. Moreover, there are no applications combining WFS and virtual reality video productions. In this paper, we are interested in exploring the applications of WFS in the context of gestural control and 360 video production. We present three projects performed by students at Aalborg University Copenhagen that explore these directions.","2016-10-04","2023-07-06 05:56:29","2023-07-06 05:56:29","2023-07-05","20–27","","","","","","Multimodal interactions, virtual reality and 360 movies","AM '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/TQSW6TJ9/Grani et al. - 2016 - Multimodal interactions, virtual reality and 360 m.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q93HDXHS","conferencePaper","2009","Taylor, Micah T.; Chandak, Anish; Antani, Lakulish; Manocha, Dinesh","RESound: interactive sound rendering for dynamic virtual environments","Proceedings of the 17th ACM international conference on Multimedia","978-1-60558-608-3","","10.1145/1631272.1631311","https://dl.acm.org/doi/10.1145/1631272.1631311","We present an interactive algorithm and system (RESound) for sound propagation and rendering in virtual environments and media applications. RESound uses geometric propagation techniques for fast computation of propagation paths from a source to a listener and takes into account specular reflections, diffuse reflections, and edge diffraction. In order to perform fast path computation, we use a unified ray-based representation to efficiently trace discrete rays as well as volumetric ray-frusta. RESound further improves sound quality by using statistical reverberation estimation techniques. We also present an interactive audio rendering algorithm to generate spatialized audio signals. The overall approach can render sound in dynamic scenes allowing source, listener, and obstacle motion. Moreover, our algorithm is relatively easy to parallelize on multi-core systems. We demonstrate its performance on complex game-like and architectural environments.","2009-10-19","2023-07-06 05:56:29","2023-07-06 05:56:29","2023-07-05","271–280","","","","","","RESound","MM '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/NY77UAF5/Taylor et al. - 2009 - RESound interactive sound rendering for dynamic v.pdf","","","sound; acoustics; raytracing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7EBPVS2M","conferencePaper","2012","Delle Monache, Stefano; Rocchesso, Davide; Qi, Jie; Buechley, Leah; De Götzen, Amalia; Cestaro, Dario","Paper mechanisms for sonic interaction","Proceedings of the Sixth International Conference on Tangible, Embedded and Embodied Interaction","978-1-4503-1174-8","","10.1145/2148131.2148146","https://dl.acm.org/doi/10.1145/2148131.2148146","Introducing continuous sonic interaction in augmented pop-up books enhances the expressive and performative qualities of movables, making the whole narrative experience more engaging and personal. The SaMPL Spring School on Sounding Popables explored the specific topic of paper-driven sonic narratives. Working groups produced several sketches of sonic interactions with movables. The most significant sketches of sounding popables are presented and analyzed.","2012-02-19","2023-07-06 05:56:29","2023-07-06 05:56:29","2023-07-05","61–68","","","","","","","TEI '12","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/A6B599KJ/Delle Monache et al. - 2012 - Paper mechanisms for sonic interaction.pdf","","","sonic interaction design; pop-up books","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6BT93W2W","conferencePaper","2008","Ďurikovič, Dominik; Ďurikovič, Roman","Quality metrics for WEB page content representation in audio space","Proceedings of the 24th Spring Conference on Computer Graphics","978-1-60558-957-2","","10.1145/1921264.1921295","https://dl.acm.org/doi/10.1145/1921264.1921295","Having well-defined measures of web page content representation can significantly improve design, development, testing and validating of audio interfaces. First metric is connected with data changes perception by reading users. Changes in the web page that are not important for the reader comparing to the previous version are called small changes on the web page. Changes with higher importance value for reader are called huge changes. Ability to catch relevant changes in source data document by audio space representation is important measurable attribute for users reading these data. This attribute is called web change perception for web pages. First, we describe evaluation of the web change perception and next we are measuring sound perception of these representations using linear regression and calculation by Mathematica system.","2008-04-21","2023-07-06 05:56:29","2023-07-06 05:56:29","2023-07-05","149–154","","","","","","","SCCG '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/LDUVBMZ4/Ďurikovič and Ďurikovič - 2008 - Quality metrics for WEB page content representatio.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"54AZNEI6","conferencePaper","2009","Zheng, Changxi; James, Doug L.","Harmonic fluids","ACM SIGGRAPH 2009 papers","978-1-60558-726-4","","10.1145/1576246.1531343","https://dl.acm.org/doi/10.1145/1576246.1531343","Fluid sounds, such as splashing and pouring, are ubiquitous and familiar but we lack physically based algorithms to synthesize them in computer animation or interactive virtual environments. We propose a practical method for automatic procedural synthesis of synchronized harmonic bubble-based sounds from 3D fluid animations. To avoid audio-rate time-stepping of compressible fluids, we acoustically augment existing incompressible fluid solvers with particle-based models for bubble creation, vibration, advection, and radiation. Sound radiation from harmonic fluid vibrations is modeled using a time-varying linear superposition of bubble oscillators. We weight each oscillator by its bubble-to-ear acoustic transfer function, which is modeled as a discrete Green's function of the Helmholtz equation. To solve potentially millions of 3D Helmholtz problems, we propose a fast dual-domain multipole boundary-integral solver, with cost linear in the complexity of the fluid domain's boundary. Enhancements are proposed for robust evaluation, noise elimination, acceleration, and parallelization. Examples are provided for water drops, pouring, babbling, and splashing phenomena, often with thousands of acoustic bubbles, and hundreds of thousands of transfer function solves.","2009-07-27","2023-07-06 05:56:29","2023-07-06 05:56:29","2023-07-05","1–12","","","","","","","SIGGRAPH '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/MVHSCR6E/Zheng and James - 2009 - Harmonic fluids.pdf","","","sound synthesis; acoustic bubbles; acoustic transfer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"644AUB9L","conferencePaper","2011","Huang, Chung-Ching; Bardzell, Jeffrey; Terrell, Jennifer","Can your pet rabbit read your email? a critical analysis of the Nabaztag rabbit","Proceedings of the 2011 Conference on Designing Pleasurable Products and Interfaces","978-1-4503-1280-6","","10.1145/2347504.2347532","https://dl.acm.org/doi/10.1145/2347504.2347532","The Nabaztag rabbit is an ambient digital device with customized functions. It was advertised as an ambient display, using strong product images suggesting that it is a pet alternative. However, after early interest, the popularity of this product did not last long. In this paper, we demonstrate interaction criticism as an approach to design research, exploring and proposing reasons for the product's decline. Specifically, we argue that the rabbit is difficult to connect with emotionally and explore several reasons this might be true. Our approach is phenomenological and hermeneutic in nature: we engaged in product usage for over twelve months, and practice a theoretically informed interpretive analysis. Using a combination of critical theories and affect research from robotics, we argue the Nabaztag product identity is confusing, which might be related to the manufactures' multiple intentions, and the gap between ideal and real users. We continue with an account of two genres of functions in the Nabaztag, revealing how they polarize of interpretation; moments when Nabaztag acted in unexpected ways; and the increased, rather than decreased, difficulty in interpreting Nabaztag the longer we used it. Interpretively understanding Nabaztag's experiential failures helps cultivate relevant design sensitivities and even implications for future designs.","2011-06-22","2023-07-06 05:56:29","2023-07-06 05:56:29","2023-07-05","1–8","","","","","","Can your pet rabbit read your email?","DPPI '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/P5NNI6SQ/Huang et al. - 2011 - Can your pet rabbit read your email a critical an.pdf","","","HCI; HRI; ambient information display; interaction criticism; Nabaztag","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FUZ265D5","conferencePaper","2009","Jeon, Myounghoon; Park, Junho; Heo, Ubeom; Yun, Jongmin","Enhanced turning point displays facilitate drivers' interaction with navigation devices","Proceedings of the 1st International Conference on Automotive User Interfaces and Interactive Vehicular Applications","978-1-60558-571-0","","10.1145/1620509.1620536","https://dl.acm.org/doi/10.1145/1620509.1620536","Recently, the use of in-vehicle navigation devices, such as PNDs (Personal or Portable Navigation Devices) has become pervasive, and the device functions have been rapidly expanded and updated. Unfortunately, drivers often have considerable difficulty using these complex technologies. To improve and optimize PND user interfaces, the present study suggested several display improvements for the turning point, which is one of the critical usability issues. Advanced Turn-By-Turn Display and Spatial Turning Sound were suggested to facilitate the preparation of the next turns. Leading Tones for Turning was also presented to help drivers tune the timing of their turns. We evaluated these new concepts with domain experts in three countries, and improved the details of the functions. We are currently implementing those features and looking forward to demonstrating new displays on the real product in our presentation at the Automotive User Interface conference.","2009-09-21","2023-07-06 05:56:29","2023-07-06 05:56:29","2023-07-05","145–148","","","","","","","AutomotiveUI '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/MYQXTNAF/Jeon et al. - 2009 - Enhanced turning point displays facilitate drivers.pdf","","","advanced turn-by-turn display; AUI; GUI; IVTs; leading tones for turning; PND; spatial turning sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WCFSFJGP","conferencePaper","2011","Stefik, Andreas M.; Hundhausen, Christopher; Smith, Derrick","On the design of an educational infrastructure for the blind and visually impaired in computer science","Proceedings of the 42nd ACM technical symposium on Computer science education","978-1-4503-0500-6","","10.1145/1953163.1953323","https://dl.acm.org/doi/10.1145/1953163.1953323","The blind and visually impaired community is significantly underrepresented in computer science. Students who wish to enter the discipline must overcome significant technological and educational barriers to succeed. In an attempt to help this population, we are engaged in a three-year research project to build an educational infrastructure for blind and visually impaired middle and high school students. Our primary research goal is to begin forging a multi-sensory educational infrastructure for the blind across the United States. We present here two preliminary results from this research: 1) a new auditory programming environment called Sodbeans, a programming language called Hop, and a multi-sensory (sound and touch) curriculum, and 2) an empirical study of our first summer workshop with the blind students. Results show that students reported a significant increase in programming self-efficacy after participating in our camp.","2011-03-09","2023-07-06 05:56:29","2023-07-06 05:56:29","2023-07-05","571–576","","","","","","","SIGCSE '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/BZKUW5JB/Stefik et al. - 2011 - On the design of an educational infrastructure for.pdf","","","accessibility; assistive technology; visual impairments; auditory debugging","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C4SAEMQR","conferencePaper","2012","Jeon, Myounghoon; Riener, Andreas; Lee, Ju-Hwan; Schuett, Jonathan; Walker, Bruce N.","Cross-cultural differences in the use of in-vehicle technologies and vehicle area network services: Austria, USA, and South Korea","Proceedings of the 4th International Conference on Automotive User Interfaces and Interactive Vehicular Applications","978-1-4503-1751-1","","10.1145/2390256.2390283","https://dl.acm.org/doi/10.1145/2390256.2390283","Vehicle area network (VAN) communications and related services are getting more pervasive [1]. However, even though user-centered design has been emphasized, VAN services have often been developed through a technology-driven approach. This paper presents cross-cultural survey results on VAN services in three different countries: Austria, USA, and South Korea. The current research compared the state-of-the-art of drivers' current in-vehicle technology use and investigated their needs and wants for plausible new services in the near future. Further, we validated our next generation in-vehicle interface concepts stemming from our previous participatory design process [2]. Results showed clear differences between Austrians vs. Americans and Koreans. Even though Koreans and Americans in our survey were older than Austrians, they seemed more open-minded to VAN services (e.g., social networks in car, V2V services, in-vehicle agent, etc) in general and rated them more positively. Through these cross-cultural needs analyses of end users, designers and practitioners are expected to gain insights into developing a standardized service across cultures as well as culturally tuned in-vehicle interfaces. Moreover, we hope that this initial international collaboration can serve as a good test bed for future research and hope to expand our consortium with more colleagues in the AutomotiveUI community for further cross-cultural studies.","2012-10-17","2023-07-06 05:56:29","2023-07-06 05:56:29","2023-07-05","163–170","","","","","","Cross-cultural differences in the use of in-vehicle technologies and vehicle area network services","AutomotiveUI '12","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/43JSL4GF/Jeon et al. - 2012 - Cross-cultural differences in the use of in-vehicl.pdf","","","cross-cultural differences; in-vehicle agents; next generation in-vehicle interfaces; social network services; VAN (vehicle area network)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U5GD9G2Q","conferencePaper","2013","Bakker, Saskia; van den Hoven, Elise; Eggen, Berry","FireFlies: physical peripheral interaction design for the everyday routine of primary school teachers","Proceedings of the 7th International Conference on Tangible, Embedded and Embodied Interaction","978-1-4503-1898-3","","10.1145/2460625.2460634","https://dl.acm.org/doi/10.1145/2460625.2460634","This paper presents a research-through-design study into interactive systems for a primary school setting to support teachers' everyday tasks. We developed an open-ended interactive system called FireFlies, which is intended to be interacted with in the periphery of the teacher's attention and thereby become an integral part of everyday routines. FireFlies uses light-objects and audio as a (background) information display. Furthermore, teachers can manipulate the light and audio through physical interaction. A working prototype of FireFlies was deployed in four different classrooms for six weeks. Qualitative results reveal that all teachers found a relevant way of working with FireFlies, which they applied every day of the evaluation. After the study had ended and the systems were removed from the schools, the teachers kept reaching for the devices and mentioned they missed FireFlies, which shows that it had become part of their everyday routine.","2013-02-10","2023-07-06 05:58:37","2023-07-06 05:58:37","2023-07-05","57–64","","","","","","FireFlies","TEI '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/G4GDRZFJ/Bakker et al. - 2013 - FireFlies physical peripheral interaction design .pdf","","","design; audio; calm technology; everyday routine; peripheral interaction; physical interaction; user exploration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HPIVM5HV","conferencePaper","2012","Fagerlönn, Johan; Lindberg, Stefan; Sirkka, Anna","Graded auditory warnings during in-vehicle use: using sound to guide drivers without additional noise","Proceedings of the 4th International Conference on Automotive User Interfaces and Interactive Vehicular Applications","978-1-4503-1751-1","","10.1145/2390256.2390269","https://dl.acm.org/doi/10.1145/2390256.2390269","Auditory signals have proven useful to guide and inform drivers in dangerous situations. Sounds can become annoying, however, thereby negatively affecting consumer acceptance of an interface or system. Auditory warnings are typically salient sounds such as sudden beeps or repetitive tones. But adding sound to the environment is not necessarily the only way to aurally alert people to a change in the environment. The present study explored the usefulness of three alternative strategies to notify drivers in early stages of a threatening situation using sound: 1. panning the radio sound from the driver's position (equal sound level in both ears) to one side; 2. reducing the sound level of the radio; and 3. a mild auditory warning signal (i.e., an added sound). The participants responded to the early warnings in a simple reaction task while performing a simulated driving task. After each condition, the drivers completed a questionnaire concerning their opinions of the early warnings. Interestingly, the results show that manipulating the sound of the radio can be a useful way to notify drivers. Panning the sound of the radio may be especially effective and tolerable. Potential benefits and issues with the investigated warning strategies are discussed.","2012-10-17","2023-07-06 05:58:37","2023-07-06 05:58:37","2023-07-05","85–91","","","","","","Graded auditory warnings during in-vehicle use","AutomotiveUI '12","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/QDUSJEKG/Fagerlönn et al. - 2012 - Graded auditory warnings during in-vehicle use us.pdf","","","notification systems; alerts; auditory warnings; driver acceptance; graded warnings; traffic safety","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ISMZVIDJ","conferencePaper","1971","Smith, David Canfield; Newey, Malcolm C.; Colby, Kenneth Mark","Automated therapy for nonspeaking autistic children","Proceedings of the May 16-18, 1972, spring joint computer conference","978-1-4503-7909-0","","10.1145/1478873.1479020","https://dl.acm.org/doi/10.1145/1478873.1479020","Earlier publications described our computer method for stimulating language development in nonspeaking children, sketched several case histories (Colby 1968), and gave statistical evidence that our high rate of success (71 percent) was due to our treatment method (Colby and Smith 1970). This paper presents some proposals for making the method more widely available.","1971-11-16","2023-07-06 05:58:37","2023-07-06 05:58:37","2023-07-05","1101–1106","","","","","","","AFIPS '72 (Spring)","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/I9KHBTWH/Smith et al. - 1971 - Automated therapy for nonspeaking autistic childre.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2SNXWJ39","conferencePaper","2010","Alvarez, Ignacio; Martin, Aqueasha; Dunbar, Jerone; Taiber, Joachim; Wilson, Dale-Marie; Gilbert, Juan E.","Voice interfaced vehicle user help","Proceedings of the 2nd International Conference on Automotive User Interfaces and Interactive Vehicular Applications","978-1-4503-0437-5","","10.1145/1969773.1969782","https://dl.acm.org/doi/10.1145/1969773.1969782","Manuals were designed to provide support and information about the usage and maintenance of the vehicle. In many cases user's manuals are the driver's only guidance. However, lack of clarity and efficiency of manuals lead to user dissatisfaction. In vehicles this problem is even more crucial given that driving a motor vehicle is, for many people, the most complex and potentially dangerous task they will perform during their lifetime. In this paper we present a voice interfaced driver manual that can potentially fix the deficiencies of its alternatives. In addition we aim to provide a case for the integration of such technology in a vehicle to reduce driver distraction, increase driver satisfaction, and manual usability, while also benefiting Original Equipment Manufacturers (OEMs) in lowering costs and reducing the documentation process.","2010-11-11","2023-07-06 05:58:37","2023-07-06 05:58:37","2023-07-05","42–49","","","","","","","AutomotiveUI '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/JV8L36FH/Alvarez et al. - 2010 - Voice interfaced vehicle user help.pdf","","","answers first; ITECH; vehicle user help; voice-interfaced manual","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V6H45XU3","conferencePaper","2015","Beattie, David; Baillie, Lynne; Halvey, Martin","A comparison of artificial driving sounds for automated vehicles","Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing","978-1-4503-3574-4","","10.1145/2750858.2807519","https://dl.acm.org/doi/10.1145/2750858.2807519","As automated vehicles currently do not provide sufficient feedback relating to the primary driving task, drivers have no assurance that an automated vehicle has understood and can cope with upcoming traffic situations [16]. To address this we conducted two user evaluations to investigate auditory displays in automated vehicles using different types of sound cues related to the primary driving sounds: acceleration, deceleration/braking, gear changing and indicating. Our first study compared earcons, speech and auditory icons with existing vehicle sounds. Our findings suggested that earcons were an effective alternative to existing vehicle sounds for presenting information related to the primary driving task. Based on these findings a second study was conducted to further investigate earcons modulated by different sonic parameters to present primary driving sounds. We discovered that earcons containing naturally mapped sonic parameters such as pitch and timbre were as effective as existing sounds in a simulated automated vehicle.","2015-09-07","2023-07-06 05:58:37","2023-07-06 05:58:37","2023-07-05","451–462","","","","","","","UbiComp '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/MAYRJPBV/Beattie et al. - 2015 - A comparison of artificial driving sounds for auto.pdf","","","auditory displays; speech; auditory icons; earcons; automated vehicles; driving simulator","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2C2FMEY8","conferencePaper","2016","Yu, Bin; Bongers, Nienke; van Asseldonk, Alissa; Hu, Jun; Funk, Mathias; Feijs, Loe","LivingSurface: Biofeedback through Shape-changing Display","Proceedings of the TEI '16: Tenth International Conference on Tangible, Embedded, and Embodied Interaction","978-1-4503-3582-9","","10.1145/2839462.2839469","https://dl.acm.org/doi/10.1145/2839462.2839469","In this paper we describe the concept, design and implementation of LivingSurface, an interactive wall-like surface as a shape-changing display of biofeedback. The surface changes its shape responding to an individual's physiological data, reflecting the internal bodily processes. The surface design basically consists of two layers: the pattern layer (front layer) and the actuating layer (back layer). The first is a complex paper-based structure with repetitive incisions created by laser cutting. The actuating layer serves as a medium transforming the force from servomotors, vibration motors or fans into an action on the pattern layer. The cutout patterns are stimulated to vibrate, swing, bulge, or rotate which is used to display physiological information in dynamic physical form. This work has been exhibited on Milan Design Week 2015; we collected and analyzed the feedback from the visitors during the exhibition and discuss the possibilities of the proposed surfaces as a shape-changing interface of biofeedback or an ambient display of information.","2016-02-14","2023-07-06 05:58:37","2023-07-06 05:58:37","2023-07-05","168–175","","","","","","LivingSurface","TEI '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/YEZHBN93/Yu et al. - 2016 - LivingSurface Biofeedback through Shape-changing .pdf","","","Biofeedback; interactive object; information visualization; physical display; shape-changing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2L522Z5F","conferencePaper","2010","Jo, Hyun; Martens, William L.; Park, Youngjin; Kim, Sunmin","Confirming the perception of virtual source elevation effects created using 5.1 channel surround sound playback","Proceedings of the 9th ACM SIGGRAPH Conference on Virtual-Reality Continuum and its Applications in Industry","978-1-4503-0459-7","","10.1145/1900179.1900200","https://dl.acm.org/doi/10.1145/1900179.1900200","Employing an array of nine speakers, five of which were at the listener's ear level, and four of which were elevated well above the listener's ear level, an experimental investigation of virtual sound source elevation was completed in each of three reproduction environments. The primary question of interest was that regarding whether the elevation of virtual sound sources could be modulated in a simple fashion using only the five ear-level speakers that form a conventional 5.1 channel surround-sound speaker layout [ITU-R. BS. 775-1. 1994]. It was found that the creation of elevated virtual sources was possible using the two surround-channel speakers or using all five ear-level speakers, and the resulting elevated virtual source imagery was compared with the source imagery associated with sound reproduced via speakers that were actually well elevated above the listener's ear level. Besides the extremely dry listening conditions of the anechoic chamber, tests were completed in a reverberation chamber, and in a moderately dry audio surround production studio, typical of the controlled acoustics featured in many critical listening spaces. The fact that similar results were observed in each of three listening environments supports the conclusion that the observed results are not idiosyncratic to any particular environment, such as the anechoic chamber, but results will likely generalize across many environments.","2010-12-12","2023-07-06 05:58:37","2023-07-06 05:58:37","2023-07-05","103–110","","","","","","","VRCAI '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/9HL8PP5X/Jo et al. - 2010 - Confirming the perception of virtual source elevat.pdf","","","5.1 channel surround sound; elevation perception; sound localization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F3AYR97Q","conferencePaper","2007","Hoggan, Eve; Brewster, Stephen","Designing audio and tactile crossmodal icons for mobile devices","Proceedings of the 9th international conference on Multimodal interfaces","978-1-59593-817-6","","10.1145/1322192.1322222","https://dl.acm.org/doi/10.1145/1322192.1322222","This paper reports an experiment into the design of crossmodal icons which can provide an alternative form of output for mobile devices using audio and tactile modalities to communicate information. A complete set of crossmodal icons was created by encoding three dimensions of information in three crossmodal auditory/tactile parameters. Earcons were used for the audio and Tactons for the tactile crossmodal icons. The experiment investigated absolute identification of audio and tactile crossmodal icons when a user is trained in one modality and tested in the other (and given no training in the other modality) to see if knowledge could be transferred between modalities. We also compared performance when users were static and mobile to see any effects that mobility might have on recognition of the cues. The results showed that if participants were trained in sound with Earcons and then tested with the same messages presented via Tactons they could recognize 85% of messages when stationary and 76% when mobile. When trained with Tactons and tested with Earcons participants could accurately recognize 76.5% of messages when stationary and 71% of messages when mobile. These results suggest that participants can recognize and understand a message in a different modality very effectively. These results will aid designers of mobile displays in creating effective crossmodal cues which require minimal training for users and can provide alternative presentation modalities through which information may be presented if the context requires.","2007-11-12","2023-07-06 05:58:37","2023-07-06 05:58:37","2023-07-05","162–169","","","","","","","ICMI '07","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/4SZQX6JH/Hoggan and Brewster - 2007 - Designing audio and tactile crossmodal icons for m.pdf","","","multimodal interaction; earcons; crossmodal interaction; mobile interaction; tactons (tactile icons)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"85D6SJVA","conferencePaper","2016","Mirnig, Alexander G.; Perterer, Nicole; Meschtscherjakov, Alexander; Krischkowsky, Alina; Neureiter, Katja; Laminger, Arno; Tscheligi, Manfred","Enhancing Telephone Communication in the Vehicle Through Audio from the Headrest: A Comparison Study","Proceedings of the 8th International Conference on Automotive User Interfaces and Interactive Vehicular Applications","978-1-4503-4533-0","","10.1145/3003715.3005415","https://dl.acm.org/doi/10.1145/3003715.3005415","The distraction potential of communication systems in the automotive context necessitates hands-free and attention undemanding systems. Today's hands-free car kits are of increasingly high quality, since bad audio quality can negatively impact the overall communication quality. Most solutions use built-in speakers for output and a microphone near the driver (e.g. on the ceiling). Thereby, audio quality can suffer e.g. from the long distance between the speaker and the listener. In a recent study, we compared perceived voice quality and social presence of a prototype with speakers installed in the headrest of a vehicle, to a high-end on-board audio system in a communication situation between a person sitting in the driver's seat and a person outside the vehicle. We found that Personal Audio received generally better results while also introducing its own set of issues, e.g., causing spatial disorientation in communication situations, in which other individuals are present in the car.","2016-10-24","2023-07-06 05:58:37","2023-07-06 05:58:37","2023-07-05","59–66","","","","","","Enhancing Telephone Communication in the Vehicle Through Audio from the Headrest","Automotive'UI 16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/BSLMPS7V/Mirnig et al. - 2016 - Enhancing Telephone Communication in the Vehicle T.pdf","","","user study; social presence; Headrest speaker; in-car audio; perceived quality of experience","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LTDEX2VF","conferencePaper","2012","McGregor, Iain; Turner, Phil","Soundscapes and repertory grids: comparing listeners' and a designer's experiences","Proceedings of the 30th European Conference on Cognitive Ergonomics","978-1-4503-1786-3","","10.1145/2448136.2448164","https://dl.acm.org/doi/10.1145/2448136.2448164","This paper reports on establishing whether listeners have the same listening experience as the person who designed the sound. Surprisingly, there is little or no evidence as to whether what is designed to be heard is what is actually heard. The study reported here is a qualitative study into these two experiences. Research approach -- A repertory grid technique was adopted using listener and designer generated constructs. One designer and 20 listeners rated 25 elements within a surround sound recording created by a soundscape generative system. The listeners' modal response was compared to the designer's. Findings/Design -- The results suggest that it is perfectly feasible to compare designers and listeners experiences and to establish points of agreement and disagreement. Research limitations/Implications -- Only UK-based university students and staff participated in the study, which limited generalisation of the findings. Originality/Value -- Demonstrates an ontology of sound based on user experience rather than designer's whim. This approach is based upon long-term experiences and our conceptualisation of sound Take away message -- Comparing listeners' experiences could allow designers to be confident with their sound designs.","2012-08-28","2023-07-06 05:58:37","2023-07-06 05:58:37","2023-07-05","131–137","","","","","","Soundscapes and repertory grids","ECCE '12","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/SXSQNCXC/McGregor and Turner - 2012 - Soundscapes and repertory grids comparing listene.pdf","","","soundscape; designers; listeners; listening; repertory grid","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H26VVPCF","conferencePaper","2018","Grahn, Hilkka; Kujala, Tuomo","Visual Distraction Effects between In-Vehicle Tasks with a Smartphone and a Motorcycle Helmet-Mounted Head-Up Display","Proceedings of the 22nd International Academic Mindtrek Conference","978-1-4503-6589-5","","10.1145/3275116.3275134","https://dl.acm.org/doi/10.1145/3275116.3275134","Besides motorists, also motorcyclists need safer user interfaces to interact with useful applications on the road. In this paper, distraction effects of in-vehicle tasks conducted with a head-up display (HUD) for motorcyclists were compared to smartphone tasks with 24 participants in a driving simulator. Compared to the smartphone tasks, the head-up display tasks decreased the percentage of inappropriately long glances by 45 percent. The head-up display tasks were also experienced as less demanding than the smartphone tasks. Additionally, the use of head-up display for navigation did not lead to gaze concentration effects compared to baseline driving. The head-up display is concluded to be a safer option for the tested tasks for motorcyclists than a smartphone. Based on earlier research, we assume that the use of peripheral vision allowed drivers to better maintain situational awareness during the head-up display tasks compared to the head-down smartphone tasks. In addition, the easy-to-learn haptic design of the head-up display handlebar controller could be used without vision.","2018-10-10","2023-07-06 05:58:37","2023-07-06 05:58:37","2023-07-05","153–162","","","","","","","Mindtrek '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/SNZ698ED/Grahn and Kujala - 2018 - Visual Distraction Effects between In-Vehicle Task.pdf","","","head-down display; head-up display; Driver distraction; head-mounted display; occlusion distance; visual demand; visual occlusion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GPA2YIRS","conferencePaper","2013","Langlotz, Tobias; Regenbrecht, Holger; Zollmann, Stefanie; Schmalstieg, Dieter","Audio stickies: visually-guided spatial audio annotations on a mobile augmented reality platform","Proceedings of the 25th Australian Computer-Human Interaction Conference: Augmentation, Application, Innovation, Collaboration","978-1-4503-2525-7","","10.1145/2541016.2541022","https://dl.acm.org/doi/10.1145/2541016.2541022","This paper describes spatially aligned user-generated audio annotations and the integration with visual augmentations into a single mobile AR system. Details of our prototype system are presented, along with an explorative usability study and technical evaluation of the design. Mobile Augmented Reality applications allow for visual augmentations as well as tagging and annotation of the surrounding environment. Texts and graphics are currently the media of choice for these applications with GPS coordinates used to determine spatial location. Our research demonstrates that the use of visually guided audio annotations that are positioned and orientated in augmented outdoor space successfully provides for additional, novel, and enhanced mobile user experience.","2013-11-25","2023-07-06 05:58:37","2023-07-06 05:58:37","2023-07-05","545–554","","","","","","Audio stickies","OzCHI '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/CBKKSQTZ/Langlotz et al. - 2013 - Audio stickies visually-guided spatial audio anno.pdf","","","augmented reality; spatial audio; mobile phone","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5SMPI9MS","conferencePaper","2011","Karuei, Idin; MacLean, Karon E.; Foley-Fisher, Zoltan; MacKenzie, Russell; Koch, Sebastian; El-Zohairy, Mohamed","Detecting vibrations across the body in mobile contexts","Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","978-1-4503-0228-9","","10.1145/1978942.1979426","https://dl.acm.org/doi/10.1145/1978942.1979426","In this paper we explore the potential and limitations of vibrotactile displays in practical wearable applications, by comparing users' detection rate and response time to stimuli applied across the body in varied conditions. We examined which body locations are more sensitive to vibrations and more affected by movement; whether visual workload, expectation of location, or gender impact performance; and if users have subjective preferences to any of these conditions. In two experiments we compared these factors using five vibration intensities on up to 13 body locations. Our contributions are comparisons of tactile detection performance under conditions typifying mobile use, an experiment design that supports further investigation in vibrotactile communication, and guidelines for optimal display location given intended use.","2011-05-07","2023-07-06 05:58:37","2023-07-06 05:58:37","2023-07-05","3267–3276","","","","","","","CHI '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/I5VW7H7C/Karuei et al. - 2011 - Detecting vibrations across the body in mobile con.pdf","","","mobile applications; vibrotactile display; wearable haptics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GXVP5TK4","conferencePaper","2012","Bakker, Saskia; van den Hoven, Elise; Eggen, Berry; Overbeeke, Kees","Exploring peripheral interaction design for primary school teachers","Proceedings of the Sixth International Conference on Tangible, Embedded and Embodied Interaction","978-1-4503-1174-8","","10.1145/2148131.2148184","https://dl.acm.org/doi/10.1145/2148131.2148184","This paper explores the concept of peripheral interactions; interactions with technology that take place in the background or periphery of the attention. We present two designs for a classroom setting. CawClock makes selected time frames audible in order to provide teachers with awareness of time. NoteLet is designed to support the teacher in observing children's behavior, by enabling him or her to take pictures of the classroom through straightforward interactions on a bracelet. A qualitative, two-week exploration of both systems in a classroom revealed that the soundscapes of CawClock indeed shifted to the periphery of the attention and supported the teacher's time awareness. The actions with NoteLet did not shift to the periphery. However, the tangible aspects of NoteLet seemed to facilitate the interaction to be quick and simple, which may indicate that it could shift to the periphery with more practice. Tangible interaction therefore seems a promising interaction style for this purpose.","2012-02-19","2023-07-06 05:58:37","2023-07-06 05:58:37","2023-07-05","245–252","","","","","","","TEI '12","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/4I4VTHFA/Bakker et al. - 2012 - Exploring peripheral interaction design for primar.pdf","","","design; tangible interaction; audio; attention; calm technology; peripheral interaction; awareness; periphery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NQWCLTMQ","conferencePaper","2018","Shim, Youngbo Aram; Lee, Jaeyeon; Lee, Geehyuk","Exploring Multimodal Watch-back Tactile Display using Wind and Vibration","Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","978-1-4503-5620-6","","10.1145/3173574.3173706","https://dl.acm.org/doi/10.1145/3173574.3173706","A tactile display on the back of a smartwatch is an attractive output option; however, its channel capacity is limited owing to the small contact area. In order to expand the channel capacity, we considered using two perceptually distinct types of stimuli, wind and vibration, together on the same skin area. The result is a multimodal tactile display that combines wind and vibration to create ""colored"" tactile sensations on the wrist. As a first step toward this goal, we conducted in this study four user experiments with a wind-vibration tactile display to examine different ways of combining wind and vibration: Individual, Sequential, and Simultaneous. The results revealed the sequential combination of wind and vibration to exhibit the highest potential, with an information transfer capacity of 3.29 bits. In particular, the transition of tactile modality was perceived at an accuracy of 98.52%. The current results confirm the feasibility and potential of a multimodal tactile display combining wind and vibration.","2018-04-19","2023-07-06 05:58:37","2023-07-06 05:58:37","2023-07-05","1–12","","","","","","","CHI '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/DENQVVS9/Shim et al. - 2018 - Exploring Multimodal Watch-back Tactile Display us.pdf","","","vibrotactile display; airflow display; multimodal tactile display; watch-back tactile display; wearable tactile display","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VR9N3H6H","conferencePaper","2009","Crossan, Andrew; McGill, Mark; Brewster, Stephen; Murray-Smith, Roderick","Head tilting for interaction in mobile contexts","Proceedings of the 11th International Conference on Human-Computer Interaction with Mobile Devices and Services","978-1-60558-281-8","","10.1145/1613858.1613866","https://dl.acm.org/doi/10.1145/1613858.1613866","Developing interfaces for mobile situations requires that devices are useable on the move. Here, we explore head tilting as an input technique to allow a user to interact with a mobile device 'hands free'. A Fitts' Law style evaluation is described where a user acquires targets, moving the cursor by head tilt. We explore d position and velocity control cursor mechanisms in both static and mobile situations to see which provided the best level of performance. Results show that participants could successfully acquire targets using head tilting. Position control was shown to be si gnificantly faster and more accurate in a static context, but exhi bited significantly poorer accuracy and longer target acquisition times when the user was on the move. We further demonstrate how analysis of user's gait shows consistent targeting biases at different stages in the gait cycle.","2009-09-15","2023-07-06 05:58:37","2023-07-06 05:58:37","2023-07-05","1–10","","","","","","","MobileHCI '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/U66YX5DP/Crossan et al. - 2009 - Head tilting for interaction in mobile contexts.pdf","","","mobile; accelerometer; Fitts' law; hands-free interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TUKJDEAB","conferencePaper","2019","Lee, Yi-Chen; Cherng, Fu-Yin; King, Jung-Tai; Lin, Wen-Chieh","To Repeat or Not to Repeat? Redesigning Repeating Auditory Alarms Based on EEG Analysis","Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","978-1-4503-5970-2","","10.1145/3290605.3300743","https://dl.acm.org/doi/10.1145/3290605.3300743","Auditory alarms that repeatedly interrupt users until they react are common, especially in the context of alarms. However, when an alarm repeats, our brains habituate to it and perceive it less and less, with reductions in both perception and attention-shifting: a phenomenon known as the repetition-suppression effect (RS). To retain users' perception and attention, this paper proposes and tests the use of pitch- and intensity-modulated alarms. Its experimental findings suggest that the proposed modulated alarms can reduce RS, albeit in different patterns, depending on whether pitch or intensity is the focus of the modulation. Specifically, pitch-modulated alarms were found to reduce RS more when the number of repetitions was small, while intensity-modulated alarms reduced it more as the number of repetitions increased. Based on these results, we make several recommendations for the design of improved repeating alarms, based on which modulation approach should be adopted in various situations.","2019-05-02","2023-07-06 05:58:37","2023-07-06 05:58:37","2023-07-05","1–10","","","","","","To Repeat or Not to Repeat?","CHI '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/MU4N6JWH/Lee et al. - 2019 - To Repeat or Not to Repeat Redesigning Repeating .pdf","","","brain-computer interface; neuroergonomics; auditory alarms","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ECCYLASS","conferencePaper","2009","Garzonis, Stavros; Jones, Simon; Jay, Tim; O'Neill, Eamonn","Auditory icon and earcon mobile service notifications: intuitiveness, learnability, memorability and preference","Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","978-1-60558-246-7","","10.1145/1518701.1518932","https://dl.acm.org/doi/10.1145/1518701.1518932","With an ever increasing number of mobile services, meaningful audio notifications could effectively inform users of the incoming services while minimising undesired and intrusive interruptions. Therefore, careful design of mobile service notification is needed. In this paper we evaluate two types of audio (auditory icons and earcons) as mobile service notifications, by comparing them on 4 measures: intuitiveness, learnability, memorability and user preference. A 4-stage longitudinal evaluation involving two lab experiments, a field study and a web-based experiment indicated that auditory icons performed significantly better in all measures. Implications for mobile audio notification design are presented.","2009-04-04","2023-07-06 05:58:37","2023-07-06 05:58:37","2023-07-05","1513–1522","","","","","","Auditory icon and earcon mobile service notifications","CHI '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/9TJKN2M7/Garzonis et al. - 2009 - Auditory icon and earcon mobile service notificati.pdf","","","auditory icons; earcons; mobile audio notifications; intuitiveness; learnability; memorability; mobile services","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FR5NHFK8","conferencePaper","2021","Jung, Jingun; Son, Sunmin; Lee, Sangyoon; Kim, Yeonsu; Lee, Geehyuk","ThroughHand: 2D Tactile Interaction to Simultaneously Recognize and Touch Multiple Objects","Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems","978-1-4503-8096-6","","10.1145/3411764.3445530","https://dl.acm.org/doi/10.1145/3411764.3445530","Users with visual impairments find it difficult to enjoy real-time 2D interactive applications on the touchscreen. Touchscreen applications such as sports games often require simultaneous recognition of and interaction with multiple moving targets through vision. To mitigate this issue, we propose ThroughHand, a novel tactile interaction that enables users with visual impairments to interact with multiple dynamic objects in real time. We designed the ThroughHand interaction to utilize the potential of the human tactile sense that spatially registers both sides of the hand with respect to each other. ThroughHand allows interaction with multiple objects by enabling users to perceive the objects using the palm while providing a touch input space on the back of the same hand. A user study verified that ThroughHand enables users to locate stimuli on the palm with a margin of error of approximately 13 mm and effectively provides a real-time 2D interaction experience for users with visual impairments.","2021-05-07","2023-07-06 05:58:37","2023-07-06 05:58:37","2023-07-05","1–13","","","","","","ThroughHand","CHI '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/9PQ4TDMK/Jung et al. - 2021 - ThroughHand 2D Tactile Interaction to Simultaneou.pdf","","","Games; Haptics; Accessibility; Real-time interaction; Shape-changing display; Visual impairments","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9F879HMM","conferencePaper","2019","Rogers, Katja; Funke, Jana; Frommel, Julian; Stamm, Sven; Weber, Michael","Exploring Interaction Fidelity in Virtual Reality: Object Manipulation and Whole-Body Movements","Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","978-1-4503-5970-2","","10.1145/3290605.3300644","https://dl.acm.org/doi/10.1145/3290605.3300644","High degrees of interaction fidelity (IF) in virtual reality (VR) are said to improve user experience and immersion, but there is also evidence of low IF providing comparable experiences. VR games are now increasingly prevalent, yet we still do not fully understand the trade-off between realism and abstraction in this context. We conducted a lab study comparing high and low IF for object manipulation tasks in a VR game. In a second study, we investigated players' experiences of IF for whole-body movements in a VR game that allowed players to crawl underneath virtual boulders and ""dangle'' along monkey bars. Our findings show that high IF is preferred for object manipulation, but for whole-body movements, moderate IF can suffice, as there is a trade-off with usability and social factors. We provide guidelines for the development of VR games based on our results.","2019-05-02","2023-07-06 05:58:37","2023-07-06 05:58:37","2023-07-05","1–14","","","","","","Exploring Interaction Fidelity in Virtual Reality","CHI '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/ADB9ADAR/Rogers et al. - 2019 - Exploring Interaction Fidelity in Virtual Reality.pdf","","","virtual reality; games; interaction fidelity; player experience; virtual objects; whole body interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y6SZXAN8","conferencePaper","2019","Wedoff, Ryan; Ball, Lindsay; Wang, Amelia; Khoo, Yi Xuan; Lieberman, Lauren; Rector, Kyle","Virtual Showdown: An Accessible Virtual Reality Game with Scaffolds for Youth with Visual Impairments","Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","978-1-4503-5970-2","","10.1145/3290605.3300371","https://dl.acm.org/doi/10.1145/3290605.3300371","Virtual Reality (VR) is a growing source of entertainment, but people who are visually impaired have not been effectively included. Audio cues are motivated as a complement to visuals, making experiences more immersive, but are not a primary cue. To address this, we implemented a VR game called Virtual Showdown. We based Virtual Showdown on an accessible real-world game called Showdown, where people use their hearing to locate and hit a ball against an opponent. Further, we developed Verbal and Verbal/Vibration Scaffolds to teach people how to play Virtual Showdown. We assessed the acceptability of Virtual Showdown and compared our scaffolds in an empirical study with 34 youth who are visually impaired. Thirty-three participants wanted to play Virtual Showdown again, and we learned that participants scored higher with the Verbal Scaffold or if they had prior Showdown experience. Our empirical findings inform the design of future accessible VR experiences.","2019-05-02","2023-07-06 05:58:37","2023-07-06 05:58:37","2023-07-05","1–15","","","","","","Virtual Showdown","CHI '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/XLARJ9I3/Wedoff et al. - 2019 - Virtual Showdown An Accessible Virtual Reality Ga.pdf","","","blind; virtual reality; low vision; spatial audio; haptics; youth","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ACLBAYUL","conferencePaper","2021","Gonçalves, David; Rodrigues, André; Richardson, Mike L.; de Sousa, Alexandra A.; Proulx, Michael J.; Guerreiro, Tiago","Exploring Asymmetric Roles in Mixed-Ability Gaming","Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems","978-1-4503-8096-6","","10.1145/3411764.3445494","https://dl.acm.org/doi/10.1145/3411764.3445494","The landscape of digital games is segregated by player ability. For example, sighted players have a multitude of highly visual games at their disposal, while blind players may choose from a variety of audio games. Attempts at improving cross-ability access to any of those are often limited in the experience they provide, or disregard multiplayer experiences. We explore ability-based asymmetric roles as a design approach to create engaging and challenging mixed-ability play. Our team designed and developed two collaborative testbed games exploring asymmetric interdependent roles. In a remote study with 13 mixed-visual-ability pairs we assessed how roles affected perceptions of engagement, competence, and autonomy, using a mixed-methods approach. The games provided an engaging and challenging experience, in which differences in visual ability were not limiting. Our results underline how experiences unequal by design can give rise to an equitable joint experience.","2021-05-07","2023-07-06 05:58:37","2023-07-06 05:58:37","2023-07-05","1–14","","","","","","","CHI '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/P7R8BQC5/Gonçalves et al. - 2021 - Exploring Asymmetric Roles in Mixed-Ability Gaming.pdf","","","visual impairment; inclusion; game accessibility; mixed-ability; social gaming","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8EFV4SU2","conferencePaper","2020","Gonçalves, David; Rodrigues, André; Guerreiro, Tiago","Playing With Others: Depicting Multiplayer Gaming Experiences of People With Visual Impairments","Proceedings of the 22nd International ACM SIGACCESS Conference on Computers and Accessibility","978-1-4503-7103-2","","10.1145/3373625.3418304","https://dl.acm.org/doi/10.1145/3373625.3418304","Games bring people together in immersive and challenging interactions. In this paper, we share multiplayer gaming experiences of people with visual impairments collected from interviews with 10 adults and 10 minors, and 140 responses to an online survey. We include the perspectives of 17 sighted people who play with someone who has a visual impairment, collected in a second online survey. Our focus is on group play, particularly on the problems and opportunities that arise from mixed-visual-ability scenarios. These show that people with visual impairments are playing diverse games, but face limitations in playing with others who have different visual abilities. What stands out is the lack of intersection in gaming opportunities, and consequently, in habits and interests of people with different visual abilities. We highlight barriers associated with these experiences beyond inaccessibility issues and discuss implications and opportunities for the design of mixed-ability gaming.","2020-10-29","2023-07-06 05:58:37","2023-07-06 05:58:37","2023-07-05","1–12","","","","","","Playing With Others","ASSETS '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/3JUA4GLK/Gonçalves et al. - 2020 - Playing With Others Depicting Multiplayer Gaming .pdf","","","visual impairment; inclusion; game accessibility; mixed-ability; social gaming","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4QGGUGTJ","conferencePaper","2010","Dingler, Tilman; Brewster, Stephen","AudioFeeds: a mobile auditory application for monitoring online activities","Proceedings of the 18th ACM international conference on Multimedia","978-1-60558-933-6","","10.1145/1873951.1874151","https://dl.acm.org/doi/10.1145/1873951.1874151","User participation has transformed the way news travel the globe. With the rise of the 'Web 2.0' phenomenon users have been empowered with the means of creating and distributing informational items, which we call social feeds. Platforms like Twitter and Facebook provide a variety of tools to facilitate real-time communication among people. But social sites are not limited to personal chat; they also provide an effective means for organizing large groups of people in response to catastrophic disasters. Monitoring these feeds can provide time-critical information, but can easily lead to information overload due to the large amount of data being shared. In this paper we introduce a mobile auditory display application called AudioFeeds that allows users to maintain an overview of activities in different social feeds. AudioFeeds runs on a mobile device and enables users to get an overview of their social networks and spot peaks in activity by sonifying social feeds and creating a spatialised soundscape around the user's head. We conducted a user study looking into different aspects of activity monitoring. Results show that our application provides an effective way for monitoring overall activity levels and allows users to identify activity peaks with 86.1% accuracy even when mobile.","2010-10-25","2023-07-06 06:06:10","2023-07-06 06:06:10","2023-07-05","1067–1070","","","","","","AudioFeeds","MM '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/LK8JQICW/Dingler and Brewster - 2010 - AudioFeeds a mobile auditory application for moni.pdf","","","auditory display; social media; mobile application","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""