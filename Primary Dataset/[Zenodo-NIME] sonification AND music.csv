"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"S6LUD74W","journalArticle","2010","Park, Sihwa; Kim, Seunghun; Lee, Samuel; Yeo, Woon Seung","Online Map Interface for Creative and Interactive","","","","10.5281/zenodo.1177877","https://www.zenodo.org/record/1177877","In this paper, we discuss the musical potential of COMPath - an online map based music-making tool - as a noveland unique interface for interactive music composition andperformance. COMPath provides an intuitive environmentfor creative music making by sonification of georeferenceddata. Users can generate musical events with simple andfamiliar actions on an online map interface; a set of local information is collected along the user-drawn route andthen interpreted as sounds of various musical instruments.We discuss the musical interpretation of routes on a map,review the design and implementation of COMPath, andpresent selected sonification results with focus on mappingstrategies for map-based composition.","2010-06-01","2023-07-05 05:30:50","2023-07-05 05:30:50","2023-07-05 05:30:50","","","","","","","","","","","","","","en","","","","","www.zenodo.org","","","","/Users/minsik/Zotero/storage/T277GZPR/Park et al. - 2010 - Online Map Interface for Creative and Interactive.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XTYQ497T","journalArticle","2007","le Groux, Sylvain; Manzolli, Jonatas; Verschure, Paul F.","VR-RoBoser : Real-Time Adaptive Sonification of Virtual Environments Based on Avatar Behavior","","","","10.5281/zenodo.1177101","https://www.zenodo.org/record/1177101","Until recently, the sonification of Virtual Environments had often been reduced to its simplest expression. Too often soundscapes and background music are predetermined, repetitive and somewhat predictable. Yet, there is room for more complex and interesting sonification schemes that can improve the sensation of presence in a Virtual Environment. In this paper we propose a system that automatically generates original background music in real-time called VR-RoBoser. As a test case we present the application of VR-RoBoser to a dynamic avatar that explores its environment. We show that the musical events are directly and continuously generated and influenced by the behavior of the avatar in three-dimensional virtual space, generating a context dependent sonification.","2007-06-01","2023-07-05 05:32:29","2023-07-05 05:32:29","2023-07-05 05:32:29","","","","","","","VR-RoBoser","","","","","","","en","","","","","www.zenodo.org","","","","/Users/minsik/Zotero/storage/CLD2GVWJ/le Groux et al. - 2007 - VR-RoBoser  Real-Time Adaptive Sonification of Vi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IDFN8ABK","journalArticle","2011","Leslie, Grace; Mullen, Tim","MoodMixer : {EEG}-based Collaborative Sonification","","","","10.5281/zenodo.1178089","https://www.zenodo.org/record/1178089","MoodMixer is an interactive installation in which participants collaboratively navigate a two-dimensional music spaceby manipulating their cognitive state and conveying thisstate via wearable Electroencephalography (EEG) technology. The participants can choose to actively manipulateor passively convey their cognitive state depending on theirdesired approach and experience level. A four-channel electronic music mixture continuously conveys the participants'expressed cognitive states while a colored visualization oftheir locations on a two-dimensional projection of cognitive state attributes aids their navigation through the space.MoodMixer is a collaborative experience that incorporatesaspects of both passive and active EEG sonification andperformance art. We discuss the technical design of the installation and place its collaborative sonification aestheticdesign within the context of existing EEG-based music andart.","2011-06-01","2023-07-05 05:32:49","2023-07-05 05:32:49","2023-07-05 05:32:49","","","","","","","MoodMixer","","","","","","","en","","","","","www.zenodo.org","","","","/Users/minsik/Zotero/storage/E8HEEUZA/Leslie and Mullen - 2011 - MoodMixer  {EEG}-based Collaborative Sonification.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TFMB23Z5","journalArticle","2005","Bowen, Adam","Soundstone: A {3-D} Wireless Music Controller","","","","10.5281/zenodo.1176711","https://www.zenodo.org/record/1176711","Soundstone is a small wireless music controller that tracks movement and gestures, and maps these signals to characteristics of various synthesized and sampled sounds. It is intended to become a general-purpose platform for exploring the sonification of movement, with an emphasis on tactile (haptic) feedback.","2005-06-01","2023-07-05 05:33:03","2023-07-05 05:33:03","2023-07-05 05:33:03","","","","","","","Soundstone","","","","","","","en","","","","","www.zenodo.org","","","","/Users/minsik/Zotero/storage/CRT6UYH5/Bowen - 2005 - Soundstone A {3-D} Wireless Music Controller.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4PLIBW7A","journalArticle","2006","Beilharz, Kirsty; Jakovich, Joanne; Ferguson, Sam","Hyper-shaku (Border-crossing): Towards the Multi-modal Gesture-controlled Hyper-Instrument","","","","10.5281/zenodo.1176867","https://www.zenodo.org/record/1176867","Hyper-shaku (Border-Crossing) is an interactive sensor environment that uses motion sensors to trigger immediate responses and generative processes augmenting the Japanese bamboo shakuhachi in both the auditory and visual domain. The latter differentiates this process from many hyper-instruments by building a performance of visual design as well as electronic music on top of the acoustic performance. It utilizes a combination of computer vision and wireless sensing technologies conflated from preceding works. This paper outlines the use of gesture in these preparatory sound and audio-visual performative, installation and sonification works, leading to a description of the Hyper-shaku environment integrating sonification and generative elements.","2006-06-01","2023-07-05 05:33:22","2023-07-05 05:33:22","2023-07-05 05:33:22","","","","","","","Hyper-shaku (Border-crossing)","","","","","","","en","","","","","www.zenodo.org","","","","/Users/minsik/Zotero/storage/LMZZTUD9/Beilharz et al. - 2006 - Hyper-shaku (Border-crossing) Towards the Multi-m.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NKSXJZ78","journalArticle","2015","Barrett, Natasha","Creating tangible spatial-musical images from physical performance gestures","","","","10.5281/zenodo.1179014","https://www.zenodo.org/record/1179014","Electroacoustic music has a longstanding relationship with gesture and space. This paper marks the start of a project investigating acousmatic spatial imagery, real gestural behaviour and ultimately the formation of tangible acousmatic images. These concepts are explored experimentally using motion tracking in a source-sound recording context, interactive parameter-mapping sonification in three-dimensional high-order ambisonics, composition and performance. The spatio-musical role of physical actions in relation to instrument excitation is used as a point of departure for embodying physical spatial gestures in the creative process. The work draws on how imagery for music is closely linked with imagery for music-related actions.","2015-06-01","2023-07-05 05:33:36","2023-07-05 05:33:36","2023-07-05 05:33:36","","","","","","","","","","","","","","en","","","","","www.zenodo.org","","","","/Users/minsik/Zotero/storage/JWT6LFP6/Barrett - 2015 - Creating tangible spatial-musical images from phys.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KMBI5B6Y","journalArticle","2008","Delle Monache, Stefano; Polotti, Pietro; Papetti, Stefano; Rocchesso, Davide","Sonically Augmented Found Objects","","","","10.5281/zenodo.1179519","https://www.zenodo.org/record/1179519","We present our work with augmented everyday objectstransformed into sound sources for music generation. The idea isto give voice to objects through technology. More specifically, theparadigm of the birth of musical instruments as a sonification ofobjects used in domestic or work everyday environments is hereconsidered and transposed into the technologically augmentedscenarios of our contemporary world.","2008-06-01","2023-07-05 05:33:51","2023-07-05 05:33:51","2023-07-05 05:33:51","","","","","","","","","","","","","","en","","","","","www.zenodo.org","","","","/Users/minsik/Zotero/storage/IH8LX5XN/Delle Monache et al. - 2008 - Sonically Augmented Found Objects.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SXETYW4D","journalArticle","2011","Mealla, Sebastián; Väaljamäae, Aleksander; Bosi, Mathieu; Jordà, Sergi","Listening to Your Brain : Implicit Interaction in Collaborative Music Performances","","","","10.5281/zenodo.1178107","https://www.zenodo.org/record/1178107","The use of physiological signals in Human Computer Interaction (HCI) is becoming popular and widespread, mostlydue to sensors miniaturization and advances in real-timeprocessing. However, most of the studies that use physiologybased interaction focus on single-user paradigms, and itsusage in collaborative scenarios is still in its beginning. Inthis paper we explore how interactive sonification of brainand heart signals, and its representation through physicalobjects (physiopucks) in a tabletop interface may enhancemotivational and controlling aspects of music collaboration.A multimodal system is presented, based on an electrophysiology sensor system and the Reactable, a musical tabletop interface. Performance and motivation variables wereassessed in an experiment involving a test ""Physio"" group(N=22) and a control ""Placebo"" group (N=10). Pairs ofparticipants used two methods for sound creation: implicitinteraction through physiological signals, and explicit interaction by means of gestural manipulation. The resultsshowed that pairs in the Physio Group declared less difficulty, higher confidence and more symmetric control thanthe Placebo Group, where no real-time sonification was provided as subjects were using pre-recorded physiological signal being unaware of it. These results support the feasibilityof introducing physiology-based interaction in multimodalinterfaces for collaborative music generation.","2011-06-01","2023-07-05 05:34:09","2023-07-05 05:34:09","2023-07-05 05:34:09","","","","","","","Listening to Your Brain","","","","","","","en","","","","","www.zenodo.org","","","","/Users/minsik/Zotero/storage/8YR6G7TK/Mealla et al. - 2011 - Listening to Your Brain  Implicit Interaction in .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZZ9JQ9WC","journalArticle","2019","Erdem, Cagri; Schia, Katja Henriksen; Jensenius, Alexander Refsum","Vrengt: A Shared Body-Machine Instrument for Music-Dance Performance","","","","10.5281/zenodo.3672918","https://www.zenodo.org/record/3672918","This paper describes the process of developing a shared instrument for music--dance performance, with a particular focus on exploring the boundaries between standstill vs motion, and silence vs sound. The piece Vrengt grew from the idea of enabling a true partnership between a musician and a dancer, developing an instrument that would allow for active co-performance. Using a participatory design approach, we worked with sonification as a tool for systematically exploring the dancer's bodily expressions. The exploration used a ""spatiotemporal matrix,"" with a particular focus on sonic microinteraction. In the final performance, two Myo armbands were used for capturing muscle activity of the arm and leg of the dancer, together with a wireless headset microphone capturing the sound of breathing. In the paper we reflect on multi-user instrument paradigms, discuss our approach to creating a shared instrument using sonification as a tool for the sound design, and reflect on the performers' subjective evaluation of the instrument.","2019-06-01","2023-07-05 05:34:28","2023-07-05 05:34:28","2023-07-05 05:34:28","","","","","","","Vrengt","","","","","","","en","","","","","www.zenodo.org","","","","/Users/minsik/Zotero/storage/WBWV6JR6/Erdem et al. - 2019 - Vrengt A Shared Body-Machine Instrument for Music.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H3TFXQ5Y","journalArticle","2013","Hamano, Takayuki; Rutkowski, Tomasz; Terasawa, Hiroko; Okanoya, Kazuo; Furukawa, Kiyoshi","Generating an Integrated Musical Expression with a Brain--Computer Interface","","","","10.5281/zenodo.1178542","https://www.zenodo.org/record/1178542","Electroencephalography (EEG) has been used to generate music for over 40 years,but the most recent developments in brain--computer interfaces (BCI) allowgreater control and more flexible expression for using new musical instrumentswith EEG. We developed a real-time musical performance system using BCItechnology and sonification techniques to generate imagined musical chords withorganically fluctuating timbre. We aim to emulate the expressivity oftraditional acoustic instruments. The BCI part of the system extracts patternsfrom the neural activity while a performer imagines a score of music. Thesonification part of the system captures non-stationary changes in the brainwaves and reflects them in the timbre by additive synthesis. In this paper, wediscuss the conceptual design, system development, and the performance of thisinstrument.","2013-06-01","2023-07-05 05:35:07","2023-07-05 05:35:07","2023-07-05 05:35:07","","","","","","","","","","","","","","en","","","","","www.zenodo.org","","","","/Users/minsik/Zotero/storage/B49FAXA9/Hamano et al. - 2013 - Generating an Integrated Musical Expression with a.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LUZMQAP2","journalArticle","2013","McGee, Ryan","VOSIS: a Multi-touch Image Sonification Interface","","","","10.5281/zenodo.1178604","https://www.zenodo.org/record/1178604","VOSIS is an interactive image sonification interface that creates complexwavetables by raster scanning greyscale image pixel data. Using a multi-touchscreen to play image regions of unique frequency content rather than a linearscale of frequencies, it becomes a unique performance tool for experimental andvisual music. A number of image filters controlled by multi-touch gestures addvariation to the sound palette. On a mobile device, parameters controlled bythe accelerometer add another layer expressivity to the resulting audio-visualmontages.","2013-06-01","2023-07-05 05:35:22","2023-07-05 05:35:22","2023-07-05 05:35:22","","","","","","","VOSIS","","","","","","","en","","","","","www.zenodo.org","","","","/Users/minsik/Zotero/storage/3AJL4RRC/McGee - 2013 - VOSIS a Multi-touch Image Sonification Interface.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PQG4RLR2","journalArticle","2011","Dahl, Luke; Herrera, Jorge; Wilkerson, Carr","TweetDreams : Making Music with the Audience and the World using Real-time Twitter Data","","","","10.5281/zenodo.1177991","https://www.zenodo.org/record/1177991","TweetDreams is an instrument and musical compositionwhich creates real-time sonification and visualization oftweets. Tweet data containing specified search terms is retrieved from Twitter and used to build networks of associated tweets. These networks govern the creation of melodiesassociated with each tweet and are displayed graphically.Audience members participate in the piece by tweeting,and their tweets are given special musical and visual prominence.","2011-06-01","2023-07-05 05:35:36","2023-07-05 05:35:36","2023-07-05 05:35:36","","","","","","","TweetDreams","","","","","","","en","","","","","www.zenodo.org","","","","/Users/minsik/Zotero/storage/DDVF29XW/Dahl et al. - 2011 - TweetDreams  Making Music with the Audience and t.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WD47GLGZ","journalArticle","2008","Hadjakos, Aristotelis; Aitenbichler, Erwin; Mühlhäuser, Max","The Elbow Piano : Sonification of Piano Playing Movements","","","","10.5281/zenodo.1179553","https://www.zenodo.org/record/1179553","The Elbow Piano distinguishes two types of piano touch: a touchwith movement in the elbow joint and a touch without. A playednote is first mapped to the left or right hand by visual tracking.Custom-built goniometers attached to the player's arms are usedto detect the type of touch. The two different types of touchesare sonified by different instrument sounds. This gives theplayer an increased awareness of his elbow movements, which isconsidered valuable for piano education. We have implementedthe system and evaluated it with a group of music students.","2008-06-01","2023-07-05 05:35:51","2023-07-05 05:35:51","2023-07-05 05:35:51","","","","","","","The Elbow Piano","","","","","","","en","","","","","www.zenodo.org","","","","/Users/minsik/Zotero/storage/5BI27H6Y/Hadjakos et al. - 2008 - The Elbow Piano  Sonification of Piano Playing Mo.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DUWMEDR5","journalArticle","2020","Vasilakos, Konstantinos n/a; Wilson, Scott; McCauley, Thomas; Yeung, Tsun Winston; Margetson, Emma; Khosravi Mardakheh, Milad","Sonification of High Energy Physics Data Using Live Coding and Web Based Interfaces.","","","","10.5281/zenodo.4813430","https://www.zenodo.org/record/4813430","This paper presents a discussion of Dark Matter, a sonification project using live coding and just-in-time programming techniques. The project uses data from proton-proton collisions produced by the Large Hadron Collider (LHC) at CERN, Switzerland, and then detected and reconstructed by the Compact Muon Solenoid (CMS) experiment, and was developed with the support of the art@CMS project. Work for the Dark Matter project included the development of a custom-made environment in the SuperCollider (SC) programming language that lets the performers of the group engage in collective improvisations using dynamic interventions and networked music systems. This paper will also provide information about a spin-off project entitled the Interactive Physics Sonification System (IPSOS), an interactive and standalone online application developed in the JavaScript programming language. It provides a web-based interface that allows users to map particle data to sound on commonly used web browsers, mobile devices, such as smartphones, tablets etc. The project was developed as an educational outreach tool to engage young students and the general public with data derived from LHC collisions.","2020-06-01","2023-07-05 05:36:07","2023-07-05 05:36:07","2023-07-05 05:36:07","","","","","","","","","","","","","","en","","","","","www.zenodo.org","","","","/Users/minsik/Zotero/storage/SRS7DXLP/Vasilakos et al. - 2020 - Sonification of High Energy Physics Data Using Liv.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7ZX98AJ6","journalArticle","2020","Gold, Nicolas E.; Wang, Chongyang; Olugbade, Temitayo; Berthouze, Nadia; Williams, Amanda","P(l)aying Attention: Multi-modal, multi-temporal music control","","","","10.5281/zenodo.4813303","https://www.zenodo.org/record/4813303","The expressive control of sound and music through body movements is well-studied. For some people, body movement is demanding, and although they would prefer to express themselves freely using gestural control, they are unable to use such interfaces without difficulty. In this paper, we present the P(l)aying Attention framework for manipulating recorded music to support these people, and to help the therapists that work with them. The aim is to facilitate body awareness, exploration, and expressivity by allowing the manipulation of a pre-recorded 'ensemble' through an interpretation of body movement, provided by a machine-learning system trained on physiotherapist assessments and movement data from people with chronic pain. The system considers the nature of a person's movement (e.g. protective) and offers an interpretation in terms of the joint-groups that are playing a major role in the determination at that point in the movement, and to which attention should perhaps be given (or the opposite at the user's discretion). Using music to convey the interpretation offers informational (through movement sonification) and creative (through manipulating the ensemble by movement) possibilities. The approach offers the opportunity to explore movement and music at multiple timescales and under varying musical aesthetics.","2020-06-01","2023-07-05 05:36:23","2023-07-05 05:36:23","2023-07-05 05:36:23","","","","","","","P(l)aying Attention","","","","","","","en","","","","","www.zenodo.org","","","","/Users/minsik/Zotero/storage/GY3Y8VVM/Gold et al. - 2020 - P(l)aying Attention Multi-modal, multi-temporal m.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LTY26Z4K","journalArticle","2010","Bryan-Kinns, Nick; Fencott, Robin; Metatla, Oussama; Nabavian, Shahin; Sheridan, Jennifer G.","Interactional Sound and Music : Listening to CSCW, Sonification, and Sound Art","","","","10.5281/zenodo.1177727","https://www.zenodo.org/record/1177727","In this paper we outline the emerging field of Interactional Sound and Music which concerns itself with multi-person technologically mediated interactions primarily using audio. We present several examples of interactive systems in our group, and reflect on how they were designed and evaluated. Evaluation techniques for collective, performative, and task oriented activities are outlined and compared. We emphasise the importance of designing for awareness in these systems, and provide examples of different awareness mechanisms.","2010-06-01","2023-07-05 05:36:41","2023-07-05 05:36:41","2023-07-05 05:36:41","","","","","","","Interactional Sound and Music","","","","","","","en","","","","","www.zenodo.org","","","","/Users/minsik/Zotero/storage/YGEQUS3U/Bryan-Kinns et al. - 2010 - Interactional Sound and Music  Listening to CSCW,.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VGAIWX8Z","journalArticle","2019","Bazoge, Nicolas; Gaugne, Ronan; Nouviale, Florian; Gouranton, Valerie; Bossis, Bruno","Expressive potentials of motion capture in musical performance","","","","10.5281/zenodo.3672954","https://www.zenodo.org/record/3672954","The paper presents the electronic music performance project Vis Insita implementing the design of experimental instrumental interfaces based on optical motion capture technology with passive infrared markers (MoCap), and the analysis of their use in a real scenic presentation context. Because of MoCap's predisposition to capture the movements of the body, a lot of research and musical applications in the performing arts concern dance or the sonification of gesture. For our research, we wanted to move away from the capture of the human body to analyse the possibilities of a kinetic object handled by a performer, both in terms of musical expression, but also in the broader context of a multimodal scenic interpretation.","2019-06-01","2023-07-05 05:36:54","2023-07-05 05:36:54","2023-07-05 05:36:54","","","","","","","","","","","","","","en","","","","","www.zenodo.org","","","","/Users/minsik/Zotero/storage/9NYH9SR4/Bazoge et al. - 2019 - Expressive potentials of motion capture in musical.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E3EFTAM2","journalArticle","2020","Olsen, Taylor J.","Animation, Sonification, and Fluid-Time: A Visual-Audioizer Prototype","","","","10.5281/zenodo.4813230","https://www.zenodo.org/record/4813230","The visual-audioizer is a patch created in Max in which the concept of fluid-time animation techniques, in tandem with basic computer vision tracking methods, can be used as a tool to allow the visual time-based media artist to create music. Visual aspects relating to the animator's knowledge of motion, animated loops, and auditory synchronization derived from computer vision tracking methods, allow an immediate connection between the generated audio derived from visuals—becoming a new way to experience and create audio-visual media. A conceptual overview, comparisons of past/current audio-visual contributors, and a summary of the Max patch will be discussed. The novelty of practice-based animation methods in the field of musical expression, considerations of utilizing the visual-audioizer, and the future of fluid-time animation techniques as a tool of musical creativity will also be addressed.","2020-06-01","2023-07-05 05:37:06","2023-07-05 05:37:06","2023-07-05 05:37:06","","","","","","","Animation, Sonification, and Fluid-Time","","","","","","","en","","","","","www.zenodo.org","","","","/Users/minsik/Zotero/storage/QJY4PI8C/Olsen - 2020 - Animation, Sonification, and Fluid-Time A Visual-.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7BQ2VP6U","journalArticle","2014","Hutchins, Charles; Ballweg, Holger; Knotts, Shelly; Hummel, Jonas; Roberts, Antonio","Soundbeam: A Platform for Sonyfing Web Tracking","","","","10.5281/zenodo.1178810","https://www.zenodo.org/record/1178810","Government spying on internet traffic has seemingly become ubiquitous. Not to be left out, the private sector tracks our online footprint via our ISP or with a little help from facebook. Web services, such as advertisement servers and Google track our progress as we surf the net and click on links. The Mozilla plugin, Lightbeam (formerly Collusion), shows the user a visual map of every site a surfer sends data to. A interconnected web of advertisers and other (otherwise) invisible data-gatherers quickly builds during normal usage. We propose modifying this plugin so that as the graph builds, its state is broadcast visa OSC. Members of BiLE will receive and interpret those OSC messages in SuperCollider and PD. We will act as a translational object in a process of live-sonification. The collected data is the material with which we will develop a set of music tracks based on patterns we may discover. The findings of our data collection and the developed music will be presented in the form of an audiovisual live performance. Snippets of collected text and URLs will both form the basis of our audio interpretation, but also be projected on to a screen, so an audience can voyeuristically experience the actions taken on their behalf by governments and advertisers. After the concert, all of the scripts and documentation related to the data collection and sharing in the piece will be posted to github under a GPL license.","2014-06-01","2023-07-05 05:37:19","2023-07-05 05:37:19","2023-07-05 05:37:19","","","","","","","Soundbeam","","","","","","","en","","","","","www.zenodo.org","","","","/Users/minsik/Zotero/storage/LNZXB9E8/Hutchins et al. - 2014 - Soundbeam A Platform for Sonyfing Web Tracking.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4JK2MFZU","journalArticle","2014","Renaud, Alain; Charbonnier, Caecilia; Chagué, Sylvain","{3D}inMotion A Mocap Based Interface for Real Time Visualisation and Sonification of Multi-User Interactions","","","","10.5281/zenodo.1178915","https://www.zenodo.org/record/1178915","This paper provides an overview of a proposed demonstration of 3DinMotion, a system using real time motion capture of one or several subjects, which can be used in interactive audiovisual pieces and network performances. The skeleton of a subject is analyzed in real time and displayed as an abstract avatar as well as sonified based on mappings and rules to make the interplay experience lively and rewarding. A series of musical pieces have been composed for the interface following cueing strategies. In addition a second display, ``the prompter"" guides the users through the piece. 3DinMotion has been developed from scratch and natively, leading to a system with a very low latency, making it suitable for real time music interactions. In addition, 3DinMotion is fully compatible with the OpenSoundControl (OSC) protocol, allowing expansion to commonly used musical and sound design applications.","2014-06-01","2023-07-05 05:38:38","2023-07-05 05:38:38","2023-07-05 05:38:38","","","","","","","","","","","","","","en","","","","","www.zenodo.org","","","","/Users/minsik/Zotero/storage/FW62GIKD/Renaud et al. - 2014 - {3D}inMotion A Mocap Based Interface for Real Time.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""