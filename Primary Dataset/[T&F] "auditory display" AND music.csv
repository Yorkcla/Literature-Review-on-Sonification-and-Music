"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"8PQVSENG","journalArticle","2018","Schito, Joram; Fabrikant, Sara Irina","Exploring maps by sounds: using parameter mapping sonification to make digital elevation models audible","International Journal of Geographical Information Science","","1365-8816","10.1080/13658816.2017.1420192","https://doi.org/10.1080/13658816.2017.1420192","This study empirically investigates the potential of auditory displays for spatial data exploration, as an additional means to broaden the accessibility and dissemination of geographic information for a diverse body of users. In a mixed factorial experiment, three parameter mapping sonification methods are empirically evaluated to interactively explore discrete and continuous digital elevation models by auditory means. Contrasting prior sonification research, this study’s unique empirical evidence suggests that participants can indeed successfully interpret sonified displays containing continuous spatial data. Specifically, the auditory variable pitch leads to significantly better response accuracy, compared to the sound variable duration. Background and training has a weak effect on data interpretation performance with the auditory display. The more immersive the experienced soundscape, the better participants can interpret the sonified terrain. These encouraging empirical results indeed suggest that interactive auditory displays might offer additional means to disseminate spatial information, and to increase the accessibility to spatial data, beyond the currently dominant visual paradigm.","2018-05-04","2023-07-12 05:33:57","2023-07-12 05:33:57","2023-07-12 05:33:48","874-906","","5","32","","","Exploring maps by sounds","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/13658816.2017.1420192","","/Users/minsik/Zotero/storage/2GKEKBCR/Schito and Fabrikant - 2018 - Exploring maps by sounds using parameter mapping .pdf","","","Auditory display; parameter mapping sonification; digital elevation model; GIS; spatial data analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CKNUXUS2","journalArticle","2022","Lau, Tsz Chun Marco; Li, Simon Y. W.; Lee, Alan L. F.; Sanderson, Penelope M.; Loeb, Robert G.","Evaluation of Preview Cues to Enhance Recall of Auditory Sequential Information","Auditory Perception & Cognition","","2574-2442","10.1080/25742442.2022.2095236","https://doi.org/10.1080/25742442.2022.2095236","In previous work, an auditory vital sign display of five patients was developed. Sounds denoting the vital signs of each patient were delivered in order, with a special sound for any patient whose vital signs were all normal. Although the display was effective, accuracy decreased as the number of abnormal patients increased. We wondered whether accuracy would improve with a preview sound indicating the number of patients with abnormal vital signs in the upcoming sequence. A 3 (preview cue type) x 4 (number of abnormal patients) mixed design was adopted. Preview cue type (between-subjects) was either time-compressed speech, an abstract sound containing white noise pulses to indicate the upcoming number of abnormal patients, or no preview cue. The number of abnormal patients (within-subjects) was zero, one, two, or three. Results showed that the preview cue did not improve non-clinician participants’ ability to identify the location in the sequence or the vital signs of patients with abnormal vital signs. Response accuracy dropped as the number of patients with abnormal vital signs increased. Although it did not hurt performance, the current preview cue did not improve performance. Adding a preview cue may improve performance, but not with the current design.","2022-10-02","2023-07-12 05:33:57","2023-07-12 05:33:57","2023-07-12 05:33:49","282-299","","3-4","5","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://doi.org/10.1080/25742442.2022.2095236","","/Users/minsik/Zotero/storage/MTYQBCG4/Lau et al. - 2022 - Evaluation of Preview Cues to Enhance Recall of Au.pdf","","","auditory display; multitasking; patient monitoring; preview cue","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9UD6V9KN","journalArticle","2006","Dean, Roger T.; Whitelaw, Mitchell; Smith, Hazel; Worrall, David","The mirage of real-time algorithmic synaesthesia: Some compositional mechanisms and research agendas in computer music and sonification","Contemporary Music Review","","0749-4467","10.1080/07494460600760981","https://doi.org/10.1080/07494460600760981","This article looks at algorithmic synaesthesia, a form of sonic intermedia involving synchronous computer-mediated manipulation of sound and image. In algorithmic synaesthesia extensively shared features are created in the two media. Examples of such work by austraLYSIS and others are discussed. What an audience member can cognitively access in such synaesthesia is considered: creators of intermedia works may overestimate this. The fact that a machine can process image and sound in parallel, and by the same algorithm, does not establish that the human brain can. The transparency of an algorithmic process to a listener-viewer-screener is a core issue in auditory display (or ‘sonification’). Sonification aims to make the segmentation of a data set more accessible than it is when represented numerically or visually, and has many practical and creative applications. Current approaches in experimental cognition may assist us in evaluating these issues.","2006-08-01","2023-07-12 05:33:57","2023-07-12 05:33:57","2023-07-12 05:33:50","311-326","","4","25","","","The mirage of real-time algorithmic synaesthesia","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://doi.org/10.1080/07494460600760981","","/Users/minsik/Zotero/storage/NML38IT9/Dean et al. - 2006 - The mirage of real-time algorithmic synaesthesia .pdf","","","Sonification; Algorithmic Synaesthesia; Cognition; Intermedia; Microsound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GN5RFGBR","journalArticle","2009","Braasch, Jonas","The Telematic Music System: Affordances for a New Instrument to Shape the Music of Tomorrow","Contemporary Music Review","","0749-4467","10.1080/07494460903422404","https://doi.org/10.1080/07494460903422404","New digital technologies enabled the Virtual Reality movement in the 1990s, which led to the euphoric belief that we could virtually teleport ourselves to a distant location with such great realism that we would lose our awareness of the enabling technology. Especially in the particular case of music collaborations, however, it has meanwhile become clear that this goal is unreachable because of the unavoidable transmission latency over long distances and other secondary factors. This article explores how we can create new and better design goals for telematic music collaborations by treating telematic systems as a new class of musical instruments. From this viewpoint, the creation of new art forms for the new medium becomes a central challenge, in addition to the development of better telematic technology. In particular, the new works need to focus on the affordances that the telematic system introduces, not just circumnavigate current system limitations.","2009-08-01","2023-07-12 05:33:57","2023-07-12 05:33:57","2023-07-12 05:33:51","421-432","","4-5","28","","","The Telematic Music System","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://doi.org/10.1080/07494460903422404","","/Users/minsik/Zotero/storage/M2DSYB9E/Braasch - 2009 - The Telematic Music System Affordances for a New .pdf","","","Telematic Music; Telepresence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6MNFFMKI","journalArticle","2007","Widmer, Gerhard; Rocchesso, Davide; Välimäki, Vesa; Erkut, Cumhur; Gouyon, Fabien; Pressnitzer, Daniel; Penttinen, Henri; Polotti, Pietro; Volpe, Gualtiero","Sound and Music Computing: Research Trends and Some Key Issues","Journal of New Music Research","","0929-8215","10.1080/09298210701859222","https://doi.org/10.1080/09298210701859222","This contribution attempts to give an overview of current research trends and open research problems in the rich field of Sound and Music Computing (SMC). To that end, the field is roughly divided into three large areas related to Sound, Music, and Interaction, respectively, and within each of these, major research trends are briefly described. In addition, for each sub-field a small number of open research (or research strategy) issues are identified that should be addressed in order to further advance the SMC field.","2007-09-01","2023-07-12 05:33:57","2023-07-12 05:33:57","2023-07-12 05:33:52","169-184","","3","36","","","Sound and Music Computing","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://doi.org/10.1080/09298210701859222","","/Users/minsik/Zotero/storage/4Z8DIFC4/Widmer et al. - 2007 - Sound and Music Computing Research Trends and Som.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V42TCYT4","journalArticle","2023","Nadri, Chihab; Ko, Sangjin; Diggs, Colin; Winters, Michael; Vattakkandy, Sreehari; Jeon, Myounghoon","Sonification Use Cases in Highly Automated Vehicles: Designing and Evaluating Use Cases in Level 4 Automation","International Journal of Human–Computer Interaction","","1044-7318","10.1080/10447318.2023.2180236","https://doi.org/10.1080/10447318.2023.2180236","The introduction of highly automated driving systems is expected to significantly change in-vehicle interactions, creating opportunities for the design of novel use cases and interactions for occupants. In this study, we sought to identify and extract these novel use cases and determine preliminary auditory display recommendations for these novel situations. We developed and generated use cases for level 4 automated vehicles through an expert workshop (N = 17) and online focus group interviews (N = 12). Most of the use cases we generated were then tested, apart from meditation, and user opinions were collected in a driving simulator study (N = 20). Results indicated participants were interested in functions that support their experience with both driving and non-driving related interactions in highly automated vehicles. Three categories of use cases for level 4 automated vehicles were developed: driving automation use cases, immersion use cases, and in-vehicle notification use cases. For the driving simulator study, we tested three display modalities for interaction with drivers: visual alert only, non-speech with visual, and speech with visual. In terms of situation awareness (SA), the non-speech with visual display was associated with significantly better SA for the use case consisting of a planned increase in automation level than the speech-with visual display. This study will provide guidance on sonification design to advance user experiences in highly automated vehicles.","2023-02-26","2023-07-12 05:33:57","2023-07-12 05:33:57","2023-07-12 05:33:53","1-11","","0","0","","","Sonification Use Cases in Highly Automated Vehicles","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/10447318.2023.2180236","","/Users/minsik/Zotero/storage/8QDNS3XP/Nadri et al. - 2023 - Sonification Use Cases in Highly Automated Vehicle.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HTJCSQHQ","journalArticle","1997","Pennycook, Bruce","Live electroacoustic music: Old problems, new solutions","Journal of New Music Research","","0929-8215","10.1080/09298219708570717","https://doi.org/10.1080/09298219708570717","This paper presents some observations on certain fundamental issues associated with the presentation of music which combines performers and electronically generated and processed signals delivered by loudspeakers. I contend that several factors have limited the growth of the genre and will attempt to illustrate their origins and consequences. These comments lead to some general recommendations to correct the situation and to some observations from my own experiences as a composer and performer. Finally, I examine a few new technologies which may soon impact the genre.","1997-03-01","2023-07-12 05:33:57","2023-07-12 05:33:57","2023-07-12 05:33:54","70-95","","1","26","","","Live electroacoustic music","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://doi.org/10.1080/09298219708570717","","/Users/minsik/Zotero/storage/ZTWPCNHX/Pennycook - 1997 - Live electroacoustic music Old problems, new solu.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VRJHMLEN","journalArticle","1996","Begault, Durand R.; Pittman, Marc T.","Three-Dimensional Audio Versus Head-Down Traffic Alert and Collision Avoidance System Displays","The International Journal of Aviation Psychology","","1050-8414","10.1207/s15327108ijap0601_5","https://doi.org/10.1207/s15327108ijap0601_5","The advantage of a head-up auditory display for situational awareness was evaluated in an experiment designed to measure and compare the acquisition time for capturing visual targets under two conditions: standard headdown Traffic Alert and Collision Avoidance System (TCAS) display and three-dimensional (3-D) audio TCAS presentation. (The technology used for 3-D audio presentation allows a stereo headphone user to potentially localize a sound at any externalized position in 3-D auditory space). Ten commercial airline crews were tested under full-mission simulation conditions at the NASA-Ames Crew-Vehicle Systems Research Facility Advanced Concepts Flight Simulator. Scenario software generated targets corresponding to aircraft that activated a 3-D aural advisory (the head-up auditory condition) or a standard, visual-audio TCAS advisory (map display with monaural audio alert). Results showed a significant difference in target acquisition time between the two conditions, favoring the 3-D audio TCAS condition by 500 ms.","1996-01-01","2023-07-12 05:33:57","2023-07-12 05:33:57","2023-07-12 05:33:55","79-93","","1","6","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1207/s15327108ijap0601_5 PMID: 11539173","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IIN9VHF3","journalArticle","2022","Ko, Sangjin; Kutchek, Kyle; Zhang, Yiqi; Jeon, Myounghoon","Effects of Non-Speech Auditory Cues on Control Transition Behaviors in Semi-Automated Vehicles: Empirical Study, Modeling, and Validation","International Journal of Human–Computer Interaction","","1044-7318","10.1080/10447318.2021.1937876","https://doi.org/10.1080/10447318.2021.1937876","In semi-automated vehicles, non-speech sounds have been prevalently used as auditory displays for control transitions since these sounds convey urgency well. However, there are no standards of specifications for warning sounds so that diverse non-speech sounds are being employed. To shed light on this, the effects of different non-speech auditory warnings on driver performance were investigated and quantified through the experimental study and human performance modeling approaches. Twenty-four young drivers drove in the driving simulator and experienced both handover and takeover transitions between manual and automated modes while performing a secondary task. The reaction times for handover and takeover, mental workload, and subjective responses were reported. Overall, a traditional warning sound with many repetitions and an indicator sound with decreasing polarity outperformed and were preferred. Additionally, a mathematical model, using the Queuing Network-Model Human Processor (QN-MHP) framework, was applied to quantify the effects of auditory warnings’ acoustic characteristics on drivers’ reaction times in response to takeover request displays. The acoustic characteristics, including the fundamental frequency, the number of repetitions, and the range of dominant frequencies were utilized in modeling. The model was able to explain 99.7% of the experimental data with a root mean square error (RMSE) of 0.148. The present study can contribute to establishing standards and design guidelines for takeover request displays in semi-automated vehicles.","2022-01-20","2023-07-12 05:33:57","2023-07-12 05:33:57","2023-07-12 05:33:56","185-200","","2","38","","","Effects of Non-Speech Auditory Cues on Control Transition Behaviors in Semi-Automated Vehicles","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/10447318.2021.1937876","","/Users/minsik/Zotero/storage/Z3T746XD/Ko et al. - 2022 - Effects of Non-Speech Auditory Cues on Control Tra.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JREAWNQ7","journalArticle","2007","Bernardini, Nicola; de Poli, Giovanni","The Sound and Music Computing Field: Present and Future","Journal of New Music Research","","0929-8215","10.1080/09298210701862432","https://doi.org/10.1080/09298210701862432","This paper is a general introduction for the theme of this special issue. It attempts to give a definition of the Sound and Music Computing research field stemming from its methodologies, aims and approaches. A brief account of the disciplines involved along with their academic organization follows, along with a short description of the areas of application involved. Since Sound and Music Computing has recently enjoyed a deep world-wide reflection upon its own goals, visions and perspectives which has resulted in several roadmapping exercises, the last part of this article provides a summary of these exercises by introducing them in the context in which they were created.","2007-09-01","2023-07-12 05:33:57","2023-07-12 05:33:57","2023-07-12 05:33:57","143-148","","3","36","","","The Sound and Music Computing Field","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://doi.org/10.1080/09298210701862432","","/Users/minsik/Zotero/storage/XKIWHXPR/Bernardini and de Poli - 2007 - The Sound and Music Computing Field Present and F.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6IV5YCJH","journalArticle","2023","Schütz, Laura; Weber, Emmanuelle; Niu, Wally; Daniel, Bruce; McNab, Jennifer; Navab, Nassir; Leuze, Christoph","Audiovisual augmentation for coil positioning in transcranial magnetic stimulation","Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization","","2168-1163","10.1080/21681163.2022.2154277","https://doi.org/10.1080/21681163.2022.2154277","Transcranial Magnetic Stimulation (TMS) is an effective non-invasive treatment method for major depressive disorder. Accurate placement of an electromagnetic coil on the patient’s head during repetitive TMS is the key for stimulation of the desired brain regions and positive treatment outcome. Neuronavigation systems constitute the state-of-the-art method to accurately stimulate the appropriate brain region. Local separation of navigation information and the patient anatomy in combination with intricate visualisations and cumbersome setup limits the benefits and usability of this method. The present study addresses these problems by proposing an audiovisual Augmented reality (AR) system for coil positioning during TMS. The system sonifies and visualises translational and rotational differences between a target and the current instrument position using a minimalistic graphical user interface and auditory display. Effects of cross-modal integration on usability and targeting precision were shown in an experiment comparing audiovisual AR, audio AR and visual neuronavigation. Our approach revealed significant improvements in task time of all proposed AR conditions over neuronavigation (p < 0.001). Conversely, the neuronavigation system achieved significantly better targeting accuracy (p < 0.001). A purely auditory guidance achieved comparable performance as the audiovisual interface designs.","2023-07-04","2023-07-12 05:35:32","2023-07-12 05:35:32","2023-07-12 05:35:22","1158-1165","","4","11","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/21681163.2022.2154277","","/Users/minsik/Zotero/storage/KFX34JEM/Schütz et al. - 2023 - Audiovisual augmentation for coil positioning in t.pdf","","","augmented reality; audiovisual; human computer interaction; Multimodal interaction; surgical navigation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G23VIJWF","journalArticle","2007","Leman, Marc; Avanzini, Federico; de Cheveigné, Alain; Bigand, Emmanuel","The Societal Contexts for Sound and Music Computing: Research, Education, Industry, and Socio-Culture","Journal of New Music Research","","0929-8215","10.1080/09298210701859164","https://doi.org/10.1080/09298210701859164","The paper addresses the various contexts that determine the societal framework for research in the field of sound and music computing. Four of these contexts are identified, namely, the research context, the educational context, the industrial context and the socio-cultural context. For each context, the major trends are analysed and summarized as short statements, thus providing a background in which the state-of-the-art and the challenges of sound and music research can be situated.","2007-09-01","2023-07-12 05:35:32","2023-07-12 05:35:32","2023-07-12 05:35:23","149-167","","3","36","","","The Societal Contexts for Sound and Music Computing","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://doi.org/10.1080/09298210701859164","","/Users/minsik/Zotero/storage/WFZHEM2Q/Leman et al. - 2007 - The Societal Contexts for Sound and Music Computin.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CNBJ4GPY","journalArticle","2002","Herder, Jens; Cohen, Michael","The Helical Keyboard: Perspectives for Spatial Auditory Displays and Visual Music","Journal of New Music Research","","0929-8215","10.1076/jnmr.31.3.269.14180","https://www.tandfonline.com/doi/abs/10.1076/jnmr.31.3.269.14180","Auditory displays with the ability to dynamically spatialize virtual sound sources under real-time conditions enable advanced applications for art and music. A listener can be deeply immersed while interacting and participating in the experience. We review some of those applications while focusing on the Helical Keyboard project and discussing the required technology. Inspired by the cyclical nature of octaves and helical structure of a scale, a model of a piano-style keyboard was prepared, which was then geometrically warped into a helicoidal configuration, one octave/revolution, pitch mapped to height and chroma. It can be driven by MIDI events, real-time or sequenced, which stream is both synthesized and spatialized by a spatial sound display. The sound of the respective notes is spatialized with respect to sinks, avatars of the human user, by default in the tube of the helix. Alternative coloring schemes can be applied, including a color map compatible with chromastereoptic eyewear. The graphical display animates polygons, interpolating between the notes of a chord across the tube of the helix. Recognition of simple chords allows directionalization of all the notes of a major triad from the position of its musical root. The system is designed to allow, for instance, separate audition of harmony and melody, commonly played by the left and right hands, respectively, on a normal keyboard. Perhaps the most exotic feature of the interface is the ability to fork one’s presence, replicating subject instead of object by installing multiple sinks at arbitrary places around a virtual scene so that, for example, harmony and melody can be separately spatialized, using two heads to normalize the octave; such a technique effectively doubles the helix from the perspective of a single listener. Rather than a symmetric arrangement of the individual helices, they are perceptually superimposed in-phase, co-extensively, so that corresponding notes in different registers are at the same azimuth.","2002-09-01","2023-07-12 05:35:32","2023-07-12 05:35:32","2023-07-12 05:35:24","269-281","","3","31","","","The Helical Keyboard","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://www.tandfonline.com/doi/pdf/10.1076/jnmr.31.3.269.14180","","/Users/minsik/Zotero/storage/U5SK3YQJ/Herder and Cohen - 2002 - The Helical Keyboard Perspectives for Spatial Aud.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NC6GNQJ4","journalArticle","2020","Snook, Kelly; Barri, Tarik; Bolles, Monica; Ericson, Petter; Fravel, Carl; Goßmann, Joachim; Green-Mateu, Susan E.; Luck, Andrew; Schedel, Margaret; Thomas, Robert","Concordia: A musical XR instrument for playing the solar system","Journal of New Music Research","","0929-8215","10.1080/09298215.2020.1714666","https://doi.org/10.1080/09298215.2020.1714666","Kepler Concordia, a new scientific and musical instrument enabling players to explore the solar system and other data within immersive extended-reality (XR) platforms, is being designed by a diverse team of musicians, artists, scientists and engineers using audio-first principles. The core instrument modules will be launched in 2019 for the 400th anniversary of Johannes Kepler's Harmonies of the World, in which he laid out a framework for the harmony of geometric form as well as the three laws of planetary motion. Kepler's own experimental process can be understood as audio-first because he employed his understanding of Western Classical music theory to investigate and discover the heliocentric, elliptical behaviour of planetary orbits. Indeed, principles of harmonic motion govern much of our physical world and show up at all scales in mathematics and physics. Few physical systems, however, offer such rich harmonic complexity and beauty as our own solar system. Concordia is a musical instrument that is modular, extensible and designed to allow players to generate and explore transparent sonifications of planetary movements rooted in the musical and mathematical concepts of Johannes Kepler as well as researchers who have extended Kepler's work, such as Hartmut Warm. Its primary function is to emphasise the auditory experience by encouraging musical explorations using sonification of geometric and relational information of scientifically accurate planetary ephemeris and astrodynamics. Concordia highlights harmonic relationships of the solar system through interactive sonic immersion. This article explains how we prioritise data sonification and then add visualisations and gamification to create a new type of experience and creative distributed-ledger powered ecosystem. Kepler Concordia facilitates the perception of music while presenting the celestial harmonies through multiple senses, with an emphasis on hearing, so that, as Kepler wrote, ‘the mind can seize upon the patterns’.","2020-01-01","2023-07-12 05:35:32","2023-07-12 05:35:32","2023-07-12 05:35:25","88-103","","1","49","","","Concordia","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://doi.org/10.1080/09298215.2020.1714666","","/Users/minsik/Zotero/storage/TXWM6VEF/Snook et al. - 2020 - Concordia A musical XR instrument for playing the.pdf","","","Sonification; astronomy; mathematics; ephemeris; immersive media; musical instruments","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V37L5UUC","journalArticle","2015","Supper, Alexandra","Data Karaoke: Sensory and Bodily Skills in Conference Presentations","Science as Culture","","0950-5431","10.1080/09505431.2015.1052390","https://doi.org/10.1080/09505431.2015.1052390","At the International Conference on Auditory Display (ICAD), an interdisciplinary conference dedicated to sonification and the use of non-speech sound to represent information, presenters make use of a variety of bodily skills and representations that appeal to the senses of their audience. In many established disciplines, the conventions that guide the use of these skills and representations are taken for granted; but within ICAD, they are often explicitly negotiated. The practice of ‘data karaoke', in which researchers mimic the sound of a sonification with their own voice, is particularly instructive for understanding these negotiations, and the ICAD community more generally. Data karaoke fulfils five functions: embodiment, highlighting, illustration, authorisation and integration. To make sense of data karaoke, we have to understand the institutional and intellectual environment in which this peculiar practice has emerged; but conversely, an understanding of data karaoke can help us throw new light on epistemological debates about the hierarchy of the senses: data karaoke is a multisensory skill engaging the whole body of the sonification researcher, and thus calls into question the dominant epistemological discourse within the ICAD community, in which the different sensory modalities are framed as competitors. The ICAD case shows that studying conferences as sites where bodies interact, and presentations as performances involving the bodies and senses of scientists, helps us to understand not only the conference cultures, but also the ideals about scientific scholarship and academic authority held by scientific communities.","2015-10-02","2023-07-12 05:35:32","2023-07-12 05:35:32","2023-07-12 05:35:26","436-457","","4","24","","","Data Karaoke","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://doi.org/10.1080/09505431.2015.1052390","","/Users/minsik/Zotero/storage/HQ6KGL4V/Supper - 2015 - Data Karaoke Sensory and Bodily Skills in Confere.pdf","","","sonification; sound; academic conferences; bodily practices; hierarchy of the senses","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YFYTJJKG","journalArticle","2015","Metatla, Oussama; Bryan-Kinns, Nick; Stockman, Tony; Martin, Fiore","Designing with and for people living with visual impairments: audio-tactile mock-ups, audio diaries and participatory prototyping","CoDesign","","1571-0882","10.1080/15710882.2015.1007877","https://doi.org/10.1080/15710882.2015.1007877","Methods used to engage users in the design process often rely on visual techniques, such as paper prototypes, to facilitate the expression and communication of design ideas. The visual nature of these tools makes them inaccessible to people living with visual impairments. In addition, while using visual means to express ideas for designing graphical interfaces is appropriate, it is harder to use them to articulate the design of non-visual displays. In this article, we present an approach to conducting participatory design with people living with visual impairments incorporating various techniques to help make the design process accessible. We reflect on the benefits and challenges that we encountered when employing these techniques in the context of designing cross-modal interactive tools.","2015-01-02","2023-07-12 05:35:32","2023-07-12 05:35:32","2023-07-12 05:35:27","35-48","","1","11","","","Designing with and for people living with visual impairments","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/15710882.2015.1007877","","/Users/minsik/Zotero/storage/F3RIUWMA/Metatla et al. - 2015 - Designing with and for people living with visual i.pdf","","","accessibility; auditory display; multimodal interaction; assistive technology; visual impairments; haptics; cross-modal interaction; low-fi non-visual design; mock-ups; participatory prototyping; tactile feedback","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PSQBR5AD","journalArticle","2015","Jeon, Myounghoon; Gable, Thomas M.; Davison, Benjamin K.; Nees, Michael A.; Wilson, Jeff; Walker, Bruce N.","Menu Navigation With In-Vehicle Technologies: Auditory Menu Cues Improve Dual Task Performance, Preference, and Workload","International Journal of Human–Computer Interaction","","1044-7318","10.1080/10447318.2014.925774","https://doi.org/10.1080/10447318.2014.925774","Auditory display research for driving has mainly examined a limited range of tasks (e.g., collision warnings, cell phone tasks). In contrast, the goal of this project was to evaluate the effectiveness of enhanced auditory menu cues in a simulated driving context. The advanced auditory cues of “spearcons” (compressed speech cues) and “spindex” (a speech-based index cue) were predicted to improve both menu navigation and driving. Two experiments used a dual task paradigm in which users selected songs on the vehicle’s infotainment system. In Experiment 1, 24 undergraduates played a simple, perceptual-motor ball-catching game (the primary task; a surrogate for driving), and navigated through an alphabetized list of 150 song titles—rendered as an auditory menu—as a secondary task. The menu was presented either in the typical visual-only manner, or enhanced with text-to-speech (TTS), or TTS plus one of three types of additional auditory cues. In Experiment 2, 34 undergraduates conducted the same secondary task while driving in a simulator. In both experiments, performance on both the primary task (success rate of the game or driving performance) and the secondary task (menu search time) was better with the auditory menus than with no sound. Perceived workload scores as well as user preferences favored the enhanced auditory cue types. These results show that adding audio, and enhanced auditory cues in particular, can allow a driver to operate the menus of in-vehicle technologies more efficiently while driving more safely. Results are discussed with multiple resources theory.","2015-01-02","2023-07-12 05:35:32","2023-07-12 05:35:32","2023-07-12 05:35:28","1-16","","1","31","","","Menu Navigation With In-Vehicle Technologies","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/10447318.2014.925774","","/Users/minsik/Zotero/storage/7HNXDQQR/Jeon et al. - 2015 - Menu Navigation With In-Vehicle Technologies Audi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YNY568M7","journalArticle","2002","Johannsen, Gunnar","Human Supervision and Control in Engineering and Music – Foundations and Transdisciplinary Views","Journal of New Music Research","","0929-8215","10.1076/jnmr.31.3.179.14187","https://www.tandfonline.com/doi/abs/10.1076/jnmr.31.3.179.14187","Views of engineering and music professionals into the transdisciplinary field of human supervision and control are contrasted. Foundations of this field are introduced, and an overview is presented based on all important aspects from the International Workshop on “Human Supervision and Control in Engineering and Music” held in September, 2001 in Kassel. The main concept of supervisory control is illustrated with engineering applications, particularly in humanmachine(-computer) interaction, as well as applied to music performance. Several activities in the performing arts and in industrial engineering are compared. Composition, analysis, and performance are discussed in more detail, in relation to the scientifically embedded concert of the workshop and its four works from three continents, including computer music with live performance as well as Japanese aesthetics. Performance theory and performance tools are also mentioned. Sensorimotor, gestural, and cognitive control are outlined, briefly supplemented by visual and auditory supervision. Finally, sound design and musical information retrieval are also identified as aspects of human supervision and control.","2002-09-01","2023-07-12 05:35:32","2023-07-12 05:35:32","2023-07-12 05:35:29","179-190","","3","31","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://www.tandfonline.com/doi/pdf/10.1076/jnmr.31.3.179.14187","","/Users/minsik/Zotero/storage/PY4TSSWS/Johannsen - 2002 - Human Supervision and Control in Engineering and M.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"864DDU96","journalArticle","2012","Katayose, Haruhiro; Hashida, Mitsuyo; De Poli, Giovanni; Hirata, Keiji","On Evaluating Systems for Generating Expressive Music Performance: the Rencon Experience","Journal of New Music Research","","0929-8215","10.1080/09298215.2012.745579","https://doi.org/10.1080/09298215.2012.745579","Research into music generation and into emulating human musical competence has attracted much attention in the field of computer science. In general, the results of academic research should be verified by assessing ‘objective effectiveness’, which is often represented by a ‘recognition ratio’. Although ‘objective effectiveness’ is also a requirement for research in music generation, it is meaningless unless subjective requisites are also satisfied. However, it is not easy for researchers to execute subjective evaluations within their individual endeavours. To address this difficulty within the research area of computer systems for generating expressive music performances, the Performance Rendering Contest (Rencon) was created. This is an international competition in which entrants present computer systems and the performances generated are graded, and has been held in conjunction with related international conferences. This paper presents an overview of Rencon history, highlighting the evaluative motivation of each contest. In addition, we discuss the possibilities of a new scientific research field in which future Rencons may play a role.","2012-12-01","2023-07-12 05:35:32","2023-07-12 05:35:32","2023-07-12 05:35:31","299-310","","4","41","","","On Evaluating Systems for Generating Expressive Music Performance","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://doi.org/10.1080/09298215.2012.745579","","/Users/minsik/Zotero/storage/HUQDVVDS/Katayose et al. - 2012 - On Evaluating Systems for Generating Expressive Mu.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H66PQJSV","journalArticle","1999","Hamman, M.","From Symbol to Semiotic: Representation, Signification, and the Composition of Music Interaction","Journal of New Music Research","","0929-8215","10.1076/jnmr.28.2.90.3117","https://doi.org/10.1076/jnmr.28.2.90.3117","This paper examines notions of interaction in order to synthesize an approach to the use of computers in the arts which respects the fact that, oftentimes, creative work is facilitated by task environments in which ”surprises” can happen. Typically, interface design concerns the rendering of an interaction such that it requires minimal cognitive engagement with the task in question, relying heavily on historically and culturally determined patterns of behavior and cognition. Ill-structured problems (like music composition), however, benefit when the interface presents concepts and interactions in ways that are novel. Computers can be understood as tools for the projection of such an interface when they are conceived as generators of semiotic rather than symbolic ordering frameworks.","1999-06-01","2023-07-12 05:35:32","2023-07-12 05:35:32","2023-07-12 05:35:32","90-104","","2","28","","","From Symbol to Semiotic","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://doi.org/10.1076/jnmr.28.2.90.3117","","/Users/minsik/Zotero/storage/VWGTFDTJ/Hamman - 1999 - From Symbol to Semiotic Representation, Significa.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IGTEVGL5","journalArticle","2018","Yu, Bin; Funk, Mathias; Hu, Jun; Feijs, Loe","Unwind: a musical biofeedback for relaxation assistance","Behaviour & Information Technology","","0144-929X","10.1080/0144929X.2018.1484515","https://doi.org/10.1080/0144929X.2018.1484515","Unwind is a musical biofeedback interface which combines nature sounds and sedative music into a form of New-Age music for relaxation exercises. The nature sounds respond to the user’s physiological data, functioning as an informative layer for biofeedback display. The sedative music aims to induce calmness and evoke positive emotions. UnWind incorporates the benefits of biofeedback and sedative music to facilitate deep breathing, moderate arousal, and promote mental relaxation. We evaluated Unwind in a 2 × 2 factorial experiment with music and biofeedback as independent factors. Forty young adults performed the relaxation exercise under one of the following conditions after experiencing a stressful task: Nature sounds only (NS), Nature sounds with music (NM), and Auditory biofeedback with nature sounds (NSBFB), and UnWind musical biofeedback (NMBFB). The results revealed a significant interaction effect between music and biofeedback on the improvement of heart rate variability. The combination of music and nature sounds also showed benefits in lowering arousal and reducing self-report anxiety. We conclude with a discussion of UnWind for biofeedback and the wider potential of blending nature sounds with music as a musical interface.","2018-08-03","2023-07-12 05:37:37","2023-07-12 05:37:37","2023-07-12 05:37:28","800-814","","8","37","","","Unwind","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/0144929X.2018.1484515","","/Users/minsik/Zotero/storage/62GD3VD4/Yu et al. - 2018 - Unwind a musical biofeedback for relaxation assis.pdf","","","sonification; Biofeedback; heart rate variability; musical display; relaxation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JPD3NJPL","journalArticle","2012","Lindroth, Scott","Teaching Composition: Artistic Growth through Confrontation, Tact, Sympathy, and Honesty","Contemporary Music Review","","0749-4467","10.1080/07494467.2012.725816","https://doi.org/10.1080/07494467.2012.725816","Teaching composition requires different strategies depending on each student's interests and experience. Reflecting on my own experiences as a composition student, I have found two teaching approaches to be especially helpful. Undergraduates frequently require compositional exercises that build technique and confidence. More experienced composers often benefit from carefully formulated confrontations that help students enlarge the scope of their musical imagination. Seminars allow students to explore new compositional approaches in the spirit of collaboration and experimentation. Such experiences can open new avenues for creative exploration.","2012-08-01","2023-07-12 05:37:37","2023-07-12 05:37:37","2023-07-12 05:37:28","297-304","","4","31","","","Teaching Composition","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://doi.org/10.1080/07494467.2012.725816","","/Users/minsik/Zotero/storage/K7AR2RPD/Lindroth - 2012 - Teaching Composition Artistic Growth through Conf.pdf","","","Composition; Pedagogy; Conservatory; University","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y44CCFZL","journalArticle","2000","Trueman, Dan; Cook, Perry","BoSSA: The Deconstructed Violin Reconstructed","Journal of New Music Research","","0929-8215","10.1076/jnmr.29.2.121.3098","https://www.tandfonline.com/doi/abs/10.1076/jnmr.29.2.121.3098","Traditional musical instruments provide compelling metaphors for human-computer interfacing, both in terms of input (physical, gestural performance activities) and output (sound diffusion). The violin, one of the most refined and expressive of traditional instruments, combines a peculiar physical interface with a rich acoustic diffuser. We have built a new instrument that includes elements of both the violin’s physical performance interface and its spatial filtering audio diffuser, yet eliminates both the resonating body and the strings. The instrument, BoSSA (Bowed-Sensor-Speaker-Array), is an amalgamation and extension of our previous work with violin interfaces, physical models, and directional tonal radiation studies. In addition to describing the various physical and software elements that make up BoSSA, we discuss some of its musical features and potentials; we are particularly impressed by the sense of presence and intimacy it provides, and by its potential for creating a new kind of electronic chamber music.","2000-06-01","2023-07-12 05:37:37","2023-07-12 05:37:37","2023-07-12 05:37:29","121-130","","2","29","","","BoSSA","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://www.tandfonline.com/doi/pdf/10.1076/jnmr.29.2.121.3098","","/Users/minsik/Zotero/storage/DKHVUTXX/Trueman and Cook - 2000 - BoSSA The Deconstructed Violin Reconstructed.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2R4CGCY5","journalArticle","2006","Jakovich, J.; Beilharz, K.; Echanove, M.","Symbiosis between participation and system design: from interactive art to urban development","CoDesign","","1571-0882","10.1080/15710880601008067","https://doi.org/10.1080/15710880601008067","In interactive art practice and community-based urban development, user participation plays a key role in the development phase of a project. The present paper draws on experience from our work in these two fields to analyse the role of participation in interactive systems as a form of cooperation assisting the ongoing design of system structure. A system is a complex of interacting and interrelated components. Relationships within the system, or the structure, define and generate behaviours between human participant(s) and other components of an art or urban environment. While urban development and interactive art deal with participation in differing contexts and scales, the goal of the practitioner is consistent: the construction of a system that enables and maintains meaningful interaction towards some goal. The aim of this paper is to compare observations from our works at both the art-lab and urban scales, and to characterize the symbiotic relationship between user participation and system design. The comparison of these two diverse fields provides unique insight into this relationship.","2006-12-01","2023-07-12 05:37:37","2023-07-12 05:37:37","2023-07-12 05:37:30","249-257","","4","2","","","Symbiosis between participation and system design","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/15710880601008067","","/Users/minsik/Zotero/storage/WKKA2A74/Jakovich et al. - 2006 - Symbiosis between participation and system design.pdf","","","Feedback; Interactive art; Motivation; Participatory system; Structure; Urban planning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SNQ932QJ","journalArticle","2004","Rocchesso, Davide","Physically-based Sounding Objects, as We Develop Them Today","Journal of New Music Research","","0929-8215","10.1080/0929821042000317868","https://doi.org/10.1080/0929821042000317868","The possibility of manipulating tapes opened up, in the 1950s, the era of phenomenological sound description and manipulation. In the following decades the emergence of computers, programming languages, and analysis/synthesis techniques, allowed the deconstruction of sound matter at the level of the elementary attributes of signals. Nowadays, we have a rich repertoire of modeling techniques that allow us to tackle the world of sound at different levels of complexity, taking both physical descriptions and phenomenological experiences into account. We can develop physically based sounding objects that can be manipulated according to everyday experience and embodied into artifacts that support continuous interaction.","2004-09-01","2023-07-12 05:37:37","2023-07-12 05:37:37","2023-07-12 05:37:31","305-313","","3","33","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://doi.org/10.1080/0929821042000317868","","/Users/minsik/Zotero/storage/TTGK2V8V/Rocchesso - 2004 - Physically-based Sounding Objects, as We Develop T.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LQ5GHQFB","journalArticle","2014","Frissen, Ilja; Guastavino, Catherine","Do whole-body vibrations affect spatial hearing?","Ergonomics","","0014-0139","10.1080/00140139.2014.910611","https://doi.org/10.1080/00140139.2014.910611","To assist the human operator, modern auditory interfaces increasingly rely on sound spatialisation to display auditory information and warning signals. However, we often operate in environments that apply vibrations to the whole body, e.g. when driving a vehicle. Here, we report three experiments investigating the effect of sinusoidal vibrations along the vertical axis on spatial hearing. The first was a free-field, narrow-band noise localisation experiment with 5- Hz vibration at 0.88 ms− 2. The other experiments used headphone-based sound lateralisation tasks. Experiment 2 investigated the effect of vibration frequency (4 vs. 8 Hz) at two different magnitudes (0.83 vs. 1.65 ms− 2) on a left–right discrimination one-interval forced-choice task. Experiment 3 assessed the effect on a two-interval forced-choice location discrimination task with respect to the central and two peripheral reference locations. In spite of the broad range of methods, none of the experiments show a reliable effect of whole-body vibrations on localisation performance.","2014-07-03","2023-07-12 05:37:37","2023-07-12 05:37:37","2023-07-12 05:37:32","1090-1101","","7","57","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/00140139.2014.910611 PMID: 24783989","","/Users/minsik/Zotero/storage/P25RP2D6/Frissen and Guastavino - 2014 - Do whole-body vibrations affect spatial hearing.pdf","","","exposure assessment; sound lateralisation; sound localisation; whole-body vibration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZL5GUXVR","journalArticle","1965","BROWN, I. D.","Effect of a Car Radio on Driving in Traffic","Ergonomics","","0014-0139","10.1080/00140136508930828","https://doi.org/10.1080/00140136508930828","Eight drivers were tested in light and heavy traffic while listening to recorded programmes of music and speech. The effects of these auditory distractions on the use of the car controls and time taken over a standard test circuit of 2·2 miles were measured by comparison with scores obtained in a quiet condition of normal driving. In light traffic, music significantly reduced the frequency with which the accelerator and brake pedals were used (p = 0·05), and in heavy traffic it increased the time taken per circuit )p < 0·05). These changes were interpreted as being beneficial. Speech had an insignificant effect on all scores, whether listening was motivated simply by interest in the programme, or by the need to remember its content.","1965-10-01","2023-07-12 05:37:37","2023-07-12 05:37:37","2023-07-12 05:37:33","475-479","","4","8","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/00140136508930828 PMID: 5854152","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"37UGV56X","journalArticle","2006","Dayé, Christian; de Campo, Alberto","Sounds sequential: sonification in the social sciences","Interdisciplinary Science Reviews","","0308-0188","10.1179/030801806X143286","https://doi.org/10.1179/030801806X143286","This article discusses the use of sound for auditory information display, and in particular its application for exploration of scientific data, known as sonification. Sonification can be defined as the use of sound to display data of scientific interest in order to investigate structures, trends or patterns in the data. Background is provided from several perspectives: the use of the senses in the history of science, the strengths of human hearing, the recent technological availability of auditory interfaces, the development of sonification itself, and differentiation of sonification from musical practices. In Western science, as in Western culture, the eye has become the predominant organ of sense; using the ear consciously in research thus implicitly questions the implications of the eye's predominance. As practical examples, two applications of sonification to data-sets from the social sciences are discussed in detail. We argue that the most promising areas of application of sonification within the social sciences are in the exploration of sequential data. Our two examples both concern sets of sequential data, one temporal, the other spatial (geographical). Discussion of these examples is followed by consideration of the practical and cultural implications of working with sonification. We thus hope to further the use of sonification in the social sciences, not as an alternative to visualisation or statistical approaches, but as a complementary tool of data analysis and exploration.","2006-12-01","2023-07-12 05:37:37","2023-07-12 05:37:37","2023-07-12 05:37:34","349-364","","4","31","","","Sounds sequential","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1179/030801806X143286","","/Users/minsik/Zotero/storage/RWRD6BIV/Dayé and de Campo - 2006 - Sounds sequential sonification in the social scie.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QH6SP9LR","journalArticle","2015","Larsson, Pontus; Niemand, Mathias","Using Sound to Reduce Visual Distraction from In-vehicle Human–Machine Interfaces","Traffic Injury Prevention","","1538-9588","10.1080/15389588.2015.1020111","https://doi.org/10.1080/15389588.2015.1020111","Objective: Driver distraction and inattention are the main causes of accidents. The fact that devices such as navigation displays and media players are part of the distraction problem has led to the formulation of guidelines advocating various means for minimizing the visual distraction from such interfaces. However, although design guidelines and recommendations are followed, certain interface interactions, such as menu browsing, still require off-road visual attention that increases crash risk. In this article, we investigate whether adding sound to an in-vehicle user interface can provide the support necessary to create a significant reduction in glances toward a visual display when browsing menus.Methods: Two sound concepts were developed and studied; spearcons (time-compressed speech sounds) and earcons (musical sounds). A simulator study was conducted in which 14 participants between the ages of 36 and 59 took part. Participants performed 6 different interface tasks while driving along a highway route. A 3 × 6 within-group factorial design was employed with sound (no sound /earcons/spearcons) and task (6 different task types) as factors. Eye glances and corresponding measures were recorded using a head-mounted eye tracker. Participants’ self-assessed driving performance was also collected after each task with a 10-point scale ranging from 1 = very bad to 10 = very good. Separate analyses of variance (ANOVAs) were conducted for different eye glance measures and self-rated driving performance.Results: It was found that the added spearcon sounds significantly reduced total glance time as well as number of glances while retaining task time as compared to the baseline (= no sound) condition (total glance time M = 4.15 for spearcons vs. M = 7.56 for baseline, p =.03). The earcon sounds did not result in such distraction-reducing effects. Furthermore, participants ratings of their driving performance were statistically significantly higher in the spearcon conditions compared to the baseline and earcon conditions (M = 7.08 vs. M = 6.05 and M = 5.99 respectively, p =.035 and p =.002).Conclusions: The spearcon sounds seem to efficiently reduce visual distraction, whereas the earcon sounds did not reduce distraction measures or increase subjective driving performance. An aspect that must be further investigated is how well spearcons and other types of auditory displays are accepted by drivers in general and how they work in real traffic.","2015-06-01","2023-07-12 05:37:37","2023-07-12 05:37:37","2023-07-12 05:37:35","S25-S30","","sup1","16","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/15389588.2015.1020111 PMID: 26027972","","/Users/minsik/Zotero/storage/YSYUR3D4/Larsson and Niemand - 2015 - Using Sound to Reduce Visual Distraction from In-v.pdf","","","auditory displays; human–machine interfaces; earcons; spearcons; distraction; inattention","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3MCNYI7Q","journalArticle","2009","Ahmad, A.M.; Stanney, K.M.; Fouad, H.","Theoretical foundations for integrating sound in interactive interfaces: identifying temporal and spatial information conveyance principles","Theoretical Issues in Ergonomics Science","","1463-922X","10.1080/14639220701734455","https://doi.org/10.1080/14639220701734455","This paper proposes theoretical foundations for conveying temporal (i.e. relating to time) and spatial (i.e. relating to space) information using auditory cues in interactive systems. Three theoretical models are developed to aid the design of auditory interfaces, including an audio integration model that outlines an end-to-end process for adding sounds to interactive interfaces, a temporal audio model that provides a framework for when to integrate these sounds to meet certain performance objectives and a spatial audio model that provides a framework for adding spatialisation cues to interface sounds. The models presented in this paper, which are each coupled with a set of design guidelines theorised from the literature, put forward a structured process for integrating sounds in interactive interfaces.","2009-03-01","2023-07-12 05:37:37","2023-07-12 05:37:37","2023-07-12 05:37:37","161-186","","2","10","","","Theoretical foundations for integrating sound in interactive interfaces","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/14639220701734455","","/Users/minsik/Zotero/storage/8BU6893L/Ahmad et al. - 2009 - Theoretical foundations for integrating sound in i.pdf","","","audio; interactive systems; spatial; temporal; training","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H5IYQM32","journalArticle","2008","Spain, Randall D.; Bliss, James P.","The effect of sonification display pulse rate and reliability on operator trust and perceived workload during a simulated patient monitoring task","Ergonomics","","0014-0139","10.1080/00140130802120234","https://doi.org/10.1080/00140130802120234","The present study investigated the effects of sonification pulse rate and sensor reliability on operator trust and mental workload. Processing resources and operator trust were sensitive to both pulse rate and sensor reliability. These findings suggest that setting pulse rates to 60 pulses per min may have considerable benefits in critical task environments.","2008-09-01","2023-07-12 05:39:39","2023-07-12 05:39:39","2023-07-12 05:39:29","1320-1337","","9","51","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/00140130802120234 PMID: 18802818","","/Users/minsik/Zotero/storage/8TB2QIPR/Spain and Bliss - 2008 - The effect of sonification display pulse rate and .pdf","","","sonification; trust; mental workload; system reliability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CH4FINUR","journalArticle","1984","Mezrich, J. J.; Frysinger, S.; Slivjanovski, R.","Dynamic Representation of Multivariate Time Series Data","Journal of the American Statistical Association","","0162-1459","10.1080/01621459.1984.10477059","https://www.tandfonline.com/doi/abs/10.1080/01621459.1984.10477059","In this article we describe a procedure for representing multivariate time series data by means of interactive, computer-generated dynamic imagery with computer-music accompaniment. This innovation conveys the novel insights that dynamic imagery can provide; yet, the imagery is developed from principles that make the representation useful when examined either statically or dynamically. This is because the development of the dynamic representation is guided by the same perceptual and technical principles used in making a motion picture. The particular implementation we describe is evaluated by a formal psychophysics experiment in which we measure the threshold correlation that can be perceived in our dynamic representation, and in each of three different types of graphical portrayals.","1984-03-01","2023-07-12 05:39:39","2023-07-12 05:39:39","2023-07-12 05:39:31","34-40","","385","79","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1984.10477059","","","","","Audiovisual; Data representation; Psychophysics experiment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PECVJPM2","journalArticle","2023","Nees, Michael A.; Liebman, Eliana","Auditory Icons, Earcons, Spearcons, and Speech: A Systematic Review and Meta-Analysis of Brief Audio Alerts in Human-Machine Interfaces","Auditory Perception & Cognition","","2574-2442","10.1080/25742442.2023.2219201","https://doi.org/10.1080/25742442.2023.2219201","Auditory displays commonly are used in safety-critical domains and are a vital component of universal and inclusive design practices. Despite several decades of research on brief auditory alerts for representing status and processes in user interfaces, there is no clear heuristic guidance for which type(s) of auditory alerts should be preferred for designing interfaces. We used evidence synthesis (systematic review and meta-analysis) to examine the effectiveness of different types of brief audio alerts. We identified articles comparing auditory icons (real-world sounds with an ecological relationship to their referent), earcons (abstract sounds with no ecological relationship to their referent), spearcons (accelerated/compressed speech), and speech alerts. We used meta-analysis to compare alerts across five different outcomes: accuracy, reaction time, subjective ratings, workload, and dual-task interference. For accuracy and reaction time, results indicated speech, spearcons, and other types of alerts (usually hybrid, e.g., spearcons plus speech) were superior to auditory icons, which in turn were superior to earcons. Earcons also were inferior to all other options with respect to subjective ratings. Analyses generally suggested parity among alert types for workload and dual-task interference. Based on currently available evidence, it appears that speech, spearcons, and hybrid (e.g., spearcons plus speech) auditory alerts result in better performance than auditory icons and especially earcons. Still, high heterogeneity in our analyses cannot rule out a wide range of possible effects, and our analyses could not directly address some of the concerns that have been raised regarding speech-based alerts. These findings can help to guide the selection of brief audio alerts in interface design.","2023-06-11","2023-07-12 05:39:39","2023-07-12 05:39:39","2023-07-12 05:39:32","1-30","","0","0","","","Auditory Icons, Earcons, Spearcons, and Speech","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://doi.org/10.1080/25742442.2023.2219201","","/Users/minsik/Zotero/storage/7KDCIRCP/Nees and Liebman - 2023 - Auditory Icons, Earcons, Spearcons, and Speech A .pdf","","","sonification; Auditory displays; assistive technology; universal design; alarms; network meta-analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8LJMAJP6","journalArticle","2020","Agné, Hans; Sommerer, Thomas; Angeler, David G.","Introducing the Sounds of Data to the Study of Politics: A Choir of Global Legitimacy Crises","New Political Science","","0739-3148","10.1080/07393148.2020.1809760","https://doi.org/10.1080/07393148.2020.1809760","This article introduces an innovative method to describe data with sounds in political science. The method, known in ecology, physics, and musicology as “sonification,” operates by linking sound signals to quantifiable observations. We us it to compose a choir of legitimacy crises in global governance from 1994 to 2014, and to negotiate a familiar divide in research on how legitimacy should be measured. Scholars predominantly prefer one of two approaches to measure legitimacy quantitatively, either looking at political trust or public contestation of political institutions. We illustrate the usefulness of sonification to subsume both positions in this divide. More generally, we argue that sonification can enhance public communication of scientific results and extract meanings from observations that go unnoticed in visual and verbal representations, in particular with relevance to describing time series data on anything from the spread of pandemics to violent conflicts and economic inequalities.","2020-07-02","2023-07-12 05:39:39","2023-07-12 05:39:39","2023-07-12 05:39:33","272-288","","3","42","","","Introducing the Sounds of Data to the Study of Politics","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://doi.org/10.1080/07393148.2020.1809760","","/Users/minsik/Zotero/storage/4XCYFLW8/Agné et al. - 2020 - Introducing the Sounds of Data to the Study of Pol.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RE3JB4ZP","journalArticle","1997","Jagacinski, Richard J.; Greenberg, Neil; Liao, Min-Ju","Tempo, Rhythm, and Aging in Golf","Journal of Motor Behavior","","0022-2895","10.1080/00222899709600830","https://doi.org/10.1080/00222899709600830","Older (n = 12) and younger (n = 12) golfers attempted to hit a golf ball into a target net a short distance away. An accelerometer attached to the back of the clubhead measured the applied force. In contrast to the more typical finding of slower perceptual-motor performance by older adults, older golfers reached their peak downswing force earlier in the shot and also exhibited a trend toward a faster overall speed or tempo of the shot. Additionally, older golfers exhibited greater changes in applied force and greater variability. A pattern of divergence among the force-time histories from multiple shots suggested that the overall person-plus-golf-club dynamics were unstable during a part of the shot. Older adults may be slower in controlling this instability. Half of the participants heard a tone whose pitch was proportional to their force. These participants had a slower follow-through; however, they did not make significantly more or fewer shots than participants who had not been presented with the tone. Analyses of the temporal covariation among the backswing, downswing, and follow-through favored a chain-like temporal structure over a hierarchical, proportional structure. The pattern of covariation suggests that the tempo and rhythm of the shot are not independent and that changing one's tempo may disrupt rhythm.","1997-06-01","2023-07-12 05:39:39","2023-07-12 05:39:39","2023-07-12 05:39:34","159-173","","2","29","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://doi.org/10.1080/00222899709600830 PMID: 12453792","","/Users/minsik/Zotero/storage/8PDGBWZB/Jagacinski et al. - 1997 - Tempo, Rhythm, and Aging in Golf.pdf","","","rhythm; aging; golf; tempo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3H8HAQKE","journalArticle","2022","Johnson, Emily K.","The Aural-Visual Rhetoric in Video Game Tutorials","Technical Communication Quarterly","","1057-2252","10.1080/10572252.2021.2021452","https://doi.org/10.1080/10572252.2021.2021452","This article asserts that auditory cues can be categorized by rhetorical function into the categories of visual rhetoric, defined by Amare and Manning under Peirce’s Ten Classes of Sign, understanding visual rhetoric to include both images and text. This article expands this definition to aural-visual rhetoric, including auditory elements as visual rhetoric to analyze multimodal Technical and Professional Communication (TPC), demonstrating this method using the opening tutorial scene from Portal 2.","2022-10-02","2023-07-12 05:39:39","2023-07-12 05:39:39","2023-07-12 05:39:35","374-384","","4","31","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://doi.org/10.1080/10572252.2021.2021452","","/Users/minsik/Zotero/storage/9CKWVJ8W/Johnson - 2022 - The Aural-Visual Rhetoric in Video Game Tutorials.pdf","","","Video games; human-computer interaction; digital technologies; rhetoric of technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QU6YDZGQ","journalArticle","2008","Park, Hyungjun; Son, Jeong-Soo; Lee, Kwan-Heng","Design evaluation of digital consumer products using virtual reality-based functional behaviour simulation","Journal of Engineering Design","","0954-4828","10.1080/09544820701474129","https://doi.org/10.1080/09544820701474129","To realize the virtual design and prototyping of every digital consumer product faithfully, it is important to allow all the people involved in the design process to experience its realistic appearance and functional behaviour. Proposed in this paper is a novel approach to design evaluation of a digital consumer product, which can satisfy such requirements using virtual reality-based functional behaviour simulation. In the approach, a product model, multimedia contents data, a functional behaviour model and a finite state machine are combined to constitute the virtual product model of a product of interest. A state transition methodology is adopted to capture the functional behaviour of the product, which is simulated by the finite state machine in a virtual reality environment. Based on the approach, a product design evaluation system has been developed and applied for the design evaluation of various products. A preliminary user study has been conducted to show the usefulness of the proposed approach by comparing it with other approaches using real products and two-dimensional screen prototypes.","2008-08-01","2023-07-12 05:39:39","2023-07-12 05:39:39","2023-07-12 05:39:36","359-375","","4","19","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/09544820701474129","","/Users/minsik/Zotero/storage/BZCJTDS9/Park et al. - 2008 - Design evaluation of digital consumer products usi.pdf","","","Functional behaviour simulation; Human–machine interaction; Product design evaluation; State transition methodology; Virtual product model","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M89BUX7K","journalArticle","2011","Kortum, Philip; Peres, S.   Camille; Stallmann, Kurt","Extensible Auditory Progress Bar Design: Performance and Aesthetics","International Journal of Human–Computer Interaction","","1044-7318","10.1080/10447318.2011.555310","https://doi.org/10.1080/10447318.2011.555310","This study investigated performance and preference differences for three different extensible Auditory Progress Bar (APB) designs. Four durations (30 s, 60 s, 120 s, 240 s) of the three APBs (Sine, Cello, and Electronic) were used in the study. There were 105 participants who listened to all durations of a single-stimulus type and were asked to determine the length of time they had listened to the stimulus and to rate the stimuli on aesthetic quality. Participants were significantly worse at time estimation with the Electronic APB. The Sine APB was preferred significantly less than either the Cello or Electronic APBs. Regardless of the stimulus, time estimation was more variable and more accurate as the duration of the APB increased. The results indicate that although they were originally envisioned as a supplement for the visual progress bar, APBs can be effective when used alone. Further, it was found that, even within the small design space presented here, APB design can influence the performance of listeners.","2011-09-01","2023-07-12 05:39:39","2023-07-12 05:39:39","2023-07-12 05:39:37","864-884","","9","27","","","Extensible Auditory Progress Bar Design","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/10447318.2011.555310","","/Users/minsik/Zotero/storage/X6AG9BGC/Kortum et al. - 2011 - Extensible Auditory Progress Bar Design Performan.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8IAC8JKU","journalArticle","1997","Stevens, Robert D.; Edwards, Alistair D.N.; Harling, Philip A.","Access to Mathematics for Visually Disabled Students Through Multimodal Interaction","Human–Computer Interaction","","0737-0024","10.1080/07370024.1997.9667240","https://www.tandfonline.com/doi/abs/10.1080/07370024.1997.9667240","Mathematics relies on visual forms of communication and is thus largely inaccessible to people who cannot communicate in this manner because of visual disabilities. This article outlines the Mathtalk project, which addressed this problem by using computers to produce multimodal renderings of mathematical information. This example is unusual in that it is essential to use multiple modalities because of the nature and the difficulty of the application. In addition, the emphasis is on nonvisual (and hence novel) modalities. Crucial to designing a usable auditory interface to algebra notation is an understanding of the differences between visual and listening reading, particularly those aspects that make the former active and the latter passive. A discussion of these differences yields the twin themes of compensation for lack of external memory and provision of control over information flow. These themes were addressed by: the introduction of prosody to convey algebraic structure in synthetically spoken expressions; the provision of structure-based browsing functions; and the use of a prosody-based musical glance based on algebra earcons. The addition of prosody, when compared to a traditional method of presenting spoken algebra, was experimentally shown to increase the recovery of algebraic structure, enhance the retention of content, and reduce mental workload. These three factors can be said to compensate for the lack of an adequate external memory. Evaluations showed that the browsing functions and associated command language gave the fast and accurate control over information flow that is necessary for active reading. The algebra earcon was experimentally shown to convey the presence, location, and size of algebraic constructs within an expression in a manner that might be used as a rapid glance. Finally, an evaluation of the integrated components showed that the design principles derived from the Mathtalk program can give a more usable, active reading of algebra notation than that possible with traditional methods.","1997-03-01","2023-07-12 05:39:39","2023-07-12 05:39:39","2023-07-12 05:39:39","47-92","","1-2","12","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://www.tandfonline.com/doi/pdf/10.1080/07370024.1997.9667240","","/Users/minsik/Zotero/storage/YKNN2R37/Stevens et al. - 1997 - Access to Mathematics for Visually Disabled Studen.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8XRDKYVG","journalArticle","2011","Knoll, Monja A.; Uther, Maria; Costall, Alan","Using the Internet for speech research: an evaluative study examining affect in speech","Behaviour & Information Technology","","0144-929X","10.1080/0144929X.2011.577192","https://doi.org/10.1080/0144929X.2011.577192","The Internet has rarely been used in auditory perception studies due to concerns about standardisation and calibration across different systems and settings. However, not all auditory research is based on the investigation of fine-grained differences in auditory thresholds. Where meaningful ‘real-world’ listening, for instance the perception of speech, is concerned, the Internet may be a more appropriate and ecologically valid setting to collect data. This study compared affective ratings of low-pass-filtered infant-, foreigner- and British adult-directed speech obtained with traditional methods in the laboratory, with those obtained from an Internet sample. Dropout rates and demographic distribution of participants in the Internet condition were also assessed. The results show that affective ratings were similar for both the Internet and laboratory samples. These findings indicate the viability of Internet-based research into affective speech perception and suggest that precise acoustic environmental control may not always be necessary.","2011-11-01","2023-07-12 05:42:35","2023-07-12 05:42:35","2023-07-12 05:42:28","845-851","","6","30","","","Using the Internet for speech research","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/0144929X.2011.577192","","/Users/minsik/Zotero/storage/MW2ZCZ43/Knoll et al. - 2011 - Using the Internet for speech research an evaluat.pdf","","","Internet; emotional affect; low-pass filtering; speech perception","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YIWMT9KS","journalArticle","2015","Supper, Alexandra; Bijsterveld, Karin","Sounds Convincing: Modes of Listening and Sonic Skills in Knowledge Making","Interdisciplinary Science Reviews","","0308-0188","10.1179/0308018815Z.000000000109","https://doi.org/10.1179/0308018815Z.000000000109","This article investigates the role of listening in the knowledge making practices of Western scientists, engineers, and physicians from the 1920s onwards. It does so by offering a two-dimensional typology of the modes of listening that they employ. Distinguishing between two dimensions allows us to make sense both of the purpose and of the ways in which scientists, engineers, and physicians have listened to their objects of study; and it also allows us to appreciate the importance of shifting between modes of listening. At the same time, we argue, understanding the role of sound in knowledge making cannot be limited to the study of listening alone; rather, we have to pay attention to how listening is embedded in broader sonic skills — including the handling of tools for the making, recording, storing, and retrieving of sounds.","2015-06-01","2023-07-12 05:42:35","2023-07-12 05:42:35","2023-07-12 05:42:29","124-144","","2","40","","","Sounds Convincing","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1179/0308018815Z.000000000109","","/Users/minsik/Zotero/storage/RWTY2UYU/Supper and Bijsterveld - 2015 - Sounds Convincing Modes of Listening and Sonic Sk.pdf","","","listening; engineering and medicine; history of science; skills; sound studies","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RAINXHLY","journalArticle","2015","Bando, Yoshiaki; Otsuka, Takuma; Mizumoto, Takeshi; Itoyama, Katsutoshi; Konyo, Masashi; Tadokoro, Satoshi; Nakadai, Kazuhiro; Okuno, Hiroshi G.","Posture estimation of hose-shaped robot by using active microphone array","Advanced Robotics","","0169-1864","10.1080/01691864.2014.981291","https://doi.org/10.1080/01691864.2014.981291","Hose-shaped rescue robots have been developed for searching narrow spaces such as under collapsed buildings. The posture estimation independent of the past history is critical, because conventional inertial-sensor-based posture estimation has two main problems; a cumulative error problem peculiar to inertial sensors, and a sudden posture change problem caused by external forces. For coping with the two problems, we developed a novel posture estimation method by putting an active microphone array, a set of microphones and loudspeakers, on the robot. The method calculates the time difference of arrival (TDOA) of the reference signal emitted from one loudspeaker, and estimates the posture from the distance obtained by TDOA. This concise method still has three problems: (1) external noise, (2) reverberation and reflection, and (3) obstacles. These problems are tackled by (1) TSP signal, (2) GCC-PHAT and a threshold-based onset detection, and (3) rejecting incorrect onset times, respectively. Experiments with simulated sounds and actual recordings demonstrate that the method attains the performance of estimation comparable to that of conventional methods, that is, less than 20 cm of the tip position error. Even without historical data, the method attains the similar performance while conventional methods fail.","2015-01-02","2023-07-12 05:42:35","2023-07-12 05:42:35","2023-07-12 05:42:30","35-49","","1","29","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/01691864.2014.981291","","/Users/minsik/Zotero/storage/72ZT565Q/Bando et al. - 2015 - Posture estimation of hose-shaped robot by using a.pdf","","","active microphone array; hose-shaped robot; localization; rescue robotics; robot audition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9TMY4PPS","journalArticle","1998","Fenger, T.   Nick","Visual-motor Integration and its Relation to EEG Neurofeedback Brain Wave Patterns, Reading, Spelling, and Arithmetic Achievement in Attention Deficit Disordered and Learning Disabled Students","Journal of Neurotherapy","","1087-4208","10.1300/J184v03n01_02","https://doi.org/10.1300/J184v03n01_02","Studies examining EEG neurofeedback treatment for Attention Deficit Disorders (ADD) and Learning Disabilities (LD) have shown relationships between Theta/Beta ratios (TBR's) and enhanced attention, and measures of cognitive functioning including visual-motor integration. Thirty-eight children, ages 8 to 18, received neurofeedback where Beta was rewarded while Theta and EMG were inhibited and demonstrated significant reductions in TBR's after an average of 46 sessions. They also demonstrated significant improvements in measures of visual-motor integration, and academic achievement. Though the changes in TBR's were not correlated with all outcome measures, post-treatment TBR's were correlated to post-treatment visual-motor integration scores. The possible intervening variable relationship of visual-motor integration with TBR's and achievement changes is discussed.","1998-07-01","2023-07-12 05:42:35","2023-07-12 05:42:35","2023-07-12 05:42:31","9-18","","1","3","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://doi.org/10.1300/J184v03n01_02","","/Users/minsik/Zotero/storage/8VWPEFMP/Fenger - 1998 - Visual-motor Integration and its Relation to EEG N.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TR5MNDND","journalArticle","2010","Laakso, Mari; Tiina Sarjakoski, L","Sonic Maps for Hiking—Use of Sound in Enhancing the Map Use Experience","The Cartographic Journal","","0008-7041","10.1179/000870410X12911298276237","https://doi.org/10.1179/000870410X12911298276237","title/>The use of soundscapes on digital maps in order to enrich our experience of the map use has been recognized as a part of other multimedia components in several previous research projects. The study described in this paper was conducted as part of ongoing research projects aiming to develop mobile and web applications for outdoor activities. The research describes the aims of finding new kinds of ways of communicating spatial information, even to those who are visually impaired, and, in general, to provide map users with a profounder use experience. The paper reviews previous studies and describes the sonic map implementations accomplished to date. The implementation examples presented cover a hiking use case in a national park. Sonic maps can help the users to plan their hike in advance and further provide those who are not able to visit the park with a multimodal experience of the atmosphere. Finally, conclusions are given and possible improvements for the future are discussed.","2010-11-01","2023-07-12 05:42:35","2023-07-12 05:42:35","2023-07-12 05:42:32","300-307","","4","47","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1179/000870410X12911298276237","","/Users/minsik/Zotero/storage/RTPX6NCX/Laakso and Tiina Sarjakoski - 2010 - Sonic Maps for Hiking—Use of Sound in Enhancing th.pdf","","","visually impaired; soundscape; perceive; sonic map; use experience","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UIA7S3FT","journalArticle","2019","Bitterman, Noemi; Klimovich, Katerina; Pillar, Giora","Home healthcare devices. Challenge of CPAP design for effective home treatment","The Design Journal","","1460-6925","10.1080/14606925.2019.1595446","https://doi.org/10.1080/14606925.2019.1595446","Background: Continuous positive airway pressure (CPAP) is among the most popular treatments for obstructive sleep apnea (OSA). Despite the great variety of devices and accessories available in the market, they still have an unfavorable image, poor adherence and subjected to complaints from patients and families. We reviewed available devices, identified problems and requirements, and suggest design concepts that may improve patient satisfaction, adherence and quality of life. Methods: Data were collected from commercial internet sources, scientific publications and observations. Comparative analysis was used to evaluate the components of the CPAP system, the profile of CPAP users and the treatment environment. Results: Main results indicate obstacles in the image and appearance of CPAP as a daily, life-time utility, in user capabilities, in performance and adaptivity to bedroom design. Conclusions: Adopting a user-centered design rather than an engineering focus will alleviate psychological issues and potentially increase adherence, satisfaction and the overall ""user experience"".","2019-04-01","2023-07-12 05:42:35","2023-07-21 08:27:18","2023-07-12 05:42:32","669-681","","sup1","22","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://doi.org/10.1080/14606925.2019.1595446","","/Users/minsik/Zotero/storage/QRA4X33Y/Bitterman et al. - 2019 - Home healthcare devices. Challenge of CPAP design .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JLTL29LG","journalArticle","2020","Kapsali, Maria","Sonic Bodies","Performance Research","","1352-8165","10.1080/13528165.2020.1842029","https://doi.org/10.1080/13528165.2020.1842029","This article examines the use of sonification, and movement sonification, as a tool for engagement with artworks in galleries and museums. It focuses on the relationship between artwork and sighted, partially sighted and non-sighted visitors and the way sonification, by offering an audio-centred mode of engagement, complicates the aesthetic experience. By drawing on Bruno Latour’s discussion on hybrids (1993) as well as with reference to examples of the use of sonification in gallery spaces, the article argues that rather than assuming the artwork to be a finished and stable object, movement sonification approaches the artwork as a blueprint of proximal, conceptual and material relationships which are animated by a moving-listening subject. As such, the article concludes, movement sonification can have radical implications for the way artworks are conceived as well as the way the aesthetic experience is configured.","2020-05-18","2023-07-12 05:42:35","2023-07-12 05:42:35","2023-07-12 05:42:33","45-53","","4","25","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://doi.org/10.1080/13528165.2020.1842029","","/Users/minsik/Zotero/storage/JTMPSNT4/Kapsali - 2020 - Sonic Bodies.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2KQNINRW","journalArticle","2014","Hussain, Ibrar; Chen, Ling; Mirza, Hamid Turab; Xing, Kong; Chen, Gencai","A Comparative Study of Sonification Methods to Represent Distance and Forward-Direction in Pedestrian Navigation","International Journal of Human–Computer Interaction","","1044-7318","10.1080/10447318.2014.925381","https://doi.org/10.1080/10447318.2014.925381","This article presents a new design of using nonspeech audio (i.e., earcons, spearcons, and short pulses) to represent distance and forward-direction for pedestrian navigation in eyes-free environment. Experiment in the field is carried out with the involvement of 15 participants using within-subject design to evaluate the newly developed earcons, spearcons, and short pulses for distance and forward-direction in pedestrian navigation. Results from the experiment suggest that spearcons are efficient in tasks completion, and it conveys distance and forward-direction information to participants more accurately compared with earcons and short pulses. Overall, participants have shown their satisfaction with spearcons as an audio feedback in pedestrian navigation.","2014-09-02","2023-07-12 05:42:35","2023-07-12 05:42:35","2023-07-12 05:42:35","740-751","","9","30","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/10447318.2014.925381","","/Users/minsik/Zotero/storage/WJYN65B7/Hussain et al. - 2014 - A Comparative Study of Sonification Methods to Rep.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3HWG7GNA","journalArticle","2009","Warm, Joel S.; Matthews, Gerald; Parasuraman, Raja","Cerebral Hemodynamics and Vigilance Performance","Military Psychology","","0899-5605","10.1080/08995600802554706","https://doi.org/10.1080/08995600802554706","Five studies are described using transcranial Doppler sonography (TCD) and near-infrared spectroscopy (NIRS) to examine brain systems in vigilance. The results indicate that the vigilance decrement, the temporal decline that typifies vigilance performance, is paralleled by a decline in cerebral blood flow velocity as indexed by TCD. In addition, both measures showed greater activity in the right than in the left cerebral hemisphere in response to a variety of psychophysical challenges, indicating a right hemispheric system in control of vigilance performance. The TCD measure was also found to be potentially useful in selecting observers for vigilance assignments.","2009-01-15","2023-07-12 05:43:55","2023-07-12 05:43:55","2023-07-12 05:43:47","S75-S100","","sup1","21","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Routledge _eprint: https://doi.org/10.1080/08995600802554706","","/Users/minsik/Zotero/storage/U8PFYT5L/Warm et al. - 2009 - Cerebral Hemodynamics and Vigilance Performance.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z8FRSW9F","journalArticle","2022","Lee, Seul Chan; Nadri, Chihab; Sanghavi, Harsh; Jeon, Myounghoon","Eliciting User Needs and Design Requirements for User Experience in Fully Automated Vehicles","International Journal of Human–Computer Interaction","","1044-7318","10.1080/10447318.2021.1937875","https://doi.org/10.1080/10447318.2021.1937875","The introduction of fully automated vehicles (FAVs) will change user experiences (UX) in personal transportation. In order for FAVs to become a life enhancing technology, it is required to design vehicular applications and user interfaces based on users’ expectations. To this end, we investigated user needs and design requirements. First, we elicited design taxonomy and use cases through literature review and trend analysis. Using these materials, expert interviews (N = 9) and focus group interviews (N = 10) were conducted. Through the qualitative analysis, we obtained twelve categories of user needs and devised design requirements based on the updated design taxonomy. While some of them have been an extension of current experiences in manual driving, completely new demands have also emerged within FAVs. Our findings contribute to designing UX in FAVs by satisfying users’ expectations and key values that can guide designers.","2022-02-07","2023-07-12 05:43:55","2023-07-12 05:43:55","2023-07-12 05:43:48","227-239","","3","38","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/10447318.2021.1937875","","/Users/minsik/Zotero/storage/DVJJRFFX/Lee et al. - 2022 - Eliciting User Needs and Design Requirements for U.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FXGSWYL9","journalArticle","1989","Wright, Patricia","Interface Alternatives for Hypertexts","Hypermedia","","0955-8543","10.1080/09558543.1989.12031162","https://doi.org/10.1080/09558543.1989.12031162","This paper seeks to make explicit the ranges of design choices among which authors select whenever they create a particular hypertext. These choices are grouped into five categories: the links authors provide which influence the routes that readers can take within the text; the design features relating to the initiation of jumps within the hypertext; the visual characteristics of the destination of a jump; the navigation support that can be offered to readers; the design implications of the tasks that people will be trying to accomplish using the hypertext. It is suggested that the interface characteristics which are most helpful for hypertexts written as tutorials may differ from those design features which benefit users wishing to gain access through hypertexts to large (encyclopaedic) information sources, or hypertexts used by people as a means of personal information management. Although there is little evidence about which design features work best in which circumstances, an understanding of the range of interface options may help authors appreciate the tradeoffs they often have to make when designing hypertexts.","1989-01-01","2023-07-12 05:43:55","2023-07-12 05:43:55","2023-07-12 05:43:49","146-166","","2","1","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/09558543.1989.12031162","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B4VYZQ3L","journalArticle","2004","Gunther, Ryan; Kazman, Rick; MacGregor, Carolyn","Using 3D sound as a navigational aid in virtual environments","Behaviour & Information Technology","","0144-929X","10.1080/01449290410001723364","https://doi.org/10.1080/01449290410001723364","As current virtual environments are less visually rich than real-world environments, careful consideration must be given to their design to ameliorate the lack of visual cues. One important design criterion in this respect is to make certain that adequate navigational cues are incorporated into complex virtual worlds. In this paper we show that adding 3D spatialized sound to a virtual environment can help people navigate through it. We conducted an experiment to determine if the incorporation of 3D sound (a) helps people find specific locations in the environment, and (b) influences the extent to which people acquire spatial knowledge about their environment. Our results show that the addition of 3D sound did reduce time taken to locate objects in a complex environment. However, the addition of sound did not increase the amount of spatial knowledge users were able to acquire. In fact, the addition of 3D auditory sound cues appears to suppress the development of overall spatial knowledge of the virtual environment.","2004-11-01","2023-07-12 05:43:55","2023-07-12 05:43:55","2023-07-12 05:43:50","435-446","","6","23","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/01449290410001723364","","/Users/minsik/Zotero/storage/JVTK666J/Gunther et al. - 2004 - Using 3D sound as a navigational aid in virtual en.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IFH5XWNL","journalArticle","1989","Gaver, William W.","The SonicFinder: An Interface That Uses Auditory Icons","Human–Computer Interaction","","0737-0024","10.1207/s15327051hci0401_3","https://www.tandfonline.com/doi/abs/10.1207/s15327051hci0401_3","The appropriate use of nonspeech sounds has the potential to add a great deal to the functionality of computer interfaces. Sound is a largely unexploited medium of output, even though it plays an integral role in our everyday encounters with the world, a role that is complementary to vision. Sound should be used in computers as it is in the world, where it conveys information about the nature of sound-producing events. Such a strategy leads to auditory icons, which are everyday sounds meant to convey information about computer events by analogy with everyday events. Auditory icons are an intuitively accessible way to use sound to provide multidimensional, organized information to users. These ideas are instantiated in the SonicFinder, which is an auditory interface I developed at Apple Computer, Inc. In this interface, information is conveyed using auditory icons as well as standard graphical feedback. I discuss how events are mapped to auditory icons in the SonicFinder, and illustrate how sound is used by describing a typical interaction with this interface. Two major gains are associated with using sound in this interface: an increase in direct engagement with the model world of the computer and an added flexibility for users in getting information about that world. These advantages seem to be due to the iconic nature of the mappings used between sound and the information it is to convey. I discuss sound effects and source metaphors as methods of extending auditory icons beyond the limitations implied by literal mappings, and I speculate on future directions for such interfaces.","1989-03-01","2023-07-12 05:43:55","2023-07-12 05:43:55","2023-07-12 05:43:51","67-94","","1","4","","","The SonicFinder","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://www.tandfonline.com/doi/pdf/10.1207/s15327051hci0401_3","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BKQLPVTH","journalArticle","1999","Brewster, Stephen A.; Crease, Murray G.","Correcting menu usability problems with sound","Behaviour & Information Technology","","0144-929X","10.1080/014492999119066","https://doi.org/10.1080/014492999119066","Future human-computer interfaces will use more than just graphical output to display information. In this paper we suggest that sound and graphics together can be used to improve interaction. We describe an experiment to improve the usability of standard graphical menus by the addition of sound. One common difficulty is slipping off a menu item by mistake when trying to select it. One of the causes of this is insufficient feedback. We designed and experimentally evaluated a new set of menus with much more salient audio feedback to solve this problem. The results from the experiment showed a significant reduction in the subjective effort required to use the new sonically-enhanced menus along with significantly reduced error recovery times. A significantly larger number of errors were also corrected with sound.","1999-01-01","2023-07-12 05:43:55","2023-07-12 05:43:55","2023-07-12 05:43:52","165-177","","3","18","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/014492999119066","","/Users/minsik/Zotero/storage/KT4WYLX8/Brewster and Crease - 1999 - Correcting menu usability problems with sound.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WNTI5WQM","journalArticle","2016","Singh, Aneesha; Piana, Stefano; Pollarolo, Davide; Volpe, Gualtiero; Varni, Giovanna; Tajadura-Jiménez, Ana; Williams, Amanda CdeC; Camurri, Antonio; Bianchi-Berthouze, Nadia","Go-with-the-Flow: Tracking, Analysis and Sonification of Movement and Breathing to Build Confidence in Activity Despite Chronic Pain","Human–Computer Interaction","","0737-0024","10.1080/07370024.2015.1085310","https://doi.org/10.1080/07370024.2015.1085310","Chronic (persistent) pain (CP) affects 1 in 10 adults; clinical resources are insufficient, and anxiety about activity restricts lives. Technological aids monitor activity but lack necessary psychological support. This article proposes a new sonification framework, Go-with-the-Flow, informed by physiotherapists and people with CP. The framework proposes articulation of user-defined sonified exercise spaces (SESs) tailored to psychological needs and physical capabilities that enhance body and movement awareness to rebuild confidence in physical activity. A smartphone-based wearable device and a Kinect-based device were designed based on the framework to track movement and breathing and sonify them during physical activity. In control studies conducted to evaluate the sonification strategies, people with CP reported increased performance, motivation, awareness of movement, and relaxation with sound feedback. Home studies, a focus group, and a survey of CP patients conducted at the end of a hospital pain management session provided an in-depth understanding of how different aspects of the SESs and their calibration can facilitate self-directed rehabilitation and how the wearable version of the device can facilitate transfer of gains from exercise to feared or demanding activities in real life. We conclude by discussing the implications of our findings on the design of technology for physical rehabilitation.","2016-07-03","2023-07-12 05:43:55","2023-07-12 05:43:55","2023-07-12 05:43:53","335-383","","3-4","31","","","Go-with-the-Flow","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/07370024.2015.1085310","","/Users/minsik/Zotero/storage/V8K7FJZG/Singh et al. - 2016 - Go-with-the-Flow Tracking, Analysis and Sonificat.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KX5CH6JT","journalArticle","1973","","Abstracting Service","Ergonomics","","0014-0139","10.1080/00140137308924487","https://doi.org/10.1080/00140137308924487","The following abstracts are of work of ergonomic interest which have appeared in other journals and research reports. They are a selection from the quarterl‘Ergonomics Abstracts’ which is published by the Ergonomics Information Analysis Centre, Department of Engineering Production, University of Birmingham, England. This journal offers a comprehensive annotated bibliography of current ergonomics publications. By special arrangement, its contents, printed on unbound adhesive backed sheets, are available at reduced rates to members of national ergonomics and human factors societies on application to the Centre. Selections of the information in the Centre's continuing collection, currently totalling some 64,000 abstracts, are available through special bibliographies prepared on request, and may cover any of the recognized subject headings in the field of ergonomics.","1973-01-01","2023-07-12 05:43:55","2023-07-12 05:43:55","2023-07-12 05:43:55","113-128","","1","16","","","","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/00140137308924487","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""