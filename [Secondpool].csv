"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"AWNG52EP","journalArticle","2010","Park, Sihwa; Kim, Seunghun; Lee, Samuel; Yeo, Woon Seung","Online Map Interface for Creative and Interactive","","","","10.5281/zenodo.1177877","","In this paper, we discuss the musical potential of COMPath - an online map based music-making tool - as a noveland unique interface for interactive music composition andperformance. COMPath provides an intuitive environmentfor creative music making by sonification of georeferenceddata. Users can generate musical events with simple andfamiliar actions on an online map interface; a set of local information is collected along the user-drawn route andthen interpreted as sounds of various musical instruments.We discuss the musical interpretation of routes on a map,review the design and implementation of COMPath, andpresent selected sonification results with focus on mappingstrategies for map-based composition.","2010","2023-07-24 06:48:42","2023-07-24 06:48:42","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XW4TNQMF","journalArticle","2007","le Groux, Sylvain; Manzolli, Jonatas; Verschure, Paul F.","VR-RoBoser : Real-Time Adaptive Sonification of Virtual Environments Based on Avatar Behavior","","","","10.5281/zenodo.1177101","","Until recently, the sonification of Virtual Environments had often been reduced to its simplest expression. Too often soundscapes and background music are predetermined, repetitive and somewhat predictable. Yet, there is room for more complex and interesting sonification schemes that can improve the sensation of presence in a Virtual Environment. In this paper we propose a system that automatically generates original background music in real-time called VR-RoBoser. As a test case we present the application of VR-RoBoser to a dynamic avatar that explores its environment. We show that the musical events are directly and continuously generated and influenced by the behavior of the avatar in three-dimensional virtual space, generating a context dependent sonification.","2007","2023-07-24 06:48:43","2023-07-24 06:48:43","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9NV22RHS","journalArticle","2011","Leslie, Grace; Mullen, Tim","MoodMixer : {EEG}-based Collaborative Sonification","","","","10.5281/zenodo.1178089","","MoodMixer is an interactive installation in which participants collaboratively navigate a two-dimensional music spaceby manipulating their cognitive state and conveying thisstate via wearable Electroencephalography (EEG) technology. The participants can choose to actively manipulateor passively convey their cognitive state depending on theirdesired approach and experience level. A four-channel electronic music mixture continuously conveys the participants'expressed cognitive states while a colored visualization oftheir locations on a two-dimensional projection of cognitive state attributes aids their navigation through the space.MoodMixer is a collaborative experience that incorporatesaspects of both passive and active EEG sonification andperformance art. We discuss the technical design of the installation and place its collaborative sonification aestheticdesign within the context of existing EEG-based music andart.","2011","2023-07-24 06:48:43","2023-07-24 06:48:43","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3NVWWPI3","journalArticle","2005","Bowen, Adam","Soundstone: A {3-D} Wireless Music Controller","","","","10.5281/zenodo.1176711","","Soundstone is a small wireless music controller that tracks movement and gestures, and maps these signals to characteristics of various synthesized and sampled sounds. It is intended to become a general-purpose platform for exploring the sonification of movement, with an emphasis on tactile (haptic) feedback.","2005","2023-07-24 06:48:43","2023-07-24 06:48:43","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FPWWMLQP","journalArticle","2006","Beilharz, Kirsty; Jakovich, Joanne; Ferguson, Sam","Hyper-shaku (Border-crossing): Towards the Multi-modal Gesture-controlled Hyper-Instrument","","","","10.5281/zenodo.1176867","","Hyper-shaku (Border-Crossing) is an interactive sensor environment that uses motion sensors to trigger immediate responses and generative processes augmenting the Japanese bamboo shakuhachi in both the auditory and visual domain. The latter differentiates this process from many hyper-instruments by building a performance of visual design as well as electronic music on top of the acoustic performance. It utilizes a combination of computer vision and wireless sensing technologies conflated from preceding works. This paper outlines the use of gesture in these preparatory sound and audio-visual performative, installation and sonification works, leading to a description of the Hyper-shaku environment integrating sonification and generative elements.","2006","2023-07-24 06:48:43","2023-07-24 06:48:43","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BDS947JB","journalArticle","2015","Barrett, Natasha","Creating tangible spatial-musical images from physical performance gestures","","","","10.5281/zenodo.1179014","","Electroacoustic music has a longstanding relationship with gesture and space. This paper marks the start of a project investigating acousmatic spatial imagery, real gestural behaviour and ultimately the formation of tangible acousmatic images. These concepts are explored experimentally using motion tracking in a source-sound recording context, interactive parameter-mapping sonification in three-dimensional high-order ambisonics, composition and performance. The spatio-musical role of physical actions in relation to instrument excitation is used as a point of departure for embodying physical spatial gestures in the creative process. The work draws on how imagery for music is closely linked with imagery for music-related actions.","2015","2023-07-24 06:48:43","2023-07-24 06:48:43","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H57CXX99","journalArticle","2008","Delle Monache, Stefano; Polotti, Pietro; Papetti, Stefano; Rocchesso, Davide","Sonically Augmented Found Objects","","","","10.5281/zenodo.1179519","","We present our work with augmented everyday objectstransformed into sound sources for music generation. The idea isto give voice to objects through technology. More specifically, theparadigm of the birth of musical instruments as a sonification ofobjects used in domestic or work everyday environments is hereconsidered and transposed into the technologically augmentedscenarios of our contemporary world.","2008","2023-07-24 06:48:43","2023-07-24 06:48:43","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6XYP69W8","journalArticle","2011","Mealla, Sebastián; Väaljamäae, Aleksander; Bosi, Mathieu; Jordà, Sergi","Listening to Your Brain : Implicit Interaction in Collaborative Music Performances","","","","10.5281/zenodo.1178107","","The use of physiological signals in Human Computer Interaction (HCI) is becoming popular and widespread, mostlydue to sensors miniaturization and advances in real-timeprocessing. However, most of the studies that use physiologybased interaction focus on single-user paradigms, and itsusage in collaborative scenarios is still in its beginning. Inthis paper we explore how interactive sonification of brainand heart signals, and its representation through physicalobjects (physiopucks) in a tabletop interface may enhancemotivational and controlling aspects of music collaboration.A multimodal system is presented, based on an electrophysiology sensor system and the Reactable, a musical tabletop interface. Performance and motivation variables wereassessed in an experiment involving a test ""Physio"" group(N=22) and a control ""Placebo"" group (N=10). Pairs ofparticipants used two methods for sound creation: implicitinteraction through physiological signals, and explicit interaction by means of gestural manipulation. The resultsshowed that pairs in the Physio Group declared less difficulty, higher confidence and more symmetric control thanthe Placebo Group, where no real-time sonification was provided as subjects were using pre-recorded physiological signal being unaware of it. These results support the feasibilityof introducing physiology-based interaction in multimodalinterfaces for collaborative music generation.","2011","2023-07-24 06:48:43","2023-07-24 06:48:43","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C6HMXMXN","journalArticle","2019","Erdem, Cagri; Schia, Katja Henriksen; Jensenius, Alexander Refsum","Vrengt: A Shared Body-Machine Instrument for Music-Dance Performance","","","","10.5281/zenodo.3672918","","This paper describes the process of developing a shared instrument for music--dance performance, with a particular focus on exploring the boundaries between standstill vs motion, and silence vs sound. The piece Vrengt grew from the idea of enabling a true partnership between a musician and a dancer, developing an instrument that would allow for active co-performance. Using a participatory design approach, we worked with sonification as a tool for systematically exploring the dancer's bodily expressions. The exploration used a ""spatiotemporal matrix,"" with a particular focus on sonic microinteraction. In the final performance, two Myo armbands were used for capturing muscle activity of the arm and leg of the dancer, together with a wireless headset microphone capturing the sound of breathing. In the paper we reflect on multi-user instrument paradigms, discuss our approach to creating a shared instrument using sonification as a tool for the sound design, and reflect on the performers' subjective evaluation of the instrument.","2019","2023-07-24 06:48:43","2023-07-24 06:48:43","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C2ZLDSNQ","journalArticle","2013","Hamano, Takayuki; Rutkowski, Tomasz; Terasawa, Hiroko; Okanoya, Kazuo; Furukawa, Kiyoshi","Generating an Integrated Musical Expression with a Brain--Computer Interface","","","","10.5281/zenodo.1178542","","Electroencephalography (EEG) has been used to generate music for over 40 years,but the most recent developments in brain--computer interfaces (BCI) allowgreater control and more flexible expression for using new musical instrumentswith EEG. We developed a real-time musical performance system using BCItechnology and sonification techniques to generate imagined musical chords withorganically fluctuating timbre. We aim to emulate the expressivity oftraditional acoustic instruments. The BCI part of the system extracts patternsfrom the neural activity while a performer imagines a score of music. Thesonification part of the system captures non-stationary changes in the brainwaves and reflects them in the timbre by additive synthesis. In this paper, wediscuss the conceptual design, system development, and the performance of thisinstrument.","2013","2023-07-24 06:48:43","2023-07-24 06:48:43","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T84PFP5I","journalArticle","2013","McGee, Ryan","VOSIS: a Multi-touch Image Sonification Interface","","","","10.5281/zenodo.1178604","","VOSIS is an interactive image sonification interface that creates complexwavetables by raster scanning greyscale image pixel data. Using a multi-touchscreen to play image regions of unique frequency content rather than a linearscale of frequencies, it becomes a unique performance tool for experimental andvisual music. A number of image filters controlled by multi-touch gestures addvariation to the sound palette. On a mobile device, parameters controlled bythe accelerometer add another layer expressivity to the resulting audio-visualmontages.","2013","2023-07-24 06:48:43","2023-07-24 06:48:43","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4YY4UA7R","journalArticle","2011","Dahl, Luke; Herrera, Jorge; Wilkerson, Carr","TweetDreams : Making Music with the Audience and the World using Real-time Twitter Data","","","","10.5281/zenodo.1177991","","TweetDreams is an instrument and musical compositionwhich creates real-time sonification and visualization oftweets. Tweet data containing specified search terms is retrieved from Twitter and used to build networks of associated tweets. These networks govern the creation of melodiesassociated with each tweet and are displayed graphically.Audience members participate in the piece by tweeting,and their tweets are given special musical and visual prominence.","2011","2023-07-24 06:48:43","2023-07-24 06:48:43","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C2AVGFFM","journalArticle","2008","Hadjakos, Aristotelis; Aitenbichler, Erwin; Mühlhäuser, Max","The Elbow Piano : Sonification of Piano Playing Movements","","","","10.5281/zenodo.1179553","","The Elbow Piano distinguishes two types of piano touch: a touchwith movement in the elbow joint and a touch without. A playednote is first mapped to the left or right hand by visual tracking.Custom-built goniometers attached to the player's arms are usedto detect the type of touch. The two different types of touchesare sonified by different instrument sounds. This gives theplayer an increased awareness of his elbow movements, which isconsidered valuable for piano education. We have implementedthe system and evaluated it with a group of music students.","2008","2023-07-24 06:48:43","2023-07-24 06:48:43","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MD55ETMP","journalArticle","2020","Vasilakos, Konstantinos n/a; Wilson, Scott; McCauley, Thomas; Yeung, Tsun Winston; Margetson, Emma; Khosravi Mardakheh, Milad","Sonification of High Energy Physics Data Using Live Coding and Web Based Interfaces.","","","","10.5281/zenodo.4813430","","This paper presents a discussion of Dark Matter, a sonification project using live coding and just-in-time programming techniques. The project uses data from proton-proton collisions produced by the Large Hadron Collider (LHC) at CERN, Switzerland, and then detected and reconstructed by the Compact Muon Solenoid (CMS) experiment, and was developed with the support of the art@CMS project. Work for the Dark Matter project included the development of a custom-made environment in the SuperCollider (SC) programming language that lets the performers of the group engage in collective improvisations using dynamic interventions and networked music systems. This paper will also provide information about a spin-off project entitled the Interactive Physics Sonification System (IPSOS), an interactive and standalone online application developed in the JavaScript programming language. It provides a web-based interface that allows users to map particle data to sound on commonly used web browsers, mobile devices, such as smartphones, tablets etc. The project was developed as an educational outreach tool to engage young students and the general public with data derived from LHC collisions.","2020","2023-07-24 06:48:43","2023-07-24 06:48:43","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V3G9ZJH2","journalArticle","2020","Gold, Nicolas E.; Wang, Chongyang; Olugbade, Temitayo; Berthouze, Nadia; Williams, Amanda","P(l)aying Attention: Multi-modal, multi-temporal music control","","","","10.5281/zenodo.4813303","","The expressive control of sound and music through body movements is well-studied. For some people, body movement is demanding, and although they would prefer to express themselves freely using gestural control, they are unable to use such interfaces without difficulty. In this paper, we present the P(l)aying Attention framework for manipulating recorded music to support these people, and to help the therapists that work with them. The aim is to facilitate body awareness, exploration, and expressivity by allowing the manipulation of a pre-recorded 'ensemble' through an interpretation of body movement, provided by a machine-learning system trained on physiotherapist assessments and movement data from people with chronic pain. The system considers the nature of a person's movement (e.g. protective) and offers an interpretation in terms of the joint-groups that are playing a major role in the determination at that point in the movement, and to which attention should perhaps be given (or the opposite at the user's discretion). Using music to convey the interpretation offers informational (through movement sonification) and creative (through manipulating the ensemble by movement) possibilities. The approach offers the opportunity to explore movement and music at multiple timescales and under varying musical aesthetics.","2020","2023-07-24 06:48:43","2023-07-24 06:48:43","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DWZ2PTKV","journalArticle","2010","Bryan-Kinns, Nick; Fencott, Robin; Metatla, Oussama; Nabavian, Shahin; Sheridan, Jennifer G.","Interactional Sound and Music : Listening to CSCW, Sonification, and Sound Art","","","","10.5281/zenodo.1177727","","In this paper we outline the emerging field of Interactional Sound and Music which concerns itself with multi-person technologically mediated interactions primarily using audio. We present several examples of interactive systems in our group, and reflect on how they were designed and evaluated. Evaluation techniques for collective, performative, and task oriented activities are outlined and compared. We emphasise the importance of designing for awareness in these systems, and provide examples of different awareness mechanisms.","2010","2023-07-24 06:48:43","2023-07-24 06:48:43","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5RNAFSI7","journalArticle","2019","Bazoge, Nicolas; Gaugne, Ronan; Nouviale, Florian; Gouranton, Valerie; Bossis, Bruno","Expressive potentials of motion capture in musical performance","","","","10.5281/zenodo.3672954","","The paper presents the electronic music performance project Vis Insita implementing the design of experimental instrumental interfaces based on optical motion capture technology with passive infrared markers (MoCap), and the analysis of their use in a real scenic presentation context. Because of MoCap's predisposition to capture the movements of the body, a lot of research and musical applications in the performing arts concern dance or the sonification of gesture. For our research, we wanted to move away from the capture of the human body to analyse the possibilities of a kinetic object handled by a performer, both in terms of musical expression, but also in the broader context of a multimodal scenic interpretation.","2019","2023-07-24 06:48:44","2023-07-24 06:48:44","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UZ6TATEZ","journalArticle","2020","Olsen, Taylor J.","Animation, Sonification, and Fluid-Time: A Visual-Audioizer Prototype","","","","10.5281/zenodo.4813230","","The visual-audioizer is a patch created in Max in which the concept of fluid-time animation techniques, in tandem with basic computer vision tracking methods, can be used as a tool to allow the visual time-based media artist to create music. Visual aspects relating to the animator's knowledge of motion, animated loops, and auditory synchronization derived from computer vision tracking methods, allow an immediate connection between the generated audio derived from visuals—becoming a new way to experience and create audio-visual media. A conceptual overview, comparisons of past/current audio-visual contributors, and a summary of the Max patch will be discussed. The novelty of practice-based animation methods in the field of musical expression, considerations of utilizing the visual-audioizer, and the future of fluid-time animation techniques as a tool of musical creativity will also be addressed.","2020","2023-07-24 06:48:44","2023-07-24 06:48:44","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YI9TZKCZ","journalArticle","2014","Hutchins, Charles; Ballweg, Holger; Knotts, Shelly; Hummel, Jonas; Roberts, Antonio","Soundbeam: A Platform for Sonyfing Web Tracking","","","","10.5281/zenodo.1178810","","Government spying on internet traffic has seemingly become ubiquitous. Not to be left out, the private sector tracks our online footprint via our ISP or with a little help from facebook. Web services, such as advertisement servers and Google track our progress as we surf the net and click on links. The Mozilla plugin, Lightbeam (formerly Collusion), shows the user a visual map of every site a surfer sends data to. A interconnected web of advertisers and other (otherwise) invisible data-gatherers quickly builds during normal usage. We propose modifying this plugin so that as the graph builds, its state is broadcast visa OSC. Members of BiLE will receive and interpret those OSC messages in SuperCollider and PD. We will act as a translational object in a process of live-sonification. The collected data is the material with which we will develop a set of music tracks based on patterns we may discover. The findings of our data collection and the developed music will be presented in the form of an audiovisual live performance. Snippets of collected text and URLs will both form the basis of our audio interpretation, but also be projected on to a screen, so an audience can voyeuristically experience the actions taken on their behalf by governments and advertisers. After the concert, all of the scripts and documentation related to the data collection and sharing in the piece will be posted to github under a GPL license.","2014","2023-07-24 06:48:44","2023-07-24 06:48:44","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WTIVWXK6","journalArticle","2014","Renaud, Alain; Charbonnier, Caecilia; Chagué, Sylvain","{3D}inMotion A Mocap Based Interface for Real Time Visualisation and Sonification of Multi-User Interactions","","","","10.5281/zenodo.1178915","","This paper provides an overview of a proposed demonstration of 3DinMotion, a system using real time motion capture of one or several subjects, which can be used in interactive audiovisual pieces and network performances. The skeleton of a subject is analyzed in real time and displayed as an abstract avatar as well as sonified based on mappings and rules to make the interplay experience lively and rewarding. A series of musical pieces have been composed for the interface following cueing strategies. In addition a second display, ``the prompter"" guides the users through the piece. 3DinMotion has been developed from scratch and natively, leading to a system with a very low latency, making it suitable for real time music interactions. In addition, 3DinMotion is fully compatible with the OpenSoundControl (OSC) protocol, allowing expansion to commonly used musical and sound design applications.","2014","2023-07-24 06:48:44","2023-07-24 06:48:44","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SDD7SYCN","journalArticle","2019","Sanyal, Shankha; Nag, Sayan; Banerjee, Archi; Sengupta, Ranjan; Ghosh, Dipak","Music of brain and music on brain: a novel EEG sonification approach","Cognitive Neurodynamics","","","10.1007/s11571-018-9502-4","","Can we hear the sound of our brain? Is there any technique which can enable us to hear the neuro-electrical impulses originating from the different lobes of brain? The answer to all these questions is YES. In this paper we present a novel method with which we can sonify the electroencephalogram (EEG) data recorded in ‘‘control’’ state as well as under the influence of a simple acoustical stimuli—a tanpura drone. The tanpura has a very simple construction yet the tanpura drone exhibits very complex acoustic features, which is generally used for creation of an ambience during a musical performance. Hence, for this pilot project we chose to study the nonlinear correlations between musical stimulus (tanpura drone as well as music clips) and sonified EEG data. Till date, there have been no study which deals with the direct correlation between a bio-signal and its acoustic counterpart and also tries to see how that correlation varies under the influence of different types of stimuli. This study tries to bridge this gap and looks for a direct correlation between music signal and EEG data using a robust mathematical microscope called Multifractal Detrended Cross Correlation Analysis (MFDXA). For this, we took EEG data of 10 participants in 2 min ‘‘control condition’’ (i.e. with white noise) and in 2 min ‘tanpura drone’ (musical stimulus) listening condition. The same experimental paradigm was repeated for two emotional music, ‘‘Chayanat’’ and ‘‘Darbari Kanada’’. These are well known Hindustani classical ragas which conventionally portray contrast emotional attributes, also verified from human response data. Next, the EEG signals from different electrodes were sonified and MFDXA technique was used to assess the degree of correlation (or the cross correlation coefficient cx) between the EEG signals and the music clips. The variation of cx for different lobes of brain during the course of the experiment provides interesting new information regarding the extraordinary ability of music stimuli to engage several areas of the brain significantly unlike any other stimuli (which engages specific domains only).","2019","2023-07-24 06:48:44","2023-07-24 06:48:44","","13-31","","1","13","","Cognitive Neurodynamics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ERIRXMGW","journalArticle","2018","Véron-Delor, Lauriane; Pinto, Serge; Eusebio, Alexandre; Velay, Jean-Luc; Danna, Jérémy; Aramaki, Mitsuko; Davies, Matthew E. P.; Kronland-Martinet, Richard; Ystad, Sølvi","Music and Musical Sonification for the Rehabilitation of Parkinsonian Dysgraphia: Conceptual Framework","Music Technology with Swing","","","","","Music has been shown to enhance motor control in patients with Parkinson’s disease (PD). Notably, musical rhythm is perceived as an external auditory cue that helps PD patients to better control movements. The rationale of such effects is that motor control based on auditory guidance would activate a compensatory brain network that minimizes the recruitment of the defective pathway involving the basal ganglia. Would associating music to movement improve its perception and control in PD? Musical sonification consists in modifying in real-time the playback of a preselected music according to some movement parameters. The validation of such a method is underway for handwriting in PD patients. When confirmed, this study will strengthen the clinical interest of musical sonification in motor control and (re)learning in PD.","2018","2023-07-24 06:48:44","2023-07-24 06:48:44","","312-326","","","11265","","Music Technology with Swing","","","","","","","","","","","","","","","Place: Cham Publisher: Springer International Publishing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LBAY7D3C","journalArticle","2016","Walus, Bartlomiej P.; Pauletto, Sandra; Mason-Jones, Amanda","Sonification and music as support to the communication of alcohol-related health risks to young people: Study design and results","Journal on Multimodal User Interfaces","","","10.1007/s12193-016-0220-0","","Excessive consumption of alcohol has been recognised as a significant risk factor impacting the health of young people. Effective communication of such risk is considered to be one key step to improve behaviour. We evaluated an innovative multimedia intervention that utilised audio (sonification—using sound to display data—and music) and interactivity to support the visual communication of alcohol health risk data. A 3-arm pilot experiment was undertaken. The trial measures included health knowledge, alcohol risk perception and user experience of the intervention. Ninetysix subjects participated in the experiment. At 1 month follow-up, alcohol knowledge and alcohol risk perception improved significantly in the whole sample. However, there was no difference between the intervention groups that experienced (1) visual presentation with interactivity (VI-Exp group) and, (2) visual presentation with audio (sonification and music) and interactivity (VAI-Exp group), when compared to the control group which experienced a (3) visual only presentation (V-Cont group). Participants reported enjoying the presentations and found them educational. The majority of participants indicated that the audio, music and sonification helped to convey the information well, and, although a larger sample size is needed to fully establish the effectiveness of the different interventions, this study provides a useful model for future similar studies.","2016","2023-07-24 06:48:44","2023-07-24 06:48:44","","235-246","","3","10","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YD5RQP6I","journalArticle","2012","Huang, Chih-Fang; Lu, Hsiang-Pin; Ren, Jenny","Algorithmic approach to sonification of classical Chinese poetry","Multimedia Tools and Applications","","","10.1007/s11042-011-0856-4","","The classical Chinese poetry is a remarkable form of art in traditional Chinese character. However, it is difficult for people who are unfamiliar with ancient Chinese to experience the artistic content of the poetry. In this study, a sonification scheme, Tx2Ms (Text-to-Music), is proposed to extract the poetry features between lines in verses; moreover, dynamics and interval relations are modeled to map those features to the movement of multi-dimensional musical elements such as durations. This conversion is based on poetry intonation and acoustic analysis of the pronunciations of poems; and a stochastic compositional algorithm is created by applying a Markov chain. In addition, the best pentatonic mode for a specific poem is recommended according to the formants analysis. Therefore, the sonification of classical Chinese poetry not only provides a novel way for people to appreciate Chinese poetry but also enriches the state of mind and imagery in the delivery process, and the experiment results show that the proposed system is successfully accepted by most people.","2012","2023-07-24 06:48:44","2023-07-24 06:48:44","","489-518","","2","61","","Multimedia Tools and Applications","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y2K8KTGS","journalArticle","2020","Newbold, Joseph; Gold, Nicolas E.; Bianchi-Berthouze, Nadia","Movement sonification expectancy model: leveraging musical expectancy theory to create movement-altering sonifications","Journal on Multimodal User Interfaces","","","10.1007/s12193-020-00322-2","","","2020","2023-07-24 06:48:44","2023-07-24 06:48:44","","153-166","","2","14","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"47IPPW3V","journalArticle","2008","Chemseddine, Maher; Noirhomme-Fraiture, Monique; Forbrig, Peter; Paternò, Fabio; Pejtersen, Annelise Mark","Complex and Dynamic Data Representation by Sonification","Human-Computer Interaction Symposium","","","","","So far, data representation has been based on visuals. The huge size of data verges on overuse of the visual capability. Thus, there is a need to reduce the almost exclusive use of visual techniques to represent data in order to increase our perception bandwidth. In this research, we aim to solve this problem by integrating the audio component. More precisely, we are interested in representing data using musical melodies. This paper presents a model for music elements based on human emotion, to express alert messages displayed by computer network monitoring.","2008","2023-07-24 06:48:44","2023-07-24 06:48:44","","195-200","","","272","","Human-Computer Interaction Symposium","","","","","","","","","","","","","","","Place: Boston, MA Publisher: Springer US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XD2XREWM","journalArticle","2012","Diniz, Nuno; Coussement, Pieter; Deweppe, Alexander; Demey, Michiel; Leman, Marc","An embodied music cognition approach to multilevel interactive sonification","Journal on Multimodal User Interfaces","","","10.1007/s12193-011-0084-2","","In this paper, a new conceptual framework and related implementation for interactive sonification is introduced. The conceptual framework consists of a combination of three components, namely, gestalt-based electroacoustic composition techniques (sound), user and body-centered spatial exploration (body), and corporeal mediation technology (tools), which are brought together within an existing paradigm of embodied music cognition. The implementation of the conceptual framework is based on an iterative process that involves the development of several use cases. Through this methodology, it is possible to investigate new approaches for structuring and to interactively explore multivariable data through sound.","2012","2023-07-24 06:48:44","2023-07-24 06:48:44","","211-219","","3","5","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AFQHGZ2S","journalArticle","2012","Gena, Peter","Apropos sonification: a broad view of data as music and sound","AI & SOCIETY","","","10.1007/s00146-011-0339-1","","Numbers have been identified with symbolic data forever. The profound association of both with acoustics, music, and sonic art from Pythagoras to current work is beyond reproach. Recently, sonification looks for ways to realize symbolic data (representing results or measurements) as well as “raw” data (signals, impulses, images, etc.) into compositions. In the strictest sense, everything in a computer is symbolic, that is, represented by 0s and 1s. In the arts, the digital age has broadened and enhanced the conceptual landscape not simply through its servitude to the creative process, but as its partner. However, there is a rich history of the use of data that no doubt has paved the way for many of today’s experiments including my own.","2012","2023-07-24 06:48:44","2023-07-24 06:48:44","","197-205","","2","27","","AI & SOCIETY","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J49A2HMA","journalArticle","2021","Polaczyk, Jakub; Croft, Katelyn; Cai, Yang; Ahram, Tareq Z.; Karwowski, Waldemar; Kalra, Jay","Compositional Sonification of Cybersecurity Data in a Baroque Style","Advances in Artificial Intelligence, Software and Systems Engineering","","","10.1007/978-3-030-80624-8_38","","Compositional sonification is a musical mapping algorithm that transforms a dataset into a soundscape for humans to hear numbers within a musical structure. In this study, we used three methods to sonify data: 1) a Baroque-style gestural mapping algorithm that emphasized timing and rhythm using an analog method of handwritten composition, 2) an electronic online pitch sequencer, and 3) the classical instrument Harp performing notated music. We investigated related sound mapping algorithms and the sensitivity of pattern representation of cybersecurity data, including the malware distribution network datasets. Our preliminary experiments showed that a Baroque-inspired timing element based on gestures plays a critical role in aural sonification. Using a sequencer with visualizers helped listeners grasp the patterns more effectively since they were able to watch and hear the results simultaneously. Performing the sonified data live on the harp helped make the compositions more relatable and thus connected with a broader audience. The harp also added a visual element which helped listeners identify patterns. We found that listeners were able to accurately identify the sonified data patterns in both the baroque-style analog handwritten compositions performed on harp and the compositions produced by the Online Sequencer. While non-musicians were able to answer questions about the data patterns they heard with a high percentage of accuracy, their results improved when visual elements were added.","2021","2023-07-24 06:48:45","2023-07-24 06:48:45","","304-312","","","","","Advances in Artificial Intelligence, Software and Systems Engineering","","","","","","","","","","","","","","","Publisher: Springer International Publishing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P338ABP8","journalArticle","2012","Eacott, John","Flood Tide: sonification as musical performance—an audience perspective","AI & SOCIETY","","","10.1007/s00146-011-0338-2","","The number of events and artifacts described as sonification has increased considerably in recent years with some works making a bridge between the representation of data and artistic expression. FloodTide which sonifies the flow of tidal water is such a work and has achieved a relatively high profile attracting good audiences for its 10 performances to date. It is not entirely obvious however what it is that attracts audiences and whether it is effective at representing the data being sonified. This paper aims to address these issues and is based on a discussion group in which these and other questions are considered.","2012","2023-07-24 06:48:45","2023-07-24 06:48:45","","189-195","","2","27","","AI & SOCIETY","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QILLJF6Q","journalArticle","2012","Varni, Giovanna; Dubus, Gaël; Oksanen, Sami; Volpe, Gualtiero; Fabiani, Marco; Bresin, Roberto; Kleimola, Jari; Välimäki, Vesa; Camurri, Antonio","Interactive sonification of synchronisation of motoric behaviour in social active listening to music with mobile devices","Journal on Multimodal User Interfaces","","","10.1007/s12193-011-0079-z","","This paper evaluates three different interactive sonifications of dyadic coordinated human rhythmic activity. An index of phase synchronisation of gestures was chosen as coordination metric. The sonifications are implemented as three prototype applications exploiting mobile devices: Sync’n’Moog, Sync’n’Move, and Sync’n’Mood. Sync’n’Moog sonifies the phase synchronisation index by acting directly on the audio signal and applying a nonlinear time-varying filtering technique. Sync’n’Move intervenes on the multi-track music content by making the single instruments emerge and hide. Sync’n’Mood manipulates the affective features of the music performance. The three sonifications were also tested against a condition without sonification.","2012","2023-07-24 06:48:45","2023-07-24 06:48:45","","157-173","","3","5","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MX9VF92W","journalArticle","2019","Polo, Antonio; Sevillano, Xavier","Musical Vision: an interactive bio-inspired sonification tool to convert images into music","Journal on Multimodal User Interfaces","","","10.1007/s12193-018-0280-4","","Musical Vision is a highly flexible, interactive and bio-inspired sonification tool that translates color images into harmonic polyphonic music by mimicking the human visual system in terms of its field of vision and photosensitive sensors. Putting the user at the center of the sonification process, Musical Vision allows the interactive design of fully configurable mappings between the color space and the MIDI instruments and audio pitch spaces to tailor the music rendering results to the application needs. Moreover, Musical Vision incorporates a harmonizer capable of introducing the necessary modifications to create melodies using harmonic chords. Above all else, Musical Vision is an extremely flexible system that the user can interactively configure to convert an image into either a few seconds or a several minutes long musical piece. Thus, it can be used, for instance, with trans-artistic purposes like the conversion of a painting into music, for augmenting vision with music, or for learning musical skills such as sol-fa. To evaluate the proposed sonification tool, we conducted a pilot user study, in which twelve volunteers were tested to interpret images containing geometric patterns from music rendered by Musical Vision. Results show that even those users with no musical education background were able to achieve nearly 70% accuracy in multiple choice tests after less than 25 min training. Moreover, users with some musical education were capable of accurately “drawing by ear” the images from no other stimuli than the sonifications.","2019","2023-07-24 06:48:45","2023-07-24 06:48:45","","231-243","","3","13","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4KK97ZVK","journalArticle","2012","Gresham-Lancaster, Scot","Relationships of sonification to music and sound art","AI & SOCIETY","","","10.1007/s00146-011-0337-3","","The definition of sonification has been reframed in recent years but remains somewhat in flux; the basic concepts and procedural flows have remained relatively unchanged. Recent definitions have focused on the objective the important uses of sonification in terms of scientific method. The full realization of the potential of the field must also include the craft and art of music composition. The author proposes examining techniques of sonification in a two-order framework: direct and procedural. The impact of new technologies and historical roots of that work argues that framing this broad topic should be in terms inclusive of scientific method and craftsmanship and art. The expressive use of sonic time-based data flows needs to be refined and expanded. The unexamined territory of how a broad-based population of listeners on a subjective, as well as objective level needs, have to be included in this new field.","2012","2023-07-24 06:48:45","2023-07-24 06:48:45","","207-212","","2","27","","AI & SOCIETY","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PDLDNNEL","journalArticle","2019","Maes, Pieter-Jan; Lorenzoni, Valerio; Six, Joren","The SoundBike: musical sonification strategies to enhance cyclists’ spontaneous synchronization to external music","Journal on Multimodal User Interfaces","","","10.1007/s12193-018-0279-x","","The spontaneous tendency of people to synchronize their movements to music is a powerful mechanism useful for the development of strategies for tempo adaptation of simple repetitive movements. In the current article, we contribute to such strategies—applied to cycling—by introducing a new strategy based on the sonification of cyclists’ motor rhythm. For that purpose, we developed the SoundBike, a stationary bike equipped with sensors that allows interactive sonification of cyclists’ motor rhythm using two distinct but compatible sonification methods. One is based on the principle of step sequencers, which are frequently used for electronic music production. The other is based on the Kuramoto model, allowing automatic and continuous phase alignment of beat-annotated music pieces to cyclists’ motor rhythm, i.e., pedal cadence. Apart from an in-depth presentation of the technical aspects of the SoundBike, we present an experimental study in which we investigated whether the SoundBike could enhance spontaneous synchronization of cyclists to external music. The results of this experiment suggest that sonification of cyclists’ motor rhythm may increase their tendency to synchronize to external music, and helps to keep a more stable pedal cadence, compared to the condition of having external music only (without sonification). Although the results are preliminary and should be followed-up by additional experiments to become more conclusive, SoundBike seems anyhow a promising interactive sonification device to assist motor learning and adaptation in the field of sports and motor rehabilitation.","2019","2023-07-24 06:48:45","2023-07-24 06:48:45","","155-166","","3","13","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IT6TE6ET","journalArticle","2012","Joy, Jerome","What NMSAT says about sonification","AI & SOCIETY","","","10.1007/s00146-011-0343-5","","This article presents a sample of references issuing directly from the existing NMSAT database. The method employed—that of systematically probing the database—reveals forms of sonification, but also hypothetical premises of sonification, covering the period from ancient times to the beginning of the twentieth century. The following are some of the categories of sonification that have emerged as a result of this search: Natural phenomenon & meteorology to sound (autophones); Image to sound; Text & communication to sound; Human & machine activities to sound (auditing); Localisation to sound (sonar); Architecture & geometry & abstract proportions to sound (scalization, transcription, & spatialization); Energy to sound; Human body to sound; Distance to sound (distance listening); Movement to sound (holophony, kynophony); and Interpreted observations to sound (naturalist music, transpositions & analogies, paraphrasing). The search also uncovered other principals and practices in the vicinity of sonification including: audification, auditing, auscultation, auralization, soniculation, transduction, mapping, earcons, auditory icons, sympathy, echometry, etc. It has been decided to summarise the results of « What NMSAT Says About Sonification » in this special issue of AI&Society, access to the unabridged version of article is available here: http://www.locusonus.org/sonification/.","2012","2023-07-24 06:48:45","2023-07-24 06:48:45","","233-244","","2","27","","AI & SOCIETY","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5DLUZJ9B","journalArticle","2019","Wolf, KatieAnna E.; Fiebrink, Rebecca","Personalised interactive sonification of musical performance data","Journal on Multimodal User Interfaces","","","10.1007/s12193-019-00294-y","","In this article, we describe methods and consequences for giving audience members interactive control over the real-time sonification of performer movement data in electronic music performance. We first briefly describe how to technically implement a musical performance in which each audience member can interactively construct and change their own individual sonification of performers’ movements, heard through headphones on a personal WiFi-enabled device, while also maintaining delay-free synchronization between performer movements and sound. Then, we describe two studies we conducted in the context of live musical performances with this technology. These studies have allowed us to examine how providing audience members with the ability to interactively sonify performer actions impacted their experiences, including their perceptions of their own role and engagement with the performance. These studies also allowed us to explore how audience members with different levels of expertise with sonification and sound, and different motivations for interacting, could be supported and influenced by different sonification interfaces. This work contributes to a better understanding of how providing interactive control over sonification may alter listeners’ experiences, of how to support everyday people in designing and using bespoke sonifications, and of new possibilities for musical performance and participation.","2019","2023-07-24 06:48:45","2023-07-24 06:48:45","","245-265","","3","13","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BWKB9GC9","journalArticle","2022","Frid, Emma; Bresin, Roberto","Perceptual Evaluation of Blended Sonification of Mechanical Robot Sounds Produced by Emotionally Expressive Gestures: Augmenting Consequential Sounds to Improve Non-verbal Robot Communication","International Journal of Social Robotics","","","10.1007/s12369-021-00788-4","","","2022","2023-07-24 06:48:45","2023-07-24 06:48:45","","357-372","","2","14","","International Journal of Social Robotics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FBVPNG7R","journalArticle","2020","Aldana Blanco, Andrea Lorena; Grautoff, Steffen; Hermann, Thomas","ECG sonification to support the diagnosis and monitoring of myocardial infarction","Journal on Multimodal User Interfaces","","","10.1007/s12193-020-00319-x","","","2020","2023-07-24 06:48:46","2023-07-24 06:48:46","","207-218","","2","14","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CXJHC7NZ","journalArticle","2021","Raglio, Alfredo; Panigazzi, Monica; Colombo, Roberto; Tramontano, Marco; Iosa, Marco; Mastrogiacomo, Sara; Baiardi, Paola; Molteni, Daniele; Baldissarro, Eleonora; Imbriani, Chiara; Imarisio, Chiara; Eretti, Laura; Hamedani, Mehrnaz; Pistarini, Caterina; Imbriani, Marcello; Mancardi, Gian Luigi; Caltagirone, Carlo","Hand rehabilitation with sonification techniques in the subacute stage of stroke","Scientific Reports","","","10.1038/s41598-021-86627-y","","After a stroke event, most survivors suffer from arm paresis, poor motor control and other disabilities that make activities of daily living difficult, severely affecting quality of life and personal independence. This randomized controlled trial aimed at evaluating the efficacy of a music-based sonification approach on upper limbs motor functions, quality of life and pain perceived during rehabilitation. The study involved 65 subacute stroke individuals during inpatient rehabilitation allocated into 2 groups which underwent usual care dayweek) respectively of standard upper extremity motor rehabilitation or upper extremity treatment with sonification techniques. The Fugl-Meyer Upper Extremity Scale, Box and Block Test and the Modified Ashworth Scale were used to perform motor assessment and the McGill Quality of Life-it and the Numerical Pain Rating Scale to assess quality of life and pain. The assessment was performed at baseline, after 2 weeks, at the end of treatment and at follow-up (1 month after the end of treatment). Total scores of the Fugl-Meyer Upper Extremity Scale (primary outcome measure) and hand and wrist sub scores, manual dexterity scores of the affected and unaffected limb in the Box and Block Test, pain scores of the Numerical Pain Rating Scale (secondary outcomes measures) significantly improved in the sonification group compared to the standard of care group (time*group interaction < 0.05). Our findings suggest that music-based sonification sessions can be considered an effective standardized intervention for the upper limb in subacute stroke rehabilitation.","2021","2023-07-24 06:48:46","2023-07-24 06:48:46","","7237","","1","11","","Scientific Reports","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2HBSBTCA","journalArticle","2012","Grond, Florian; Hermann, Thomas","Aesthetic strategies in sonification","AI & SOCIETY","","","10.1007/s00146-011-0341-7","","Sound can be listened to in various ways and with different intentions. Multiple factors influence how and what we perceive when listening to sound. Sonification, the acoustic representation of data, is in essence just sound. It functions as sonification only if we make sure to listen attentively in order to access the abstract information it contains. This is difficult to accomplish since sound always calls the listener’s attention to concrete—whether natural or musical—points of references. Important aspects determining how we listen to sonification are discussed in this paper: elicited sounds, repeated sounds, conceptual sounds, technologically mediated sounds, melodic sounds, familiar sounds, multimodal sounds and vocal sounds. We discuss how these aspects help the listener engage with the sound, but also how they can become points of reference in and of themselves. The various sonic qualities employed in sonification can potentially open but also risk closing doors to the accessibility and perceptibility of the sonified data.","2012","2023-07-24 06:48:46","2023-07-24 06:48:46","","213-222","","2","27","","AI & SOCIETY","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PWJN99J2","journalArticle","2010","Vogt, Katharina; Pirrò, David; Kobenz, Ingo; Höldrich, Robert; Eckel, Gerhard; Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","PhysioSonic - Evaluated Movement Sonification as Auditory Feedback in Physiotherapy","Auditory Display","","","10.1007/978-3-642-12439-6_6","","We detect human body movement interactively via a tracking system. This data is used to synthesize sound and transform sound files (music or text). A subject triggers and controls sound parameters with his or her movement within a pre-set range of motion. The resulting acoustic feedback enhances new modalities of perception and the awareness of the body movements. It is ideal for application in physiotherapy and other training contexts.","2010","2023-07-24 06:48:46","2023-07-24 06:48:46","","103-120","","","","","Auditory Display","","","","","","","","","","","","","","","Publisher: Springer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YED3IWSR","journalArticle","2012","Knees, Peter; Pohle, Tim; Widmer, Gerhard","Sound/tracks: artistic real-time sonification of train journeys","Journal on Multimodal User Interfaces","","","10.1007/s12193-011-0089-x","","We present an application of sonification in an artistic context, namely to augment the visual impressions of train journeys. While sonification and auditory displays are typically used as means to present data and to inform the user, our project sound/tracks aims at enhancing the visual experience of looking out of a moving train’s window at the passing landscape by adding a sound dimension. This allows for reflecting upon the visual impressions and deepening the state of contemplation. To this end, sound/tracks translates the perceived movement of the scenery and other visual impressions, such as passing trains, into music. The continuously changing view outside the window is captured with a camera and transformed into instantaneously played music. The application can be run on mobile phones with a built-in camera and on laptops with a Web-cam. The paper proposes and discusses different sonification approaches and presents different application scenarios.","2012","2023-07-24 06:48:46","2023-07-24 06:48:46","","87-93","","1","6","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2ISAXS4V","journalArticle","2017","Dyer, J. F.; Stapleton, P.; Rodger, M. W. M.","Advantages of melodic over rhythmic movement sonification in bimanual motor skill learning","Experimental Brain Research","","","10.1007/s00221-017-5047-8","","n important question for skill acquisition is whether and how augmented feedback can be designed to improve the learning of complex skills. Auditory information triggered by learners’ actions, movement sonification, can enhance learning of a complex bimanual coordination skill, specifically polyrhythmic bimanual shape tracing. However, it is not clear whether the coordination of polyrhythmic sequenced movements is enhanced by auditory-specified timing information alone or whether more complex sound mappings, such as melodic sonification, are necessary. Furthermore, while short-term retention of bimanual coordination performance has been shown with movement sonification training, longer term retention has yet to be demonstrated. In the present experiment, participants learned to trace a diamond shape with one hand while simultaneously tracing a triangle with the other to produce a sequenced 4:3 polyrhythmic timing pattern. Two groups of participants received real-time auditory feedback during training: melodic sonification (individual movements triggered a separate note of a melody) and rhythmic sonification (each movement triggered a percussive sound), while a third control group received no augmented feedback. Task acquisition and performance in immediate retention were superior in the melodic sonification group as compared to the rhythmic sonification and control group. In a 24-h retention phase, a decline in performance in the melodic sonification group was reversed by brief playback of the target pattern melody. These results show that melodic sonification of movement can provide advantages over augmented feedback which only provides timing information by better structuring the sequencing of timed actions, and also allow recovery of complex target patterns of movement after training. These findings have important implications for understanding the role of augmented perceptual information in skill learning, as well as its application to real-world training or rehabilitation scenarios.","2017","2023-07-24 06:48:47","2023-07-24 06:48:47","","3129-3140","","10","235","","Experimental Brain Research","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TFXC3FGZ","journalArticle","2019","Frid, Emma; Elblaus, Ludvig; Bresin, Roberto","Interactive sonification of a fluid dance movement: an exploratory study","Journal on Multimodal User Interfaces","","","10.1007/s12193-018-0278-y","","In this paper we present three different experiments designed to explore sound properties associated with fluid movement: (1) an experiment in which participants adjusted parameters of a sonification model developed for a fluid dance movement, (2) a vocal sketching experiment in which participants sketched sounds portraying fluid versus nonfluid movements, and (3) a workshop in which participants discussed and selected fluid versus nonfluid sounds. Consistent findings from the three experiments indicated that sounds expressing fluidity generally occupy a lower register and has less high frequency content, as well as a lower bandwidth, than sounds expressing nonfluidity. The ideal sound to express fluidity is continuous, calm, slow, pitched, reminiscent of wind, water or an acoustic musical instrument. The ideal sound to express nonfluidity is harsh, non-continuous, abrupt, dissonant, conceptually associated with metal or wood, unhuman and robotic. Findings presented in this paper can be used as design guidelines for future applications in which the movement property fluidity is to be conveyed through sonification.","2019","2023-07-24 06:48:47","2023-07-24 06:48:47","","181-189","","3","13","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3N4XMX8J","journalArticle","2009","Kildal, Johan; Gross, Tom; Gulliksen, Jan; Kotzé, Paula; Oestreicher, Lars; Palanque, Philippe; Prates, Raquel Oliveira; Winckler, Marco","Aspects of Auditory Perception and Cognition for Usable Display Resolution in Data Sonification","Human-Computer Interaction – INTERACT 2009","","","","","Sonification of data via the mapping of values to frequency of sound is an auditory data analysis technique commonly used to display graph information. The goal for any form of graph is to display numerical information with accuracy and neutrality while exploiting perceptual and cognitive processes. Conveying information in frequency of sound is subject to aspects of pitch perception, largely overlooked to date, that can influence these properties of auditory graphing. This paper identifies some of these aspects and describes potential design limitations and opportunities derived from the musical nature of auditory data representations.","2009","2023-07-24 06:48:47","2023-07-24 06:48:47","","467-470","","","5726","","Human-Computer Interaction – INTERACT 2009","","","","","","","","","","","","","","","Place: Berlin, Heidelberg Publisher: Springer Berlin Heidelberg","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SDL3FLVK","journalArticle","2017","Dyer, John; Stapleton, Paul; Rodger, Matthew","Transposing musical skill: sonification of movement as concurrent augmented feedback enhances learning in a bimanual task","Psychological Research","","","10.1007/s00426-016-0775-0","","Concurrent feedback provided during acquisition can enhance performance of novel tasks. The ‘guidance hypothesis’ predicts that feedback provision leads to dependence and poor performance in its absence. However, appropriately structured feedback information provided through sound (‘sonification’) may not be subject to this effect. We test this directly using a rhythmic bimanual shape-tracing task in which participants learned to move at a 4:3 timing ratio. Sonification of movement and demonstration was compared to two other learning conditions: (1) Sonification of task demonstration alone and (2) completely silent practice (control). Sonification of movement emerged as the most effective form of practice, reaching significantly lower error scores than control. Sonification of solely the demonstration, which was expected to benefit participants by perceptually unifying task requirements, did not lead to better performance than control. Good performance was maintained by participants in the Sonification condition in an immediate retention test without feedback, indicating that the use of this feedback can overcome the guidance effect. On a 24-h retention test, performance had declined and was equal between groups. We argue that this and similar findings in the feedback literature are best explained by an ecological approach to motor skill learning which places available perceptual information at the highest level of importance.","2017","2023-07-24 06:48:47","2023-07-24 06:48:47","","850-862","","4","81","","Psychological Research","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2IV565CL","journalArticle","2023","Toffa, O. K.; Mignotte, M.","Dataset and semantic based-approach for image sonification","Multimedia Tools and Applications","","","10.1007/s11042-022-12914-z","","This paper presents an image-audio dataset and a mid-level image sonification system that strives to help visually impaired users understand the semantic content of an image and access visual information via a combination of semantic audio and an easily decodable audio generated in real time, both triggered by sliding, taping, holding actions when the users explore the image on a touch screen or with a pointer. Firstly, we segmented the original image using a label fusion model and based on the user position in the image, a sonified signal is generated using musical notes and meaningful visual information within the active region like the color and the luminance, then the gradient and the texture. Secondly, we integrated the semantic understanding of the image into our model using DeepLab semantic segmentation of the image and created a dataset of audio and images aligned on the 20 classes of the PASCAL VOC 2012 dataset. The dataset of images are organized based on color, gradient, texture for low-level sonification and on semantic content with sounds for mid-level sonification. Thirdly, in order to provide both types of information in a complementary way, the slide, tap and hold actions of a touch screen are incorporated in the model. The semantic audio providing a brief description of the visual object is played on slide action, the generated signal with color details of the object on the tap action, gradient and texture of the object on hold action. Finally, we validated our sonification model on the provided dataset during a pilot study and the subjects were generally able to identify the objects in the image, the color of the objects and even provide a general description of the scene of the image. Our system could be useful to visually impaired persons in a photo sharing application using a smartphone or for painting art description in a digital museum.","2023","2023-07-24 06:48:47","2023-07-24 06:48:47","","1505-1518","","1","82","","Multimedia Tools and Applications","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZG225CF7","journalArticle","2022","Vishnevsky, Andrey; Abbas, Nadezda; Rocha, Alvaro; Adeli, Hojjat; Dzemyda, Gintautas; Moreira, Fernando","Sonification of Information Security Incidents in an Organization Using a Multistep Cooperative Game Model","Information Systems and Technologies","","","10.1007/978-3-031-04826-5_30","","This work is devoted to the development of computer attacks detection tool with a sound interface. Information security tools transmit visual signals to characterize the behavior of violators, but various types of conflict processes could be expressed by the plot of musical compositions. This approach could be used to encode the interaction of a protective computer system with an attacker in a harmonious way. In presented work, as an example of the conflict situation, was used a stochastic multistep cooperative game between the units of the attacked organization. An attempt to express audibly the stability of the cooperative game is made. It was realized with the help of the musical harmony of several voices of musical instruments. The program code for calculating the positional consistency of the proposed game-theoretic model and fragments of musical compositions for voicing the state of the protected organization are also proposed. Combining the basics of musical composition with game-theoretic modeling could offer a number of new possibilities for creating an ergonomic auditory human-computer interfaces.","2022","2023-07-24 06:48:47","2023-07-24 06:48:47","","306-314","","","","","Information Systems and Technologies","","","","","","","","","","","","","","","Publisher: Springer International Publishing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KRCWXP7P","journalArticle","2019","Rönnberg, Niklas","Sonification supports perception of brightness contrast","Journal on Multimodal User Interfaces","","","10.1007/s12193-019-00311-0","","In complex visual representations, there are several possible challenges for the visual perception that might be eased by adding sound as a second modality (i.e. sonification). It was hypothesized that sonification would support visual perception when facing challenges such as simultaneous brightness contrast or the Mach band phenomena. This hypothesis was investigated with an interactive sonification test, yielding objective measures (accuracy and response time) as well as subjective measures of sonification benefit. In the test, the participant’s task was to mark the vertical pixel line having the highest intensity level. This was done in a condition without sonification and in three conditions where the intensity level was mapped to different musical elements. The results showed that there was a benefit of sonification, with higher accuracy when sonification was used compared to no sonification. This result was also supported by the subjective measurement. The results also showed longer response times when sonification was used. This suggests that the use and processing of the additional information took more time, leading to longer response times but also higher accuracy. There were no differences between the three sonification conditions.","2019","2023-07-24 06:48:47","2023-07-24 06:48:47","","373-381","","4","13","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8BHJR453","journalArticle","2021","Plaisier, Heleen; Meagher, Thomas R.; Barker, Daniel","DNA sonification for public engagement in bioinformatics","BMC Research Notes","","","10.1186/s13104-021-05685-7","","","2021","2023-07-24 06:48:47","2023-07-24 06:48:47","","273","","1","14","","BMC Research Notes","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HI74VY9A","journalArticle","2006","Rutkowski, Tomasz M.; Vialatte, Francois; Cichocki, Andrzej; Mandic, Danilo P.; Barros, Allan Kardec; Gabrys, Bogdan; Howlett, Robert J.; Jain, Lakhmi C.","Auditory Feedback for Brain Computer Interface Management – An EEG Data Sonification Approach","Knowledge-Based Intelligent Information and Engineering Systems","","","10.1007/11893011_156","","An auditory feedback for Brain Computer Interface (BCI) applications is proposed. This is achieved based on the so-called sonification of the mental states of humans, captured by Electro-Encephalogram (EEG) recordings. Two time-frequency signal decomposition techniques, the Bump Modelling and Empirical Mode Decomposition (EMD), are used to map the EEG recordings onto musical scores. This auditory feedback proves to have extremely high potential in the development of on-line BCI interfaces. Examples based on the responses from visual stimuli support the analysis.","2006","2023-07-24 06:48:48","2023-07-24 06:48:48","","1232-1239","","","","","Knowledge-Based Intelligent Information and Engineering Systems","","","","","","","","","","","","","","","Publisher: Springer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XZQH2VKH","journalArticle","2012","Gresham-Lancaster, Scot","Waveguide synthesis for sonification of distributed sensor arrays","AI & SOCIETY","","","10.1007/s00146-011-0357-z","","A decade of work is outlined based on the use of sensors on plants that are used to change the parameters of a fixed rotation of overlapping pitches. The use of waveguide, physical modeling synthesis, allows the repeated music figures to be changed in timbral space in real time in a discernable set of ongoing parameter mapping from a large data set being generated by various biological and atmospheric sensors.","2012","2023-07-24 06:48:48","2023-07-24 06:48:48","","289-292","","2","27","","AI & SOCIETY","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EX9VKNX2","journalArticle","2023","Torresan, Christian; Bernardes, Gilberto; Caetano, Elsa; Restivo, Teresa; Brooks, Anthony L.","The Singing Bridge: Sonification of a Stress-Ribbon Footbridge","ArtsIT, Interactivity and Game Creation","","","","","Stress-ribbon footbridges are often prone to excessive vibrations induced by environmental phenomena (e.g., wind and rain) and human actions (e.g., walking and jumping) and their liveliness is strongly associated with their slenderness. In earlier studies, multiple dynamic responses of a stress-ribbon footbridge were observed on the campus of the Faculty of Engineering of the University of Porto (FEUP) in Portugal. Although extreme vibrations have never been reported, vertical oscillations are clearly perceptible under pedestrian excitement. While monitoring the bridge, the technology revealed physical phenomena that are invisible to humans. This project aims to adopt sonification techniques as a compositional tool to create a sonic manifestation that shows the dynamic response of the bridge. In this study, two different sonification techniques (audification and parameter mapping) were used to extrapolate the same phenomena using different strategies. For what concerns sound synthesis, the first technique used an FM synthesizer, while the second one used external VSTs and generative approaches to create a compelling musical sonification. In order to evaluate the proposed sonification techniques’ reliability, an online listening test was conducted to assess the three main dimensions of a collected dataset: the number of people crossing the bridge, their walking speed, and the steadiness of their pace. Respondents were required to complete both a blind test and one after a short training to assess the intuitiveness and reliability of both methods. According to the results, it is clear that the training significantly improves the participants’ accuracy in identifying the correct categories. In fact, almost all values have increased after the short training. Therefore, this suggests that the success of both sonification techniques could improve significantly with deeper training. Additionally, the overall trend shows parameter mapping sonification as a more intuitive and precise technique than audification.","2023","2023-07-24 06:48:48","2023-07-24 06:48:48","","359-373","","","479","","ArtsIT, Interactivity and Game Creation","","","","","","","","","","","","","","","Place: Cham Publisher: Springer Nature Switzerland","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I3ZKJ9MP","journalArticle","2019","Lorenzoni, Valerio; Van Den Berghe, Pieter; Maes, Pieter-Jan; De Bie, Tijl; De Clercq, Dirk; Leman, Marc","Design and validation of an auditory biofeedback system for modification of running parameters","Journal on Multimodal User Interfaces","","","10.1007/s12193-018-0283-1","","Real-time auditory feedback during sports activities is becoming increasingly popular in view of opportunities for monitoring and movement (re)training in ecological environments. However, the design of an effective feedback strategy is difficult. In this paper, we present a methodical approach to the design of an auditory feedback strategy for running gait modification of recreational runners, using distortion of a musical baseline. First tests were conducted to select the best performing auditory distortion signal in terms of clarity and level perception, and to derive the relative perception curve. This was found to be pink noise with an exponential response curve. Further tests were carried out to determine the just noticeable difference of this signal in actual running conditions. Finally, validation tests were performed to examine if the real-time auditory biofeedback, combined with music, could alter the runner’s steps per minute (SPM) during treadmill-based running. The results show that our sonification strategy can alter the mean running SPM in a clear and non-disturbing way, and that our noise-based continuous feedback approach performs better than standard verbal instructions. Even though some of the participants did not respond effectively to the feedback, a large majority of the participants rated the feedback system as pleasant and indicated that they would use such system to improve their running style.","2019","2023-07-24 06:48:48","2023-07-24 06:48:48","","167-180","","3","13","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XLX8XTVG","journalArticle","2014","Braund, Edward; Miranda, Eduardo; Aramaki, Mitsuko; Derrien, Olivier; Kronland-Martinet, Richard; Ystad, Sølvi","Music with Unconventional Computing: A System for Physarum Polycephalum Sound Synthesis","Sound, Music, and Motion","","","","","The field of computer music is evolving in tandem with advances in computer science. Our research is interested in how the developing field of unconventional computation may provide new pathways for music and music technologies. In this paper we present the development of a system for harnessing the biological computing substrate Physarum Polycephalum for sonification. Physarum Polycephalum is a large single cell with a myriad of diploid nuclei, which moves like a giant amoeba in its pursuit for food. The organism is amorphous, and although without a brain or any serving centre of control, can respond to the environmental conditions that surround it.","2014","2023-07-24 06:48:48","2023-07-24 06:48:48","","175-189","","","8905","","Sound, Music, and Motion","","","","","","","","","","","","","","","Place: Cham Publisher: Springer International Publishing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EHAY2Z2X","journalArticle","2023","Fink, Thomas; Akdag Salah, Alkim Almila; Johnson, Colin; Rodríguez-Fernández, Nereida; Rebelo, Sérgio M.","Extending the Visual Arts Experience: Sonifying Paintings with AI","Artificial Intelligence in Music, Sound, Art and Design","","","10.1007/978-3-031-29956-8_7","","Sonification of visual information is a relatively new research line that aims to create a new way to access and experience visual displays, especially for the visually impaired. When applied to artworks, sonification needs to translate the aesthetic experience as well. This is attempted via a handful studies in the literature, where most of the transformation and music generation is done manually, or only by using the low level visual features of artworks. In this paper, we present a sonification model that uses both low level and high level features such as color, edge information, saliency, object and scene detection to create a pleasant and descriptive sonification of artworks with the use of a fully automatic pipeline. The results of the model are tested via interviews done with experts in music theory and generative music models. We found a high agreement among experts for the evaluation of a small set of sonified paintings. Addition of high level features such as sounds extracted from the scene played a big role in this. Among the challenges observed during the interviews was the need to add emotion and mood information as well as semantic information to the sonification in order to create more descriptive melodies and sounds. The complexity and ambiguity of the visual information generated the most disagreement among experts both in their interpretation of the paintings as well as their sonifications.","2023","2023-07-24 06:48:48","2023-07-24 06:48:48","","100-116","","","","","Artificial Intelligence in Music, Sound, Art and Design","","","","","","","","","","","","","","","Publisher: Springer Nature Switzerland","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KJN4AIYV","journalArticle","2021","Iber, Michael; Lechner, Patrik; Jandl, Christian; Mader, Manuel; Reichmann, Michael","Auditory augmented process monitoring for cyber physical production systems","Personal and Ubiquitous Computing","","","10.1007/s00779-020-01394-3","","","2021","2023-07-24 06:48:48","2023-07-24 06:48:48","","691-704","","4","25","","Personal and Ubiquitous Computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7XYZG5LH","journalArticle","2017","Temple, Mark D.","An auditory display tool for DNA sequence analysis","BMC Bioinformatics","","","10.1186/s12859-017-1632-x","","Background: DNA Sonification refers to the use of an auditory display to convey the information content of DNA sequence data. Six sonification algorithms are presented that each produce an auditory display. These algorithms are logically designed from the simple through to the more complex. Three of these parse individual nucleotides, nucleotide pairs or codons into musical notes to give rise to 4, 16 or 64 notes, respectively. Codons may also be parsed degenerately into 20 notes with respect to the genetic code. Lastly nucleotide pairs can be parsed as two separate frames or codons can be parsed as three reading frames giving rise to multiple streams of audio. Results: The most informative sonification algorithm reads the DNA sequence as codons in three reading frames to produce three concurrent streams of audio in an auditory display. This approach is advantageous since start and stop codons in either frame have a direct affect to start or stop the audio in that frame, leaving the other frames unaffected. Using these methods, DNA sequences such as open reading frames or repetitive DNA sequences can be distinguished from one another. These sonification tools are available through a webpage interface in which an input DNA sequence can be processed in real time to produce an auditory display playable directly within the browser. The potential of this approach as an analytical tool is discussed with reference to auditory displays derived from test sequences including simple nucleotide sequences, repetitive DNA sequences and coding or non-coding genes. Conclusion: This study presents a proof-of-concept that some properties of a DNA sequence can be identified through sonification alone and argues for their inclusion within the toolkit of DNA sequence browsers as an adjunct to existing visual and analytical tools.","2017","2023-07-24 06:48:48","2023-07-24 06:48:48","","221","","1","18","","BMC Bioinformatics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8PV9K259","journalArticle","2015","Braund, Edward; Miranda, Eduardo; Johnson, Colin; Carballal, Adrian; Correia, João","Music with Unconventional Computing: Towards a Step Sequencer from Plasmodium of Physarum Polycephalum","Evolutionary and Biologically Inspired Music, Sound, Art and Design","","","","","The field of computer music has evolved in tandem with advances made in computer science. We are interested in how the developing field of unconventional computation may provide new pathways for music and related technologies. In this paper, we outline our initial work into harnessing the behaviour of the biological computing substrate Physarum polycephalum for a musical step sequencer. The plasmodium of Physarum polycephalum is an amorphous unicellular organism, which moves like a giant amoeba as it navigates its environment for food. Our research manipulates the organism’s route-efficient propagation characteristics in order to create a growth environment for musical/sound arrangement. We experiment with this device in two different scenarios: sample triggering and MIDI note triggering using sonification techniques.","2015","2023-07-24 06:48:48","2023-07-24 06:48:48","","15-26","","","9027","","Evolutionary and Biologically Inspired Music, Sound, Art and Design","","","","","","","","","","","","","","","Place: Cham Publisher: Springer International Publishing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZZSVTVGU","journalArticle","2019","Ezquerro, L.; Simón, J. L.","Geomusic as a New Pedagogical and Outreach Resource: Interpreting Geoheritage with All the Senses","Geoheritage","","","10.1007/s12371-019-00364-3","","The scientific, rational approach to the knowledge of Earth can be complemented and enhanced with an emotional approach by means of arts. Sonification of sedimentary series, by converting distinct lithology, facies or geochemical parameters into notes, and bed thickness into duration of sounds, provides a new viewpoint on both their sequential features and the cultural meaning of geoheritage. A total of 14 musical compositions have been achieved according to that procedure, based on successions of diverse ages and sedimentary environments within the Iberian Peninsula. Some of these successions exhibit cyclic features that have been analyzed by a number of authors. Cyclostratigraphy shows how certain sedimentary patterns can reveal climatic oscillations related to periodic variations of Earth orbital cycles. Geomusic elaborated from sonification of such sedimentary cycles could be therefore linked with Music of the Spheres postulated by Pythagoras in ancient Greece. Its hidden message deals with asking for a New Culture of Earth, for a renewed, friendly relationship with our planet. Its applied development could extend to soundtracks of scientific documentaries, background music at museums or geoparks, or performances at outreach events, or as a motivating factor in Earth Sciences learning.","2019","2023-07-24 06:48:49","2023-07-24 06:48:49","","1187-1198","","3","11","","Geoheritage","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P4F8JQ4U","journalArticle","2021","Buongiorno Nardelli, Marco; Kronland-Martinet, Richard; Ystad, Sølvi; Aramaki, Mitsuko","MUSICNTWRK: Data Tools for Music Theory, Analysis and Composition","Perception, Representations, Image, Sound, Music","","","","","We present the API for MUSICNTWRK, a python library for pitch class set and rhythmic sequences classification and manipulation, the generation of networks in generalized music and sound spaces, deep learning algorithms for timbre recognition, and the sonification of arbitrary data. The software is freely available under GPL 3.0 and can be downloaded at www.musicntwrk.com.","2021","2023-07-24 06:48:49","2023-07-24 06:48:49","","190-215","","","12631","","Perception, Representations, Image, Sound, Music","","","","","","","","","","","","","","","Place: Cham Publisher: Springer International Publishing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RVMC3LYV","journalArticle","2018","O’Brien, Benjamin; Juhas, Brett; Bieńkiewicz, Marta; Pruvost, Laurent; Buloup, Frank; Bringnoux, Lionel; Bourdin, Christophe; Aramaki, Mitsuko; Davies, Matthew E. P.; Kronland-Martinet, Richard; Ystad, Sølvi","Considerations for Developing Sound in Golf Putting Experiments","Music Technology with Swing","","","","","This chapter presents the core interests and challenges of using sound for learning motor skills and describes the development of sonification techniques for three separate golf-putting experiments. These studies are part of the ANR SoniMove project, which aims to develop new Human Machine Interfaces (HMI) that provide gestural control of sound in the areas of sports and music. After a brief introduction to sonification and sound-movement studies, the following addresses the ideas and sound synthesis techniques developed for each experiment.","2018","2023-07-24 06:48:49","2023-07-24 06:48:49","","338-358","","","11265","","Music Technology with Swing","","","","","","","","","","","","","","","Place: Cham Publisher: Springer International Publishing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2ZXKI3S7","journalArticle","2005","Hepting, Daryl H.; Gerhard, David; Wiil, Uffe Kock; Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","Collaborative Computer-Aided Parameter Exploration for Music and Animation","Computer Music Modeling and Retrieval","","","","","Although many artists have worked to create associations between music and animation, this has traditionally be done by developing one to suit the pre-existing other, as in visualization or sonification. The approach we employ in this work is to enable the simultaneous development of both music and sound from a common and rather generic central parameter variation, which may simply indicate a structure for periodic repetitions. This central parameter variation is then simultaneously mapped to appropriate musical and graphical variables by the musician and the animator, thereby contributing their own interpretations. The result of this mapping is then rendered in an intermediate form where music and animation are allowed to iteratively influence each other. The main piece of software in this development is the system which allows exploration of parameter mappings. The software interface allows both musician and animator to meaningfully experiment with the other’s mappings since the interface permits access in a common form, without requiring additional skills to interpret.","2005","2023-07-24 06:48:49","2023-07-24 06:48:49","","158-172","","","3310","","Computer Music Modeling and Retrieval","","","","","","","","","","","","","","","Place: Berlin, Heidelberg Publisher: Springer Berlin Heidelberg","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EK9V54V6","journalArticle","2022","Hamilton-Fletcher, Giles; Alvarez, James; Obrist, Marianna; Ward, Jamie","SoundSight: a mobile sensory substitution device that sonifies colour, distance, and temperature","Journal on Multimodal User Interfaces","","","10.1007/s12193-021-00376-w","","","2022","2023-07-24 06:48:49","2023-07-24 06:48:49","","107-123","","1","16","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IPKB5GR8","journalArticle","2013","Schmitz, Gerd; Mohammadi, Bahram; Hammer, Anke; Heldmann, Marcus; Samii, Amir; Münte, Thomas F; Effenberg, Alfred O","Observation of sonified movements engages a basal ganglia frontocortical network","BMC Neuroscience","","","10.1186/1471-2202-14-32","","","2013","2023-07-24 06:48:49","2023-07-24 06:48:49","","32","","1","14","","BMC Neuroscience","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PUCP6F4A","journalArticle","2007","DeWitt, Anna; Bresin, Roberto; Paiva, Ana C. R.; Prada, Rui; Picard, Rosalind W.; Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","Sound Design for Affective Interaction","Affective Computing and Intelligent Interaction","","","","","Different design approaches contributed to what we see today as the prevalent design paradigm for Human Computer Interaction; though they have been mostly applied to the visual aspect of interaction. In this paper we presented a proposal for sound design strategies that can be used in applications involving affective interaction. For testing our approach we propose the sonification of the Affective Diary, a digital diary with focus on emotions, affects, and bodily experience of the user. We applied results from studies in music and emotion to sonic interaction design. This is one of the first attempts introducing different physics-based models for the real-time complete sonification of an interactive user interface in portable devices.","2007","2023-07-24 06:48:49","2023-07-24 06:48:49","","523-533","","","4738","","Affective Computing and Intelligent Interaction","","","","","","","","","","","","","","","Place: Berlin, Heidelberg Publisher: Springer Berlin Heidelberg","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WAMDLCVY","journalArticle","2019","Ghai, Shashank; Ghai, Ishan","Effects of (music-based) rhythmic auditory cueing training on gait and posture post-stroke: A systematic review & dose-response meta-analysis","Scientific Reports","","","10.1038/s41598-019-38723-3","","Gait dysfunctions are common post-stroke. Rhythmic auditory cueing has been widely used in gait rehabilitation for movement disorders. However, a consensus regarding its influence on gait and postural recovery post-stroke is still warranted. A systematic review and meta-analysis was performed to analyze the effects of auditory cueing on gait and postural stability post-stroke. Nine academic databases were searched according to PRISMA guidelines. The eligibility criteria for the studies were a) studies were randomized controlled trials or controlled clinical trials published in English, German, Hindi, Punjabi or Korean languages b) studies evaluated the effects of auditory cueing on spatiotemporal gait and/or postural stability parameters post-stroke c) studies scored ≥4 points on the PEDro scale. Out of 1,471 records, 38 studies involving 968 patients were included in this present review. The review and meta-analyses revealed beneficial effects of training with auditory cueing on gait and postural stability. A training dosage of 20–45 minutes session, for 3–5 times a week enhanced gait performance, dynamic postural stability i.e. velocity (Hedge’s g: 0.73), stride length (0.58), cadence (0.75) and timed-up and go test (−0.76). This review strongly recommends the incorporation of rhythmic auditory cueing based training in gait and postural rehabilitation, post-stroke.","2019","2023-07-24 06:48:50","2023-07-24 06:48:50","","2183","","1","9","","Scientific Reports","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3DD8XBZ6","journalArticle","2010","Sobue, Shin-ichi; Araki, Hiroshi; Tazawa, Seiichi; Noda, Hirotomo; Kamiya, Izumi; Yamamoto, Aya; Fujita, Takeo; Higashiizumi, Ichiro; Okumura, Hayato; Elleithy, Khaled","An Application of Lunar GIS with Visualized and Auditory Japan’s Lunar Explorer “Kaguya” Data","Advanced Techniques in Computing Sciences and Software Engineering","","","","","This paper describes an application of a geographical information system with visualized and sonification lunar remote sensing data provided by Japan’s lunar explorer (SELENE “KAGUYA”). Web based GIS is a very powerful tool which lunar scientists can use to visualize and access remote sensing data with other geospatial information. We discuss enhancement of the pseudo-colored visual map presentation of lunar topographical altimetry data derived from LALT and the map of the data to several sound parameters (Interval, harmony, and tempo). This paper describes an overview of this GIS with a sonification system, called “Moonbell”.","2010","2023-07-24 06:48:50","2023-07-24 06:48:50","","159-163","","","","","Advanced Techniques in Computing Sciences and Software Engineering","","","","","","","","","","","","","","","Place: Dordrecht Publisher: Springer Netherlands","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N4UQWNZJ","journalArticle","2023","Sørensen, Vibeke; Lansing, J. Stephen","Art, technology and the Internet of Living Things","AI & SOCIETY","","","10.1007/s00146-023-01667-4","","Intelligence augmentation was one of the original goals of computing. Artificial Intelligence (AI) inherits this project and is at the leading edge of computing today. Computing can be considered an extension of brain and body, with mathematical prowess and logic fundamental to the infrastructure of computing. Multimedia computing—sensing, analyzing, and translating data to and from visual images, animation, sound and music, touch and haptics, as well as smell—is based on our human senses and is now commonplace. We use data visualization and sonification, as well as data mining and analysis, to sort through the complexity and vast volume of data coming from the world inside and around us. It helps us ‘see’ in new ways. We can think of this capacity as a new kind of “digital glasses”. The Internet of Living Things (IOLT) is potentially an even more profound extension of ourselves to the world: a network of electronic devices embedded into objects, but now with subcutaneous, ingestible devices, and embedded sensors that include people and other living things. Like the Internet of Things (IOT), living things are connected; we call those connections “ecology”. As the IOT becomes increasingly synonymous with the IOLT, the question of ethics that is at the centre of aesthetics and the arts will move to the forefront of our experience of and regard for the world in and around us.","2023","2023-07-24 06:48:50","2023-07-24 06:48:50","","","","","","","AI & SOCIETY","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W8SCK4NV","journalArticle","2014","Houix, Olivier; Misdariis, Nicolas; Susini, Patrick; Bevilacqua, Frédéric; Gutierrez, Florestan; Aramaki, Mitsuko; Derrien, Olivier; Kronland-Martinet, Richard; Ystad, Sølvi","Sonically Augmented Artifacts: Design Methodology Through Participatory Workshops","Sound, Music, and Motion","","","","","Participatory workshops have been organized within the framework of the ANR project Legos that concerns gesture-sound interactive systems. These workshops addressed both theoretical issues and experimentation with prototypes. The first goal was to stimulate new ideas related to the control of everyday objects using sound feedback, and then, to create and experiment with new sonic augmented objects. The second aim was educational. We investigated how sonic interaction design can be introduced to people without backgrounds in sound and music. We present in this article an overview of three workshops. The first workshop focused on the analysis and the possible sonification of everyday objects. New usage scenarios were obtained and tested. The second workshop focused on sound metaphor, questioning the relationship between sound and gesture. The last one was a workshop organized during a summer school for students. During these workshops, we experimented a cycle of design process: analysis, creation and testing.","2014","2023-07-24 06:48:50","2023-07-24 06:48:50","","20-40","","","8905","","Sound, Music, and Motion","","","","","","","","","","","","","","","Place: Cham Publisher: Springer International Publishing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JBVGJVXU","journalArticle","2014","Han, Yoon Chung; Han, Byeong-jun","Virtual pottery: a virtual 3D audiovisual interface using natural hand motions","Multimedia Tools and Applications","","","10.1007/s11042-013-1382-3","","In this paper, we present our approach towards designing and implementing a virtual 3D sound sculpting interface that creates audiovisual results using hand motions in real time. In the interface “Virtual Pottery,” we use the metaphor of pottery creation in order to adopt the natural hand motions to 3D spatial sculpting. Users can create their own pottery pieces by changing the position of their hands in real time, and also generate 3D sound sculptures based on pre-existing rules of music composition. The interface of Virtual Pottery can be categorized by shape design and camera sensing type. This paper describes how we developed the two versions of Virtual Pottery and implemented the technical aspects of the interfaces. Additionally, we investigate the ways of translating hand motions into musical sound. The accuracy of the detection of hand motions is crucial for translating natural hand motions into virtual reality. According to the results of preliminary evaluations, the accuracy of both motion-capture tracking system and portable depth sensing camera is as high as the actual data. We carried out user studies, which took into account information about the two exhibitions along with the various ages of users. Overall, Virtual Pottery serves as a bridge between the virtual environment and traditional art practices, with the consequence that it can lead to the cultivation of the deep potential of virtual musical instruments and future art education programs.","2014","2023-07-24 06:48:50","2023-07-24 06:48:50","","917-933","","2","73","","Multimedia Tools and Applications","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PPEPV6QP","journalArticle","2022","Kania, Damian; Szurmik, Tomasz; Bibrowicz, Karol; Romaniszyn-Kania, Patrycja; Czak, Mirosław; Mańka, Anna; Rosiak, Maria; Turner, Bruce; Pollak, Anita; Mitas, Andrzej W.; Pietka, Ewa; Badura, Pawel; Kawa, Jacek; Wieclawek, Wojciech","The Effect of Therapeutic Commands on the Teaching of Maintaining Correct Static Posture","Information Technology in Biomedicine","","","","","The article presents the results of a preliminary study analy-sing the physiological parameters obtained during exercises that teach the patient’s correct body posture while sitting. Electrodermal activity (EDA), blood volume pulse (BVP), and electromyographic (EMG) signals were recorded and analysed during the training process for position shaping. A music preference and musicality questionnaire was carried out before the study. The JAWS questionnaire was completed twice by the respondent, before and after exercises. The physiotherapists provided instructions with respect to the stimulation of the autonomic nervous system, observed in EDA, heart rate and the subsequent motor units. While performing the exercises, the subjects felt positive emotions, which can be perceived as a positive experience for the probands and suggests their willingness to learn and maintain correct body posture while sitting. The sonification of the therapist’s commands and their sonic emotional content is further researched.","2022","2023-07-24 06:48:50","2023-07-24 06:48:50","","393-405","","","1429","","Information Technology in Biomedicine","","","","","","","","","","","","","","","Place: Cham Publisher: Springer International Publishing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CAUIHIXL","journalArticle","2012","Grond, Florian","Safety Certificate: an audification performance of high-speed trains","AI & SOCIETY","","","10.1007/s00146-011-0351-5","","Safety Certificate is a musical performance based on sensor data from high-speed trains. The original purpose of this data is to provide a basis for the assessments of the mechanical aspects of train safety. In this performance, the data, which represents dynamical processes below the audible range, are converted into sound through audification. The sound that is generated live during the performance is manipulated through the Manta control interface, which allows for the convenient layering of 48 different timbres. Safety Certificate was premiered at Seconde Nature in Aix-en-Provence in March 2010 during the Sonification symposium–What, Where, How, Why, organized by Locus Sonus. The following short article gives details about the data, the audification technique, use of the control interface, and the musical structure of the performance.","2012","2023-07-24 06:48:50","2023-07-24 06:48:50","","293-295","","2","27","","AI & SOCIETY","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TPMBPHQX","journalArticle","2021","Takabatake, Kazuhiko; Kunii, Naoto; Nakatomi, Hirofumi; Shimada, Seijiro; Yanai, Kei; Takasago, Megumi; Saito, Nobuhito","Musical Auditory Alpha Wave Neurofeedback: Validation and Cognitive Perspectives","Applied Psychophysiology and Biofeedback","","","10.1007/s10484-021-09507-1","","","2021","2023-07-24 06:48:51","2023-07-24 06:48:51","","323-334","","4","46","","Applied Psychophysiology and Biofeedback","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BB94SWEA","journalArticle","2015","Sun, Yuanjing; Jeon, Myounghoon; Marcus, Aaron","Lyricon (Lyrics + Earcons) Improves Identification of Auditory Cues","Design, User Experience, and Usability: Users and Interactions","","","","","Auditory researchers have developed various non-speech cues in designing auditory user interfaces. A preliminary study of “lyricons” (lyrics + earcons [1]) has provided a novel approach to devising auditory cues in electronic products, by combining the concurrent two layers of musical speech and earcons (short musical motives). An experiment on sound-function meaning mapping was conducted between earcons and lyricons. It demonstrated that lyricons significantly more enhanced the relevance between the sound and the meaning compared to earcons. Further analyses on error type and confusion matrix show that lyricons showed a higher identification rate and a shorter mapping time than earcons. Factors affecting auditory cue identification and application directions of lyricons are discussed.","2015","2023-07-24 06:48:51","2023-07-24 06:48:51","","382-389","","","9187","","Design, User Experience, and Usability: Users and Interactions","","","","","","","","","","","","","","","Place: Cham Publisher: Springer International Publishing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8HUBCEH3","journalArticle","2018","Black, David; Hahn, Horst K.; Kikinis, Ron; Wårdell, Karin; Haj-Hosseini, Neda","Auditory display for fluorescence-guided open brain tumor surgery","International Journal of Computer Assisted Radiology and Surgery","","","10.1007/s11548-017-1667-5","","","2018","2023-07-24 06:48:51","2023-07-24 06:48:51","","25-35","","1","13","","International Journal of Computer Assisted Radiology and Surgery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DMJUBQWS","journalArticle","2009","Bologna, Guido; Deville, Benoît; Pun, Thierry; Mira, José; Ferrández, José Manuel; Álvarez, José R.; De La Paz, Félix; Toledo, F. Javier","Blind Navigation along a Sinuous Path by Means of the See ColOr Interface","Bioinspired Applications in Artificial and Natural Computation","","","","","The See ColOr interface transforms a small portion of a coloured video image into sound sources represented by spatialized musical instruments. This interface aims at providing visually impaired people with a capability of perception of the environment. In this work, the purpose is to verify the hypothesis that it is possible to use sounds from musical instruments to replace colour. Compared to state of the art devices, a quality of the See ColOr interface is that it allows the user to receive a feed-back auditory signal from the environment and its colours, promptly. An experiment based on a head mounted camera has been performed. Specifically, this experiment is related to outdoor navigation for which the purpose is to follow a sinuous path. Our participants successfully went along a red serpentine path for more than 80 meters.","2009","2023-07-24 06:48:51","2023-07-24 06:48:51","","235-243","","","5602","","Bioinspired Applications in Artificial and Natural Computation","","","","","","","","","","","","","","","Place: Berlin, Heidelberg Publisher: Springer Berlin Heidelberg","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FJY8WYYQ","journalArticle","2014","Kiriella, Dawpadee B.; Kumari, Shyama C.; Ranasinghe, Kavindu C.; Jayaratne, Lakshman","Music Training Interface for Visually Impaired through a Novel Approach to Optical Music Recognition","GSTF Journal on Computing (JoC)","","","10.7603/s40601-013-0045-6","","","2014","2023-07-24 06:48:51","2023-07-24 06:48:51","","45","","4","3","","GSTF Journal on Computing (JoC)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3RJQFKNN","journalArticle","1998","Rigas, Dimitrios I.; Alty, James L.; Johnson, Hilary; Nigay, Lawrence; Roast, Christopher","How Can Multimedia Designers Utilize Timbre?","People and Computers XIII","","","","","When musical sound is required during development of auditory or multimedia interfaces, designers often need to utilize different musical voices or timbre (usually produced via a multiple timbre synthesizer or a sound card) in order to communicate information. Currently, there is a limited set of guidelines assisting multimedia designers to select appropriate timbre. This paper reports a set of recall and recognition experiments on timbres produced by a multiple timbre synthesizer. Results indicate that a number of instruments were successfully recalled and recognized. A set of empirically derived guidelines are suggested to assist multimedia designers in selecting timbre.","1998","2023-07-24 06:48:51","2023-07-24 06:48:51","","273-286","","","","","People and Computers XIII","","","","","","","","","","","","","","","Place: London Publisher: Springer London","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X7NRQVH3","journalArticle","2011","Rutkowski, Tomasz M.; Cooper, Eric W.; Kryssanov, Victor V.; Ogawa, Hitoshi; Brewster, Stephen","Auditory Brain-Computer/Machine-Interface Paradigms Design","Haptic and Audio Interaction Design","","","","","The paper discusses novel and interesting, from users’ point of view, design of auditory brain-computer/machine interfaces (BCI/ BMI) utilizing human auditory responses. Two concepts of auditory stimuli BCI/BMI are presented. The first paradigm is based on steady-state tonal or musical stimuli yielding satisfactory EEG response classification for several seconds long stimuli. The second discussed paradigm is based on spatial sound localization and the brain evoked responses estimation, requiring shorter than a second stimuli presentation. In conclusion the preliminary results are discussed and suggestions for further applications are drawn.","2011","2023-07-24 06:48:51","2023-07-24 06:48:51","","110-119","","","6851","","Haptic and Audio Interaction Design","","","","","","","","","","","","","","","Place: Berlin, Heidelberg Publisher: Springer Berlin Heidelberg","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RNVWLZEQ","journalArticle","1999","Barrass, Stephen; Kramer, Gregory","Using sonification","Multimedia Systems","","","10.1007/s005300050108","","The idea behind sonification is that synthetic non-verbal sounds can represent numerical data and provide support for information processing activities of many different kinds. This article describes some of the ways that sonification has been used in assistive technologies, remote collaboration, engineering analyses, scientific visualisations, emergency services and aircraft cockpits. Approaches for designing sonifications are surveyed, and issues raised by the existing approaches and applications are outlined. Relations are drawn to other areas of knowledge where similar issues have also arisen, such as human-computer interaction, scientific visualisation, and computer music. At the end is a list of resources that will help you delve further into the topic.","1999","2023-07-24 06:48:52","2023-07-24 06:48:52","","23-31","","1","7","","Multimedia Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2W4WQB5U","journalArticle","2018","Matinfar, Sasan; Nasseri, M. Ali; Eck, Ulrich; Kowalsky, Michael; Roodaki, Hessam; Navab, Navid; Lohmann, Chris P.; Maier, Mathias; Navab, Nassir","Surgical soundtracks: automatic acoustic augmentation of surgical procedures","International Journal of Computer Assisted Radiology and Surgery","","","10.1007/s11548-018-1827-2","","","2018","2023-07-24 06:48:52","2023-07-24 06:48:52","","1345-1355","","9","13","","International Journal of Computer Assisted Radiology and Surgery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GASRPPF7","journalArticle","2018","Seiça, Mariana; Lopes, Rui; Martins, Pedro; Cardoso, F. Amílcar; Aramaki, Mitsuko; Davies, Matthew E. P.; Kronland-Martinet, Richard; Ystad, Sølvi","Sonifying Twitter’s Emotions Through Music","Music Technology with Swing","","","","","","2018","2023-07-24 06:48:52","2023-07-24 06:48:52","","586-608","","","11265","","Music Technology with Swing","","","","","","","","","","","","","","","Place: Cham Publisher: Springer International Publishing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G3UBHPJ4","journalArticle","2020","Roddy, Stephen; Bridges, Brian","Mapping for meaning: the embodied sonification listening model and its implications for the mapping problem in sonic information design","Journal on Multimodal User Interfaces","","","10.1007/s12193-020-00318-y","","This is a theoretical paper that considers the mapping problem, a foundational issue which arises when designing a sonification, as it applies to sonic information design. We argue that this problem can be addressed by using models from the field of embodied cognitive science, including embodied image schema theory, conceptual metaphor theory and conceptual blends, and from research which treats sound and musical structures using these models, when mapping data to sound. However, there are currently very few theoretical frameworks for applying embodied cognition principles in a sonic information design context. This article describes one such framework, the Embodied Sonification Listening Model, which provides a theoretical description of sonification listening in terms of Conceptual Metaphor Theory.","2020","2023-07-24 06:48:52","2023-07-24 06:48:52","","143-151","","2","14","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HMXI53QJ","journalArticle","2022","Hagen, Edward H.","The Biological Roots of Music and Dance: Extending the Credible Signaling Hypothesis to Predator Deterrence","Human Nature","","","10.1007/s12110-022-09429-9","","After they diverged from panins, hominins evolved an increasingly committed terrestrial lifestyle in open habitats that exposed them to increased predation pressure from Africa’s formidable predator guild. In the Pleistocene, Homo transitioned to a more carnivorous lifestyle that would have further increased predation pressure. An effective defense against predators would have required a high degree of cooperation by the smaller and slower hominins. It is in the interest of predator and potential prey to avoid encounters that will be costly for both. A wide variety of species, including carnivores and apes and other primates, have therefore evolved visual and auditory signals that deter predators by credibly signaling detection and/or the ability to effectively defend themselves. In some cooperative species, these predator deterrent signals involve highly synchronized visual and auditory displays among group members. Hagen and Bryant (Human Nature, 14(1), 21–51, 2003) proposed that synchronized visual and auditory displays credibly signal coalition quality. Here, this hypothesis is extended to include credible signals to predators that they have been detected and would be met with a highly coordinated defensive response, thereby deterring an attack. Within-group signaling functions are also proposed. The evolved cognitive abilities underlying these behaviors were foundations for the evolution of fully human music and dance.","2022","2023-07-24 06:48:52","2023-07-24 06:48:52","","261-279","","3","33","","Human Nature","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QSSAAYME","journalArticle","2001","Johannsen, Gunnar","Auditory Displays in Human–Machine Interfaces of Mobile Robots for Non-Speech Communication with Humans","Journal of Intelligent and Robotic Systems","","","10.1023/A:1013953213049","","Auditory displays are developed and investigated for mobile service robots in a human–machine environment. The service robot domain was chosen as an example for future use of auditory displays within multimedia process supervision and control applications in industrial, transportation, and medical systems. The design of directional sounds and of additional sounds for robot states as well as the design of more complicated robot sound tracks are explained. Basic musical elements and robot-movement sounds are combined. Experimental studies on the auditory perception of directional sounds as well as of sound tracks for the predictive display of intended robot trajectories in a simulated supermarket scenario are described.","2001","2023-07-24 06:48:52","2023-07-24 06:48:52","","161-169","","2","32","","Journal of Intelligent and Robotic Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SPCD7JXP","journalArticle","2012","Fabiani, Marco; Bresin, Roberto; Dubus, Gaël","Interactive sonification of expressive hand gestures on a handheld device","Journal on Multimodal User Interfaces","","","10.1007/s12193-011-0076-2","","We present here a mobile phone application called MoodifierLive which aims at using expressive music performances for the sonification of expressive gestures through the mapping of the phone’s accelerometer data to the performance parameters (i.e. tempo, sound level, and articulation). The application, and in particular the sonification principle, is described in detail. An experiment was carried out to evaluate the perceived matching between the gesture and the music performance that it produced, using two distinct mappings between gestures and performance. The results show that the application produces consistent performances, and that the mapping based on data collected from real gestures works better than one defined a priori by the authors.","2012","2023-07-24 06:48:52","2023-07-24 06:48:52","","49-57","","1-2","6","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9JS39462","journalArticle","1974","Cutting, James E.; Rosner, Burton S.","Categories and boundaries in speech and music*","Perception & Psychophysics","","","10.3758/BF03198588","","Perceptual categories and boundaries arise when Ss respond to continuous variation on a physical dimension in a discontinuous fashion. It is more difficult to discriminate between members of the same category than to discriminate between members of different categories, even though the amount of physical difference between both pairs is the same. Speech stimuli have been the sole class of auditory signals to yield such perception; for example, each different consonant phoneme serves as a category label. Experiment I demonstrates that categories and boundaries occur for both speech and nonspeech stimuli differing in rise time. Experiment II shows that rise time cues categorical differences in both complex and simple nonspeech waveforms. Taken together, these results suggest that certain aspects of speech perception are intimately related to processes and mechanisms exploited in other domains. The many categories in speech may be based on categories that occur elsewhere in auditory perception.","1974","2023-07-24 06:48:53","2023-07-24 06:48:53","","564-570","","3","16","","Perception & Psychophysics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RZVWUTQQ","journalArticle","2006","Vialatte, François B.; Cichocki, Andrzej; King, Irwin; Wang, Jun; Chan, Lai-Wan; Wang, DeLiang; Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","Sparse Bump Sonification: A New Tool for Multichannel EEG Diagnosis of Mental Disorders; Application to the Detection of the Early Stage of Alzheimer’s Disease","Neural Information Processing","","","","","This paper investigates the use of sound and music as a means of representing and analyzing multichannel EEG recordings. Specific focus is given to applications in early detection and diagnosis of early stage of Alzheimer’s disease. We propose here a novel approach based on multi channel sonification, with a time-frequency representation and sparsification process using bump modeling. The fundamental question explored in this paper is whether clinically valuable information, not available from the conventional graphical EEG representation, might become apparent through an audio representation. Preliminary evaluation of the obtained music score – by sample entropy, number of notes, and synchronous activity – incurs promising results.","2006","2023-07-24 06:48:53","2023-07-24 06:48:53","","92-101","","","4234","","Neural Information Processing","","","","","","","","","","","","","","","Place: Berlin, Heidelberg Publisher: Springer Berlin Heidelberg","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U2BX7DNK","journalArticle","2022","Bruder, Alexandra L.; Rothwell, Clayton D.; Fuhr, Laura I.; Shotwell, Matthew S.; Edworthy, Judy Reed; Schlesinger, Joseph J.","The Influence of Audible Alarm Loudness and Type on Clinical Multitasking","Journal of Medical Systems","","","10.1007/s10916-021-01794-9","","In high-consequence industries such as health care, auditory alarms are an important aspect of an informatics system that monitors patients and alerts providers attending to multiple concurrent tasks. Alarms levels are unnecessarily high and alarm signals are uninformative. In a laboratory-based task setting, we studied 25 anesthesiology residents’ responses to auditory alarms in a multitasking paradigm comprised of three tasks: patient monitoring, speech perception/intelligibility, and visual vigilance. These tasks were in the presence of background noise plus/minus music, which served as an attention-diverting stimulus. Alarms signified clinical decompensation and were either conventional alarms or a novel informative auditory icon alarm. Both alarms were presented at four different levels. Task performance (accuracy and response times) were analyzed using logistic and linear mixed-effects regression. Salient findings were 1), the icon alarm had similar performance to the conventional alarm at a +2 dB signal-to-noise-ratio (SNR) (accuracy: OR 1.21 (95% CI 0.88, 1.67), response time: 0.04 s at 2 dB (95% CI: –0.16, 0.24), which is a much lower level than current clinical environments; 2) the icon alarm was associated with 27% greater odds (95% CI: 18%, 37%) of correctly addressing the vigilance task, regardless of alarm SNR, suggesting crossmodal/multisensory multitasking benefits; and 3) compared to the conventional alarm, the icon alarm was associated with an absolute improvement in speech perception of 4% in the presence of an attention-diverting auditory stimulus (p = 0.031). These findings suggest that auditory icons can provide multitasking benefits in cognitively demanding clinical environments.","2022","2023-07-24 06:48:53","2023-07-24 06:48:53","","5","","1","46","","Journal of Medical Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SKU8DWSD","journalArticle","2022","Su, Isabelle; Hattwick, Ian; Southworth, Christine; Ziporyn, Evan; Bisshop, Ally; Mühlethaler, Roland; Saraceno, Tomás; Buehler, Markus J.","Interactive exploration of a hierarchical spider web structure with sound","Journal on Multimodal User Interfaces","","","10.1007/s12193-021-00375-x","","3D spider webs exhibit highly intricate fiber architectures and owe their outstanding performance to a hierarchical organization that spans orders of magnitude in length scale from the molecular silk protein, to micrometer-sized fibers, and up to cm-scale web. Similarly, but in a completely different physical manifestation, music has a hierarchical structure composed of elementary sine wave building blocks that can be combined with other waveforms to create complex timbres, which are then arranged within larger-scale musical compositions. Although apparently different, spider webs and music have many similarities, as we point out in this work. Here, we propose an intuitive and interactive way to explore and visualize a 3D Cyrtophora citricola spider web geometry that has been digitally modeled with micron-scale details from full-scale laboratory experiments. We use model-based sonification to translate the web architecture into sound, allowing for aural perception and interpretation of its essential topological features. We implement this sonification using Unity3D and Max/MSP to create an interactive spider web environment in which a user travels through a virtual spider web. Each silk fiber in their field of view is sonified using different sine waves. Together, the sonified fibers create new and more complex timbres that reflects the architecture of 3D spider webs. These concepts are implemented into a spider web-based instrument for live performances, art installations and data exploration. It provides an unprecedented and creative way to immerse the composer, audience and user in an immersive multimedia experience generated by the complexity of a 3D spider web.","2022","2023-07-24 06:48:53","2023-07-24 06:48:53","","71-85","","1","16","","Journal on Multimodal User Interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T5IY9XUK","journalArticle","2006","Chen, Xiaoyu; Tremaine, Marilyn; Lutz, Robert; Chung, Jae-woo; Lacsina, Patrick","AudioBrowser: a mobile browsable information access for the visually impaired","Universal Access in the Information Society","","","10.1007/s10209-006-0019-y","","Although a large amount of research has been conducted on building interfaces for the visually impaired that allows users to read web pages and generate and access information on computers, little development addresses two problems faced by the blind users. First, sighted users can rapidly browse and select information they find useful, and second, sighted users can make much useful information portable through the recent proliferation of personal digital assistants (PDAs). These possibilities are not currently available for blind users. This paper describes an interface that has been built on a standard PDA and allows its user to browse the information stored on it through a combination of screen touches coupled with auditory feedback. The system also supports the storage and management of personal information so that addresses, music, directions, and other supportive information can be readily created and then accessed anytime and anywhere by the PDA user. The paper describes the system along with the related design choices and design rationale. A user study is also reported.","2006","2023-07-24 06:48:53","2023-07-24 06:48:53","","4-22","","1","5","","Universal Access in the Information Society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E7G52UQG","journalArticle","2021","Paroiu, Razvan; Trăuşan-Matu, Ştefan","A new approach for chat sonification","2021 23rd International Conference on Control Systems and Computer Science (CSCS)","","","10.1109/CSCS52396.2021.00080","","This paper presents a new approach of chat sonification based on a deep neural network for music generation. The advantage of chat sonification is that the feeling of something artificial in the generated music is less present than for other artificial intelligence approaches. The new method of sonification introduced in the paper is based on the polyphonic model theory, similarly to the idea used in the MusicXML Creator Platform, the novelty being that it uses sequence-to-sequence neural networks for an improved pitch generation. The duration generation algorithm remains the same as it was implemented in the MusicXML Creator. The results are evaluated by multiple participants","2021","2023-07-24 06:48:53","2023-07-24 06:48:53","","453-456","","","","","2021 23rd International Conference on Control Systems and Computer Science (CSCS)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DFIWBL5Y","journalArticle","2016","Hananoi, Shunsuke; Muraoka, Kazuki; Kiyoki, Yasushi","A music composition system with time-series data for sound design in next-generation sonification environment","2016 International Electronics Symposium (IES)","","","10.1109/ELECSYM.2016.7861035","","This research suggests an idea of synthesizing time series data and human sense. This software can help human to know environmental invisible changes that human cannot detect. Player can use time series data for music composition. We have already had a way of sonification as relating research. However, this research aims having more flexibility in composition. For the motivation, we set player flexibility as tonality editing and data conversion range control system. In our software, player can edit melody and its range of sampling. For example, if composer want to express data as bright atmosphere music, you can edit some settings in the tool and play it. We have tried to use this software to use foreign exchange data. In future work, we try to use this software in virtual reality. And we will test the listener recognition change.","2016","2023-07-24 06:48:53","2023-07-24 06:48:53","","380-384","","","","","2016 International Electronics Symposium (IES)","","","","","","","","","","","","","","","Publisher: IEEE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ELCEA4MT","journalArticle","2021","Meytin, Sophia","A Novel Method for Protein-Protein Interface Analysis Using Sonification","2021 IEEE MIT Undergraduate Research Technology Conference (URTC)","","","10.1109/URTC54388.2021.9701622","","Auditory inspection reduces analytical subjectivity through concrete musical parameters. Here, it is applied to oligomerically varied protein-protein interfaces (PPIs) in a harmony-based sonification method. Amino acids in the PPI bonded contacts for homo- and hetero- di/tri/tetramers were grouped using four qualitative systems. PPI hydrogen bonds and salt bridges were examined to determine groups’ bonded interaction frequencies, compared using the chi-squared test of independence. Highly significant p-values (<<0.05) for homo- and hetero-oligomers in all systems’ oligomeric states strongly suggested that bonded interactions’ frequencies differ. These frequencies were successfully sonified using variations in consonance, waveform, chord structure, and pitch.","2021","2023-07-24 06:48:54","2023-07-24 06:48:54","","1-5","","","","","2021 IEEE MIT Undergraduate Research Technology Conference (URTC)","","","","","","","","","","","","","","","Publisher: IEEE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7YH8EDKP","journalArticle","2015","Huang, Chih-Fang; Nien, Wei-Po","A sonification system based on geographic and meteorologic data","2015 8th International Conference on Ubi-Media Computing (UMEDIA)","","","10.1109/UMEDIA.2015.7297465","","Most algorithmic composition system is implemented based on the input of music parameters, which needs professional music and technology training to complete the automated composition. This paper proposed an innovated way called Geographic and Meteorologic Data Sonification System (GMDSS), to perform the data mappings into music for various devices such as hand phone or other hand-hold devices, and the driver can retrieve the environment information in real time to let people concentrate on driving without the need of paying attention to the long-term complicated data visualization. The mapping relationship between geographic/meteorologic data and music features is discussed, and the result shows the implementation of GIS and weather data which can compose the proper correspondent music accordingly.","2015","2023-07-24 06:48:54","2023-07-24 06:48:54","","259-262","","","","","2015 8th International Conference on Ubi-Media Computing (UMEDIA)","","","","","","","","","","","","","","","Publisher: IEEE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GGKFTELR","journalArticle","2020","Hall, Lawton","Leander: Navigating Musical Possibility Space Through Color Data Sonification","2020 IEEE VIS Arts Program (VISAP)","","","10.1109/VISAP51628.2020.00011","","HDQGHU is an experimental film that sonifies color data to generate its musical soundtrack. The colors of Lake Michigan, captured in time lapse video, constitute ever-changing probability vectors that govern the behavior of musical sound-events over time. This VWRFKDVWLF, or probabilistic approach to data sonification imagines the musical experience as movement through a virtual possibility space, rather than the end result of a causal process. This pictorial describes how color data guides the various musical parameters at play in /HDQGHU through weighted chance.","2020","2023-07-24 06:48:54","2023-07-24 06:48:54","","45-60","","","","","2020 IEEE VIS Arts Program (VISAP)","","","","","","","","","","","","","","","Publisher: IEEE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HR569S9G","journalArticle","2018","Morawitz, Falk","Quantum: An art-science case study on sonification and sound design in virtual reality","2018 IEEE 4th VR Workshop on Sonic Interactions for Virtual Environments (SIVE)","","","10.1109/SIVE.2018.8577080","","Molecular sonification is the transformation of chemical data into sound and has been used to gain insight into chemical systems and for the creation of contemporary music compositions. The combination of sonification with a virtual reality environment offers potential benefits such as providing a visual frame of reference, an increased sense of immersion, nuanced spatial information through binaural audio cues and ease of interactivity. To explore how strategies developed in sonification research and contemporary electroacoustic music composition can be adapted to virtual reality, the art-science installation ’Quantum’ was created. The multi-media work consists of computer-generated molecules in a virtual space producing sound created via the sonification of nuclear magnetic resonance data. Upon user interaction with different molecules, the overall composition and complexity of the sound world develop. The binaural sound material can migrate back and forth from the molecules to the non-binaural background composition and, depending on user input, develop in terms of timbre, spectral complexity, and gestural content. ‘Quantum’ is an exploration of the combination of sonification and virtual reality and offers first points of discussion that can be elaborated upon in future artworks, games or educational content.","2018","2023-07-24 06:48:54","2023-07-24 06:48:54","","1-5","","","","","2018 IEEE 4th VR Workshop on Sonic Interactions for Virtual Environments (SIVE)","","","","","","","","","","","","","","","Publisher: IEEE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SMEE7RXI","journalArticle","2020","Zahray, Lisa; Savery, Richard; Syrkett, Liana; Weinberg, Gil","Robot Gesture Sonification to Enhance Awareness of Robot Status and Enjoyment of Interaction","2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","","","10.1109/RO-MAN47096.2020.9223452","","We present a divergent approach to robotic soniﬁcation with the goal of improving the quality and safety of human-robot interactions. Soniﬁcation (turning data into sound) has been underutilized in robotics, and has broad potential to convey robotic movement and intentions to users without requiring visual engagement. We design and evaluate six different soniﬁcations of movements for a robot with four degrees of freedom. Our soniﬁcation techniques include a direct mapping from each degree of freedom to pitch and timbre changes, emotion-based sound mappings, and velocity-based mappings using different types of sounds such as motors and music. We evaluate these soniﬁcations using metrics for ease of use, enjoyment/appeal, and conveyance of movement information. Based on our results, we make recommendations to inform decisions for future robot soniﬁcation design. We suggest that when using soniﬁcation to improve safety of human-robot collaboration, it is necessary not only to convey sufﬁcient information about movements, but also to convey that information in a pleasing and even social way to to enhance the human-robot relationship.","2020","2023-07-24 06:48:54","2023-07-24 06:48:54","","978-985","","","","","2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","","","","","","","","","","","","","","","Publisher: IEEE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XTL3IQQI","journalArticle","2013","Sanchez, A.; Valderrama, M.","Sonification of EEG signals based on musical structures","2013 Pan American Health Care Exchanges (PAHCE)","","","10.1109/PAHCE.2013.6568291","","This short communication proposes a new method to translate human EEG recordings into music. The sonification method is based on the relation of primary concepts of musical composition to different time-frequency characteristics of electrical signals from the brain that give information about the mental states. In general terms, normalization and thresholding procedures were applied to wavelet transforms of analyzed signals in order to extract amplitude and frequency parameters that were mapped to several synthetic or natural sounds following basic musical composition structures. The method seeks to provide a new perspective about the brain activity by means of an auditory feedback which not only facilitates the long term monitoring in clinical contexts like polysomnographics studies or epilepsy but also offers a new tool for a self-understating of the different human body processes modulated by the brain.","2013","2023-07-24 06:48:55","2023-07-24 06:48:55","","1-1","","","","","2013 Pan American Health Care Exchanges (PAHCE)","","","","","","","","","","","","","","","Publisher: IEEE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4GBWH2SJ","journalArticle","2019","Colombo, R.; Raglio, A.; Panigazzi, M.; Mazzone, A.; Bazzini, G.; Imarisio, C.; Molteni, D.; Caltagirone, C.; Imbriani, M.","The SonicHand Protocol for Rehabilitation of Hand Motor Function: A Validation and Feasibility Study","IEEE Transactions on Neural Systems and Rehabilitation Engineering","","","10.1109/TNSRE.2019.2905076","","Musical sonification therapy is a new technique that can reinforce conventional rehabilitation treatments by increasing therapy intensity and engagement through challenging and motivating exercises. The aim of this paper is to evaluate the feasibility and validity of the SonicHand protocol, a new training and assessment method for the rehabilitation of hand function. The study was conducted in 15 healthy individuals and 15 stroke patients. The feasibility of implementation of the training protocol was tested in stroke patients only, who practiced a series of exercises concurrently to music sequences produced by specific movements. The assessment protocol evaluated hand motor performance during pronation/supination, wrist horizontal flexion/extension, and hand grasp without sonification. From hand position data, 15 quantitative parameters were computed evaluating mean velocity, movement smoothness, and angular excursions of hand/fingers. We validated this assessment in terms of its ability to discriminate between patients and healthy subjects, test-retest reliability and concurrent validity with the upper limb section of the Fugl-Meyer scale (FM), the functional independence measure (FIM), and the Box and Block Test (BBT). All patients showed a good understanding of the assigned tasks and were able to correctly execute the proposed training protocol, confirming its feasibility. A moderateto-excellent intraclass correlation coefficient was found in 8/15 computed parameters. The moderate-to-strong correlation was found between the measured parameters and the clinical scales. The SonicHand training protocol is feasible and the assessment protocol showed good to excellent between-group discrimination ability, reliability, and concurrent validity, thus enabling the implementation of new personalized and motivating training programs employing sonification for the rehabilitation of hand function.","2019","2023-07-24 06:48:55","2023-07-24 06:48:55","","664-672","","4","27","","IEEE Transactions on Neural Systems and Rehabilitation Engineering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NNG4HMZW","journalArticle","2012","Dailly, Anabel Immoos; Sigrist, Roland; Kim, Yeongmi; Wolf, Peter; Erckens, Hendrik; Cerny, Joachim; Luft, Andreas; Gassert, Roger; Sulzer, James","Can simple error sonification in combination with music help improve accuracy in upper limb movements?","2012 4th IEEE RAS & EMBS International Conference on Biomedical Robotics and Biomechatronics (BioRob)","","","10.1109/BioRob.2012.6290908","","While repetitive training is widely regarded to be a useful rehabilitation strategy, such training requires motivation that may be lacking. In order to improve motivation in a potentially inexpensive and simple manner, we introduce in this proof-of-concept study a combination of error sonification and music for upper limb training. Twelve healthy participants trained a figure tracing task for the upper limb, six receiving feedback in terms of error sonification and music and six without receiving feedback in the control group. The error-sonified feedback group decreased its amount of error significantly compared to the control group. Thus this particular paradigm can help teach planar reaching movements. Eventually this paradigm may become simple and useful enough to enhance existing therapeutic intervention in stroke rehabilitation.","2012","2023-07-24 06:48:55","2023-07-24 06:48:55","","1423-1427","","","","","2012 4th IEEE RAS & EMBS International Conference on Biomedical Robotics and Biomechatronics (BioRob)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JCR4SNYM","journalArticle","1996","Jameson, D.H.","Building real-time music tools visually with Sonnet","Proceedings Real-Time Technology and Applications","","","10.1109/RTTAS.1996.509518","","We are building a variety of interactive music tools using Sonnet, a visual programming language in use at our center. Originally designed for sonification experiments for monitoring and debugging programs, Sonnet has grown into a more general system with a focus on real-time event-driven applications. In this paper, we describe some of the features of Sonnet followed by some examples of how it is being used.","1996","2023-07-24 06:48:55","2023-07-24 06:48:55","","11-18","","","","","Proceedings Real-Time Technology and Applications","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WWQ4X628","journalArticle","2018","Freitas, Tiago Abreu; Miranda, Hudson; Rabelo, Cassiano; Jorio, Ado","Instrumentation and Algorithms for the Sonification of Scanning Probe Microscopy Output","2018 3rd International Symposium on Instrumentation Systems, Circuits and Transducers (INSCIT)","","","10.1109/INSCIT.2018.8546705","","The present work reports on the development of scientific instrumentation in scanning probe microscopy (SPM), a field of wide application and versatility that allows exploring surface properties at the atomic and molecular levels. Usually, SPM systems deliver information to the user with images that represent the surface properties. Here we implemented a realtime data sonification system that enables visually impaired people to perform SPM experiments and provides other information that is not nicely captured by the human eye. Based on a Raspberry Pi 3B + board and Python algorithms, our system generates parameters based on an SPM scan for sonification that is tested here using the MIDI (Musical Instrument Digital Interface).","2018","2023-07-24 06:48:55","2023-07-24 06:48:55","","1-6","","","","","2018 3rd International Symposium on Instrumentation Systems, Circuits and Transducers (INSCIT)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R3RUTLWI","journalArticle","2019","Giariskanis, Fotis; Parthenios, Panagiotis; Mania, Katerina","ARCHIMUSIC3D: Multimodal Playful Transformations between Music and Refined Urban Architectural Design","2019 11th International Conference on Virtual Worlds and Games for Serious Applications (VS-Games)","","","10.1109/VS-Games.2019.8864525","","The commonly used 3D architectural design tools for urban environments fail to capture aspects of an urban design which are aesthetic as well as functional. This paper describes an innovative multimodal user interface through which an urban designer can work on the music transcription of a specific urban environment applying music compositional rules and filters in order to identify discordant entities, highlight imbalanced parts and make design corrections. The proposed platform offers sonification of an Urban Virtual Environment (UVE), simulating a real-world cityscape, offering visual interpretation and musically playful modification of its soundscape. The system presented offers: The ability to view and convert an urban street to music (ready to play) based on a specific grammar of converting architectural elements to musical elements; secondly, the ability to transform this music in order to `harmonize' it based on musical rules as if composing and playing music; and finally, the prospect of converting back the aesthetically and harmonically “corrected” musical piece to a newly refined street or urban design, visualized in 3D. The presented platform comprises of three scenes, which compile the three main parts of the system's multimodal interface; e.g., the 3D scene, the Digital Audio Workstation (DAW) scene and the TouchOSC mobile controller. The purpose of this paper is to assist architects and urban designers in 1) identifying urban dissonances, 2) refining their design using musical rules and 3) interactively presenting the output both visually and acoustically.","2019","2023-07-24 06:48:55","2023-07-24 06:48:55","","1-4","","","","","2019 11th International Conference on Virtual Worlds and Games for Serious Applications (VS-Games)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"75E7B2NZ","journalArticle","2011","Stewart, Rebecca; Sandler, Mark","The amblr: A mobile spatial audio music browser","2011 IEEE International Conference on Multimedia and Expo","","","10.1109/ICME.2011.6012203","","Music collections are often visualized in a two-dimensional space to show relationships between songs. Some user inter faces interacting with these two-dimensional maps of songs use spatial auditory display to allow easier access to the au dio content. A common auditory display is to have multiple songs playing simultaneously from differing spatial locations around the user. However, when using this style of interface the sonification of the collection needs to be limited to a lo cal subset of the collection, usually three to six songs. This paper presents the amblr, a spatial audio music browser that allows a user to auralize the collection. It combines effective design from previous work with new approaches to create a novel interface. This allows for a more intuitive navigation of a virtual space populated by a large collection songs without relying on textual metadata.","2011","2023-07-24 06:48:56","2023-07-24 06:48:56","","1-6","","","","","2011 IEEE International Conference on Multimedia and Expo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZJR784UK","journalArticle","2000","Van Scoy, F.L.","Sonification of remote sensing data: initial experiment","2000 IEEE Conference on Information Visualization. An International Conference on Computer Visualization and Graphics","","","10.1109/IV.2000.859796","","The author is generating music from a particular view of a multi-dimensional geographic information system (GIS) data set to alert a viewer to the existence of hidden clusters of data points. The author describes some results from an early experiment with generating music from a 2D slice of the data and testing whether these representations are easily understood by beginning users.","2000","2023-07-24 06:48:56","2023-07-24 06:48:56","","453-460","","","","","2000 IEEE Conference on Information Visualization. An International Conference on Computer Visualization and Graphics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UHJ2KKEV","journalArticle","2021","Fernandes, Carlos M.; Migotina, Daria; Rosa, Agostinho C.","Brain's Night Symphony (BraiNSy): A Methodology for EEG Sonification","IEEE Transactions on Affective Computing","","","10.1109/TAFFC.2018.2850008","","This paper describes a method for converting sleep Electroencephalogram (EEG) signals into music. For that purpose, a new segmentation procedure is used for extracting relevant information from the sleep EEG that is then translated into sequences of notes, chords, arpeggios and pauses, with a varying tempo that is defined by sleep stages. The final outcome is a direct time-domain conversion of the brain activity during sleep into sound. Since typical sleep EEGs vary with age and sleep disorders, different groups of subjects were used in the experiments: babies, sane adults and patients with air-flow limitation.","2021","2023-07-24 06:48:56","2023-07-24 06:48:56","","103-112","","1","12","","IEEE Transactions on Affective Computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2KQTN47R","journalArticle","2021","Su, Isabelle; Qin, Zhao; Saraceno, Tomás; Bisshop, Ally; Mühlethaler, Roland; Ziporyn, Evan; Buehler, Markus J.","Sonification of a 3-D Spider Web and Reconstitution for Musical Composition Using Granular Synthesis","Computer Music Journal","","","10.1162/comj_a_00580","","Three-dimensional spider webs feature highly intricate fiber architectures, which can be represented via 3-D scanning and modeling. To allow novel interpretations of the key features of a 3-D Cyrtophora citricola spider web, we translate complex 3-D data from the original web model into music, using data sonification. We map the spider web data to audio parameters such as pitch, amplitude, and envelope. Paired with a visual representation, the resulting audio allows a unique and holistic immersion into the web that can describe features of the 3-D architecture (fiber distance, lengths, connectivity, and overall porosity of the structure) as a function of spatial location in the web. Using granular synthesis, we further develop a method to extract musical building blocks from the sonified web, transforming the original representation of the web data into new musical compositions. We build a new virtual, interactive musical instrument in which the physical 3-D web data are used to generate new variations in sound through exploration of different spatial locations and grain-processing parameters. The transformation of sound from grains to musical arrangements (variations of melody, rhythm, harmony, chords, etc.) is analogous to the natural bottom–up processing of proteins, resembling the design of sequence and higher-level hierarchical protein material organization from elementary chemical building blocks. The tools documented here open possibilities for creating virtual instruments based on spider webs for live performances and art installations, suggesting new possibilities for immersion into spider web data, and for exploring similarities between protein folding, on the one hand, and assembly and musical expression, on the other.","2021","2023-07-24 06:48:56","2023-07-24 06:48:56","","43-59","","4","44","","Computer Music Journal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YSVEV5P2","journalArticle","2015","Turchet, Luca; Bresin, Roberto","Effects of Interactive Sonification on Emotionally Expressive Walking Styles","IEEE Transactions on Affective Computing","","","10.1109/TAFFC.2015.2416724","","This paper describes two experiments conducted to investigate the role of sonically simulated ground materials in modulating both production and recognition of walks performed with emotional intentions. The results of the first experiment showed that the involved auditory feedbacks affected the pattern of emotional walking in different ways, although such an influence manifested itself in more than one direction. The results of the second experiment showed the absence of an influence of the sound conditions on the recognition of the emotions from acoustic information alone. Similar results were found in both experiments for musically-trained and untrained participants. Our results suggest that tempo and sound level are two acoustical features important in both production and recognition of emotions in walking. In addition, the similarities of the presented results with those reported in the music performance domain, as well as the absence of an influence of musical expertise lend support to the “motor origin hypothesis of emotional expression in music” according to which a motor origin for the expression of emotions is common in all those domains of human activity that result in the generation of an acoustical signal.","2015","2023-07-24 06:48:56","2023-07-24 06:48:56","","152-164","","2","6","","IEEE Transactions on Affective Computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JDDQ7NUA","journalArticle","2022","Kaplan, Cyril; Husa, Pavel; Mikovec, Zdenek","Scale_it: Converting time series data into musical scales","2022 13th IEEE International Conference on Cognitive Infocommunications (CogInfoCom)","","","10.1109/CogInfoCom55841.2022.10081873","","This paper introduces open-source sonification software that represents a real-time stream of time-series data as notes on a musical scale. The software comprises a data processing unit and a signal-to-sound conversion unit. Before being expressed as a note on a musical scale of predefined length and character, each chunk of the incoming data stream is transformed into a parameter representing its relative position in the original population.","2022","2023-07-24 06:48:56","2023-07-24 06:48:56","","000021-000022","","","","","2022 13th IEEE International Conference on Cognitive Infocommunications (CogInfoCom)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B9WSUCA6","journalArticle","2022","Husa, Pavel; Kaplan, Cyril; Mikovec, Zdenek","Towards modular sonic EEG neurofeedback interface","2022 13th IEEE International Conference on Cognitive Infocommunications (CogInfoCom)","","","10.1109/CogInfoCom55841.2022.10081815","","Abstract This paper introduces an electroencephalogram (EEG) to the sound transformation tool written in Python and Pure Data using Faust. The proposed signal chain performs a real-time sonification representing the distance of the level from the threshold. We present four different music games implemented in Pure Data using our custom object Scale_it in a neurofeedback application. These games were tested and evaluated by users. The subjective impression of the participants (N=53) of gaining control of the game after the session is overall 3.59 on a five-level Likert scale.","2022","2023-07-24 06:48:56","2023-07-24 06:48:56","","000019-000020","","","","","2022 13th IEEE International Conference on Cognitive Infocommunications (CogInfoCom)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HAA8I854","journalArticle","2020","Ramírez, Andrea Valenzuela; Hornero, Gemma; Royo, Daniel; Aguilar, Angel; Casas, Oscar","Assessment of Emotional States Through Physiological Signals and Its Application in Music Therapy for Disabled People","IEEE Access","","","10.1109/ACCESS.2020.3008269","","Disabled people have more difficulties for expression and interaction on a non-verbal level because of the physical, sensory or communication difficulties that they usually present. Regarding music therapy, the existing adapted interfaces target disabled people with controlled motor abilities, which still suggests a barrier for the most affected users presenting truly little or uncontrolled movements. In this work, we have developed a device to build an adapted musical instrument through physiological signals. The instrument uses the electrocardiogram (ECG), the electrical activity of the skin (EDA), the respiration signal and the movement of the user to generate music via sonification of the characteristic features of each physiological signal. Furthermore, the ECG and EDA signals have been used to assess the emotional state of the person to provide to the music therapist objective information in real-time about the adaptation of the user to the techniques and interfaces used in their sessions. The adapted instrument has been tested by people with cerebral palsy showing its high degree of adaptability to the user. In four months, an increase in participation of the most affected users in their music therapy sessions has been achieved. In concrete, the results show that the considered users have achieved six of the so-called aids to analysis which are commonly used to evaluate such practices. The results of the assessment of emotional states indicate that the state of a person can be extracted from the ECG in periods of ten seconds while the evolution of the EDA reveals if the person is relaxed or excited. The device has generated a lot of interest among educators since it outperforms the state-of-the-art techniques allowing the integration of the most affected users by eliminating the aforementioned barrier because of non-controlled movements while assessing the emotional state of the person when facing the activity.","2020","2023-07-24 06:48:56","2023-07-24 06:48:56","","127659-127671","","","8","","IEEE Access","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A29R73ZL","journalArticle","2004","Matta, S.; Kumar, D.K.; Yu, Xinghuo; Burry, M.","An approach for image sonification","First International Symposium on Control, Communications and Signal Processing, 2004.","","","10.1109/ISCCSP.2004.1296321","","This paper presents a new approach for image to sound mapping. The proposed method utilizes the music parameters such as pitch and rhythm to support translation of images into sounds. Many people have tried image-to-sound mapping or data-to-sound mapping and failed to prove the useful results and many people haven't followed the principles of psychoacoustics in implementing image to sound conversion methods. The important bottleneck in these kinds of experiments is that humans can't remember the normal sounds as compared to music. A method is developed to overcome this bottleneck by utilizing musical parameters. Most of the available tools have been tested on the participants and it has been discovered that the technology available to convert data streams into sounds was not sufficient and needed an improvement.","2004","2023-07-24 06:48:57","2023-07-24 06:48:57","","431-434","","","","","First International Symposium on Control, Communications and Signal Processing, 2004.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R6XMC3XU","journalArticle","2000","Ballora, M.; Pennycook, B.; Ivanov, P.Ch.; Goldberger, A.; Glass, L.","Detection of obstructive sleep apnea through auditory display of heart rate variability","Computers in Cardiology 2000. Vol.27 (Cat. 00CH37163)","","","10.1109/CIC.2000.898630","","A data set of interbeat interval times is read into a music software synthesis program to become the basis of a ""soundtrack"" or auditory display. Here the authors present examples of the sonifications, and discuss potential advantages in listening to, as opposed to viewing, heart rate variability data. This method treats the diagnosis of obstructive sleep apnea as a problem of orchestration and melody.","2000","2023-07-24 06:48:57","2023-07-24 06:48:57","","739-740","","","","","Computers in Cardiology 2000. Vol.27 (Cat. 00CH37163)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YYMZVICU","journalArticle","2009","Ng, Kia","Interactive Multimedia Interface and Multimodal Analysis for Technology-Enhanced Learning and Performance Preservation","2009 Fifth International Joint Conference on INC, IMS and IDC","","","10.1109/NCM.2009.107","","This paper presents one of the interactive multimedia tools resulted from a research and development project in the context of technology-enhanced learning for music, and the usage of the multimodal interface for performing arts preservation. Firstly, the paper briefly introduces the i-Maestro project (see www.i-maestro.org). With particular focuses on the interactive multimedia interface for 3D motion visualization, the paper discusses the applications and usages of this gestural interface for technology-enhanced learning for string instruments. The interface utilizes online and offline multimodal data analysis and provide interactive feedback using 3D graphics and sonification. It provides additional level of details and data for stylistic and performance analysis. Next, the paper presents an ongoing research project which is working on digital preservation of interactive music performance using this interface.","2009","2023-07-24 06:48:57","2023-07-24 06:48:57","","1401-1405","","","","","2009 Fifth International Joint Conference on INC, IMS and IDC","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PYM36AGG","journalArticle","2018","Maçãs, Catarina; Martins, Pedro; Machado, Penousal","Consumption as a Rhythm: A Multimodal Experiment on the Representation of Time-Series","2018 22nd International Conference Information Visualisation (IV)","","","10.1109/iV.2018.00093","","Through Data Visualisation and Sonification models, we present a study of multimodal representations to characterise the Portuguese consumption patterns, which were gathered from Portuguese hypermarkets and supermarkets over the course of two years. We focus on the rhythmic nature of the data to create and discuss audio and visual representations that highlight disruptions and sudden changes in the normal consumption patterns. For this study, we present two distinct visual and audio representations and discuss their strengths and limitations.","2018","2023-07-24 06:48:57","2023-07-24 06:48:57","","504-509","","","","","2018 22nd International Conference Information Visualisation (IV)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IN7ZR789","journalArticle","2015","Yang, Jiajun; Hermann, Thomas","A Zen Garden interface for the interactive control of sonic ambiences in smart environment","2015 6th IEEE International Conference on Cognitive Infocommunications (CogInfoCom)","","","10.1109/CogInfoCom.2015.7390648","","In this paper we introduce SoZen, a novel interface for parameterizing aspects of ambient soundscapes including ambient music and peripheral sonifications using a Zen Garden. The SoZen system is both a stylish and aesthetic artifact in daily living environments and a computer-vision-based tangible and malleable representation for sound features. Due to the immediate correspondence between the arrangement of stones and the shape of sand with sonic features it also becomes a persistent visualization of the current sonic ambience. Thus it offers a conceptually different and `calm' way of reviewing and specifying sounds. The paper focuses on the conceptual ideas and showcases our current system. We demonstrate some initial ambiences and conclude with current ideas on how to embed and evaluate SoZen in the context of a smart apartment environment.","2015","2023-07-24 06:48:57","2023-07-24 06:48:57","","523-524","","","","","2015 6th IEEE International Conference on Cognitive Infocommunications (CogInfoCom)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T9AC5U2Q","journalArticle","2006","Kori, H.; Tezuka, T.; Tanaka, K.","Ranking of Regional Blogs by Suitability for Sonification","22nd International Conference on Data Engineering Workshops (ICDEW'06)","","","10.1109/ICDEW.2006.125","","Media content presented to a vehicle driver is mainly auditory, since visual content is distracting and viewing it increases the risk of an accident. Music and radio are thus commonly listened to while driving. However, these types of content rarely reflect regional characteristics and are therefore not well suited for tourists who want to get information about the region they are visiting. We have developed the Blog Car Radio system that presents blog entries in auditory style using sonification (speech synthesis). Blog entries are obtained from blog search engines, selected by distances from the vehicle’s current location, and ranked based on their suitability for sonification and relevance to the userspecified category. By using Blog Car Radio, a driver can obtain local information with only a small amount of distraction. In this paper, we particularly discussed the method to rank text contents which is suitable for sonification.","2006","2023-07-24 06:48:57","2023-07-24 06:48:57","","x132-x132","","","","","22nd International Conference on Data Engineering Workshops (ICDEW'06)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X3SD2IKH","journalArticle","2013","Milsap, Griffin; Fifer, Matthew; Crone, Nathan; Thakor, Nitish","Listening to the music of the brain: Live analysis of ECoG recordings using digital audio workstation software","2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)","","","10.1109/NER.2013.6696026","","A process is presented for analyzing electrocorticographic (ECoG) recordings and prototyping brain computer interfaces in which complex signal processing chains are able to be rapidly developed and iterated in digital audio workstation (DAW) software. DAW software includes many built-in “drag and drop” blocks that perform common, low-level signal processing algorithms such as filtering and envelope extraction. In addition to being optimized for real-time performance, DAW software also produces audio output, allowing for listening to raw and processed signals. Hearing these sonifications can impart new insights that may not be apparent in purely visual representations. A simple functional mapping analysis is performed in a DAW called Pure Data and compared to the results from a more traditional spatiotemporal analysis in MATLAB. Channels exhibiting qualitative activation in the resulting functional maps were further analyzed in another DAW called Renoise, wherein several high frequency (i.e., >400 Hz) features were observed. This study demonstrates an example use of DAW software, which we suggest is an easy-to-use and intuitive environment for real-time exploratory analyses and sophisticated sonification of ECoG recordings.","2013","2023-07-24 06:48:57","2023-07-24 06:48:57","","682-685","","","","","2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9H62PIJ8","journalArticle","2014","McIntosh, Shane; Legere, Katie; Hassan, Ahmed E.","Orchestrating change: An artistic representation of software evolution","2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering (CSMR-WCRE)","","","10.1109/CSMR-WCRE.2014.6747192","","Several visualization tools have been proposed to highlight interesting software evolution phenomena. These tools help practitioners to navigate large and complex software systems, and also support researchers in studying software evolution. However, little work has explored the use of sound in the context of software evolution. In this paper, we propose the use of musical interpretation to support exploration of software evolution data. In order to generate music inspired by software evolution, we use parameter-based sonification, i.e., a mapping of dataset characteristics to sound. Our approach yields musical scores that can be played synthetically or by a symphony orchestra. In designing our approach, we address three challenges: (1) the generated music must be aesthetically pleasing, (2) the generated music must accurately reflect the changes that have occurred, and (3) a small group of musicians must be able to impersonate a large development team. We assess the feasibility of our approach using historical data from Eclipse, which yields promising results.","2014","2023-07-24 06:48:57","2023-07-24 06:48:57","","348-352","","","","","2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering (CSMR-WCRE)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V3F2652X","journalArticle","2004","Hermann, T.; Ritter, H.","Sound and meaning in auditory data display","Proceedings of the IEEE","","","10.1109/JPROC.2004.825904","","Auditory data display is an interdisciplinary field linking auditory perception research, sound engineering, data mining, and human-computer interaction in order to make semantic contents of data perceptually accessible in the form of (nonverbal) audible sound. For this goal it is important to understand the different ways in which sound can encode meaning. We discuss this issue from the perspectives of language, music, functionality, listening modes, and physics, and point out some limitations of current techniques for auditory data display, in particular when targeting high-dimensional data sets. As a promising, potentially very widely applicable approach, we discuss the method of model-based sonification (MBS) introduced recently by the authors and point out how its natural semantic grounding in the physics of a sound generation process supports the design of sonifications that are accessible even to untrained, everyday listening. We then proceed to show that MBS also facilitates the design of an intuitive, active navigation through ""acoustic aspects"", somewhat analogous to the use of successive two-dimensional views in three-dimensional visualization. Finally, we illustrate the concept with a first prototype of a ""tangible"" sonification interface which allows us to ""perceptually map"" sonification responses into active exploratory hand motions of a user, and give an outlook on some planned extensions.","2004","2023-07-24 06:48:57","2023-07-24 06:48:57","","730-741","","4","92","","Proceedings of the IEEE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9MJPTKGV","journalArticle","2003","Osmanovic, N.; Hrustemovic, N.; Myler, H.R.","A testbed for auralization of graphic art","Annual Technical Conference IEEE Region 5, 2003","","","10.1109/REG5.2003.1199709","","Sonification is the use of non-speech audio to convey information and auralization, a similar but distinct area, is an aural metaphor for visualization - the process by which objects and scenes are interpreted by the human visual system. A fundamental goal of this study is to produce a mechanism by which the visually impaired can experience the artistic content of images using an alternate modality, hearing. In this project, the auralization of images using music is explored. A software algorithm to map pixel data from images to corresponding sound sequences is presented. This conversion is based on a frequency relationship that exists between color and sound in human perception. For example, the frequency of the G tone at 392 Hz, 40 times redoubled (octaved), delivers the frequency of the color red (431 /spl times/ 10/sup 12/ Hz). In addition, the volume of the mapped sounds is used to represent color intensity.","2003","2023-07-24 06:48:57","2023-07-24 06:48:57","","45-49","","","","","Annual Technical Conference IEEE Region 5, 2003","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZSBUUPS9","journalArticle","2001","Rigas, D.; Memery, D.; Yu, H.","Experiments in using structured musical sound, synthesised speech and environmental stimuli to communicate information: is there a case for integration and synergy?","Proceedings of 2001 International Symposium on Intelligent Multimedia, Video and Speech Processing. ISIMP 2001 (IEEE Cat. No.01EX489)","","","10.1109/ISIMP.2001.925434","","This paper describes three sets of experiments in auditory information processing using structured musical sound, synthesised speech and environmental stimuli or special effects. The first experiment examines auditory information processing of small sequences of rhythmic musical tones. The second experiment examines auditory information processing of some environmental sounds. The third experiment examines auditory information processing when sound and synthesised speech are simultaneously presented. The results of this investigation aim to help multimedia and user interface developers to design auditory messages that incorporate structured musical stimuli, synthesised speech and environmental sounds or special effects. These type of auditory messages can complement visual displays or communicate information on their own in auditory interfaces. The results of these initial experiments indicate a prima facie case for integrating various types of auditory stimuli in one single message. The simultaneous presentation of sound and speech was easily recognised and interpreted. The paper concludes with some initial practical guidelines for designers and a discussion of further work.","2001","2023-07-24 06:48:58","2023-07-24 06:48:58","","465-468","","","","","Proceedings of 2001 International Symposium on Intelligent Multimedia, Video and Speech Processing. ISIMP 2001 (IEEE Cat. No.01EX489)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"94MXZ4BR","journalArticle","2002","Vickers, Paul; Alty, James L","When bugs sing","Interacting with Computers","","","10.1016/S0953-5438(02)00026-7","","In The Songs of Insects, Pierce (1949) described the striped ground cricket, Nemobius fasciatus-fasciatus, which chirps at a rate proportional to ambient air temperature. Twenty chirps-per-second tell us it is 31.4 °C; 16 chirps and it is 27 °C. This is a natural example of an auditory display, a mechanism for communicating data with sound. By applying auditory display techniques to computer programming we have attempted to give the bugs that live in software programs their own songs. We have developed the CAITLIN musical program auralisation system Vickers and Alty, 2002b) to allow structured musical mappings to be made of the constructs in Pascal programs. Initial experimental evaluation [Interacting with Computers (2002a,b)] showed that subjects could interpret the musical motifs used to represent the various Pascal language constructs.In this paper we describe how the CAITLIN system was used to study the effects of musical program auralisation on debugging tasks performed by novice Pascal programmers. The results of the experiment indicate that a formal musical framework can act as a medium for communicating information about program behaviour, and that the information communicated could be used to assist with the task of locating bugs in faulty programs.","2002","2023-07-24 06:48:58","2023-07-24 06:48:58","","793-819","","6","14","","Interacting with Computers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T2LMENWN","journalArticle","1993","Mayer-Kress, G.; Choi, I.; Weber, N.; Barger, R.; Hubler, A.","Musical signals from Chua's circuit","IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing","","","10.1109/82.246172","","Chua's circuit can produce a very rich variety of signals that are both periodic and chaotic. The authors explore some classes of these attractors with respect to their auditory display and musical properties. They discuss the fast control of the circuit through a specially developed computer-controlled electronic resistor and how chaotic control methods might be applied to optimally switch between different attractors. The Chua circuit has parameter regions where noisy frequency and amplitude modulated sounds are generated, each of which is related to a certain transition to chaos. The authors discovered a period-adding sequence of bassoon-like sounds that produces interesting almost harmonic pitch changes. Finally, they emphasize the importance of transient dynamics especially in the context of percussion-like sounds.<>","1993","2023-07-24 06:48:58","2023-07-24 06:48:58","","688-695","","10","40","","IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7LULFKW8","journalArticle","2006","Kim, Hyunho; Seo, Changhoon; Lee, Junhun; Ryu, Jeha; Yu, Si-bok; Lee, Sooyoung","Vibrotactile Display for Driving Safety Information","2006 IEEE Intelligent Transportation Systems Conference","","","10.1109/ITSC.2006.1706802","","Vehicle driving support systems such as navigation systems and dead-angle warning systems make us safer and more comfortable. These kinds of systems provide only visual and auditory information. However, visual driving support systems are restricted in driver's field-of-view and auditory warning signals can lose in radio music, engine noise, or conversation noise. Tactile displays using vibration motors can provide useful information in spite of restricted field-of-view and noisy environment. It may quickly draw the attention of the driver when important events occur: for instance, collision warning and directional cues. These intuitive and quick cues may be combined together with the visual and auditory display to give multimodal feedback to the driver. In this paper, we present a vibrotactile display device for providing safety information to drivers. User studies with the vibrotactile device on the top of the foot show 86.7% recognition rate for alphabet characters after some training and 83.9% for providing driving safety information","2006","2023-07-24 06:48:58","2023-07-24 06:48:58","","573-577","","","","","2006 IEEE Intelligent Transportation Systems Conference","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VFKH4SJ4","journalArticle","2016","Goudarzi, Visda","Exploration of sonification design process through an interdisciplinary workshop","Proceedings of the Audio Mostly 2016","","","10.1145/2986416.2986422","","In sonification of scientific data, designers know very little about the domain science and domain scientists are not familiar with the sonification methodology. The knowledge about the domain science is not given, but evolved during the problem-solving process. We discuss design challenges in auditory display design regarding user-centered approaches and suggest a method to involve domain scientists throughout sonification designs. We explore this within a workshop in which sonification experts, domain experts, and programmers worked together to better understand and solve problems collaboratively. The sonification framework that is used during the workshops is briefly described and the workshop process and how each group worked together during the workshop sessions are examined. Participants worked on pre-defined and exploratory tasks to sonify climate data. Furthermore, they grasped each other's domains; climate scientists especially became more open to use auditory display and sonification as a tool in their data mining tasks. Resulting sonification prototypes and workshop sessions are documented on a wiki to be used by the sonification community. To get started, we used some of the sonification designs created during the workshop for an online study where participants from science, engineering, and humanities were asked questions about the data behavior by listening to sonifcations of bivariate time series. Results indicate that sonic representation of data from resulting sonification allows most users (even with little or no knowledge of sound and music) to successfully complete some common data exploration tasks.","2016","2023-07-24 06:48:58","2023-07-24 06:48:58","","147–153","","","","","Proceedings of the Audio Mostly 2016","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VA2KCZVH","journalArticle","2021","Rönnberg, Niklas","Sonification for Conveying Data and Emotion","Proceedings of the 16th International Audio Mostly Conference","","","10.1145/3478384.3478387","","In the present study a sonification of running data was evaluated. The aim of the sonification was to both convey information about the data and convey a specific emotion. The sonification was evaluated in three parts, firstly as an auditory graph, secondly together with additional text information, and thirdly together with an animated visualization, with a total of 150 responses. The results suggest that the sonification could convey an emotion similar to that intended, but at the cost of less good representation of the data. The addition of visual information supported understanding of the sonification, and the auditory representation of data. The results thus suggest that it is possible to design sonification that is perceived as both interesting and fun, and convey an emotional impression, but that there may be a trade off between musical experience and clarity in sonification.","2021","2023-07-24 06:48:58","2023-07-24 06:48:58","","56–63","","","","","Proceedings of the 16th International Audio Mostly Conference","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GS9VCKEW","journalArticle","2015","Seibert, Gabriela; Hug, Daniel; Cslovjecsek, Markus","Towards an Enactive Swimming Sonification: Exploring Multisensory Design and Musical Interpretation","Proceedings of the Audio Mostly 2015 on Interaction With Sound","","","10.1145/2814895.2814902","","In this paper we present a design method that integrates the exploration of visual representations and musical expertise in the process of creating a swimming sonification, and initial results of the method's application in an explorative study. Our focus lies on the creation of a sonic representation that facilitates the affective, intuitive reproduction of the crawl swim movement. The method integrates artistic creativity and a systematic design process. By combining the linguistic-conceptual, visual and auditory representation of the (imagined) movement, we aim to advance the expressive quality of the sonic representation as well as the design method in a crossmodal, holistic way. Finally we report on a qualitative evaluation of the potential of this approach to support the affective, intuitive re-enactment of the swimming movement.","2015","2023-07-24 06:48:58","2023-07-24 06:48:58","","1–8","","","","","Proceedings of the Audio Mostly 2015 on Interaction With Sound","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XB9TVSGS","journalArticle","2008","Last, Mark; Gorelik, Anna","Using sonification for mining time series data","Proceedings of the 9th International Workshop on Multimedia Data Mining: held in conjunction with the ACM SIGKDD 2008","","","10.1145/1509212.1509220","","In recent years, there is a growing interest in mining time series databases by both automated and interactive tools. In this paper, we present an interactive methodology for mining of time series data using a novel sonification technique which uses some important properties of time series and tonal music to achieve effective (accurate) and efficient (fast) results. We have created an experimental website, where participants were asked to perform some basic data exploration and mining tasks by listening to a musical display of several time series. The initial results indicate that the proposed methodology for musical representation of data allows, on one hand, to efficiently perform some decision-making tasks ""on the fly"" - by only listening to some short music examples, and on the other hand, it provides an alternative data representation for blind or visually impaired users or users who are due to their professional or personal activities (e.g., driving) cannot use their sense of vision for watching a visual display of data, but still need to get some important time-based information by using their other senses.","2008","2023-07-24 06:48:58","2023-07-24 06:48:58","","63–72","","","","","Proceedings of the 9th International Workshop on Multimedia Data Mining: held in conjunction with the ACM SIGKDD 2008","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AZMV5V9N","journalArticle","2022","Joo, Woohun","Graphic-to-Sound Sonification for Visual and Auditory Communication Design","Proceedings of the 17th International Audio Mostly Conference","","","10.1145/3561212.3561214","","I designed two sonification platforms designed for visual/auditory communication design studies and audiovisual art. The purpose of this study was to examine whether test participants can associate visuals and sound without any prior training and sonification approaches in this paper can be utilized as an interactive musical expression. The platform for the communication design study was developed first and the artistic audiovisual platform with the same sonification methodology followed next. In this paper, I introduce the (former) sonification platform designed for the image-to-sound association studies, their sonification methodologies, and present the study results. The object-oriented sonification method that I newly developed describes each shape sonically. The five image-sound association studies were conducted to see whether people can successfully associate sounds and fundamental shapes (i.e., a circle, a triangle, a square, lines, curves, and other custom shapes). Regardless of age and educational background, the correct answer rate was high.","2022","2023-07-24 06:48:59","2023-07-24 06:48:59","","1–6","","","","","Proceedings of the 17th International Audio Mostly Conference","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"23EVTPDE","journalArticle","2020","Giomi, Andrea","Somatic Sonification in Dance Performances. From the Artistic to the Perceptual and Back","Proceedings of the 7th International Conference on Movement and Computing","","","10.1145/3401956.3404226","","Since the end of the 1980s, interactive musical systems have played an increasingly relevant role in dance performances. More recently, the use of interactive auditory feedback for sensorimotor learning such as movement sonification has gained currency and scientific attention in a variety of fields ranging from rehabilitation to sport training, neuroscience and product design. This paper investigates the convergence between interactive music/dance systems and movement sonification in the field of dance. The main question we address is whether the emergence of the notion of sonification can foster new perspectives for practice-based artistic research. In this context, we highlight a fundamental shift of perspective from musical interactivity per se to the somatic knowledge provided by the real time sonification of movement, which can be considered as a major somatic-sonification turn.","2020","2023-07-24 06:48:59","2023-07-24 06:48:59","","1–8","","","","","Proceedings of the 7th International Conference on Movement and Computing","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y7E6V7KJ","journalArticle","2013","Seibert, Gabriela; Hug, Daniel","Bringing musicality to movement sonification: design and evaluation of an auditory swimming coach","Proceedings of the 8th Audio Mostly Conference","","","10.1145/2544114.2544127","","In this paper we describe a novel approach to the sonification of crawl swim movement. The design method integrates task and data analysis from a sport science perspective with subjective experience of swimmers and swimming coaches, and strongly relies on the skills of musicians in order to define the basic sonic design. We report on the design process, and on the implementation and evaluation of a first prototype.","2013","2023-07-24 06:48:59","2023-07-24 06:48:59","","1–6","","","","","Proceedings of the 8th Audio Mostly Conference","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WWHUQLIW","journalArticle","2017","Pigrem, Jon; Barthet, Mathieu","Datascaping: Data Sonification as a Narrative Device in Soundscape Composition","Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences","","","10.1145/3123514.3123537","","Soundscape composition is an art form that has grown from acoustic ecology and soundscape studies. Current practices foster a wide range of approaches, from the educational and documentary function of the world soundscape project (WSP) to the creation of imaginary sonic worlds supported by theories of acousmatic and electroacoustic music. Sonification is the process of rendering audio in response to data, and is often used in scenarios where visual representations of data are impractical. The field of auditory display has grown in isolation to soundscape composition, however fosters conceptual similarities in its representation of information in sonic form. This paper investigates the use of data sonification as a narrative tool in soundscape composition. A soundscape has been created using traditional concrete sounds (fixed media recorded sound objects), augmented with sonified real-time elements. An online survey and listening experiment was conducted, which asked participants to rate the soundscape on its ability to communicate specific detail with regard to environmental and social elements contained within. Research data collected shows a strong ability in participants to decode and comprehend additional layers of narrative information communicated through the soundscape.","2017","2023-07-24 06:48:59","2023-07-24 06:48:59","","1–8","","","","","Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CHNCKTGT","journalArticle","2020","Kalampratsidou, Vilelmini; Torres, Elizabeth B.","Sonification of heart rate variability can entrain bodies in motion","Proceedings of the 7th International Conference on Movement and Computing","","","10.1145/3401956.3404186","","In this work, we introduce a co-adaptive closed-loop interface driven by audio augmented with a parameterization of the dancer's heart-rate in near real-time. In our set-up, two salsa dancers perform their routine dance (previously choreographed and well-trained) and a spontaneously improvised piece lead by the male dancer. They firstly dance their pieces while listening to the original version of the song (baseline condition). Then, we ask them to dance while listening to the music, as altered by the heart rate extracted from the female dancer in near real-time. Salsa dancing is always led by the male. As such, their challenge is to adapt, their movements, as a dyad, to the real-time change induced by the female's heart activity. Our work offers a new co-adaptive set up for dancers, new data types and analytical methods to study two forms of dance: well-rehearsed choreography and improvisation. We show that the small variations in heart activity, despite its robustness for autonomic function, can distinguish well between these two modes of dance.","2020","2023-07-24 06:48:59","2023-07-24 06:48:59","","1–8","","","","","Proceedings of the 7th International Conference on Movement and Computing","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L8E3Y2KR","journalArticle","2017","Roddy, S.","Composing The Good Ship Hibernia and the Hole in the Bottom of the World","Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences","","","10.1145/3123514.3123520","","This paper explores topics in embodied cognition, soundscape composition and sonification. It explains the compositional decisions and technical considerations that went into the composition of the piece The Good Ship Hibernia, which is an example of embodied soundscape sonification. This explanation is undertaken within the context of an approach to both sonification design and music composition that accounts for and exploits the embodied aspects of meaning-making in auditory cognition as described in the embodied cognitive science literature.","2017","2023-07-24 06:48:59","2023-07-24 06:48:59","","1–6","","","","","Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WF6LSCGS","journalArticle","2014","Donnarumma, Marco","Notes on Bimodal Muscle Sensing for the Sonification of Indeterminate Motion","Proceedings of the 2014 International Workshop on Movement and Computing","","","10.1145/2617995.2618028","","This article offers an overview of a musical performance instrument that leverages bimodal muscle sensing for the sonification of motion. Namely, the instrument a) captures the sound produced by a performer's muscles and makes it available for real-time audio processing, and b) enables a performer to drive the processing parameters using high-level features extracted from muscle activity. This enables the performer to produce and finely shape sound with gestures that do not need to be specified beforehand as a vocabulary of a finite number of movements. This allows the performer and the software to create an open-ended range of sonified movements which arise from the interplay of bodily mechanisms and software processes.","2014","2023-07-24 06:48:59","2023-07-24 06:48:59","","170–171","","","","","Proceedings of the 2014 International Workshop on Movement and Computing","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TH4LPQVS","journalArticle","2023","Guarese, Renan; Zambetta, Fabio; van Schyndel, Ron","Evaluating micro-guidance sonification methods in manual tasks for Blind and Visually Impaired people","Proceedings of the 34th Australian Conference on Human-Computer Interaction","","","10.1145/3572921.3572929","","This paper presents a user evaluation of seven sonification methods in two-dimensional (2D) manual micro-guidance tasks, which can be used as building blocks for spatialized audio in Mixed and Virtual Reality to model next-generation guidance aids for the Blind and Visually Impaired (BVI). The methods were tested in comparable interactive sonifications of 2D positions in a series of hand-navigation assessments with BVI and blindfolded sighted users, to validate the different approaches in environments without any visual feedback. Results highlighted that alternation and spatiality can be useful resources in sonified guidance, and that users accustomed to faster-than-regular audio speed replay tend to have more precise performances, while musical literacy only had a performance effect on methods highly dependent on aural skills. Ultimately, this work corroborates the notion that sonification may help BVI users perform better in day-to-day manual micro-guidance tasks such as retrieving items from a pantry, handling kitchen appliances, and properly discarding trash.","2023","2023-07-24 06:48:59","2023-07-24 06:48:59","","260–271","","","","","Proceedings of the 34th Australian Conference on Human-Computer Interaction","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L2F8CTMD","journalArticle","2020","Mitchell, Thomas J.; Jones, Alex J.; O'Connor, Michael B.; Wonnacott, Mark D.; Glowacki, David R.; Hyde, Joseph","Towards molecular musical instruments: interactive sonifications of 17-alanine, graphene and carbon nanotubes","Proceedings of the 15th International Audio Mostly Conference","","","10.1145/3411109.3411143","","Scientists increasingly rely on computational models of atoms and molecules to observe, understand and make predictions about the microscopic world. Atoms and molecules are in constant motion, with vibrations and structural fluctuations occurring at very short time-scales and corresponding length-scales. But can these microscopic oscillations be converted into sound? And, what would they sound like? In this paper we present our initial steps towards a generalised approach for sonifying data produced by a real-time molecular dynamics simulation. The approach uses scanned synthesis to translate real-time geometric simulation data into audio. The process is embedded within a stand alone application as well as a variety of audio plugin formats to enable the process to be used as an audio synthesis method for music making. We review the relevant background literature before providing an overview of our system. Simulations of three molecules are then considered: 17-alanine, graphene and a carbon nanotube. Four examples are then provided demonstrating how the technique maps molecular features and parameters onto the auditory character of the resulting sound. A case study is then provided in which the sonification/synthesis method is used within a musical composition.","2020","2023-07-24 06:48:59","2023-07-24 06:48:59","","214–221","","","","","Proceedings of the 15th International Audio Mostly Conference","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NHJZJCDB","journalArticle","2008","Pohle, Tim; Knees, Peter; Widmer, Gerhard","Sound/tracks: real-time synaesthetic sonification and visualisation of passing landscapes","Proceedings of the 16th ACM international conference on Multimedia","","","10.1145/1459359.1459440","","When travelling on a train, many people enjoy looking out of the window at the landscape passing by. We present sound/tracks, an application that translates the perceived movement of the scenery and other visual impressions, such as passing trains, into music. The continuously changing view outside the window is captured with a camera and translated into MIDI events that are replayed instantaneously. This allows for a reflection of the visual impression, adding a sound dimension to the visual experience and deepening the state of contemplation. The application is intended to be run on both mobile phones (with built-in camera) and on laptops (with a connected Web-cam). We propose and discuss different approaches to translating the video signal into an audio stream, present different application scenarios, and introduce a method to visualise the dynamics of complete train journeys by ""re-transcribing"" the captured video frames used to generate the music.","2008","2023-07-24 06:49:00","2023-07-24 06:49:00","","599–608","","","","","Proceedings of the 16th ACM international conference on Multimedia","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9B7E953I","journalArticle","2021","Sugawa, Moe; Furukawa, Taichi; Chernyshov, George; Hynds, Danny; Han, Jiawen; Padovani, Marcelo; Zheng, Dingding; Marky, Karola; Kunze, Kai; Minamizawa, Kouta","Boiling Mind: Amplifying the Audience-Performer Connection through Sonification and Visualization of Heart and Electrodermal Activities","Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction","","","10.1145/3430524.3440653","","In stage performances, an invisible wall in front of the stage often weakens the connections between the audience and performers. To amplify this performative connection, we present the concept ”Boiling Mind”. Our design concept is based on streaming sensor data related to heart and electrodermal activities from audience members and integrating this data into staging elements, such as visual projections, music, and lighting. Thus, the internal states of the audience directly influence the staging. Artists can have a more direct perception of the inner reactions of audience members and can create physical expressions in response to them. In this paper, we present the wearable sensing system as well as design considerations of mapping heart and electrodermal activity to changes in the staging elements. We evaluated our design and setup over three live performances.","2021","2023-07-24 06:49:00","2023-07-24 06:49:00","","1–10","","","","","Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZCD7W55M","journalArticle","2021","Winters, R. Michael; Walker, Bruce N.; Leslie, Grace","Can You Hear My Heartbeat?: Hearing an Expressive Biosignal Elicits Empathy","Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems","","","10.1145/3411764.3445545","","Interfaces designed to elicit empathy provide an opportunity for HCI with important pro-social outcomes. Recent research has demonstrated that perceiving expressive biosignals can facilitate emotional understanding and connection with others, but this work has been largely limited to visual approaches. We propose that hearing these signals will also elicit empathy, and test this hypothesis with sounding heartbeats. In a lab-based within-subjects study, participants (N = 27) completed an emotion recognition task in different heartbeat conditions. We found that hearing heartbeats changed participants’ emotional perspective and increased their reported ability to “feel what the other was feeling.” From these results, we argue that auditory heartbeats are well-suited as an empathic intervention, and might be particularly useful for certain groups and use-contexts because of its musical and non-visual nature. This work establishes a baseline for empathic auditory interfaces, and offers a method to evaluate the effects of future designs.","2021","2023-07-24 06:49:00","2023-07-24 06:49:00","","1–11","","","","","Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8RQJU2HW","journalArticle","2016","Weber, Maximilian; Kuhn, Marco","KONTRAKTION: Sonification of Metagestures with electromyographic Signals","Proceedings of the Audio Mostly 2016","","","10.1145/2986416.2986421","","'Kontraktion' is an embodied musical interface using biosignals to create an immersive sonic performance setup. It explores the energetic coupling between digital synthesis and musical expression by reducing the interface to an embodied instrument and therefore tightening the connection between intention and sound. By using the setup as a biofeedback system the user explores his own subconscious gestures with a heightened sensitivity. Even subtle, usually unaware neural impulses are brought to conscious awareness by sensing muscle contractions with an armband and projecting them outward into space with sound in realtime. The users gestural expressions are embodied in sound and allow for an expressive energetic coupling between the users body and a virtual agent. Utilizing the newly adopted awareness of his body the user can take control of the sound and perform with it using the metagestures of his body as an embodied interface. The body itself is transformed into a musical instrument, controlled by neurological impulses and sonified by a virtual interpreter.","2016","2023-07-24 06:49:00","2023-07-24 06:49:00","","132–138","","","","","Proceedings of the Audio Mostly 2016","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"STY24XRN","journalArticle","2014","Rovithis, Emmanouel; Mniestris, Andreas; Floros, Andreas","Educational audio game design: sonification of the curriculum through a role-playing scenario in the audio game 'Kronos'","Proceedings of the 9th Audio Mostly: A Conference on Interaction With Sound","","","10.1145/2636879.2636902","","Audio-Games (AGs) are electronic games that feature partially or completely auditory interfaces to express the game's plot and mechanics. The required concentration on sonic information makes AGs a suitable medium not only for entertainment, but also for education on (and not limited to) music and sound studies curricula. This paper presents a novel educational AG entitled Kronos that implements a role-playing scenario to facilitate the sonification of the relevant curriculum and to create an educational platform that combines an audio-based gaming environment with a musical instrument. In that process a methodology suggested by the authors has been used. The sonic symbols assigned to create the game's narrative content will be explained and future developments will be mentioned.","2014","2023-07-24 06:49:00","2023-07-24 06:49:00","","1–6","","","","","Proceedings of the 9th Audio Mostly: A Conference on Interaction With Sound","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DZ9RFJNF","journalArticle","2018","Godbout, Andrew; Popa, Iulius A. T.; Boyd, Jeffrey E.","Emotional Musification","Proceedings of the Audio Mostly 2018 on Sound in Immersion and Emotion","","","10.1145/3243274.3243303","","We present a method for emotional musification that utilizes the musical game MUSE. We take advantage of the strong links between music and emotion to represent emotions as music. While we provide a prototype for measuring emotion using facial expression and physiological signals our sonification is not dependent on this. Rather we identify states within MUSE that elicit certain emotions and map those onto the arousal and valence spatial representation of emotion. In this way our efforts are compatible with emotion detection methods which can be mapped to arousal and valence. Because MUSE is based on states and state transitions we gain the ability to transition seamlessly from one state to another as new emotions are detected thus avoiding abrupt changes between music types.","2018","2023-07-24 06:49:01","2023-07-24 06:49:01","","1–6","","","","","Proceedings of the Audio Mostly 2018 on Sound in Immersion and Emotion","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BCYG9SR3","journalArticle","2007","Ng, Kia C.; Weyde, Tillman; Larkin, Oliver; Neubarth, Kerstin; Koerselman, Thijs; Ong, Bee","3d augmented mirror: a multimodal interface for string instrument learning and teaching with gesture support","Proceedings of the 9th international conference on Multimodal interfaces","","","10.1145/1322192.1322252","","Multimodal interfaces can open up new possibilities for music education, where the traditional model of teaching is based predominantly on verbal feedback. This paper explores the development and use of multimodal interfaces in novel tools to support music practice training. The design of multimodal interfaces for music education presents a challenge in several respects. One is the integration of multimodal technology into the music learning process. The other is the technological development, where we present a solution that aims to support string practice training with visual and auditory feedback. Building on the traditional function of a physical mirror as a teaching aid, we describe the concept and development of an ""augmented mirror"" using 3D motion capture technology.","2007","2023-07-24 06:49:01","2023-07-24 06:49:01","","339–345","","","","","Proceedings of the 9th international conference on Multimodal interfaces","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JTH8NHBT","journalArticle","2016","Chernyshov, George; Chen, Jiajun; Lai, Yenchin; Noriyasu, Vontin; Kunze, Kai","Ambient Rhythm: Melodic Sonification of Status Information for IoT-enabled Devices","Proceedings of the 6th International Conference on the Internet of Things","","","10.1145/2991561.2991564","","In this paper we explore how to embed status information of IoT-enabled devices in the acoustic atmosphere using melodic ambient sounds while limiting obtrusiveness for the user. The user can use arbitrary sound samples to represent the devices he wants to monitor. Our system combines these sound samples into a melodic ambient rhythm that contains information on all the processes or variables that user is monitoring. We focus on continuous rather than binary information (e.g. ""monitoring progress status"" rather then ""new message received""). We evaluate our system in a machine monitoring scenario focusing on 5 distinct machines/processes to monitor with 6 priority levels for each. 9 participants use our system to monitor these processes with an up to 92.44% detection rate, if several levels are combined. Participants had no previous experience with this or similar systems and had only 5-10 minute training session before the tests.","2016","2023-07-24 06:49:01","2023-07-24 06:49:01","","1–6","","","","","Proceedings of the 6th International Conference on the Internet of Things","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZN3IHT78","journalArticle","2021","Hermann, Thomas; Reinsch, Dennis","sc3nb: a Python-SuperCollider Interface for Auditory Data Science","Proceedings of the 16th International Audio Mostly Conference","","","10.1145/3478384.3478401","","This paper introduces sc3nb, a Python package for audio coding and interactive control of the SuperCollider programming environment. sc3nb supports Jupyter notebooks, enables flexible means for sound and music computing such as sound synthesis and analysis and is particularly tailored for sonification. We present the main concepts and interfaces and illustrate how to use sc3nb at hand of selected code examples for basic sonification approaches, such as audification and parameter-mapping sonification. Finally, we introduce TimedQueues which enable coordinated audiovisual displays, e.g. to synchronize matplotlib data and sc3nb-based sound rendition. sc3nb enables interactive sound applications right in the center of the pandas/numpy/scipy data science ecosystem. The open source package is hosted at GitHub and available via the Python Package Index PyPI.","2021","2023-07-24 06:49:01","2023-07-24 06:49:01","","208–215","","","","","Proceedings of the 16th International Audio Mostly Conference","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WGZ23DCF","journalArticle","2010","Dulyan, Aram; Edmonds, Ernest","AUXie: initial evaluation of a blind-accessible virtual museum tour","Proceedings of the 22nd Conference of the Computer-Human Interaction Special Interest Group of Australia on Computer-Human Interaction","","","10.1145/1952222.1952280","","Remotely accessible audio-based virtual tours can offer great utility for blind or vision impaired persons, eliminating the difficulties posed by travel to unfamiliar locations, and allowing truly independent exploration. This paper draws upon sonification techniques used in previous implementations of audio-based 3D environments to develop a prototype of blind-accessible virtual tours specifically tailored to the needs of cultural sites. A navigable 3D world is presented using spatially positioned musical earcons, accompanied by synthesised speech descriptions and navigation aids. The worlds are read from X3D models enhanced with metadata to identify and describe the rooms and exhibits, thus enabling an audio modality for existing 3D worlds and simplifying the tour creation process. The prototype, named AUXie, was evaluated by 11 volunteers with total blindness to establish a proof of concept and identify the problematic aspects of the interface. The positive response obtained confirmed the validity of the approach and yielded valuable insight into how such tours can be further improved.","2010","2023-07-24 06:49:01","2023-07-24 06:49:01","","272–275","","","","","Proceedings of the 22nd Conference of the Computer-Human Interaction Special Interest Group of Australia on Computer-Human Interaction","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K9PTQYUI","journalArticle","2021","Bakogiannis, Konstantinos; Andreopoulou, Areti; Georgaki, Anastasia","The development of a dance-musification model with the use of machine learning techniques under COVID-19 restrictions","Proceedings of the 16th International Audio Mostly Conference","","","10.1145/3478384.3478407","","Interactive technologies enable dancers to control the music in real-time with their movement. This paper presents the design and development of a model which takes as input a dancer’s movement and outputs music, structurally related to dance, with the use of machine learning techniques. Both the technical and artistic aspects of the model development are described in detail. In particular, the paper compares the use of machine learning techniques to traditional coding, in interactive dance and music applications. Moreover, it describes the significant discrimination between movement sonification and dance musification and explains why the model presented here falls into the second category. Special focus is given to the implications of the COVID-19 restrictions regarding the established collaboration with the dancer.","2021","2023-07-24 06:49:01","2023-07-24 06:49:01","","81–88","","","","","Proceedings of the 16th International Audio Mostly Conference","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PBFR4PM2","journalArticle","2008","Dodiya, Janki; Alexandrov, Vassil N.","Use of auditory cues for wayfinding assistance in virtual environment: music aids route decision","Proceedings of the 2008 ACM symposium on Virtual reality software and technology","","","10.1145/1450579.1450615","","This paper addresses the crucial problem of wayfinding assistance in the Virtual Environments (VEs). A number of navigation aids such as maps, agents, trails and acoustic landmarks are available to support the user for navigation in VEs, however it is evident that most of the aids are visually dominated. This work-in-progress describes a sound based approach that intends to assist the task of 'route decision' during navigation in a VE using music. Furthermore, with use of musical sounds it aims to reduce the cognitive load associated with other visually as well as physically dominated tasks. To achieve these goals, the approach exploits the benefits provided by music to ease and enhance the task of wayfinding, whilst making the user experience in the VE smooth and enjoyable.","2008","2023-07-24 06:49:02","2023-07-24 06:49:02","","171–174","","","","","Proceedings of the 2008 ACM symposium on Virtual reality software and technology","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V8XGJ9IP","journalArticle","2021","Reed, Courtney N.; McPherson, Andrew P.","Surface Electromyography for Sensing Performance Intention and Musical Imagery in Vocalists","Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction","","","10.1145/3430524.3440641","","Through experience, the techniques used by professional vocalists become highly ingrained and much of the fine muscular control needed for healthy singing is executed using well-refined mental imagery. In this paper, we provide a method for observing intention and embodied practice using surface electromyography (sEMG) to detect muscular activation, in particular with the laryngeal muscles. Through sensing the electrical neural impulses causing muscular contraction, sEMG provides a unique measurement of user intention, where other sensors reflect the results of movement. In this way, we are able to measure movement in preparation, vocalised singing, and in the use of imagery during mental rehearsal where no sound is produced. We present a circuit developed for use with the low voltage activations of the laryngeal muscles; in sonification of these activations, we further provide feedback for vocalists to investigate and experiment with their own intuitive movements and intentions for creative vocal practice.","2021","2023-07-24 06:49:02","2023-07-24 06:49:02","","1–11","","","","","Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"78NG9ZRN","journalArticle","2011","Jylhä, Antti; Erkut, Cumhur","Auditory feedback in an interactive rhythmic tutoring system","Proceedings of the 6th Audio Mostly Conference: A Conference on Interaction with Sound","","","10.1145/2095667.2095683","","We present the recent developments in the design of audio-visual feedback in iPalmas, the interactive Flamenco rhythm tutor. Based on evaluation of the original implementation, we have re-designed the interface to better support the user in learning and performing rhythmic patterns. The system measures the performance parameters of the user and provides auditory feedback on the performance with different sounds corresponding to different performance attributes. The design of these sounds is informed by several attributes derived from the evaluation. We propose informative, non-intrusive. and archetypal sounds to be used in the system.","2011","2023-07-24 06:49:02","2023-07-24 06:49:02","","109–115","","","","","Proceedings of the 6th Audio Mostly Conference: A Conference on Interaction with Sound","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W84Z52UV","journalArticle","2020","Hebling, Eduardo D.; Partesotti, Elena; Santana, Charles P.; Figueiredo, André; Dezotti, Cássio G.; Botechia, Tales; da Silva, César A. Pereira; da Silva, Micael A.; Rossetti, Danilo; de Oliveira, Victor A. W.; Cielavin, Sandra; Moroni, Artemis S.; Manzolli, Jônatas","MovieScape: Audiovisual Landscapes for Silent Movie: Enactive Experience in a Multimodal Installation","Proceedings of the 9th International Conference on Digital and Interactive Arts","","","10.1145/3359852.3359883","","The multimodal installation MovieScape articulates sound, image, and movement with the poetics of silent film. It is an immersive mixed-reality environment in which the boundaries between the real and the virtual are imprecise and ambiguous. In MovieScape, audiovisuals generated in real time induce the emergence of recurring motor patterns performed by the visitor. An audiovisual-scape is selected with an imaginary steering wheel that is activated when the visitor closes both fists. After this initial gesture, a sphere covered with images of the silent movie is modified with rotations and displacements that alter the sequence of scenes, music track and the sound landscape. The interaction between perception, action and movement articulates with the concept of Embodied Cognition, bringing about an enactive experience in which the participant immerses in the possibilities of the silent movie. The basic setup of the installation is presented, as well as technical aspects concerning the network communication, motion capture, sonification and visualization. A participative methodology, which is (itself) a factor of knowledge construction and artistic expression, supports the creative process of MovieScape.","2020","2023-07-24 06:49:02","2023-07-24 06:49:02","","1–7","","","","","Proceedings of the 9th International Conference on Digital and Interactive Arts","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D4RYZJMV","journalArticle","2022","Bernard, Corentin; Monnoyer, Jocelyn; Ystad, Sølvi; Wiertlewski, Michael","Eyes-Off Your Fingers: Gradual Surface Haptic Feedback Improves Eyes-Free Touchscreen Interaction","Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems","","","10.1145/3491102.3501872","","Moving a slider to set the music volume or control the air conditioning is a familiar task that requires little attention. However, adjusting a virtual slider on a featureless touchscreen is much more demanding and can be dangerous in situations such as driving. Here, we study how a gradual tactile feedback, provided by a haptic touchscreen, can replace visual cues. As users adjust a setting with their finger, they feel a continuously changing texture, which spatial frequency correlates to the value of the setting. We demonstrate that, after training with visual and auditory feedback, users are able to adjust a setting on a haptic touchscreen without looking at the screen, thereby reducing visual distraction. Every learning strategy yielded similar performance, suggesting an amodal integration. This study shows that surface haptics can provide intuitive and precise tuning possibilities for tangible interfaces on touchscreens.","2022","2023-07-24 06:49:02","2023-07-24 06:49:02","","1–10","","","","","Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MTAVS4VZ","journalArticle","2009","Kim, Si-Jung; Thangjitham, Jennifer; Winchester, Woodrow","Assessing the efficacy of a mixed-modal auditory display system for enhancing auditory sensation","Proceedings of the 47th Annual Southeast Regional Conference","","","10.1145/1566445.1566520","","This paper presents the design and evaluation of a new type of mixed-modal auditory display for enhancing auditory sensation. The purpose of this study is to determine whether or not a light apparatus in which LED lights are installed in front of a speaker panel and alternating lights based on sound frequency and intensity can support user awareness of the auditory source. Five different auditory sources were used, and a user study with 20 participants revealed that all auditory sources were better recognized when listened to with the light apparatus. The music and the emergency alert auditory source were best represented by the light apparatus. The experiment showed synchronized visual and audio representation enhanced the user's auditory sensation. Findings suggest that the light apparatus could be useful when auditory signal displays are not universally applicable because people who suffer from hearing loss are unable to use them effectively. It is expected that the result of this study could contribute to the design of hearing aids, emergency alarm device/mechanisms, communication trust or other assistive technologies that seek to provide context to received data.","2009","2023-07-24 06:49:02","2023-07-24 06:49:02","","1–2","","","","","Proceedings of the 47th Annual Southeast Regional Conference","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DA5SMJR5","journalArticle","2017","Feltham, Frank; Loke, Lian","Felt Sense through Auditory Display: A Design Case Study into Sound for Somatic Awareness while Walking","Proceedings of the 2017 ACM SIGCHI Conference on Creativity and Cognition","","","10.1145/3059454.3059461","","Walking is an everyday act that we, as humans, often take for granted. To walk requires the synergy of somatosensory, neurological and physiological processes for us to move at a regular pace by lifting and setting down each foot in turn. It can be argued that walking is also a source of creativity and exploration when conducted as an intentional act of somatic or self-awareness. This design case study aims to explore the kinds of somatic awareness and aesthetic engagement of walking apparent through the introduction of a pressure mediated sound generating surface with a group of Feldenkrais movement practitioners. These explorations reveal that there is an awareness of tempo and rhythm during the step cycle. This awareness takes on an internal focus as shifts in attention and bodily organization. Another key finding is that exploration and play are enabled due to the rich timbral qualities of the pressure mediated auditory feedback. The significance and contribution in this work is in the implications it has for the design of technologies that support kinaesthetic awareness through aesthetic and exploratory strategies.","2017","2023-07-24 06:49:02","2023-07-24 06:49:02","","287–298","","","","","Proceedings of the 2017 ACM SIGCHI Conference on Creativity and Cognition","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MGG9BM2M","journalArticle","2011","Encelle, Benoît; Ollagnier-Beldame, Magali; Pouchot, Stéphanie; Prié, Yannick","Annotation-based video enrichment for blind people: a pilot study on the use of earcons and speech synthesis","The proceedings of the 13th international ACM SIGACCESS conference on Computers and accessibility","","","10.1145/2049536.2049560","","Our approach to address the question of online video accessibility for people with sensory disabilities is based on video annotations that are rendered as video enrichments during the playing of the video. We present an exploratory work that focuses on video accessibility for blind people with audio enrichments composed of speech synthesis and earcons (i.e. nonverbal audio messages). Our main results are that earcons can be used together with speech synthesis to enhance understanding of videos; that earcons should be accompanied with explanations; and that a potential side effect of earcons is related to video rhythm perception.","2011","2023-07-24 06:49:03","2023-07-24 06:49:03","","123–130","","","","","The proceedings of the 13th international ACM SIGACCESS conference on Computers and accessibility","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KVFAN4F7","journalArticle","2018","Lorenzoni, Valerio; Maes, Pieter-Jan; Van den Berghe, Pieter; De Clercq, Dirk; de Bie, Tijl; Leman, Marc","A biofeedback music-sonification system for gait retraining","Proceedings of the 5th International Conference on Movement and Computing","","","10.1145/3212721.3212843","","Auditory feedbacks are becoming increasingly popular in sports providing opportunities for monitoring and gait (re)training in ecological environments. We present the design process of a sonification strategy for modification of running parameters. The sonification provides real-time feedback of the performance through introduction of distortion of a baseline music track. The music BPM is continuously matched to the runners' cadence. The noise-based continuous feedback was able to significantly alter the mean running cadence in a non-instructed and non-disturbing way and performed better than standard verbal instructions. Although some of the participants did not respond effectively to the feedback, a large majority of the participants positively rated the feedback system in terms of pleasantness and motivation.","2018","2023-07-24 06:49:03","2023-07-24 06:49:03","","1–5","","","","","Proceedings of the 5th International Conference on Movement and Computing","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5U2P5XII","journalArticle","2016","Volioti, Christina; Hadjidimitriou, Stelios; Manitsaris, Sotiris; Hadjileontiadis, Leontios; Charisis, Vasileios; Manitsaris, Athanasios","On mapping emotional states and implicit gestures to sonification output from the 'Intangible Musical Instrument'","Proceedings of the 3rd International Symposium on Movement and Computing","","","10.1145/2948910.2948950","","Sonification is an interdisciplinary field of research, aiming at generating sound from data based on systematic, objective and reproducible transformations. Towards this direction, expressive gestures play an important role in music performances facilitating the artistic perception by the audience. Moreover, emotions are linked with music, as sound has the ability to evoke emotions. In this vein, a combinatory approach which aims at gesture and emotion sonification in the context of music composition and performance is presented here. The added value of the proposed system is that both gesture and emotion are able to continuously manipulate the reproduced sound in real-time.","2016","2023-07-24 06:49:03","2023-07-24 06:49:03","","1–5","","","","","Proceedings of the 3rd International Symposium on Movement and Computing","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EZ2RY7QV","journalArticle","2019","Ronnberg, Niklas","Musical Elements in Sonification Support Visual Perception","Proceedings of the 31st European Conference on Cognitive Ergonomics","","","10.1145/3335082.3335097","","Visual representations of data are commonly used to communicate research results. However, such representations might introduce several possible challenges for the human visual perception system, for example in perceiving brightness levels. Sonification, adding sound to the visual representation, might be used to overcome these challenges. As sonification provides additional information, sonification could be useful in supporting interpretations of a visual perception. In the present study, usefulness in terms of accuracy of sonification was investigated with an interactive sonification test. In the experiment, participants were asked to identify the highest brightness level in a monochrome visual representation. The task was performed in four conditions, one with no sonification and three with different sonification settings. The results show that sonification is useful, as measured by higher task accuracy.","2019","2023-07-24 06:49:03","2023-07-24 06:49:03","","114–117","","","","","Proceedings of the 31st European Conference on Cognitive Ergonomics","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DNM6C889","journalArticle","2021","Emsley, Iain; Chamberlain, Alan","Sounding out the System: Multidisciplinary Web Science Platforms for Creative Sonification","Companion Publication of the 13th ACM Web Science Conference 2021","","","10.1145/3462741.3466667","","In this paper, we present our initial findings in using digital methods to consider the way that different devices can connect to the same object. We take a more experimental view of the ways in which network data might be used in compositions to help us to move beyond traditional sonification techniques into more musical territories which enables us to start to understand the ways in which archival data and tools might be used as a creative response to the data and provide a more human way of engaging with the data archive. Such approaches can inform the ways in which future research platforms for Web Science can be developed in a truly multidisciplinary way which matches the needs of the wider research community and supports public engagement.","2021","2023-07-24 06:49:03","2023-07-24 06:49:03","","50–52","","","","","Companion Publication of the 13th ACM Web Science Conference 2021","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9WX3PTWN","journalArticle","2016","Kolykhalova, Ksenia; Alborno, Paolo; Camurri, Antonio; Volpe, Gualtiero","A serious games platform for validating sonification of human full-body movement qualities","Proceedings of the 3rd International Symposium on Movement and Computing","","","10.1145/2948910.2948962","","In this paper we describe a serious games platfrom for validating sonification of human full-body movement qualities. This platform supports the design and development of serious games aiming at validating (i) our techniques to measure expressive movement qualities, and (ii) the mapping strategies to translate such qualities in the auditory domain, by means of interactive sonification and active music experience. The platform is a part of a more general framework developed in the context of the EU ICT H2020 DANCE ""Dancing in the dark"" Project n.645553 that aims at enabling the perception of nonverbal artistic whole-body experiences to visual impaired people.","2016","2023-07-24 06:49:03","2023-07-24 06:49:03","","1–5","","","","","Proceedings of the 3rd International Symposium on Movement and Computing","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TT6T99FD","journalArticle","2017","Blanco, Andrea Lorena Aldana; Grautoff, Steffen; Hermann, Thomas","CardioSounds: Real-time Auditory Assistance for Supporting Cardiac Diagnostic and Monitoring","Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences","","","10.1145/3123514.3123542","","This paper presents a real-time sonification system for Electrocardiography (ECG) monitoring and diagnostic. We introduce two novel sonification designs: (a) Auditory magnification loupe, a method to sonify important beat-to-beat variations when doing sports activities, and (b) ST-segment water ambience sonification, which aims to assist clinicians in the diagnostic process by building a soundscape that exhibits ECG signal abnormalities as the analysed signal deviates from a healthy ECG. The proposed methods were designed to assist users to unobtrusively monitor their own (or their patients') heart signal in situations when a visual-only representation is not convenient for the proper fulfilment of a given task. Using CardioSounds users receive auditory feedback in order to monitor important heart rhythm disturbances (e.g. Arrhythmia) or pathologies due to a blocking of the heart's vessels.","2017","2023-07-24 06:49:03","2023-07-24 06:49:03","","1–4","","","","","Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZCQDKCIN","journalArticle","2008","Knees, Peter; Pohle, Tim; Widmer, Gerhard","Sound/tracks: real-time synaesthetic sonification of train journeys","Proceedings of the 16th ACM international conference on Multimedia","","","10.1145/1459359.1459592","","Travelling on a train and looking out of the window at the moving scenery reveals a composition of ""visual music"" with its own tempo and rhythm, its own colours and harmonies. The project sound/tracks aims at capturing these visual impressions and translates them into a musical composition in real-time - producing an immediate and unique soundtrack to the train journey based on the passing landscape. To this end, the outside impressions are captured with a camera and translated into instantaneously played back piano music. The immediately added sound dimension allows for reflection of the visual impression and deepening of the state of contemplation. For the resulting compositions, the passing scenery can be considered the score. ""Re-transcription"" of this score to an image gives a panoramic overview over the complete journey and exhibits some interesting effects caused by the movement of the train, such as compression and stretching of passing objects. In addition to intensifying the experience of a train journey, sound/tracks permits to persistently capture and archive the fleeting impressions of journey and composition and allows for re-experiencing the trip both visually and acoustically at a later point.","2008","2023-07-24 06:49:03","2023-07-24 06:49:03","","1117–1118","","","","","Proceedings of the 16th ACM international conference on Multimedia","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W8DLUT24","journalArticle","2020","Winters, R. Michael; Koziej, Stephanie","An auditory interface for realtime brainwave similarity in dyads","Proceedings of the 15th International Audio Mostly Conference","","","10.1145/3411109.3411147","","We present a case-study in the development of a""hyperscanning"" auditory interface that transforms realtime brainwave-similarity between interacting dyads into music. Our instrument extends reality in face-to-face communication with a musical stream reflecting an invisible socio-neurophysiological signal. This instrument contributes to the historical context of brain-computer interfaces (BCIs) applied to art and music, but is unique because it is contingent on the correlation between the brainwaves of the dyad, and because it conveys this information using entirely auditory feedback. We designed the instrument to be i) easy to understand, ii) relatable and iii) pleasant for members of the general public in an exhibition context. We present how this context and user group led to our choice of EEG hardware, inter-brain similarity metric, and our auditory mapping strategy. We discuss our experience following four public exhibitions, as well as future improvements to the instrument design and user experience.","2020","2023-07-24 06:49:03","2023-07-24 06:49:03","","261–264","","","","","Proceedings of the 15th International Audio Mostly Conference","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RZMS989A","journalArticle","2022","Liu, Wanyu; Magalhaes, Michelle Agnes; Mackay, Wendy E.; Beaudouin-Lafon, Michel; Bevilacqua, Frédéric","Motor Variability in Complex Gesture Learning: Effects of Movement Sonification and Musical Background","ACM Transactions on Applied Perception","","","10.1145/3482967","","With the increasing interest in movement sonification and expressive gesture-based interaction, it is important to understand which factors contribute to movement learning and how. We explore the effects of movement sonification and users’ musical background on motor variability in complex gesture learning. We contribute an empirical study in which musicians and non-musicians learn two gesture sequences over three days, with and without movement sonification. Results show the interlaced interaction effects of these factors and how they unfold in the three-day learning process. For gesture 1, which is fast and dynamic with a direct “action-sound” sonification, movement sonification induces higher variability for both musicians and non-musicians on day 1. While musicians reduce this variability to a similar level as no auditory feedback condition on day 2 and day 3, non-musicians remain to have significantly higher variability. Across three days, musicians also have significantly lower variability than non-musicians. For gesture 2, which is slow and smooth with an “action-music” metaphor, there are virtually no effects. Based on these findings, we recommend future studies to take into account participants’ musical background, consider longitudinal study to examine these effects on complex gestures, and use awareness when interpreting the results given a specific design of gesture and sound.","2022","2023-07-24 06:49:04","2023-07-24 06:49:04","","2:1–2:21","","1","19","","ACM Transactions on Applied Perception","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2H65EGCF","journalArticle","2023","Clark, Matthew; Doryab, Afsaneh","Sounds of Health: Using Personalized Sonification Models to Communicate Health Information","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","","","10.1145/3570346","","This paper explores the feasibility of using sonification in delivering and communicating health and wellness status on personal devices. Ambient displays have proven to inform users of their health and wellness and help them to make healthier decisions, yet, little technology provides health assessments through sounds, which can be even more pervasive than visual displays. We developed a method to generate music from user preferences and evaluated it in a two-step user study. In the first step, we acquired general healthiness impressions from each user. In the second step, we generated customized melodies from music preferences in the first step to capture participants' perceived healthiness of those melodies. We deployed our surveys for 55 participants to complete on their own over 31 days. We analyzed the data to understand commonalities and differences in users' perceptions of music as an expression of health. Our findings show the existence of clear associations between perceived healthiness and different music features. We provide useful insights into how different musical features impact the perceived healthiness of music, how perceptions of healthiness vary between users, what trends exist between users' impressions, and what influences (or does not influence) a user's perception of healthiness in a melody. Overall, our results indicate validity in presenting health data through personalized music models. The findings can inform the design of behavior management applications on personal and ubiquitous devices.","2023","2023-07-24 06:49:04","2023-07-24 06:49:04","","206:1–206:31","","4","6","","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PDHKM9AG","journalArticle","2008","Andersen, Tue Haste; Zhai, Shumin","“Writing with music”: Exploring the use of auditory feedback in gesture interfaces","ACM Transactions on Applied Perception","","","10.1145/1773965.1773968","","We investigate the use of auditory feedback in pen-gesture interfaces in a series of informal and formal experiments. Initial iterative exploration showed that gaining performance advantage with auditory feedback was possible using absolute cues and state feedback after the gesture was produced and recognized. However, gaining learning or performance advantage from auditory feedback tightly coupled with the pen-gesture articulation and recognition process was more difficult. To establish a systematic baseline, Experiment 1 formally evaluated gesture production accuracy as a function of auditory and visual feedback. Size of gestures and the aperture of the closed gestures were influenced by the visual or auditory feedback, while other measures such as shape distance and directional difference were not, supporting the theory that feedback is too slow to strongly influence the production of pen stroke gestures. Experiment 2 focused on the subjective aspects of auditory feedback in pen-gesture interfaces. Participants' rating on the dimensions of being wonderful and stimulating was significantly higher with musical auditory feedback. Several lessons regarding pen gestures and auditory feedback are drawn from our exploration: a few simple functions such as indicating the pen-gesture recognition results can be achieved, gaining performance and learning advantage through tightly coupled process-based auditory feedback is difficult, pen-gesture sets and their recognizers can be designed to minimize visual dependence, and people's subjective experience of gesture interaction can be influenced using musical auditory feedback. These lessons may serve as references and stepping stones toward future research and development in pen-gesture interfaces with auditory feedback.","2008","2023-07-24 06:49:04","2023-07-24 06:49:04","","17:1–17:24","","3","7","","ACM Transactions on Applied Perception","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KVRQJXWF","journalArticle","2013","Merer, Adrien; Aramaki, Mitsuko; Ystad, Sølvi; Kronland-Martinet, Richard","Perceptual characterization of motion evoked by sounds for synthesis control purposes","ACM Transactions on Applied Perception","","","10.1145/2422105.2422106","","This article addresses the question of synthesis and control of sound attributes from a perceptual point of view. We focused on an attribute related to the general concept of motion evoked by sounds. To investigate this concept, we tested 40 monophonic abstract sounds on listeners via a questionnaire and drawings, using a parametrized custom interface. This original procedure, which was defined with synthesis and control perspectives in mind, provides an alternative means of determining intuitive control parameters for synthesizing sounds evoking motion. Results showed that three main shape categories (linear, with regular oscillations, and with circular oscillations) and three types of direction (rising, descending, and horizontal) were distinguished by the listeners. In addition, the subjects were able to perceive the low-frequency oscillations (below 8 Hz) quite accurately. Three size categories (small, medium, and large) and three levels of randomness (none, low amplitude irregularities, and high amplitude irregularities) and speed (constant speed and speeds showing medium and large variations) were also observed in our analyses of the participants' drawings. We further performed a perceptual test to confirm the relevance of the contribution of some variables with synthesized sounds combined with visual trajectories. Based on these results, a general typology of evoked motion was drawn up and an intuitive control strategy was designed, based on a symbolic representation of continuous trajectories (provided by devices such as motion capture systems, pen tablets, etc.). These generic tools could be used in a wide range of applications such as sound design, virtual reality, sonification, and music.","2013","2023-07-24 06:49:04","2023-07-24 06:49:04","","1:1–1:24","","1","10","","ACM Transactions on Applied Perception","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UULN22QN","journalArticle","2018","Manaris, Bill","Computing in the arts: the algorithm is the medium","Journal of Computing Sciences in Colleges","","","","","Algorithms have existed for at least 2,000 years (e.g., Euclid's algorithm). In music and art, algorithms appear as early as Guido d'Arezzo (ca. 1000 A.D.), and in compositions by Bach, Mozart, John Cage, Iannis Xenakis, among others. Modern examples include data sonification for scientific or aesthetic purposes, such as sonifying biosignals, images, orbits of planets, and human movement (e.g., dance), among others. This talk will focus on Computing in the Arts (CITA), an NSF-funded model curriculum, which combines creativity, problem solving, and computer programming to prepare students for graduate school and careers in technology and arts industries of the 21st century. CITA is part of the new movement to combine art and design with science, technology, engineering and math (STEM + Art = STEAM). Several examples will be presented, including: • SoundMorpheus (an innovative interface for positioning sounds via arm movements); • Diving into Infinity (a motion-based system which explores depictions of infinity in M.C. Escher's works); and • JythonMusic (a programming environment for developing interactive music experiences and systems). Bill Manaris is Professor of Computer Science, and Director of the Computing in the Arts program at the College of Charleston. His areas of expertise include computer music, human-computer interaction and artificial intelligence. He explores interaction design, modeling of aesthetics and creativity, sound spatialization, and telematics. As an undergraduate, he studied computer science and music at the University of New Orleans, and holds M.S. and Ph.D. degrees in Computer Science from the University of Louisiana. He also studied classical and jazz guitar. Recently, he published a textbook in Computer Music and Creative Programming. His research has been supported by the National Science Foundation, Google, IBM, the Louisiana Board of Regents, and the Stavros Niarchos Foundation.","2018","2023-07-24 06:49:04","2023-07-24 06:49:04","","5–6","","6","33","","Journal of Computing Sciences in Colleges","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9QKQULH9","journalArticle","2017","Rector, Kyle; Salmon, Keith; Thornton, Dan; Joshi, Neel; Morris, Meredith Ringel","Eyes-Free Art: Exploring Proxemic Audio Interfaces For Blind and Low Vision Art Engagement","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","","","10.1145/3130958","","Engagement in the arts1 is an important component of participation in cultural activities, but remains a largely unaddressed challenge for people with sensory disabilities. Visual arts are generally inaccessible to people with visual impairments due to their inherently visual nature. To address this, we present Eyes-Free Art, a design probe to explore the use of proxemic audio for interactive sonic experiences with 2D art work. The proxemic audio interface allows a user to move closer and further away from a painting to experience background music, a novel sonification, sound effects, and a detailed verbal description. We conducted a lab study by creating interpretations of five paintings with 13 people with visual impairments and found that participants enjoyed interacting with the artwork. We then created a live installation with a visually impaired artist to iterate on this concept to account for multiple users and paintings. We learned that a proxemic audio interface allows for people to feel immersed in the artwork. Proxemic audio interfaces are similar to visual because they increase in detail with closer proximity, but are different because they need a descriptive verbal overview to give context. We present future research directions in the space of proxemic audio interactions.","2017","2023-07-24 06:49:04","2023-07-24 06:49:04","","93:1–93:21","","3","1","","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WY4RSNHU","journalArticle","2016","Yu, Bin; Hu, Jun; Funk, Mathias; Feijs, Loe","A Study on User Acceptance of Different Auditory Content for Relaxation","Proceedings of the Audio Mostly 2016","","","10.1145/2986416.2986418","","The use of auditory interface at the relaxation-assisted interactive system is becoming increasingly popular. This study aims to investigate the effects of different types of auditory content on the subjective relaxation experience. The participants listened to fifteen sound samples from five categories: (a) nature white noise, (b) natural soundscape, (c) ambient music, (d) instrumental music, (e) instrumental music mixed with the natural soundscape. These auditory contents were selected or designed specifically for assisting relaxation. The study measured the subjective relaxation rating after listening to each sample and interviewed the listeners to understand what causes the differences in relaxation experience. The results indicate that the instrumental music and the combination of nature soundscape and music might be a better auditory content or audio form to induce relaxation compared to the ambient music, pure natural soundscape, and nature white noise. The findings of this study can be used in the design of musical and auditory display in many interactive systems for stress mitigation and relaxation exercises.","2016","2023-07-24 06:49:04","2023-07-24 06:49:04","","69–76","","","","","Proceedings of the Audio Mostly 2016","","","","","","","","","","","","","","","Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PGYZQD26","journalArticle","2019","Winters, R. Michael; Joshi, Neel; Cutrell, Edward; Morris, Meredith Ringel","Strategies for Auditory Display of Social Media","Ergonomics in Design","","","10.1177/1064804618788098","","Social media is an overwhelmingly visual medium, and we ask the simple question: How can the data and images of social media posts be transformed into something as meaningful and vivid in the auditory sense? Such a design would be useful for eyes-free browsing and could enhance the existing visual media. Our strategy first uses artificial intelligence systems to transform low-level input data into high-level sociocultural features. These features are then conveyed using a multifactored temporal design that uses speech, sonification, auditory scenes, and music.","2019","2023-07-24 06:49:04","2023-07-24 06:49:04","","11-15","","1","27","","Ergonomics in Design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LVIWYLXJ","journalArticle","2002","Flowers, John H.; Grafel, Douglas C.","Perception of Sonified Daily Weather Records","Proceedings of the Human Factors and Ergonomics Society Annual Meeting","","","10.1177/154193120204601711","","Human participants performed a perceptual task in which they sorted auditory (musical) displays of one-month long daily weather summaries (temperature, rainfall, and snowfall) based on perceived similarity of weather patterns. Displays for fifteen winter months and fifteen summer months were sorted by separate groups of participants. Each group sorted two sets of displays that varied in presentation speed. Multidimensional scaling analyses indicated that these displays were effective in conveying weather features important for climate comparisons for both winter and summer months, and that the faster (7.1 sec) displays were more effective than the slower (14.2 sec) displays. These results show that sonification can be an effective tool for exploring multivariate time series data, but that optimization of such displays may require consideration of the temporal constraints of auditory sensory memory and working memory.","2002","2023-07-24 06:49:05","2023-07-24 06:49:05","","1579-1583","","17","46","","Proceedings of the Human Factors and Ergonomics Society Annual Meeting","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NCELT4CN","journalArticle","2021","Šabić, Edin; Chen, Jing; MacDonald, Justin A.","Toward a Better Understanding of In-Vehicle Auditory Warnings and Background Noise","Human Factors","","","10.1177/0018720819879311","","ObjectiveThe effectiveness of three types of in-vehicle warnings was assessed in a driving simulator across different noise conditions.BackgroundAlthough there has been much research comparing different types of warnings in auditory displays and interfaces, many of these investigations have been conducted in quiet laboratory environments with little to no consideration of background noise. Furthermore, the suitability of some auditory warning types, such as spearcons, as car warnings has not been investigated.MethodTwo experiments were conducted to assess the effectiveness of three auditory warnings (spearcons, text-to-speech, auditory icons) with different types of background noise while participants performed a simulated driving task.ResultsOur results showed that both the nature of the background noise and the type of auditory warning influenced warning recognition accuracy and reaction time. Spearcons outperformed text-to-speech warnings in relatively quiet environments, such as in the baseline noise condition where no music or talk-radio was played. However, spearcons were not better than text-to-speech warnings with other background noises. Similarly, the effectiveness of auditory icons as warnings fluctuated across background noise, but, overall, auditory icons were the least efficient of the three warning types.ConclusionOur results supported that background noise can have an idiosyncratic effect on a warning’s effectiveness and illuminated the need for future research into ameliorating the effects of background noise.ApplicationThis research can be applied to better present warnings based on the anticipated auditory environment in which they will be communicated.","2021","2023-07-24 06:49:05","2023-07-24 06:49:05","","312-335","","2","63","","Human Factors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D7ZAMH6Q","journalArticle","2016","Lacherez, Philippe; Donaldson, Liam; Burt, Jennifer S.","Do Learned Alarm Sounds Interfere With Working Memory?","Human Factors","","","10.1177/0018720816662733","","Objective:To assess whether identifying (or ignoring) learned alarm sounds interferes with performance on a task involving working memory.Background:A number of researchers have suggested that auditory alarms could interfere with working memory in complex task environments, and this could serve as a caution against their use. Changing auditory information has been shown to interfere with serial recall, even when the auditory information is to be ignored. However, previous researchers have not examined well-learned patterns, such as familiar alarms.Method:One group of participants learned a set of alarms (either a melody, a rhythmic pulse, or a spoken nonword phrase) and subsequently undertook a digits-forward task in three conditions (no alarms, identify the alarm, or ignore the alarm). A comparison group undertook the baseline and ignore conditions but had no prior exposure to the alarms.Results:All alarms interfered with serial recall when participants were asked to identify them; however, only the nonword phrase interfered with recall when ignored. Moreover, there was no difference between trained and untrained participants in terms of recall performance when ignoring the alarms, suggesting that previous training does not make alarms less ignorable.Conclusion:Identifying any alarm sound may interfere with immediate working memory; however, spoken alarms may interfere even when ignored.Application:It is worth considering the importance of alarms in environments requiring high working memory performance and in particular avoiding spoken alarms in such environments.","2016","2023-07-24 06:49:05","2023-07-24 06:49:05","","1044-1051","","7","58","","Human Factors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BFMBVCPF","journalArticle","2013","Walker, Bruce N.; Lindsay, Jeffrey; Nance, Amanda; Nakano, Yoko; Palladino, Dianne K.; Dingler, Tilman; Jeon, Myounghoon","Spearcons (Speech-Based Earcons) Improve Navigation Performance in Advanced Auditory Menus","Human Factors","","","10.1177/0018720812450587","","","2013","2023-07-24 06:49:05","2023-07-24 06:49:05","","157-182","","1","55","","Human Factors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZF86JWTI","journalArticle","2000","Blattner, Meera M","A Design for Auditory Display","Proceedings of the Human Factors and Ergonomics Society Annual Meeting","","","10.1177/154193120004400159","","Auditory messages in home, office, and medical environments consist of little more than beeps, bells, and buzzers. Warning signals are particularly inappropriate for these environments because they startle, annoy, but do not inform listeners as to the source of the difficulty. As home automation becomes prevalent, unpleasant sounds will not be tolerated. Earcons are musical sounds that are easy to learn; yet designed to inform listeners of the status of automated systems. We discuss problems such as simultaneous sounds and levels of urgency below.","2000","2023-07-24 06:49:05","2023-07-24 06:49:05","","219-222","","1","44","","Proceedings of the Human Factors and Ergonomics Society Annual Meeting","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3MP7ZPYW","journalArticle","2013","Ho, Anson; Burns, Catherine","Music as an Auditory Display: Interaction Effects of Mode and Tempo on Perceived Urgency","Proceedings of the Human Factors and Ergonomics Society Annual Meeting","","","10.1177/1541931213571256","","Currently, there are very few guidelines on parameters needed to create an effective auditory display. Auditory displays can be intrusive and may not be used effectively if they are poorly designed. However, music is often in our environments as ambient noise and, instead of being intrusive, can be perceived as making the environment calmer and more productive. We present the initial steps of exploring the option of using music as a medium to develop an auditory display capable of conveying normal state information and warning information. An important feature that may impact the effectiveness of auditory warnings is perceived urgency: the impression of urgency that a sound evokes on a listener. To explore whether music could convey urgency as needed for auditory warnings, we evaluated four different musical phrases that varied in time and key signature as a method of measuring the effects of mode and tempo on perceived urgency. The effectiveness of the study was tested with twenty subjects split into a two by two factorial design: gender (male vs. female) and musical experience (experienced vs. non-experienced). The applications of this research can help develop concrete guidelines when designing effective auditory displays in order to improve users’ performance when dealing with complex interfaces.","2013","2023-07-24 06:49:05","2023-07-24 06:49:05","","1149-1153","","1","57","","Proceedings of the Human Factors and Ergonomics Society Annual Meeting","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GMWCQE3L","journalArticle","1993","Gerth, Jeffrey M.","Identification of Sounds with Multiple Timbres","Proceedings of the Human Factors and Ergonomics Society Annual Meeting","","","10.1177/154193129303700905","","The present research examined identification of complex sounds created by simultaneously playing two or more component sounds in various combinations. Sixteen component sounds were used, created by imposing four distinct temporal patterns on four basic timbres, two musical timbres and two complex real-world timbres. In the present experiment, complex sounds were created by simultaneously playing one to four component sounds, each with a different timbre. Subjects heard a complex sound, followed by a second complex sound that always differed from the first by adding a component, deleting a component or substituting a component. Subjects indicated which component had been added, deleted, or substituted. Sound changes were identified with moderate accuracy (above 60 percent). The errors committed varied with temporal pattern, timbre, sound change and density. The analyses of identification confusions indicated that subjects identified the correct timbre of the sound change even when temporal patterning was confused. The finding that temporal patterns were confused largely within the sound category of the correct response limits the previous interpretation of other research, which found that similar temporal patterns are confusable even with differences in spectra. Results of the present investigation suggest that multiple, temporal patterns with varying timbres can be presented from a single physical location to convey a change in state or status of an informative sound source. Design contributions of the present research to auditory information systems such as virtual reality are discussed. For such an application, a combination of physical separation and multiple patterns with varying timbres could provide a coherent, yet informationally complex, auditory display.","1993","2023-07-24 06:49:05","2023-07-24 06:49:05","","539-543","","9","37","","Proceedings of the Human Factors and Ergonomics Society Annual Meeting","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H39AA9M9","journalArticle","2012","Duarte, Emília; Rebelo, Francisco; Teles, Júlia; Wogalter, Michael S.","A Personalized Speech Warning Facilitates Compliance in an Immersive Virtual Environment","Proceedings of the Human Factors and Ergonomics Society Annual Meeting","","","10.1177/1071181312561427","","The effect of a personalized technology-based warning on compliance was assessed using an immersive virtual environment (IVE). Sixty university students performed an end-of-day routine security check in the IVE. Participants were asked to search for and activate safety-related devices, which involved entering several rooms. Just prior to abandoning the first room, participants were incidentally exposed to a posted warning (mandatory to disconnect the music generator) consisting of either a personal warning (i.e., a speech message with the participant’s first name) or an impersonal warning (i.e., a auditory beep signal). Compliance was determined by observing whether or not the participants pressed the button-switch as directed by the warning. Results reveal that compliance rate was significantly greater when the warning was personalized. No significant gender differences were found. Implications of these results are discussed in terms of the benefits of effective warnings.","2012","2023-07-24 06:49:06","2023-07-24 06:49:06","","2045-2049","","1","56","","Proceedings of the Human Factors and Ergonomics Society Annual Meeting","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N6VBP5LP","journalArticle","2009","Sanderson, Penelope M.","Auditory alarms for medical equipment: How do we ensure they convey their meanings?","Proceedings of the Human Factors and Ergonomics Society Annual Meeting","","","10.1177/154193120905300422","","For systems to be effective and to earn their users' trust, their signals must be readily interpreted. An international standard IEC 60601–1–8 was released in 2005 that provides guidelines on how to make auditory alarms on medical electrical equipment more recognisable and discriminable. Since the release of the standard, there have been concerns about the adequacy of its recommendations and, in particular, its proposal that manufacturers should use melodies to distinguish alarms from different sources. The melodies presented in the standard are just suggestions, but the standard does not indicate how an acceptable set of melodies can be established. Moreover, the standard does not require that developers perform thorough testing with representative users before implementing any melodies. The paper reviews studies performed over the last few years that demonstrate that the melodies suggested in IEC 60601–1–8 are ineffective. The paper also critiques suggestions that have been put forward for alternative alarm sounds, using speech synthesis techniques, better urgency mapping, and so on. Finally, criteria for future design and evaluation efforts are indicated.","2009","2023-07-24 06:49:06","2023-07-24 06:49:06","","264-268","","4","53","","Proceedings of the Human Factors and Ergonomics Society Annual Meeting","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XVTA3KQQ","journalArticle","2012","Gresham-Lancaster, Scot; Sinclair, Peter","Sonification and Acoustic Environments","Leonardo Music Journal","","","","","Sonification can allow us to connect sound and/or music via data to the environment; in another sense, by ""displaying"" data through sound, sonification participates in creating our acoustic environment. The authors consider here the significance of certain aspects of this relationship.","2012","2023-07-24 06:49:06","2023-07-24 06:49:06","","67-71","","","22","","Leonardo Music Journal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DR3QPWKJ","journalArticle","2006","Barrass, Stephen; Whitelaw, Mitchell; Bailes, Freya","Listening to the Mind Listening: An Analysis of Sonification Reviews, Designs and Correspondences","Leonardo Music Journal","","","","","Listening to the Mind Listening (LML) explored whether sonifications can be more than just ""noise"" in terms of perceived information and musical experience. The project generated an unprecedented body of 27 multichannel sonifications of the same dataset by 38 composers. The design of each sonification was explicitly documented, and there are 88 analytical reviews of the works. The public concert presenting 10 of these sonifications at the Sydney Opera House Studio drew a capacity audience. This paper presents an analysis of the reviews, the designs and the correspondences between timelines of these works.","2006","2023-07-24 06:49:06","2023-07-24 06:49:06","","13-19","","","16","","Leonardo Music Journal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3SBINHNZ","journalArticle","2005","Sturm, Bob L.","Pulse of an Ocean: Sonification of Ocean Buoy Data","Leonardo","","","","","The author presents his work in sonifying ocean buoy data for scientific, pedagogical and compositional purposes. Mapping the spectral buoy data to audible frequencies creates interesting and illuminating sonifications of ocean wave dynamics. Several phenomena can be heard, including both those visible and those invisible in graphical representations of the data. The author has worked extensively with this data to compose music and to produce ""Music from the Ocean,"" a multimedia CD-ROM demonstrating the data, the phenomena and the sonification. After a brief introduction to physical oceanography, many examples are presented and a composition and installation created from the sonifications are discussed.","2005","2023-07-24 06:49:06","2023-07-24 06:49:06","","143-149","","2","38","","Leonardo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G9X2TIJJ","journalArticle","1999","Dunn, John; Clark, Mary Anne","""Life Music"": The Sonification of Proteins","Leonardo","","","","","An artist and a biologist have collaborated on the sonification of protein data to produce the audio compact disc ""Life Music."" Here they describe the process by which this collaboration merges scientific knowledge and artistic expression to produce soundscapes from the basic building blocks of life. The soundscapes may be encountered as aesthetic experiences, as scientific inquiries or as both. The authors describe the rationale both for the artistic use of science and for the scientific use of art from the separate viewpoints of artist and scientist.","1999","2023-07-24 06:49:06","2023-07-24 06:49:06","","25-32","","1","32","","Leonardo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7SHBCZDK","journalArticle","2004","Ballora, Mark; Pennycook, Bruce; Ivanov, Plamen C.; Glass, Leon; Goldberger, Ary L.","Heart Rate Sonification: A New Approach to Medical Diagnosis","Leonardo","","","","","Ever since 1819, when Theophile Laënnec first put a block of wood to a patient's chest in order to listen to her heartbeat, physicians have used auscultation to help diagnose cardiopulmonary disorders. Here the authors describe a novel diagnostic method based in music technology. Digital musicsynthesis software is used to transform the sequence of time intervals between consecutive heartbeats into an electroacoustic soundtrack. The results show promise as a diagnostic tool and also provide the basis of an interesting musical soundscape.","2004","2023-07-24 06:49:07","2023-07-24 06:49:07","","41-46","","1","37","","Leonardo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B6TVMAH3","journalArticle","2016","Eloul, Shaltiel; Zissu, Gil; Amo, Yehiel H.; Jacoby, Nori","Motion Tracking of a Fish as a Novel Way to Control Music Performance","Leonardo","","","","","The authors have mapped the three-dimensional motion of a fish onto various electronic music performance gestures, including loops, melodies, arpeggio and DJ-like interventions. They combine an element of visualization, using an LED screen installed on the back of an aquarium, to create a link between the fish's motion and the sonified music. This visual addition provides extra information about the fish's role in the music, enabling the perception of versatile and developing auditory structures during the performance that extend beyond the sonification of the momentary motion of objects.","2016","2023-07-24 06:49:07","2023-07-24 06:49:07","","203-210","","3","49","","Leonardo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P4NJWFHF","journalArticle","2016","Barrass, Stephen","An Annotated Portfolio of Research through Design in Acoustic Sonification","Leonardo","","","","","The Hypertension Singing Bowl was shaped from a year of the author's blood pressure readings, and 3D printed in stainless steel so it would ring. This digitally fabricated singing bowl is an ""ultimate particular"" that establishes the design space of Acoustic Sonifications. This paper presents early experiments with Acoustic Sonification and analyses them using an Annotated Portfolio to identify interaction, perception, aesthetics and contemplation as important axes of the domain. This illustrates how Annotated Portfolios could also be used to analyse New Interfaces for Musical Expression.","2016","2023-07-24 06:49:07","2023-07-24 06:49:07","","72-73","","1","49","","Leonardo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WBAG6YZX","journalArticle","2014","Gingrich, Oliver; Renaud, Alain; Emets, Eugenia; Xiao, Zhidong","Transmission: A Telepresence Interface for Neural and Kinetic Interaction","Leonardo","","","","","Transmission is both a telepresence performance and a research project. As a real-time visualization tool, Transmission creates alternate representations of neural activity through sound and vision, investigating the effect of interaction on human consciousness. As a sonification project, it creates an immersive experience for two users: a soundscape created by the human mind and the influence of kinetic interaction. An electroencephalographic (EEG) headset interprets a user's neural activity. An Open Sound Control (OSC) script then translates this data into a real-time particle stream and sound environment at one end. A second user in a remote location modifies this stream in real time through body movement. Together they become a telematic musical interface—communicating through visual and sonic representation of their interactions.","2014","2023-07-24 06:49:07","2023-07-24 06:49:07","","375-385","","4","47","","Leonardo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GEJEH4BL","journalArticle","2012","Taylor, Sean; Fernström, Mikael","Marbh Chrios","Leonardo","","","","","Marbh Crhios (Dead Zone) is a multimedia artwork, part of the Lovely Weather Donegal Residencies Project, that reflects upon climate change in the context of a local community in Killybegs in County Donegal, Ireland. The work was based on scientific data about contested marine 'dead zones' that the authors represented with algorithmically generated music, sonifications and visualizations in a live performance in Mooney's Boatyard in Killybegs, involving three local ensembles.","2012","2023-07-24 06:49:07","2023-07-24 06:49:07","","192-193","","2","45","","Leonardo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8KCEI9UR","journalArticle","2007","Shi, X. J.; Cai, Y. Y.; Chan, C. W.","Electronic Music for Bio-Molecules Using Short Music Phrases","Leonardo","","","","","The authors explore protein sonification issues using Morse code theory. Short musical phrases based on protein amino acids are used to compose protein music. Rhythms and tunes familiar to teenagers are also investigated, with the aim of producing different genres of protein music. A special musical instrument, the Chinese guzheng, can be employed to play the protein music. Experiment is carried out with different proteins, including the HIV main protease. It is hoped that this study can help unveil the mysteries of nature and motivate students to learn biology.","2007","2023-07-24 06:49:07","2023-07-24 06:49:07","","137-141","","2","40","","Leonardo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JW5G6MIJ","journalArticle","2004","Ben-Tal, Oded; Berger, Jonathan","Creative Aspects of Sonification","Leonardo","","","","","A goal of sonification research is the intuitive audio representation of complex, multidimensional data. The authors present two facets of this research that may provide insight into the creative process. First, they discuss aspects of categorical perception in nonverbal auditory scene analysis and propose that these characteristics are simplified models of creative engagement with sound. Second, they describe the use of sonified data in musical compositions by each of the authors and observe aspects of the creative process in the purely aesthetic use of sonified statistical data.","2004","2023-07-24 06:49:08","2023-07-24 06:49:08","","229-232","","3","37","","Leonardo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R52EEFMG","journalArticle","2016","Barrett, Natasha","Interactive Spatial Sonification of Multidimensional Data for Composition and Auditory Display","Computer Music Journal","","","","","This article presents a new approach to interactive spatial sonification of multidimensional data as a tool for spatial sound synthesis, for composing temporal-spatial musical materials, and as an auditory display for scientists to analyze multidimensional data sets in time and space. The approach applies parameter-mapping sonification and is currently implemented in an application called Cheddar, which was programmed in Max/MSP. Cheddar sonifies data in real time, where the user can modify a wide variety of temporal, spatial, and sonic parameters during the listening process, and thus more easily uncover patterns and processes in the data than when applying non-real-time, noninteractive techniques. The design draws on existing literature concerning perception and acoustics, and it applies the author's practical experience in acousmatic composition, spectromorphology, and sound semantics, while addressing accuracy, flexibility, and ease of use. Although previous sonification applications have addressed some degree of real-time control and spatialization, this approach integrates space and sound in an interactive framework. Spatial information is sonified in high-order 3-D ambisonics, where the user can interactively move the virtual listening position to reveal details easily missed from fixed or noninteractive spatial views. Sounds used as input to the sonification take advantage of the rich spectra and extramusical attributes of acoustic sources, which, although previously theorized, are investigated here in a practical context thoroughly tested alongside acoustic and psychoacoustic considerations. Furthermore, when using Cheddar, no specialized knowledge of programming, acoustics, or psychoacoustics is required. These approaches position Cheddar at the junction between science and art. With one application serving both disciplines, the patterns and processes of science are more fluently appropriated into music or sound art, and vice versa for scientific research, science public outreach, and education.","2016","2023-07-24 06:49:08","2023-07-24 06:49:08","","47-69","","2","40","","Computer Music Journal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BJARZ8YH","journalArticle","2019","Bonet, Núria","Musical Borrowing in Sonification","Organised Sound","","","10.1017/S1355771819000220","","Sonification presents some challenges in communicating information, particularly because of the large difference between possible data to sound mappings and cognitively valid mappings. It is an information transmission process which can be described through the Shannon-Weaver Theory of Mathematical Communication. Musical borrowing is proposed as a method in sonification which can aid the information transmission process as the composer’s and listener’s shared musical knowledge is used. This article describes the compositional process of Wasgiischwashäsch (2017) which uses Rossini’s William Tell Overture (1829) to sonify datasets relating to climate change in Switzerland. It concludes that the familiarity of audiences with the original piece, and the humorous effect produced by the distortion of a well-known piece, contribute to a more effective transmission process.","2019","2023-07-24 06:49:08","2023-07-24 06:49:08","","184-194","","2","24","","Organised Sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NBBEQQQ3","journalArticle","2014","Straebel, Volker; Thoben, Wilm","Alvin Lucier's Music for Solo Performer: Experimental music beyond sonification","Organised Sound","","","10.1017/S135577181300037X","","Alvin Lucier's Music for Solo Performer (1965), often referred to as the ‘brain wave piece’, has become a key work of experimental music. Its setup, in which the brain waves of a solo performer are made to excite percussion instruments, has given the work a central place in the discourse on artistic sonification. However, only a small number of the authors making reference to the work seem to have studied the score, and even fewer have given thought to the score's implications for performance practice and aesthetic reflection. This paper pays detailed attention to these yet overlooked aspects, drawing on accounts of early performances as well as the authors’ participation in a 2012 performance led by the composer. We also trace the history of live-electronic equipment used for Music for Solo Performer and discuss the work's reception in sonification research.","2014","2023-07-24 06:49:08","2023-07-24 06:49:08","","17-29","","1","19","","Organised Sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VE2D4GTP","journalArticle","2014","Ballora, Mark","Sonification, Science and Popular Music: In search of the ‘wow’","Organised Sound","","","10.1017/S1355771813000381","","Sonification is described as an under-utilised dimension of the ‘wow!’ factor in science engagement multi-media. It is suggested that sonification's potential value, like much of the scientific visualisation content, probably lies less in hard facts and more in how it may serve as a stimulant for curiosity. Sound is described as a multi-dimensional phenomenon, and a number of approaches to creating sonifications are reviewed. Design strategies are described for five types of phenomena that were sonified for works created by cosmologist George Smoot III and percussionist/ethnomusicologist Mickey Hart, most particularly for their film Rhythms of the Universe (Hart and Smoot 2013).","2014","2023-07-24 06:49:08","2023-07-24 06:49:08","","30-40","","1","19","","Organised Sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JY4TW68V","journalArticle","2015","Whalley, Ian","Developing Telematic Electroacoustic Music: Complex networks, machine intelligence and affective data stream sonification","Organised Sound","","","10.1017/S1355771814000478","","This paper proposes expanding telematic electroacoustic music practice through the consideration of affective computing and integration with complex data streams. Current telematic electroacoustic music practice, despite the distances involved, is largely embedded in older music/sonic arts paradigms. For example, it is dominated by using concert halls, by concerns about the relationship between people and machines, and by concerns about geographically distributed cultures and natural environments. A more suitable environment for telematic sonic works is found in the inter-relationship between ‘players’ and broader contemporary networked life – one embedded in multiple real-time informational data streams. These streams will increase rapidly with the expansion of the Internet of Things (IoT), and with the increasing deployment of algorithmic decision-making and machine learning software. While collated data streams, such as news feeds, are often rendered visually, they are also partly interpreted through embodied cognition that is similar to music and sonic art interpretation. A meeting point for telematic electroacoustic music and real-time data sonification is in affective composition/performance models and data sonification. These allow for the sonic exploration of participants’ place in a matrix of increasingly networked relationships.","2015","2023-07-24 06:49:08","2023-07-24 06:49:08","","90-98","","1","20","","Organised Sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EVLKPCHB","journalArticle","2014","Worrall, David","Can Micro-Gestural Inflections Be Used to Improve the Soniculatory Effectiveness of Parameter Mapping Sonifications?","Organised Sound","","","10.1017/S135577181300040X","","Parameter mapping sonification is the most widely used technique for representing multi-dimensional data in sound. However, it is known to be unreliable when used for detecting information in some types of data. This is generally thought to be the result of the co-dependency of the psychoacoustic dimensions used in the mapping.Positing its perceptual basis in a theory of embodied cognition, the most common approach to overcoming this limitation involves techniques that afford the interactive exploration of the data using gross body gestures. In some circumstances, such exploration is not possible and, even when it is, it may be neither necessary nor sufficient.This article explores some other possible reasons for the unreliability of parameter mapping sonification and, drawing from the experience of expressive musical performance, suggests that the problem lies not in the parametric approach per se, nor in the lack of interactivity, but in the extent to which the parameters employed contribute to coherent gestalts. A method for how this might be achieved that relies on the use of micro-gestural information is proposed. While this is speculative, the use of such gestural inflections is well known in music performance, is supported by findings in neuroscience and lends itself to empirical testing.","2014","2023-07-24 06:49:08","2023-07-24 06:49:08","","52-59","","1","19","","Organised Sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y3LG547K","journalArticle","2014","Wolfe, Kristina","Sonification and the Mysticism of Negation","Organised Sound","","","10.1017/S1355771814000296","","Sonification has become a commonly used tool for data analysis, auditory feedback and compositional inspiration. It is often described in scientific terms as a means of uncovering previously unknown patterns in information or data through the use of the auditory sense. This goal seems to be objective, but the results and methodologies can be highly subjective. Moreover, the techniques and sources of information are strikingly similar to those used in mysticism, especially mysticisms of negation, even though the frames of reference and underlying perceptions of the world are markedly different. Both practitioners of sonification and apophatic mystics believe that certain types of information are incomprehensible through traditional analytic means and can only be understood through experience. In this way, sonification can be thought of as a source of mystical information.In this paper, I will discuss the similarities between sonification and apophatic mysticism, or the mysticism of negation. I will argue that the practice of sonification, as a source of mystical information, is ideally suited for creative contemplation, particularly in electronic music. I will start by providing some historical background on the mysticism of negation. I will then present several ways in which sonified knowledge (sound) is often imagined, discussed and perceived akin to a mystical object. Finally, I will discuss specific ways in which sonification exemplifies apophatic mysticism and reveals mystical information. This information – whatever its nature – can be used for creative contemplation and is a potentially invaluable source of compositional and spiritual inspiration.","2014","2023-07-24 06:49:08","2023-07-24 06:49:08","","304-309","","3","19","","Organised Sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VLQGB9BX","journalArticle","2009","Tittel, Claudia","Sound Art as Sonification, and the Artistic Treatment of Features in our Surroundings","Organised Sound","","","10.1017/S1355771809000089","","The article tries to explain different aspects of sound art in public space in the context of an understanding of a modified language of twentieth-century visual art and music. It gives a description of different approaches to colouring situations and contexts with sound. Various examples of sonification and artistic treatments in our surrounding are shown. It shows an artistic practice which is linked to social-cultural aspects and their critical role in art. Therefore sound installations are placed into a genealogy of installation practice in public space.","2009","2023-07-24 06:49:08","2023-07-24 06:49:08","","57-64","","1","14","","Organised Sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I34XB9SJ","journalArticle","2023","Lemmon, Eric","The Politics of Aesthetic Preference in Participatory Music","Organised Sound","","","10.1017/S1355771822000012","","Bringing together an array of interdisciplinary subjects, this article seeks to proffer a theory of political aesthetic preference emergent within participatory musical works. Beginning with an overview of imitation in music and then recapping the critical work advanced by Kofi Agawu and Jean-Jacques Nattiez on musical semiology, this article first delves into how musical signs are interpreted and propagated within participatory settings. Subsequently, using Jürgen Habermas’s influential theory on the public sphere as well as the critical revisions to said theory proposed by Nancy Fraser and Michael Warner, participatory musics are conceptualised as constituting the formal space of a public in which the aesthetic direction of a participatory music work is negotiated among participants. Based on an analysis of Luke Dahl, Jorge Herrera and Carr Wilkerson’s multi-user instrument and participatory work TweetDreams, this article discusses the ways in which participant inputs and choices impact the poietic process of the work due to the clear rules that are set up within its interactive and algorithmic protocols for sonification. It concludes by pointing towards other recent research on participatory works, where the framing of participatory musics as a political–aesthetic space leads to broader questions about audience power and how the latter is negotiated and shared, then poses questions for future research on the audience’s choice in refusal and dissensus.","2023","2023-07-24 06:49:09","2023-07-24 06:49:09","","53-63","","1","28","","Organised Sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CMTHWVZ3","journalArticle","2022","Robinson, Frederic Anthony; Velonaki, Mari; Bown, Oliver","Crafting the Language of Robotic Agents: A vision for electroacoustic music in human–robot interaction","Organised Sound","","","10.1017/S1355771822000358","","This article discusses the role of electroacoustic music practice in the context of human–robot interaction (HRI), illustrated by the first author’s work creating the sonic language of interactive robotic artwork Diamandini. It starts with a discussion of the role of sound in social robotics and surveys various notable conceptual approaches to robot sound. The central thesis of the article is that electroacoustic music can provide a valuable source of aesthetic considerations and creative practice for the creation of richer and more refined sonic HRIs by giving practitioners new ways to create believable sounding objects, to convey fiction, agency and animacy, and to communicate causality in auditory feedback. To demonstrate this, the authors describe the rationale for treating robot sound design as a compositional process and discuss the implications of the endeavour’s non-linear and site-specific nature. These considerations are illustrated using sound examples and design decisions made throughout the creation process of the robotic artwork. The authors conclude with observations on how the compositional process is affected by this particular application context.","2022","2023-07-24 06:49:09","2023-07-24 06:49:09","","206-218","","2","27","","Organised Sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M7HSDPRS","journalArticle","2014","Winters, R. Michael; Wanderley, Marcelo M.","Sonification of Emotion: Strategies and results from the intersection with music","Organised Sound","","","10.1017/S1355771813000411","","Emotion is a word not often heard in sonification, though advances in affective computing make the data type imminent. At times the relationship between emotion and sonification has been contentious due to an implied overlap with music. This paper clarifies the relationship, demonstrating how it can be mutually beneficial. After identifying contexts favourable to auditory display of emotion, and the utility of its development to research in musical emotion, the current state of the field is addressed, reiterating the necessary conditions for sound to qualify as a sonification of emotion. With this framework, strategies for display are presented that use acoustic and structural cues designed to target select auditory-cognitive mechanisms of musical emotion. Two sonifications are then described using these strategies to convey arousal and valence though differing in design methodology: one designed ecologically, the other computationally. Each model is sampled at 15-second intervals at 49 evenly distributed points on the AV space, and evaluated using a publically available tool for computational music emotion recognition. The computational design performed 65 times better in this test, but the ecological design is argued to be more useful for emotional communication. Conscious of these limitations, computational design and evaluation is supported for future development.","2014","2023-07-24 06:49:09","2023-07-24 06:49:09","","60-69","","1","19","","Organised Sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IY8KUL6W","journalArticle","2014","Ikeshiro, Ryo","Audification and Non-Standard Synthesis in Construction in Self","Organised Sound","","","10.1017/S1355771813000435","","The author's Construction in Self (2009) belongs to the interdisciplinary context of auditory display/music. Its use of data at audio rate could be described as both audification and non-standard synthesis. The possibilities of audio-rate data use and the relation between the above descriptions are explored, and then used to develop a conceptual and theoretical basis of the work.Vickers and Hogg's term ‘indexicality’ is used to contrast audio with control rate. The conceptual implications of its use within the digital medium and the possibility for the formation of higher-order structures are discussed. Grond and Hermann's notion of ‘familiarity’ is used to illustrate the difference between audification and non-standard synthesis, and the contexts of auditory display and music respectively. Familiarity is given as being determined by Dombois and Eckel's categories of data. Kubisch's Electrical Walks, Xenakis's GENDYN and the audification of seismograms are used as examples. Bogost's concept of the alien is introduced, and its relevance to the New Aesthetic and Algorave are discussed. Sound examples from Construction in Self are used to demonstrate the varying levels of familiarity or noise possible and suggested as providing a way of bridging the divide between institutional and underground electronic music.","2014","2023-07-24 06:49:09","2023-07-24 06:49:09","","78-89","","1","19","","Organised Sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZPZNABUJ","journalArticle","2012","Ikeshiro, Ryo","Audiovisual Harmony: The realtime audiovisualisation1 of a single data source in Construction in Zhuangzi","Organised Sound","","","10.1017/S1355771812000076","","This paper explores the context and technical and aesthetic considerations behind the author's generative and improvisational audiovisual work, Construction in Zhuangzi (2011), and in particular the approach of ‘audiovisualising’ the same source of data and its validation, and its possibilities as an artistic practice. First, the origins of integrated audiovisual art in the output of John Whitney are explored. Then, metaphors based on musical textures are used to describe different approaches to the audiovisual medium. Research into perception and auditory displays are next used to justify the simultaneous representation of the same data in both the audio and the video. The aesthetic potential of this practice is then corroborated using Michel Chion's theory of sound in cinema. In concluding, its possibilities for providing an appropriate form and aesthetic approach to the audiovisual material are discussed.","2012","2023-07-24 06:49:09","2023-07-24 06:49:09","","148-155","","2","17","","Organised Sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CUD79PQW","journalArticle","1996","Barrass, Stephen","Sculpting a sound space with information properties","Organised Sound","","","10.1017/S1355771896000040","","","1996","2023-07-24 06:49:09","2023-07-24 06:49:09","","125-136","","2","1","","Organised Sound","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T2XQ36D2","journalArticle","2022","Helmuth, Mara; Schedel, Margaret","Links between sonification and generative music","Interdisciplinary Science Reviews","","","10.1080/03080188.2022.2035106","","The authors investigate works by four composers who employed technology in the creation of music employing audification, sonification and algorithmic composition techniques. These compositions involve interdisciplinary collaborations either with scientific researchers, in the case of Annea Lockwood's Dusk and Carla Scaletti's hàgg, or artificial intelligence, in George Lewis's Voyager, and Bob Sturm's The Waters of Heanny. Each composer's choice of input data, custom-designed tools and personal compositional processes result in unique, expressive works that challenge the listener to expand their view of music and reality.","2022","2023-07-24 06:49:09","2023-07-24 06:49:09","","215-242","","2","47","","Interdisciplinary Science Reviews","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"82C3I3C7","journalArticle","2021","Devroop, Chatradari ‘Chats’; Titlestad, Michael","Sonification and Music: Science meets Art","English Studies in Africa","","","10.1080/00138398.2021.1969120","","The opposites, science and the arts, have always enjoyed a relationship. Recently, this relationship has been expressed in sonification, a branch of science seeking to add sound to data, giving data music-like intelligibility. Scientists believe that our aural capabilities are a potentially rich source of data that could assist in problem solving. In 2020, a sonic realization of the coronavirus was generated using its spike protein data. This sonification endeavoured to probe the coronavirus aurally. However, the creators of this sonified scientific probe are now claiming that their experiment is also a music composition. We examine this claim. This paper is underpinned by the conviction that not all sound is music. Music cannot represent anything other than itself because our understanding of music is always via allegory. Therefore, the efforts of Buehler, it is argued, are misdirected and trivial when placed in the stressed socio-political context of COVID-19.","2021","2023-07-24 06:49:09","2023-07-24 06:49:09","","181-191","","1-2","64","","English Studies in Africa","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QGZIQF6Y","journalArticle","2019","Franjou, Sebastian L.; Milazzo, Mario; Yu, Chi-Hua; Buehler, Markus J.","Sounds interesting: can sonification help us design new proteins?","Expert Review of Proteomics","","","10.1080/14789450.2019.1697236","","Introduction: The practice of turning scientific data into music, a practice known as sonification, is a growing field. Driven by analogies between the hierarchical structures of proteins and many forms of music, multiple attempts of mapping proteins to music have been made. Previous works have either worked at a low level, mapping amino acid to notes, or at a higher level, using the overall structure as a basis for composition.Areas covered: We report a comprehensive mapping strategy that encompasses the encoding of the geometry of proteins, in addition to the amino acid sequence and secondary structure information. This leads to a piece of music that is both more complete and closely linked to the original protein. By using this mapping, we can invert the process and map music to proteins, retrieving not only the amino acid sequence but also the secondary structure and folding from musical data.Expert opinion: We can train a machine learning model on ‘protein music’ to generate new music that can be translated to new proteins. By selecting proper datasets and conditioning parameters on the generative model, we could tune de novo proteins with high level parameters to achieve certain protein design features.","2019","2023-07-24 06:49:09","2023-07-24 06:49:09","","875-879","","11-12","16","","Expert Review of Proteomics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MDSRBK9J","journalArticle","2006","Dean, Roger T.; Whitelaw, Mitchell; Smith, Hazel; Worrall, David","The mirage of real-time algorithmic synaesthesia: Some compositional mechanisms and research agendas in computer music and sonification","Contemporary Music Review","","","10.1080/07494460600760981","","This article looks at algorithmic synaesthesia, a form of sonic intermedia involving synchronous computer-mediated manipulation of sound and image. In algorithmic synaesthesia extensively shared features are created in the two media. Examples of such work by austraLYSIS and others are discussed. What an audience member can cognitively access in such synaesthesia is considered: creators of intermedia works may overestimate this. The fact that a machine can process image and sound in parallel, and by the same algorithm, does not establish that the human brain can. The transparency of an algorithmic process to a listener-viewer-screener is a core issue in auditory display (or ‘sonification’). Sonification aims to make the segmentation of a data set more accessible than it is when represented numerically or visually, and has many practical and creative applications. Current approaches in experimental cognition may assist us in evaluating these issues.","2006","2023-07-24 06:49:09","2023-07-24 06:49:09","","311-326","","4","25","","Contemporary Music Review","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3N9EPVJ6","journalArticle","2019","Rönnberg, Niklas","Musical sonification supports visual discrimination of color intensity","Behaviour & Information Technology","","","10.1080/0144929X.2019.1657952","","Visual representations of data introduce several possible challenges for the human visual perception system in perceiving brightness levels. Overcoming these challenges might be simplified by adding sound to the representation. This is called sonification. As sonification provides additional information to the visual information, sonification could be useful in supporting the visual perception. In the present study, usefulness (in terms of accuracy and response time) of sonification was investigated with an interactive sonification test. In the test, participants were asked to identify the highest brightness level in a monochrome visual representation. The task was performed in four conditions, one with no sonification and three with different sonification settings. The results show that sonification is useful, as measured by higher task accuracy, and that the participant's musicality facilitates the use of sonification with better performance when sonification was used. The results were also supported by subjective measurements, where participants reported an experienced benefit of sonification.","2019","2023-07-24 06:49:10","2023-07-24 06:49:10","","1028-1037","","10","38","","Behaviour & Information Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4CWB9JV4","journalArticle","2022","Nadri, Chihab; Anaya, Chairunisa; Yuan, Shan; Jeon, Myounghoon","From Visual Art to Music: Sonification Can Adapt to Painting Styles and Augment User Experience","International Journal of Human–Computer Interaction","","","10.1080/10447318.2022.2091210","","Advances in the fields of data processing and sonification have been applied to transcribe a variety of visual experiences into an auditory format. Although image sonification examples exist, the application of these principles to visual art has not been examined thoroughly. We sought to develop and evaluate a set of guidelines for the sonification of visual artworks. Through conducting expert interviews (N = 11), we created an initial sonification algorithm that accounts for art style, lightness, and color diversity to modulate the sonified output in terms of tempo and pitch. This algorithm was evaluated through user evaluations (N = 22). User study responses supported expert interview findings, the notion that sonification can be designed to match the experience of viewing an artwork, and showed interesting interaction effects among art styles, visual components, and musical parameters. We suggest the proposed guidelines can augment visitor experiences at art exhibits and provide the basis for further experimentation.","2022","2023-07-24 06:49:10","2023-07-24 06:49:10","","1-13","","0","0","","International Journal of Human–Computer Interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D6EKXYH4","journalArticle","2005","Ekdale, A. A.; Tripp, Alan C.","Paleontological Sonification: Letting Music Bring Fossils to Your Ears","Journal of Geoscience Education","","","10.5408/1089-9995-53.3.271","","Sonification is the process of translating any type of data into sound. In paleontology, it is possible to render various aspects of fossil shapes, such as cephalopod suture patterns or brachiopod commissure lines, as a series of musical tones that can be recognized easily by the human ear. Paleontological applications of sonification might enable auditory perception of morphologic patterns in fossils that may or may not be visually apparent. Some simple classroom demonstrations can help students understand the potential of using sound to identify different types of fossils with their eyes closed (i.e., using their ears alone).","2005","2023-07-24 06:49:10","2023-07-24 06:49:10","","271-280","","3","53","","Journal of Geoscience Education","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8JG3PASU","journalArticle","2021","Reddy, Andrew","On the use of nuclear magnetic resonance spectroscopy in music composition- principles, practice and possibilities","Journal of New Music Research","","","10.1080/09298215.2022.2043389","","Nuclear Magnetic Resonance (NMR) spectroscopy is an analytical technique commonly used across the natural sciences that works upon the detection of magnetically spin-aligned atomic nuclei resonating with an externally applied radio source. This work presents an overview of music created using NMR data and outlines principles for its use in composition. Methods for the sonification and musical interpretation of NMR spectra are demonstrated and opportunities for its development as a musical technique are identified.Abbreviations: 12-TET: Twelve-Tone Equal Temperament; ADC: Analogue to Digital Converter; COSY: Correlation Spectroscopy; FID: Free Induction Decay; FT: Fourier Transform; NMR: Nuclear Magnetic Resonance; PCM: Pulse Code Modulation; PPM: Parts per million; SNR: Signal-to-noise ratio; TMS: Tetramethylsilane","2021","2023-07-24 06:49:10","2023-07-24 06:49:10","","487-501","","5","50","","Journal of New Music Research","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DDIX8G3Z","journalArticle","2016","Godøy, Rolf Inge; Song, Minho; Nymoen, Kristian; Haugen, Mari Romarheim; Jensenius, Alexander Refsum","Exploring Sound-Motion Similarity in Musical Experience","Journal of New Music Research","","","10.1080/09298215.2016.1184689","","People tend to perceive many and also salient similarities between musical sound and body motion in musical experience, as can be seen in countless situations of music performance or listening to music, and as has been documented by a number of studies in the past couple of decades. The so-called motor theory of perception has claimed that these similarity relationships are deeply rooted in human cognitive faculties, and that people perceive and make sense of what they hear by mentally simulating the body motion thought to be involved in the making of sound. In this paper, we survey some basic theories of sound-motion similarity in music, and in particular the motor theory perspective. We also present findings regarding sound-motion similarity in musical performance, in dance, in so-called sound-tracing (the spontaneous body motions people produce in tandem with musical sound), and in sonification, all in view of providing a broad basis for understanding sound-motion similarity in music.","2016","2023-07-24 06:49:10","2023-07-24 06:49:10","","210-222","","3","45","","Journal of New Music Research","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9GQAR6ZI","journalArticle","2018","Yu, Bin; Funk, Mathias; Hu, Jun; Feijs, Loe","Unwind: a musical biofeedback for relaxation assistance","Behaviour & Information Technology","","","10.1080/0144929X.2018.1484515","","Unwind is a musical biofeedback interface which combines nature sounds and sedative music into a form of New-Age music for relaxation exercises. The nature sounds respond to the user’s physiological data, functioning as an informative layer for biofeedback display. The sedative music aims to induce calmness and evoke positive emotions. UnWind incorporates the benefits of biofeedback and sedative music to facilitate deep breathing, moderate arousal, and promote mental relaxation. We evaluated Unwind in a 2 × 2 factorial experiment with music and biofeedback as independent factors. Forty young adults performed the relaxation exercise under one of the following conditions after experiencing a stressful task: Nature sounds only (NS), Nature sounds with music (NM), and Auditory biofeedback with nature sounds (NSBFB), and UnWind musical biofeedback (NMBFB). The results revealed a significant interaction effect between music and biofeedback on the improvement of heart rate variability. The combination of music and nature sounds also showed benefits in lowering arousal and reducing self-report anxiety. We conclude with a discussion of UnWind for biofeedback and the wider potential of blending nature sounds with music as a musical interface.","2018","2023-07-24 06:49:10","2023-07-24 06:49:10","","800-814","","8","37","","Behaviour & Information Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P2REASWT","journalArticle","2019","Ballatore, Andrea; Gordon, David; Boone, Alexander P.","Sonifying data uncertainty with sound dimensions","Cartography and Geographic Information Science","","","10.1080/15230406.2018.1495103","","The communication of data uncertainty is a crucial problem in data science, information visualization, and geographic information science (GIScience). Effective ways to communicate the uncertainty of data enables data consumers to interpret the data as intended by the producer, reducing the possibilities of misinterpretation. In this article, we report on an empirical investigation of how sound can be used to convey information about data uncertainty in an intuitive way. To answer the research question How intuitive are sound dimensions to communicate uncertainty? we carry out a cognitive experiment, where participants were asked to interpret the certainty/uncertainty level in two sounds A and B (N = 33). We produce sound stimuli by varying sound dimensions, including loudness, duration, location, pitch, register, attack, decay, rate of change, noise, timbre, clarity, order, and harmony. In the stimuli, both synthetic and natural sounds are used to allow comparison. The experiment results identify three sound dimensions (loudness, order, and clarity) as significantly more intuitive to communicate uncertainty, providing guidelines for sonification and information visualization practitioners.","2019","2023-07-24 06:49:10","2023-07-24 06:49:10","","385-400","","5","46","","Cartography and Geographic Information Science","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CNB5S92Y","journalArticle","2008","Balakrishnan, G.; Sainarayanan, G.","Stereo Image Processing Procedure for Vision Rehabilitation","Applied Artificial Intelligence","","","10.1080/08839510802226777","","This article presents a review on vision-aided systems and proposes an approach for visual rehabilitation using stereo vision technology. The proposed system utilizes stereo vision, image processing methodology, and a sonification procedure to support blind mobilization. The developed system includes wearable computer, stereo cameras as vision sensor, and stereo earphones, all molded in a helmet. The image of the scene in front of the visually handicapped is captured by the vision sensors. The captured images are processed to enhance the important features in the scene in front for mobilization assistance. The image processing is designed as a model of human vision by identifying the obstacles and their depth information. The processed image is mapped onto musical stereo sound for the blind's understanding of the scene in front. The developed method has been tested in the indoor and outdoor environments and the proposed image processing methodology is found to be effective for object identification.","2008","2023-07-24 06:49:10","2023-07-24 06:49:10","","501-522","","6","22","","Applied Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NIB4638H","journalArticle","2019","Burdick, Kendall J.; Bell, Abigail S.; McCoy, Mary C.; Samuels, Jonathan L.; Jolly, Alex S.; Patel, Seema S.; Balas, Julia B.; Patten, K. Jakob; Schlesinger, Joseph J.","Using Multisensory Haptic Integration to Improve Monitoring in the Intensive Care Unit","Auditory Perception & Cognition","","","10.1080/25742442.2020.1773194","","Introduction Alarm fatigue and medical alarm mismanagement reduces the quality of patient care and creates stressful work environments for clinicians. Here, the feasibility of a novel “pre-alarm” system that utilizes multisensory integration of auditory and haptic stimuli is examined as a possible solution. Methods Three vital signs (heart rate, blood pressure, and blood oxygenation) were represented by three musically distinct sounds that were combined into soundscapes and progressed through five pre-alarm zones (very low to very high). Three haptic conditions were tested with the auditory stimulus to determine the best combination of auditory and haptic stimulation. Qualitative data was collected through surveys and the NASA TLX index. Results Alterations in frequency and timbre were most effective at transmitting information regarding changing vital sign zones with comparatively higher accuracy and quicker reaction time (RT), p <.01. The addition of haptic stimuli to the auditory soundscape caused no significant decline in study participant accuracy or RT. However, two weeks after training, participants performed the tasks significantly faster (p <.001) and felt the alarm monitoring task was significantly less cognitively demanding (p <.01), compared to the unisensory condition. Participants also felt more confident in identifying changing vital signs with the addition of haptic stimuli. Discussion The current study demonstrates that multisensory signals do not diminish the perception of transmitted information and suggest efficient training benefits over unimodal signals. Multisensory training may be beneficial over time compared to unisensory training due to a stronger consolidation effect. The potential integration of haptic input with existing auditory alarm systems and training is supported.","2019","2023-07-24 06:49:11","2023-07-24 06:49:11","","188-206","","4","2","","Auditory Perception & Cognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4AGWEMJ2","journalArticle","2002","Herder, Jens; Cohen, Michael","The Helical Keyboard: Perspectives for Spatial Auditory Displays and Visual Music","Journal of New Music Research","","","10.1076/jnmr.31.3.269.14180","","Auditory displays with the ability to dynamically spatialize virtual sound sources under real-time conditions enable advanced applications for art and music. A listener can be deeply immersed while interacting and participating in the experience. We review some of those applications while focusing on the Helical Keyboard project and discussing the required technology. Inspired by the cyclical nature of octaves and helical structure of a scale, a model of a piano-style keyboard was prepared, which was then geometrically warped into a helicoidal configuration, one octave/revolution, pitch mapped to height and chroma. It can be driven by MIDI events, real-time or sequenced, which stream is both synthesized and spatialized by a spatial sound display. The sound of the respective notes is spatialized with respect to sinks, avatars of the human user, by default in the tube of the helix. Alternative coloring schemes can be applied, including a color map compatible with chromastereoptic eyewear. The graphical display animates polygons, interpolating between the notes of a chord across the tube of the helix. Recognition of simple chords allows directionalization of all the notes of a major triad from the position of its musical root. The system is designed to allow, for instance, separate audition of harmony and melody, commonly played by the left and right hands, respectively, on a normal keyboard. Perhaps the most exotic feature of the interface is the ability to fork one’s presence, replicating subject instead of object by installing multiple sinks at arbitrary places around a virtual scene so that, for example, harmony and melody can be separately spatialized, using two heads to normalize the octave; such a technique effectively doubles the helix from the perspective of a single listener. Rather than a symmetric arrangement of the individual helices, they are perceptually superimposed in-phase, co-extensively, so that corresponding notes in different registers are at the same azimuth.","2002","2023-07-24 06:49:11","2023-07-24 06:49:11","","269-281","","3","31","","Journal of New Music Research","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XP83STB2","journalArticle","2020","Snook, Kelly; Barri, Tarik; Bolles, Monica; Ericson, Petter; Fravel, Carl; Goßmann, Joachim; Green-Mateu, Susan E.; Luck, Andrew; Schedel, Margaret; Thomas, Robert","Concordia: A musical XR instrument for playing the solar system","Journal of New Music Research","","","10.1080/09298215.2020.1714666","","Kepler Concordia, a new scientific and musical instrument enabling players to explore the solar system and other data within immersive extended-reality (XR) platforms, is being designed by a diverse team of musicians, artists, scientists and engineers using audio-first principles. The core instrument modules will be launched in 2019 for the 400th anniversary of Johannes Kepler's Harmonies of the World, in which he laid out a framework for the harmony of geometric form as well as the three laws of planetary motion. Kepler's own experimental process can be understood as audio-first because he employed his understanding of Western Classical music theory to investigate and discover the heliocentric, elliptical behaviour of planetary orbits. Indeed, principles of harmonic motion govern much of our physical world and show up at all scales in mathematics and physics. Few physical systems, however, offer such rich harmonic complexity and beauty as our own solar system. Concordia is a musical instrument that is modular, extensible and designed to allow players to generate and explore transparent sonifications of planetary movements rooted in the musical and mathematical concepts of Johannes Kepler as well as researchers who have extended Kepler's work, such as Hartmut Warm. Its primary function is to emphasise the auditory experience by encouraging musical explorations using sonification of geometric and relational information of scientifically accurate planetary ephemeris and astrodynamics. Concordia highlights harmonic relationships of the solar system through interactive sonic immersion. This article explains how we prioritise data sonification and then add visualisations and gamification to create a new type of experience and creative distributed-ledger powered ecosystem. Kepler Concordia facilitates the perception of music while presenting the celestial harmonies through multiple senses, with an emphasis on hearing, so that, as Kepler wrote, ‘the mind can seize upon the patterns’.","2020","2023-07-24 06:49:11","2023-07-24 06:49:11","","88-103","","1","49","","Journal of New Music Research","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q2E9ATSK","journalArticle","2006","Dayé, Christian; de Campo, Alberto","Sounds sequential: sonification in the social sciences","Interdisciplinary Science Reviews","","","10.1179/030801806X143286","","This article discusses the use of sound for auditory information display, and in particular its application for exploration of scientific data, known as sonification. Sonification can be defined as the use of sound to display data of scientific interest in order to investigate structures, trends or patterns in the data. Background is provided from several perspectives: the use of the senses in the history of science, the strengths of human hearing, the recent technological availability of auditory interfaces, the development of sonification itself, and differentiation of sonification from musical practices. In Western science, as in Western culture, the eye has become the predominant organ of sense; using the ear consciously in research thus implicitly questions the implications of the eye's predominance. As practical examples, two applications of sonification to data-sets from the social sciences are discussed in detail. We argue that the most promising areas of application of sonification within the social sciences are in the exploration of sequential data. Our two examples both concern sets of sequential data, one temporal, the other spatial (geographical). Discussion of these examples is followed by consideration of the practical and cultural implications of working with sonification. We thus hope to further the use of sonification in the social sciences, not as an alternative to visualisation or statistical approaches, but as a complementary tool of data analysis and exploration.","2006","2023-07-24 06:49:11","2023-07-24 06:49:11","","349-364","","4","31","","Interdisciplinary Science Reviews","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7SWZPCAU","journalArticle","2015","Larsson, Pontus; Niemand, Mathias","Using Sound to Reduce Visual Distraction from In-vehicle Human–Machine Interfaces","Traffic Injury Prevention","","","10.1080/15389588.2015.1020111","","Objective: Driver distraction and inattention are the main causes of accidents. The fact that devices such as navigation displays and media players are part of the distraction problem has led to the formulation of guidelines advocating various means for minimizing the visual distraction from such interfaces. However, although design guidelines and recommendations are followed, certain interface interactions, such as menu browsing, still require off-road visual attention that increases crash risk. In this article, we investigate whether adding sound to an in-vehicle user interface can provide the support necessary to create a significant reduction in glances toward a visual display when browsing menus.Methods: Two sound concepts were developed and studied; spearcons (time-compressed speech sounds) and earcons (musical sounds). A simulator study was conducted in which 14 participants between the ages of 36 and 59 took part. Participants performed 6 different interface tasks while driving along a highway route. A 3 × 6 within-group factorial design was employed with sound (no sound /earcons/spearcons) and task (6 different task types) as factors. Eye glances and corresponding measures were recorded using a head-mounted eye tracker. Participants’ self-assessed driving performance was also collected after each task with a 10-point scale ranging from 1 = very bad to 10 = very good. Separate analyses of variance (ANOVAs) were conducted for different eye glance measures and self-rated driving performance.Results: It was found that the added spearcon sounds significantly reduced total glance time as well as number of glances while retaining task time as compared to the baseline (= no sound) condition (total glance time M = 4.15 for spearcons vs. M = 7.56 for baseline, p =.03). The earcon sounds did not result in such distraction-reducing effects. Furthermore, participants ratings of their driving performance were statistically significantly higher in the spearcon conditions compared to the baseline and earcon conditions (M = 7.08 vs. M = 6.05 and M = 5.99 respectively, p =.035 and p =.002).Conclusions: The spearcon sounds seem to efficiently reduce visual distraction, whereas the earcon sounds did not reduce distraction measures or increase subjective driving performance. An aspect that must be further investigated is how well spearcons and other types of auditory displays are accepted by drivers in general and how they work in real traffic.","2015","2023-07-24 06:49:11","2023-07-24 06:49:11","","S25-S30","","sup1","16","","Traffic Injury Prevention","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PXLV3Q7R","journalArticle","1997","Stevens, Robert D.; Edwards, Alistair D.N.; Harling, Philip A.","Access to Mathematics for Visually Disabled Students Through Multimodal Interaction","Human–Computer Interaction","","","10.1080/07370024.1997.9667240","","Mathematics relies on visual forms of communication and is thus largely inaccessible to people who cannot communicate in this manner because of visual disabilities. This article outlines the Mathtalk project, which addressed this problem by using computers to produce multimodal renderings of mathematical information. This example is unusual in that it is essential to use multiple modalities because of the nature and the difficulty of the application. In addition, the emphasis is on nonvisual (and hence novel) modalities. Crucial to designing a usable auditory interface to algebra notation is an understanding of the differences between visual and listening reading, particularly those aspects that make the former active and the latter passive. A discussion of these differences yields the twin themes of compensation for lack of external memory and provision of control over information flow. These themes were addressed by: the introduction of prosody to convey algebraic structure in synthetically spoken expressions; the provision of structure-based browsing functions; and the use of a prosody-based musical glance based on algebra earcons. The addition of prosody, when compared to a traditional method of presenting spoken algebra, was experimentally shown to increase the recovery of algebraic structure, enhance the retention of content, and reduce mental workload. These three factors can be said to compensate for the lack of an adequate external memory. Evaluations showed that the browsing functions and associated command language gave the fast and accurate control over information flow that is necessary for active reading. The algebra earcon was experimentally shown to convey the presence, location, and size of algebraic constructs within an expression in a manner that might be used as a rapid glance. Finally, an evaluation of the integrated components showed that the design principles derived from the Mathtalk program can give a more usable, active reading of algebra notation than that possible with traditional methods.","1997","2023-07-24 06:49:11","2023-07-24 06:49:11","","47-92","","1-2","12","","Human–Computer Interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HHJDZG8F","journalArticle","2018","Axon, Louise; Goldsmith, Michael; Creese, Sadie","Sonification Mappings: Estimating Effectiveness, Polarities and Scaling in an Online Experiment","J. Audio Eng. Soc","","","","","Sonification is a technique to present data arrays as sound, thereby taking advantage of the human ability to hear patterns that might otherwise not be apparent. Mappings from parameters of data to parameters of sound form the basis of parameter-mapping sonification. The choice of mappings and their design can influence both the utility of the sonification system and the ability of users to interpret the sounds. In this article the authors demonstrate the use of a time-efficient methodology with an experimental online platform for assessing mappings. Experiments explored the effectiveness of various mappings, and the discussions explore the implications of each approach. Based on the responses of 100 participants in an online Magnitude Estimation experiment, the effectiveness of 16 data-sound mappings was explored. Results showed that mappings involving certain sound parameters were generally effective, while those using other sound parameters varied in their effectiveness. In some cases the ability to interpret mappings and the polarities with which they were perceived varied among individuals using them. The mappings that used the tempo parameter were generally perceived effectively, while those using other sound parameters varied. Exploratory observations suggest that differences among participants might be related to different levels of musical experience.","2018","2023-07-24 06:49:12","2023-07-24 06:49:12","","1016–1032","","12","66","","J. Audio Eng. Soc","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T4SMXYFA","journalArticle","2012","Bujacz, Michal; Skulimowski, Piotr; Strumillo, Pawel","Naviton—A Prototype Mobility Aid for Auditory Presentation of Three-Dimensional Scenes to the Visually Impaired","J. Audio Eng. Soc","","","","","To augment the task of navigation and orientation of blind individuals, a new travel aid uses 3D scene sonification to present information about the environment using nonverbal audio. The model is composed of two classes of objects: obstacles and planes. The algorithm uses scene image segmentation, personalized spatial audio, musical tones, and sonar-like sound patterns. Individually measured head-related transfer functions were used to provide users with the illusion of sounds originating from the locations of sonified scene elements. Using a segmented and parametric description overcomes the sensory mismatch between visual and auditory perception. In a pilot study using both blind and sighted volunteers, subjects were able to utilize the prototype for spatial orientation and obstacle avoidance after a few minutes of training, attaining 90% accuracy in estimating the direction and depth of obstacles.","2012","2023-07-24 06:49:12","2023-07-24 06:49:12","","696–708","","9","60","","J. Audio Eng. Soc","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7HFL7NJV","journalArticle","2018","Roma, Gerard; Xambó, Anna; Freeman, Jason","User-independent Accelerometer Gesture Recognition for Participatory Mobile Music","J. Audio Eng. Soc","","","","","With the widespread use of smartphones that have multiple sensors and sound processing capabilities, there is a great potential for increased audience participation in music performances. This paper proposes a framework for participatory mobile music based on mapping arbitrary accelerometer gestures to sound synthesizers. The authors describe Handwaving, a system based on neural networks for real-time gesture recognition and sonification on mobile browsers. Based on a multiuser dataset, results show that training with data from multiple users improves classification accuracy, supporting the use of the proposed algorithm for user-independent gesture recognition. This illustrates the relevance of user-independent training for multiuser settings, especially in participatory music. The system is implemented using web standards, which makes it simple and quick to deploy software on audience devices in live performance settings.","2018","2023-07-24 06:49:12","2023-07-24 06:49:12","","430–438","","6","66","","J. Audio Eng. Soc","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A2CS7LA8","journalArticle","2012","Schaffert, Nina; Gehret, Reiner; Mattes, Klaus","Modeling the Rowing Stroke Cycle Acoustically","J. Audio Eng. Soc","","","","","Because elite athletes require an unconscious and automated sense of time, and because sound is especially appropriate for conveying timing information, acoustic feedback can be especially useful in training of rowers. In the context of human movement, rhythm is a time accurate sequence of motor actions. Rhythm and synchronization are inseparable within a moving context. An auditory feedback signal based on boat acceleration helps rowers control their activities, and this sonified data can be stored in an audio file for later training and analysis. The improved sensitivity to the time-critical nature of the rowing cycle yielded an improved synchronization among the crew, as well as an improvement of individual athlete’s rowing technique.","2012","2023-07-24 06:49:12","2023-07-24 06:49:12","","551–560","","7/8","60","","J. Audio Eng. Soc","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4PPAHZA5","journalArticle","2012","Stewart, Rebecca; Sandler, Mark","Spatial Auditory Display in Music Search and Browsing Applications","J. Audio Eng. Soc","","","","","User interfaces for searching and browsing collections of music often use nonaudio for presenting information about the contents of the collection. This study reviews the literature to unify the various ways in which auditory spatialization can be used to augment the presentation of data. The authors examined 22 user interfaces that use such concepts as auditory icons, perceived location, amplitude panning, and a usability evaluation. Commonalities among the designs are discussed including the chosen spatialization approaches and evaluation methods.","2012","2023-07-24 06:49:12","2023-07-24 06:49:12","","936–946","","11","60","","J. Audio Eng. Soc","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PDLPHE4V","journalArticle","2018","Stolfi, Ariane; Sokolovskis, Janis; Goródscy, Fábio; Iazzetta, Fernando; Barthet, Mathieu","Audio Semantics: Online Chat Communication in Open Band Participatory Music Performances","J. Audio Eng. Soc","","","","","Technology-mediated audience participation is an emergent topic in creative music technology with a blurred distinction between audience and performers. This paper analyzes communication patterns occurring in the online chat of the Open Band system for participatory live music performance. In addition to acting as a multi-user messaging tool, the chat system also serves as a control interface for the sonification of textual messages from the audience. Open Band performances have been presented at various festivals and conferences since 2016. Its web-based platform enables collective “sound dialogues” that are opened to everyone regardless of musical skills. Drawing on interactive participatory art and networked music performance, the system aims to provide engaging social experiences in colocated music-making situations. The authors collected data from four public performances including over 3,000 anonymous messages sent by audiences. After presenting the design of the system, the authors analyzed the semantic content of messages using thematic and statistical methods. Findings show how different sonification mechanisms alter the nature of the communication between participants who articulate both linguistic and musical self-expression. One of the design goals was to provide a platform for free audience expression as a web “agora.” The various themes that emerged from the analyses endorse this idea, as participants felt free to discuss subjects ranging from love to political opinions.","2018","2023-07-24 06:49:12","2023-07-24 06:49:12","","910–921","","11","66","","J. Audio Eng. Soc","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZW67CE5P","journalArticle","2020","Liew, Kongmeng; Lindborg, PerMagnus","A Sonification of Cross-Cultural Differences in Happiness-Related Tweets","J. Audio Eng. Soc","","","","","Sonification can be defined as any technique that translates data into non-speech sound with a systematic, describable, and reproducible method, in order to reveal or facilitate communication, interpretation, or discovery of meaning that is latent in the data. This paper describes an approach for communicating cross-cultural differences in sentiment data through sonification, which is a powerful technique for the translation of patterns into sounds that are understandable, accessible, and musically pleasant. A machine-learning classifier was trained on sentiment information of two samples of Tweets from Singapore and New York with the keyword of ""happiness."" Positive-valence words that relate to the concept of happiness showed stronger influences on the classifier than negative words. For mapping, Tweet frequency differences of the semantic variable ""anticipation"" affected tempo, positive-affected pitch, and joy-affected loudness, while ""trust"" affected rhythmic regularity. The authors evaluated sonification of the original data from the two cities, together with a control condition generated from random mappings in a listening experiment. Results suggest that the original was rated as significantly more pleasant.","2020","2023-07-24 06:49:12","2023-07-24 06:49:12","","25–33","","1/2","68","","J. Audio Eng. Soc","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BP5FAMX4","journalArticle","2022","Hayes, Ben; Saitis, Charalampos; Fazekas, György","Disembodied Timbres: A Study on Semantically Prompted FM Synthesis","J. Audio Eng. Soc","","","","","Disembodied electronic sounds constitute a large part of the modern auditory lexicon, but research into timbre perception has focused mostly on the tones of conventional acoustic musical instruments. It is unclear whether insights from these studies generalize to electronic sounds, nor is it obvious how these relate to the creation of such sounds. This work presents an experiment on the semantic associations of sounds produced by FM synthesis with the aim of identifying whether existing models of timbre semantics are appropriate for such sounds. A novel experimental paradigm, in which experienced sound designers responded to semantic prompts by programming a synthesizer, was applied, and semantic ratings on the sounds they created were provided. Exploratory factor analysis revealed a five-dimensional semantic space. The first two factors mapped well to the concepts of luminance, texture, and mass. The remaining three factors did not have clear parallels, but correlation analysis with acoustic descriptors suggested an acoustical relationship to luminance and texture. The results suggest that further inquiry into the timbres of disembodied electronic sounds, their synthesis, and their semantic associations would be worthwhile and that this could benefit research into auditory perception and cognition and synthesis control and audio engineering.","2022","2023-07-24 06:49:13","2023-07-24 06:49:13","","373–391","","5","70","","J. Audio Eng. Soc","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QN8HN5J4","journalArticle","2013","Kostek, Bożena","Auditory Display From The Music Technology Perspective","","","","","","This paper presents some applications of Auditory Displays (AD) in the domain of music technology. First, the scope of music technology and auditory display areas are shortly outlined. Then, the research trends and system solutions within the fields of music technology, music information retrieval and music recommendation are discussed. Finally, an example of an auditory display that facilities music annotation process based on gaze tracking is shown.","2013","2023-07-24 06:49:13","2023-07-24 06:49:13","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C3KMF3X6","journalArticle","2019","Li, Grace; Walker, Bruce N.","Mixed speech and non-speech auditory displays: impacts of design, learning, and individual differences in musical engagement","","","","","","Information presented in auditory displays is often spread across multiple streams to make it easier for listeners to distinguish between different sounds and changes in multiple cues. Due to the limited resources of the auditory sense and the fact that they are often untrained compared to the visual senses, studies have tried to determine the limit to which listeners are able to monitor different auditory streams while not compromising performance in using the displays. This study investigates the difference between non-speech auditory displays, speech auditory displays, and mixed displays; and the effects of the different display designs and individual differences on performance and learnability. Results showed that practice with feedback significantly improves performance regardless of the display design and that individual differences such as active engagement in music and motivation can predict how well a listener is able to learn to use these displays. Findings of this study contribute to understanding how musical experience can be linked to usability of auditory displays, as well as the capability of humans to learn to use their auditory senses to overcome visual workload and receive important information.","2019","2023-07-24 06:49:13","2023-07-24 06:49:13","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T684ZIHN","journalArticle","2007","Mauney, Lisa M.; Walker, Bruce N.","Individual Differences and the Field of Auditory Display: Past Research, A Present Study, and an Agenda for the Future","","","","","","There has been some interest in the study of individual differences in the field of auditory displays, but we argue that there is a much greater potential than has been realized, to date. Relevant types of individual differences that may be applicable to interpreting auditory information include perceptual abilities, cognitive abilities, musical abilities, and learning styles. There are many measures of these individual differences available; however, they have not been thoroughly utilized in the auditory display arena. We discuss several types of individual differences relevant to auditory displays. We then present some examples of past research, along with the results of a current investigation of individual differences in auditory displays. Finally, we propose an agenda as to what research and tests should be used to further study this area.","2007","2023-07-24 06:49:13","2023-07-24 06:49:13","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"46HN4TBR","journalArticle","2000","Tran, Quan T.; Mynatt, Elizabeth D.","Music monitor: Dynamic data display","","","","","","In this demo, we present an interface prototype of Music Monitor, an application targeted for home use that dynamically conducts music in real-time to reflect parallel activities in disparate locations (e.g., preparing food in the kitchen while entertaining guests in the living room) to help the user balance attention appropriately between them. Music Monitor interprets salient information from the monitored activities to map into simple music profiles (e.g., instrument selection, tempo). The prototype design focuses on providing continuous, ambient music as a peripheral auditory data display for the primary user without sacrificing aesthetic musical value for other participants and listeners.","2000","2023-07-24 06:49:13","2023-07-24 06:49:13","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P6H84B7M","journalArticle","2005","Alty, James L.; Rigas, Dimitrios; Vickers, Paul","Music and speech in auditory interfaces: When is one mode more appropriate than another?","","","","","","A number of experiments, which have been carried out using non-speech auditory interfaces, are reviewed and the advantages and disadvantages of each are discussed. The possible advantages of using non-speech audio media such as music are discussed – richness of the representations possible, the aesthetic appeal, and the possibilities of such interfaces being able to handle abstraction and consistency across the interface.","2005","2023-07-24 06:49:13","2023-07-24 06:49:13","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CYQH6P9E","journalArticle","2005","Won, Sook Young","Auditory display of genome data: Human chromosome 21","","","","","","The motivation for this paper is to systematically explore the efficacy of mapping data to sound parameters with the specific aim of sonifying statistical trends and hearing the `gist' [1] of the data. In this paper, we consider the task of searching through a gene sequence of the human chromosome 21 for CpG islands and type of gene evidence. Musical intervals and rhythm is proposed for detecting CpG islands and musical timber is used for representing the gene evidence. We extract human genome data from the NCBI (National Center for Biotechnology Information) [2] database and use list processing and synthesis capabilities of CLM [3].","2005","2023-07-24 06:49:13","2023-07-24 06:49:13","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P53YIZZX","journalArticle","2002","Neuhoff, J. G.; Knight, R.; Wayand, J.","Pitch change, sonification, and musical expertise: Which way is up?","","","","","","Frequency change is one of the most widely used acoustic dimensions in auditory display, and pitch perception is among the most widely researched topics in audition. Nonetheless, there is little research on the appropriate mapping and scaling of information to acoustic frequency in sonification. Here, we show that musical training is a contributing factor to the mapping, scaling, and conceptual relationships that exist between the information to be sonified and its acoustic representation. In Experiment 1, three groups of listeners that varied in musical expertise moved a slider to indicate the amount of pitch change that they heard in ten non-standard musical intervals. Listeners with more musical training showed greater slider movement in response to pitch change than musical novices, but not in response to brightness in a visual control condition. Novices also made significantly more errors in identifying the direction of pitch change for intervals that were well above discrimination thresholds. Experiment 2 showed that the errors by novices were due primarily to conceptual errors in labeling `rising' and `falling' pitch with a small but significant number of perceptual discrimination errors. The results suggest that musical training is an important factor in the mapping, scaling, and conceptual relationships used in sonification.","2002","2023-07-24 06:49:14","2023-07-24 06:49:14","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TIKJFNQE","journalArticle","2004","Vickers, P.","External auditory representations of programs: Past, present, and future - an aesthetic perspective","","","","","","This paper provides a summary of previous work done in the area of external auditory representations of programs (known as program auralisation). A brief historical review is given followed by a short summary of the characteristics of the main program auralisation systems that have been reported in the literature. As program auralisation systems tend to use musical representations they are necessarily affected by artistic considerations. The influence of art theory and practice on the design of computer systems and artefacts (known as aesthetic computing [1, 2]) has grown to the extent that there is now a growing field devoted to its study. Therefore, it is instructive to explore the design of program auralisation systems in the light of aesthetic computing ideas. The remainder of this paper then, discusses the main principles of aesthetic computing in relation to program auralisation, and finishes with some views on how aesthetic computing can influence the future development of program auralisation systems.","2004","2023-07-24 06:49:14","2023-07-24 06:49:14","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B4X88IIR","journalArticle","2006","Hogg, B.; Vickers, P.","Sonification abstraite/sonification concrete: An 'aesthetic persepctive space' for classifying auditory displays in the ars musica domain","","","","","","This paper discusses æsthetic issues of sonifications and the relationships between sonification (ars informatica) and music & sound art (ars musica). It is posited that many sonifications have suffered from poor internal ecological validity which makes listening more difficult, thereby resulting in poorer data extraction and inference on the part of the listener. Lessons are drawn from the electroacoustic music and musique concrète communities as it is argued that it is not instructive to distinguish between sonifications and music/sound art. Edgard Varèse defined music as organised sound, and sonifications organise sound to reflect mimetically the thing being sonified. Therefore, an æsthetic perspective space onto which sonifications and musical compositions alike can be mapped is proposed. The resultant map allows sonifications to be compared with works in the ars musica domain with which they share characteristics. The æsthetics of those ars musica counterparts can then be interrogated revealing useful design and organisation constructs that can be used to improve the sonifications' communicative ability.","2006","2023-07-24 06:49:14","2023-07-24 06:49:14","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QQM5CBN3","journalArticle","2002","Joseph, A. J.; Lodha, S. K.","Musart: Musical audio transfer function real-time toolkit","","","","","","This work describes the design and implementation of a sonification toolkit. MUSART (MUSical Audio transfer function Realtime Toolkit) is a sonification toolkit which produces musical sound maps that are played in real-time. Register, pitch, timbre, chords, duration, silence, loudness, beats, and panning are the musical concepts used to create melodic sound maps. Univariate and multivariate data sets are sonified using various sound parameter combinations and music tracks. Users have the flexibility to create personalized auditory displays by mapping each data dimension of a data set to one or more sound parameters. MUSART is designed to be flexible so that it can be used with many applications. In this work, we use musical auditory maps to explore seismic volumes used for detecting areas for drilling oil.","2002","2023-07-24 06:49:14","2023-07-24 06:49:14","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JUAA3MGJ","journalArticle","2007","Fagerlon, Johan","Expressive Musical Warning Signs","","","","","","Warning signals are often very simple and monotone sounds. This paper focuses on taking a more musical approach to the design of warnings and alarms than has been the case in the past. We present an experimental pilot study in which we explore the possibilities of using short musical pieces as warning signals in a vehicle cab. In the study, 18 experienced drivers experienced five different driving scenarios with different levels of urgency. Each scenario was presented together with an auditory icon, a traditional abstract warning sound, and a musical warning sound designed in collaboration with a composer. The test was carried out in an “audio-only” environment. Drivers were required to rate the perceived urgency, annoyance and appropriateness for every sound. They also had a chance to talk freely about the different warning signals. The results indicate interestingly that drivers may be able to understand the intended meaning of musical warning signals. It seems like the musical warning signals may prove useful primarily in situations of low and medium levels of urgency.","2007","2023-07-24 06:49:14","2023-07-24 06:49:14","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GZP5I7PE","journalArticle","2011","Winters, R. Michael","1/F Noise and Auditory Aesthetics: Sonification of a Driven Bead Pile","","","","","","“1=f noise” describes the behavior of many naturally occurring complex dynamical systems over time. Perhaps more surprising than its ubiquity in nature is its prevalence in speech and especially music. Current research suggests that aspects of the human emotional response to music can be predicted using analysis based upon 1=f distributions. Musical compositions in which the pitch and duration of notes over time correspond to 1=f distributions have been found to be more pleasant than non-1=f distributions. The current research compares the sonification of a driven bead pile to an experimentally contrived deviation. Preliminary results suggest that the findings of pitch and duration may be extended to timbre and spatialization. The benefits of continued research into the application 1=f distributions in sonification for the auditory display community are discussed.","2011","2023-07-24 06:49:14","2023-07-24 06:49:14","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TIGHBM8F","journalArticle","2005","Childs, Edward","Auditory graphs of real-time data","","","","","","The advantages of auditory display for monitoring real-time data are discussed. Parallels between the structures of real-time data and music are emphasized as potentially fruitful areas of research. The Accentus LLC design philosophy is described, followed by several examples of auditory graphs. Areas of future research are recommended.","2005","2023-07-24 06:49:14","2023-07-24 06:49:14","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XIH5MM6V","journalArticle","2018","Tislar, Kay; Duford, Zackery; Nelson, Brittany; Peabody, Madeline; Jeon, Myounghoon","Examining the learnability of auditory displays: Music, earcons, spearcons, and lyricons","","","","","","Auditory displays are a useful platform to convey information to users for a variety of reasons. The present study sought to examine the use of different types of sounds that can be used in auditory displays—music, earcons, spearcons, and lyricons—to determine which sounds have the highest learnability when presented in sequences. Participants were self-trained on sound meanings and then asked to recall meanings after listening to sequences of varying lengths. The relatedness of sounds and their attributed meanings, or the intuitiveness of the sounds, was also examined. The results show that participants were able to learn and recall lyricons and spearcons the best, and related meaning is an important contributing variable to learnability and memorability of all sound types. This should open the door for future research and experimentation of lyricons and spearcons presented in auditory streams.","2018","2023-07-24 06:49:14","2023-07-24 06:49:14","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"USUWLJMX","journalArticle","2012","Winters, R. Michael; Wanderley, Marcelo M.","New Directions for Sonification of Expressive Movement in Music","","","","","","Expert musical performance is rich with visual cues that facilitate expressive communication. Previous sonifications in this domain have evaluated the saliency of mapping based upon a sound's direct visual correspondence. Although this approach may be important for motion sonification more generally, a little flexibility might be necessary for expressive gesture. A review of literature on gesture in music performance suggests that for evaluation, it matters less what is moving than how and how much. New efforts focus on the potential use of sonification with the underlying performance audio. A series of videos and sonifications are presented as examples of this point.","2012","2023-07-24 06:49:15","2023-07-24 06:49:15","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JFVC22ZQ","journalArticle","2005","Gossmann, Joachim","Towards an auditory representation of complexity","","","","","","In applications of sonification, the information inferred by the sonification strategy applied often supersedes the amount of information which can be retrieved by the ear about the object of sonification. This paper focuses on the representation of complex geometric formation through sound, drawing on the development of an interactive installation sonifying escape time fractals as an example. The terms “auditory emergence and formation” are introduced and an attempt is made to interpret them for music composition, data display and information theory. The example application,. “Audio Fraktal”, is a public installation in the permanent exhibition of the Museum for Media Art at ZKM, Karlsruhe. The design of the audiovisual display system that allows the shared experience of interactive spatial auditory formation is described. The work was produced by the author at the Institute for Music and Acoustics at ZKM, Karlsruhe.","2005","2023-07-24 06:49:15","2023-07-24 06:49:15","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4BJNNZZW","journalArticle","2011","Preti, Constanza; Schubert, Emery","Sonification of Emotions II: Live music in a pediatric hospital","","","","","","In this paper, we argue that music can be used to sonify emotions. Further, we propose that the ‘sonification of emotion’ conceptualization can explain some aspects of the practice of playing music in hospitals. Music can be constantly adjusted to reflect (but more accurately, we argue, sonify) various aspects of the emotional situations unfolding in a hospital, namely the interaction between the child/patient, their caregivers and the hospital staff. The case study of a musical interaction between a musician and a child/patient is presented and led to the development of a Cycle of Sonification model, where the musician collects complex environmental cues and then portrays them through the music to adjust the emotional ‘temperature’. That is, the emotion of the environment is reflected (sonified) by the music, but additionally the music is also calibrated so as to allow the regulation of the emotional mood (including distraction) in the otherwise stressful environment. As well as adjusting the mood, the music provides a ‘reading’ or measure of emotion through the auditory, non-speech mode, consistent with some pertinent definitions of sonification","2011","2023-07-24 06:49:15","2023-07-24 06:49:15","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9VMRHTUJ","journalArticle","1997","Lodha, Suresh K.; Beahan, John; Heppe, Travis; Joseph, Abigail; Zane-Ulman, Brett","MUSE: A musical data sonification toolkit","","","","","","Data sonification is the representation of data using sound. Last year we presented a flexible, interactive and portable data sonification toolkit called LISTEN, that allows mapping of data to several sound parameters such as pitch, volume, timbre and duration [20]. One of the potential drawbacks of LISTEN is that since the sounds generated are non-musical, they can be fatiguing when exploring large data sets over extended periods of time. A primary goal in the design of MUSE – a MUsical Sonification Environment – is to map scientific data to musical sounds. The challenge is to ensure that the data meanings are preserved and brought out by these mappings. MUSE provides flexible data mappings to musical sounds using parameters such as pitch (melody), rhythm, tempo, volume, timbre and harmony. MUSE is written in C++ for the SGI platform and works with the freely available sound specification software CSound developed at MIT. We have applied MUSE to map uncertainty in some scientific data sets to musical sounds.","1997","2023-07-24 06:49:15","2023-07-24 06:49:15","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D3G45H39","book","2010","Straebel, Volker","The Sonification Metaphor in Instrumental Music and Sonification's Romantic Implications","","","","","","","2010","2023-07-24 06:49:15","2023-07-24 06:49:15","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MRQFY6XS","journalArticle","1996","Vickers, Paul; Alty, James L.","CAITLIN: A musical problem auralisation tool to assist novice programmers with debugging","","","","","","In the field of auditory display relatively little work has focused on the use of sound to aid program debugging. In this paper, we describe CAITLIN , a pre-processor for Turbo Pascal programs that musically auralises programs with a view to assisting novice programmers with locating errors in their code. A discussion follows of an experiment which showed that programmers could use the musical feedback to visualise and describe program structure. We then present conclusions and a discussion of future work.","1996","2023-07-24 06:49:15","2023-07-24 06:49:15","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HLLBHP7W","journalArticle","2012","Ash, Kingsley","Affected States: Analysis and Sonification of Twitter Music Trends","","","","","","This paper describes an approach to the sonification of real- time twitter music trend data realized for the ICAD 2012 Sonification Competition: Listening to the World Listening. The paper will discuss the techniques used to create the sonification and the motivations behind them, including details of the data analysis, mapping strategies, visual display and sonification output. The system analyses the Twitter Music Trends data feed, which aggregates music listening data from Twitter by artist, as well as the Echo Nest REST API to determine the perceived emotional affect and prevailing descriptions of a selection of the latest trending artists. The resulting data is visualized and sonified in real-time to facilitate analysis and generate an appealing visual and auditory display of the resulting data. Experience with the system suggests that it is successful in allowing users to determine perceived emotional affect and quality for a number of artists simultaneously, and could allow further investigation into the correlation between these factors. The system also generates appealing visual music that reaches beyond the practice of scientific investigation to reach out to a wider audience.","2012","2023-07-24 06:49:16","2023-07-24 06:49:16","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NVLVEWZL","journalArticle","2003","Sturm, Bob L.","Ocean buoy spectral data sonification: Research update","","","","","","Since first presenting this work at the 2002 International Conference on Auditory Display the author has professionally produced a multimedia CD that explores the sonification of ocean wave data. “Music from the Ocean” is a multimedia CD that explores the sonification of ocean wave data for oceanography, science pedagogy, and music and sound synthesis. Turning this data into sound provides new ways of experiencing and comprehending the phenomena involved; the processes come alive and are more comprehensible, memorable and exciting than graphs of the data. This CD contains over 55 minutes of sound examples as well as an interactive Flash presentation and research paper explaining the methods in more detail. The accompanying 16-page booklet features graphics and information about the data, phenomena, and music. The CD appeals to a wide variety of consumers, from oceanographers, science teachers, experimental computer music enthusiasts, and music therapists.","2003","2023-07-24 06:49:16","2023-07-24 06:49:16","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"87HHPG47","journalArticle","2012","Oswald, David","Non-Speech Audio-Semiotics: A Review and Revision of Auditory Icon and Earcon Theory","","","","","","The aim of this paper is to develop a theory and taxonomy of auditory signs, based on semiotics. For more than two decades, the discourse on non-speech audio interfaces has been dominated by a dichotomy between auditory icons, which are based on everyday hearing, and earcons, which are based on musical hearing. The corresponding theory behind these concepts has to be revised for several reasons. First, the authors of these theories partly use semiotic concepts and terminology, but not always in a correct way. Second, the classification of auditory icons as ""iconic"", and earcons as ""abstract"" is too simple and based on the questionable premise that everyday sounds are per se iconic and musical motives are per se abstract and symbolic. Third, this widespread idea ignores the crucial role of the user in the process of perception. In addition, the users' perception of visual and auditory signs in computer interfaces is fundamentally different today, from how it was in the early years of graphical user interfaces — the time when the first auditory interfaces and the corresponding theories were developed.","2012","2023-07-24 06:49:16","2023-07-24 06:49:16","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FYQSXKAH","journalArticle","2011","Schubert, Emery; Ferguson, Sam; Farrar, Natasha; McPherson, Gary E.","Sonification of Emotion I: Film Music","","","","","","This paper discusses the uses of sound to provide information about emotion. The review of the literature suggests that music is able to communicate and express a wide variety of emotions. The novel aspect of the present study is a reconceptualisation of this literature by considering music as having the capacity to sonify emotions. A study was conducted in which excerpts of non-vocal film music were selected to sonify six putative emotions. Participants were then invited to identify which emotions each excerpt sonified. The results demonstrate a good specificity of emotion sonification, with errors attributable to selection of emotions close in meaning to the target (excited confused with happy, but not with sad, for example). While ‘sonification’ of emotions has been applied in opera and film for some time, the present study allows a new way of conceptualizing the ability of sound to communicate affect through music. Philosophical and psychological implications are considered.","2011","2023-07-24 06:49:16","2023-07-24 06:49:16","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9FLDKBAZ","journalArticle","2006","Margounakis, D.; Politis, D.","Converting images to music using their colour properties","","","","","","Music is associated to colors since ancient years. Different mappings between attributes of sound and images allow the efficient conversion between the two types of media. The proposed method for converting images to music using the concept of chromaticism provides the area of computer music with a parameterized environment for audio-visual presentations. The auditory display of colour images may bring the different ways that a listener perceives a musical piece (because of colour transitions) to light. A design template for chromatic synthesis is described. A short example, based on a graphical digital icon, demonstrates the preliminary results.","2006","2023-07-24 06:49:16","2023-07-24 06:49:16","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YPLDBYR3","journalArticle","2013","Worrall, David","Understanding The Need For Micro-Gestural Inflections In Parameter-Mapping Sonification","","","","","","Most of the software tools used for data sonification have been adopted or adapted from those designed to compose computer music, which in turn, adopted them from abstractly notated scores. Such adoptions are not value-free; by using them, the cultural paradigms underlying the music for which the tools were made have influenced the conceptualization and, it is argued, the effectiveness of data sonifications. Recent research in cognition supports studies in empirical musicology that suggest that listening is not a passive ingestion of organised sounds but is an embodied activity that invisibly enacts gestures of what is heard. This paper outlines an argument for why sonifiers using parametric-mapping sonification should consider incorporating micro-gestural inflections if they are to mitigate The Mapping Problem in enhancing the intelligibility of sonified data.","2013","2023-07-24 06:49:17","2023-07-24 06:49:17","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FVZC8PKA","journalArticle","2008","Jung, Ralf","Ambience for Auditory Displays: Embedded Musical Instruments As Peripheral Audio Cues","","","","","","From alarm signals and data sonification to multimodal interfaces, auditory displays are omnipresent in our everyday life and they become more and more popular. But there are some challenges we have to meet because of the differentness of the auditory sense compared to the visual sense. Usually, audio notification signals are limited to simple warning cues and system feedback that are in most cases intrusive be- cause they differ from the environmental noise. That has the effect that people present in the room could be distracted from their current tasks because they cannot “close their ears.” To prevent the disturbing effect of traditional notification signals we developed the novel concept of non-speech audio notification embedded in ambient soundscapes to provide multi-user notification in a more discreet and non-disturbing way. Instead of using well-known non-speech cues like auditory icons and earcons, we decided to compose and record peripheral soundscapes and notification instruments by ourselves towards a more aesthetic approach. In this paper, we give an overview of our location-aware system with two applications (PAAN, AeMN) and sketch a real life scenario in a wine department of a supermarket. We will also present findings from a user study and provide a small collection of notification instruments and soundscapes as audio samples.","2008","2023-07-24 06:49:17","2023-07-24 06:49:17","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6EXHSPBU","journalArticle","2012","Schmele, Timothy; Gomez, Imanol","Exploring 3D audio for brain sonification","","","","","","Brain activity data, measured by functional Magnetic Resonance Imaging (fMRI), produces extremely high dimensional, sparse and noisy signals which are difficult to visualize, monitor and analyze. The use of spatial music can be particularly appropriate to represent its contained patterns. The literature describes several research done on sonifying neuroimaging data as well as different techniques to use spatialization as a musical language. In this paper, we discuss an artistic approach to fMRI sonification exploiting new compositional paradigms in spatial music. There fore, we have consider the brain activity as audio base material of a the spatial musical composition. Our approach attempts to explore the aesthetic potential of brain sonification not by transforming the data beyond the recognizable, and presenting the data as direct as possible.","2012","2023-07-24 06:49:17","2023-07-24 06:49:17","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X9ESPVDH","book","2010","Spondike, David","A Strategy for Composing Music using the Sonification of ""Snapshot"" type Data Collections ""Schnappschuss von der Erde""","","","","","","Transforming the sonification of data into a musical experience that is satisfying on scientific as well as aesthetic grounds requires balancing similar and competing objectives and sensibilities. One possible solution, described here, is used to create the digital music composition Schnappschuss von der Erde in response to the call for compositions by the International Community for Auditory Display (ICAD) Conference.","2010","2023-07-24 06:49:17","2023-07-24 06:49:17","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SSREHG8G","journalArticle","2018","Rönnberg, Niklas; Löwgren, Jonas","Photone: Exploring modal synergy in photographic images and music","","","","","","We present Photone, an interactive installation combining photographic images and musical sonification. An image is displayed, and a dynamic musical score is generated based on the overall color properties of the image and the color value of the pixel under the cursor. Hence, the music changes as the user moves the cursor. This simple approach turns out to have interesting experiential qualities in use. The composition of images and music invites the user to explore the combination of hues and textures, and musical sounds. We characterize the resulting experience in Photone as one of modal synergy where visual and auditory output combine holistically with the chosen interaction technique. This tentative finding is potentially relevant to further research in auditory displays and multimodal interaction.","2018","2023-07-24 06:49:17","2023-07-24 06:49:17","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M75ZF83G","journalArticle","2012","Gossmann, Joachim","A perspective on the limited potential for simultaneity in auditory display","","","","","","The auditory environment is frequently described as a juxtaposition between an array of pre-disclosed auditory streams and a process of attentional selection. The orientation of attentional selection toward environmental streams is characterized by a differentiation in the types of stream: Speech, music and sound effects are only three examples in an open polymorphism of what could be described as perceptual strategies through which we access the sounding world. The differentiable-simultaneous manifold of environmental streams allows perceptual participation only in a certain number of streams at the same time - only one speaking voice, one sense of ""harmony"", a single ""rhythm"", and so forth: We propose a re-basing of sonfication strategies not on the definition of external mechanisms, but on the definition and application of new modal strategies that are circumscribed and accessible through 'what is not possible to perceive at the same time'.","2012","2023-07-24 06:49:17","2023-07-24 06:49:17","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NNMEQ729","journalArticle","2012","Vigani, Andrea","Sonic Window #1 [2011] — A Real Time Sonification","","","","","","This is a real time audio installation in Max/MSP. It is a sonification of an abstract process: the writing on Twitter about music listening experiences on the web by people around the world. My purpose is not to sonify the effects of this process on a musical structure (the musical structure of the listened songs) like a real-time-echo-web-mix or a new version of J.Cage “Imaginary landscape n°4”, but to sonify the structure of the process itself, with its languages transducers, its media, its rules. For this purpose I created a musical instrument played by the data, like a windchimes but here all the sounds are created by the web data itself, like if the material of a windchimes is the wind itself.It’s like an open windows on the web listeners, you observe the action of listening and talking about music, but you don’t hear the music listened and you search for connections, reactions, interactions among the listeners, among them, the transmission media and the code language.","2012","2023-07-24 06:49:17","2023-07-24 06:49:17","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8H7587K8","book","2009","Schoon, Andi; Dombois, Florian","Sonification in Music","","","","","","Seen from the vantage point of cultural history, contemporary sonification is essentially characterised by two aspects that to date have seldom been considered in combination: the first aspect is sonification as the transformation of the inaudible into the sphere of the audible, and the second its use as an instrument for gaining knowledge via the concrete listening experience. One of the aims of the research project “Denkgeräusche” (""Sounds of Thought""), conducted at Bern University of the Arts, was to make a contribution to a (yet unwritten) cultural history of sonification. With this target in mind, a database was compiled of historical and contemporary musical compositions in which procedures associated with sonification were employed.","2009","2023-07-24 06:49:18","2023-07-24 06:49:18","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XDSANPX3","journalArticle","2005","Pape, Daniel","Is pitch perception and discrimination of vowels language-dependent and influencd by the vowels spectral properties?","","","","","","Pitch discrimination and accuracy has been found to depend on different factors. However, little work has been done (1) on the cross-linguistic influence of the listeners' native language and (2) on the influence of the spectral structure on the pitch perception of vowels as well as (3) cross-linguistic differences regarding different levels of muscial education. If differences in pitch discrimination between different language families exist this would be a crucial knowledge in the design and failure-safe application of auditory displays driven by pitch differences in speech control. Therefore the current study examines pitch discrimination of German vowels with a similar vowel height differing in rounding and tenseness for (1) native German listeners and (2) native Catalan listeners. Significant differences in the sensitivity of pitch perception between these two languages were found. Catalan listeners, independent of their musical education, were mostly insensitive to even large pitch differences in the vowels to be judged. The accuracy of pitch judgements for German listeners were significantly different for musically educated listeners in comparison to musically uneducated listeners. Further, both languages show a significant pitch difference for rounded vowels compared to the unrounded vowels. The current study provides evidence that pitch discrimination is language-dependent, at least partially.","2005","2023-07-24 06:49:18","2023-07-24 06:49:18","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZI2S58R3","journalArticle","2017","Andreopoulou, Areti; Goudarzi, Visda","Reflections on the Representation of Women in the International Conferences on Auditory Displays (ICAD)","","","","","","This paper investigates the representation of women researchers and artists in the conferences of the International Community for Auditory Display (ICAD). In the absence of an organized membership mechanism and / or publicly available records of conference attendees, this topic was approached through the study of publication and authorship patterns of female researchers in ICAD conferences. Temporal analysis showed that, even though there has been an increase in the number of publications co-authored by female researchers, the annual percentage of female authors remained in relatively unchanged levels (mean = 17.9%) throughout the history of ICAD conferences. This level, even though low, remains within the reported percentages of female representation in other communities with related disciplines, such as the International Computer Music Association (ICMA) and the Conferences of the International Society for Music Information Retrieval (ISMIR), and significantly higher than in more audio engineering-related communities, such as the Audio Engineering Society (AES).","2017","2023-07-24 06:49:18","2023-07-24 06:49:18","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3YSQEVVP","journalArticle","2014","Jeon, Myounghoon; Sun, Yuanjing","Design and Evaluation of Lyricons (Lyrics + Earcons) for Semantic and Aesthetic Improvements of Auditory Cues","","","","","","Auditory researchers have developed various non-speech cues in designing auditory user interfaces. A preliminary study of “Lyricons” (lyrics + earcons) has provided a novel approach to devising auditory cues in electronic products, by combining the concurrent two layers of musical speech and earcons (short musical motives). The purpose of the present study is to introduce iterative design processes and to validate the effectiveness of lyricons compared to earcons, whether people can more intuitively grasp functions that lyricons imply than those of earcons. Results favor lyricons over earcons. Future work and practical application directions are also discussed.","2014","2023-07-24 06:49:18","2023-07-24 06:49:18","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ITSBE7AV","journalArticle","2018","Middleton, Jonathan; Hakulinen, Jaakko; Tiitinen, Katariina; Hella, Juhu; Keskinen, Tuuli; Huuskonen, Pertti; Linna, Juhani; Turunen, Markku; Ziat, Mounia; Raisamo, Roope","Sonification with musical characteristics: a path guided by user engagement","","","","","","Sonification with musical characteristics can engage users, and this dynamic carries value as a mediator between data and human perception, analysis, and interpretation. A user engagement study has been designed to measure engagement levels from conditions within primarily melodic, rhythmic, and chordal contexts. This paper reports findings from the melodic portion of the study, and states the challenges of using musical characteristics in sonifications via the perspective of form and function – a long standing debate in Human-Computer Interaction. These results can guide the design of more complex sonifications of multivariable data suitable for real life use.","2018","2023-07-24 06:49:18","2023-07-24 06:49:18","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UNKD9KTK","journalArticle","2013","Avissar, Daniel; Leider, Colby; Bennett, Hristopher; Gailey, Robert","An audio game app using interactive Movement sonification for Targeted posture control","","","","","","Interactive movement sonification has been gaining validity as a technique for biofeedback and auditory data mining in research and development for gaming, sports, and physiotherapy. Naturally, the harvesting of kinematic data over recent years has been a function of an increased availability of more portable, high-precision sensory technologies, such as smart phones, and dynamic real time programming environments, such as Max/MSP. Whereas the overlap of motor skill coordination and acoustic events has been a staple to musical pedagogy, musicians and music engineers have been surprisingly less involved than biomechanical, electrical, and computer engineers in research efforts in these fields. Thus, this paper proposes a prototype for an accessible virtual gaming interface that uses music and pitch training as positive reinforcement in the accomplishment of target postures.","2013","2023-07-24 06:49:18","2023-07-24 06:49:18","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V48DB363","book","2015","Walus, Bartlomiej P.; Pauletto, Sandra; Mason-Jones, Amanda","Evaluating the use of sonification and music to support the communication of alcohol health risk to young people: Initial results","","","","","","The interdisciplinary research project, Using Sonification to COmmunicate public health Risk data (SCORe), aims to experimentally test how sonification, interactivity in combination with music, could increase the communicative potential of a visual presentation directed to young people and focused on health risk data of alcohol consumption. Specifically, we are studying how this type of presentation can support engagement with health information, and the effective interpretation and recall of data. In order to explore the possible influence of sound in understanding important health risk messages, a 3-arm pilot randomised control (participant-blinded) trial was designed. We compared a visual presentation augmented by sonification, music and interaction with a simple visual presentation and a visual presentation augmented by simple user interaction. This paper describes the most complex of the three health presentations (the audio-visual and interactive presentation) and presents initial findings that relate to this presentation only.","2015","2023-07-24 06:49:19","2023-07-24 06:49:19","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U37A6KAF","journalArticle","2008","Polotti, Pietro; Benzi, Carlo","Rhetorical Schemes for Audio Communication","","","","","","The application of rhetorical techniques to the use of non-verbal sound in the interaction between humans and technologies is the core idea of this paper. We present our ideas at a general level and illustrate an exploratory case based on the application of rhetorical schemes to the sonification of computer operating system events. Both cases of musical sounds and everyday sounds are investigated. This work is intended as a preliminary study aiming at motivating a larger scale and more rigorous research about the potentiality of the use of rhetoric in the domain of Auditory Display (AD) and Sonic Interaction Design (SID).","2008","2023-07-24 06:49:19","2023-07-24 06:49:19","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M5V9C8NQ","journalArticle","2003","Quinn, Marty; Quinn, Wendy; Hatcher, Ben","For those who died: A 9/11 tribute","","","","","","For Those Who Died is a 6-minute dance and music tribute to those who died on 9/11. It premiered 9/11/2002 as part of a larger event entitled “Reflections, A Gift to the Community” that occurred at the Music Hall in Portsmouth NH. It features extensive textual sonification, dance, and two layers of visual presentation of nine textual datasets containing the names of those who died on September 11, 2001, the US Constitution and some DNA from chromosomes 1 and 9. The music was produced using five Yamaha QY70 and QY100 synthesizers and presented in surround sound.","2003","2023-07-24 06:49:19","2023-07-24 06:49:19","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J68SY8VN","journalArticle","2019","Neuhoff, John G.","Is sonification doomed to fail?","","","","","","Despite persistent research and design efforts over the last twenty years, widespread adoption of sonification to display complex data has largely failed to materialize, and many of the challenges to successful sonification identified in the past persist. Major impediments to the widespread adoption sonification include fundamental perceptual differences between vision and audition, large individual differences in auditory perception, musical biases of sonification researchers, and the interdisciplinary nature of sonification research and design. The historical and often indiscriminate mingling of art and science in sonification design may be a root cause of some of these challenges. Future sonification design efforts that explicitly strive to meet either artistic or scientific goals may lead to greater clarity and success in the field and more widespread adoption of useful sonification techniques.","2019","2023-07-24 06:49:19","2023-07-24 06:49:19","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TYFYSKHC","book","2015","Winters, R. Michael; Weinberg, Gil","Sonification of the Tohoku earthquake: Music, popularization & the auditory sublime","","","","","","The past century has witnessed the emergence of expressive musical forms that originate in appropriated technologies and practices. In most cases, this appropriation is performed without protest— but not always. Carefully negotiating a space for sound as an objective, scientific medium, the field of sonification has cautiously guarded the term from subjective and affective endeavors. This paper explores the tensions arising in sonification popularization through a formal analysis of Sonification of the Tohoku Earthquake, a two-minute YouTube video that combined audification with a time-aligned seismograph, text and heatmap. Although the many views the video has received speak to a high public impact, the features contributing to this popularity have not been formalized, nor the extent to which these characteristics further sonifications’ scientific mission. For this purpose, a theory of popularization based upon “sublime listening experiences” is applied. The paper concludes by drawing attention to broader themes in the history of music and technology and presents guidelines for designing effective public-facing examples.","2015","2023-07-24 06:49:19","2023-07-24 06:49:19","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LWDDA7H9","journalArticle","1996","Tkaczevski, Alejandro","Auditory interface problems and solutions for commercial multimedia products","","","","","","This paper will explore the various aesthetic, technical, and musical issues that sound designers face when creating audio for commercial products. I will draw from my experience at and use materials from Broderbund Software. In the first section, I will discuss issues concerning interface sonification. In the second section, I will briefly illustrate a quick musical solution to a potentially large logistical problem. In the last section, I will show a solution for balancing digital audio and MIDI on a variety of devices, drivers, and operation systems.","1996","2023-07-24 06:49:19","2023-07-24 06:49:19","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WTZJ322Q","journalArticle","2019","Kleinberger, Rebecca; George, Stefanakis; Franjou, Sebastian","Speech companions: Evaluating the effects of musically modulated auditory feedback on the voice","","","","","","Changing the way one hears one's own voice, for instance by adding delay or shifting the pitch in real-time, can alter vocal qualities such as speed, pitch contour, or articulation. We created new types of auditory feedback called Speech Companions that generate live musical accompaniment to the spoken voice. Our system generates harmonized chorus effects layered on top of the speakerﾒs voice that change chord at each pseudo-beat detected in the spoken voice. The harmonization variations follow predetermined chord progressions. For the purpose of this study we generated two versions: one following a major chord progression and the other one following a minor chord progression. We conducted an evaluation of the effects of the feedback on speakers and we present initial findings assessing how different musical modulations might potentially affect the emotions and mental state of the speaker as well as semantic content of speech, and musical vocal parameters.","2019","2023-07-24 06:49:20","2023-07-24 06:49:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W8M9Q8KT","book","2010","Diniz, Nuno; Deweppe, Alexander; Demey, Michiel; Leman, Marc","A Framework for Music-based Interactive Sonification","","","","","","In this paper, a framework for interactive sonification is introduced. It is argued that electroacoustic composition techniques can provide a methodology for structuring and presenting multivariable data through sound. Furthermore, an embodied music cognition driven interface is applied to provide an interactive exploration of the generated music-based output. The motivation and theoretical foundation for this work are presented as well as the framework’s implementation and an exploratory use case.","2010","2023-07-24 06:49:20","2023-07-24 06:49:20","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T9U2RLRV","journalArticle","2013","Henry, Ashley G.; Bruce, Carrie M.; Winton, Riley J.; Walker, Bruce N.","Sonification Mapping Configurations: Pairings Of Real-Time Exhibits And Sound","","","","","","Visitors to aquariums typically rely on their vision to interact with live exhibits that convey rich descriptive and aesthetic visual information. However, some visitors may prefer or need to have an alternative interpretation of the exhibitÕs visual scene to improve their experience. Musical sonification has been explored as an interpretive strategy for this purpose and related work provides some guidance for sonification design, yet more empirical work on developing and validating the music-to-visual scene mappings needs to be completed. This paper discusses work to validate mappings that were developed through an investigation of musician performances for two specific live animal exhibits at the Georgia Aquarium. In this proposed study, participants will provide feedback on musical mapping examples which will help inform design of a real-time sonification system for aquarium exhibits. Here, we describe our motivation, methods, and expected contributions.","2013","2023-07-24 06:49:20","2023-07-24 06:49:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H2ZIJDMK","journalArticle","2021","Roddy, Stephen; Bridges, Brian","The design of a smart city sonification system using a conceptual blending and musical framework, web audio and deep learning techniques","","","","","","This paper describes an auditory display system for smart city data for Dublin City, Ireland. It introduces and describes the different layers of the system and outlines how they operate individually and interact with one another. The system uses a deep learning model called a variational autoencoder to generate musical content to represent data points. Further data-to-sound mappings are introduced via parameter mapping sonification techniques during sound synthesis and post-processing. Conceptual blending and music theory provide frameworks, which govern the design of the system. The paper ends with a discussion of the design process that contextualizes the contribution, highlighting the interdisciplinary nature of the project, which spans data analytics, music composition and human-computer interaction.","2021","2023-07-24 06:49:20","2023-07-24 06:49:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"88QH9YKX","book","2010","Leonard, Neil","Sonification: Celestial Data and Poetic Inquiry","","","","","","This paper describes a composition/sonification project to be realized by faculty and students from the Electronic Production and Design department (EP/D) at Berklee College of Music in Boston. The goal of the project is compose music for a 30- minute interdisciplinary-networked performance to be premiered in Boston, Lyon and Havana involving artists from each city. In the process, artists are examining new modes of expression and the construction of knowledge and artistic dialog. Kelly Snook, Ph.D. Astrophysicist, Division of Solar System Exploration, NASA Goddard Spaceflight Center is working with the group to choose scientific data for sonification including compelling new planetary science and solar system data.","2010","2023-07-24 06:49:20","2023-07-24 06:49:20","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G7V66LEL","journalArticle","2017","Fox, K. Michael; Stewart, Jeremy; Hamilton, Rob","madBPM: Musical and Auditory Display for Biological Predictive Modeling","","","","","","The modeling of biological data can be carried out using structured sound and musical process in conjunction with integrated visualizations. With a future goal of improving the speed and accuracy of techniques currently in use for the production of synthetic high value chemicals through the greater understanding of data sets, the madBPM project couples real-time audio synthesis and visual rendering with a highly flexible data-ingestion engine. Each component of the madBPM system is modular, allowing for customization of audio, visual and data-based processing.","2017","2023-07-24 06:49:20","2023-07-24 06:49:20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"772WQKY4","journalArticle","2016","Landry, Steven; Sun, Yuangjing; Slade, Darnishia; Jeon, Myounghoon","Tempo-Fit Heart Rate App: Using Heart Rate Sonification As Exercise Performance Feedback","","","","","","Physical inactivity is a worldwide issue causing a variety of health problems. Exploring novel ways to encourage people to engage in physical activity is a topic at the forefront of research for countless stakeholders. Based upon a review of the literature, a pilot study, and exit interviews, we propose an app prototype that utilizes music tempo manipulation to guide users into a target heart rate zone during an exercise session. A study was conducted with 26 participants in a fifteen-minute cycling session using different sonification mappings and combinations of audiovisual feedback based on the user's current heart rate. Results suggest manipulating the playback speed of music in real time based on heart rate zone departures can be an effective motivational tool for increasing or decreasing activity levels of the listener. Participants vastly preferred prescriptive sonifications mappings over descriptive mappings, due to people's natural inclination to follow the tempo of music.","2016","2023-07-24 06:49:21","2023-07-24 06:49:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9C5YRCJN","journalArticle","2011","Mealla, Sebastian; Bosi, Mathieu; Jorda, Sergi; Valjamae, Aleksander","Sonification of Brain and Body Signals in Collaborative Tasks Using a Tabletop Musical Interface","","","","","","Physiological Computing has been applied in different disciplines, and is becoming popular and widespread in Human-Computer In- teraction, due to device miniaturization and improvements in real- time processing. However, most of the studies on physiology- based interfaces focus on single-user systems, while their use in Computer-Supported Collaborative Work (CSCW) is still emerg- ing. The present work explores how sonification of human brain and body signals can enhance user experience in collaborative mu- sic composition. For this task, a novel multimodal interactive sys- tem is built using a musical tabletop interface (Reactable) and a hybrid Brain-Computer Interface (BCI). The described system al- lows performers to generate and control sounds using their own or their fellow team member’s physiology. Recently, we assessed this physiology-based collaboration system in a pilot experiment. Dis- cussion on the results and future work on new sonifications will be accompanied by practical demonstration during the conference.","2011","2023-07-24 06:49:21","2023-07-24 06:49:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WTFGHCNZ","journalArticle","2011","Tissberger, Johann P.; Wersenyi, Gyorgy","Sonification Solutions for Body Movements in Rehabilitation of Locomotor Disorders","","","","","","One of the recent fields of sonification focuses on the sonification of body movements in sports or rehabilitation. This is usually some kind of monitoring of real-time measurement data and auditory feedback for the patient. This paper presents two sonification approaches in medicine: a balancing coordination system and a robot for moving the legs after serious injuries of the lower body parts. These two systems are evaluated and compared based on the method of sonification, and transmission and analysis of the auditory information. Finally, a supposed method for using musical notes and measures is presented, and a selection method for the length of sonification based on the initial time interval is suggested.","2011","2023-07-24 06:49:21","2023-07-24 06:49:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FRDDHUVR","journalArticle","2013","Lukasik, Ewa; Materski, Michal","Sonification Of A Virtual Model Of The Old Rare Musical Instrument","","","","","","The paper describes a project whose goal was to enable users realistically interact with a 3D virtual model of a historical musical instrument – the clavichord attributed to the famous 18th century maker. A challenge of enabling the user to play the virtual copy of the instrument was resolved by using the dynamic MIDI keyboard. The prerecorded clavichord sound samples are controlled by the software using the MIDI commands. Playing the real keyboard is synchronized with the movement of virtual keys and other parts of sound generating mechanism. The sound effects, characteristic for the clavichord Tragen der Tonne and Bebung may be imitated, which gives the user the impression of playing the real instrument.","2013","2023-07-24 06:49:21","2023-07-24 06:49:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"89DLHWK2","journalArticle","2017","Landry, Steven; Jeon, Myounghoon","Participatory Design Research Methodologies: A Case Study in Dancer Sonification","","","","","","Given that embodied interaction is widespread in Human-Computer Interaction, interests on the importance of body movements and emotions are gradually increasing. The present paper describes our process of designing and testing a dancer sonification system using a participatory design research methodology. The end goal of the dancer sonification project is to have dancers generate aesthetically pleasing music in real-time based on their dance gestures, instead of dancing to pre-recorded music. The generated music should reflect both the kinetic activities and affective contents of the dancer’s movement. To accomplish these goals, expert dancers and musicians were recruited as domain experts in affective gesture and auditory communication. Much of the dancer sonification literature focuses exclusively on describing the final performance piece or the techniques used to process motion data into auditory control parameters. This paper focuses on the methods we used to identify, select, and test the most appropriate motion to sound mappings for a dancer sonification system.","2017","2023-07-24 06:49:21","2023-07-24 06:49:21","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H58UGAJ8","journalArticle","2001","Chafe, Chris; Leistikow, Randal","Levels of temporal resolution in sonification of network performance","","","","","","The standard “ping” utility provides a momentary measurement of round trip time. Sequences of ping events are used to gather longer-term statistics about jitter and packet loss in order to describe the quality of service of a network path. A more finegrained tool is needed to evaluate paths which carry interactive media streams for collaborative environments. Natural interaction depends on obtaining consistent low-latency, low-jitter service, something which normally requires several ping “takes” to assess and even then only provides an averaged picture of quality of service. We have designed a stream-based method which directly displays the critical qualities to the ear by continuously driving a bidirectional connection to create sound waves. The network path itself becomes the acoustic medium which our probe sets into vibration. The granularity of this display better matches the time-scales of variance that are important in interactive applications (for example, bidirectional audio streams for long-distance musical collaboration or high-quality teleconference applications). The ear's acuity for pitch fluctuation and timbral constancy make this an unforgiving test. A related sonification technique is discussed which is a sonarlike mapping of momentary ping data to musical tones. Temporal levels of musical foreground, middleground and background can be heard in the melodies derived from the data and correspond to structures that are of importance in the analysis of network performance.","2001","2023-07-24 06:49:22","2023-07-24 06:49:22","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UTIHUGD2","journalArticle","2014","Winters, R. Michael; Cumming, Julie E.","Sonification of Symbolic Music in the Elvis Project","","","","","","This paper presents the development of sonification in the ELVIS project, a collaboration in interdisciplinary musicology targeting large databases of symbolic music and tools for their systematic analysis. An sonification interface was created to rapidly explore and analyze collections of musical intervals originating from various composers, genres, and styles. The interface visually displays imported musical data as a sound-file, and maps data events to individual short, discrete pitches or intervals. The user can interact with the data by visually zoom in, making selections, playing through the data at various speeds, and adjusting the transposition and frequency spread of the pitches to maximize acoustic comfort and clarity. A study is presented in which rapid pitchmapping is applied to compare differences between similar corpora. A group of 11 participants were able to correctly order collections of sonifications for three composers (Monteverdi, Bach, and Beethoven) and three presentation speeds (10², 10³, and 10⁴ notes/second). Benefits of sonification are discussed including the ability to quickly differentiate composers, find non-obvious patterns in the data, and ‘direct mapping’. The interface is made available as a MacOSX standalone application written in Super- Collider.","2014","2023-07-24 06:49:22","2023-07-24 06:49:22","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N3RFT73Q","journalArticle","2001","Barra, Maria; Cillo, Tania; de Santis, Antonio; Petrillo, Umberto Ferraro; Negro, Alberto; Scarano, Vittoro; Matlock, Teenie; Maglio, Paul P.","Personal webmelody: Customized sonification of web servers","","","","","","This paper presents Personal WebMelody, a sonified web server that informs its administrator of both normal and abnormal operation through background music. It allows customization and full integration of system-generated music representing web server activity with external music sources (audio CD, MP3, etc) selected by the administrator. Our sonification technique works by associating MIDI or WAV sound tracks with web server events. In an attempt to enable the webmaster to listen to such system-generated music for a long period without becoming fatigued, we introduce the opportunity of mixing an external music source with systemgenerated music. In this way, the administrator can hear the status of the web server while listening to his or her preferred music. We present an empirical study that shows how our web server sonification can convey useful information efficiently.","2001","2023-07-24 06:49:22","2023-07-24 06:49:22","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"76JDWVN5","journalArticle","2016","Coop, Allan D.","Sonification, Musification, and Synthesis of Absolute Program Music","","","","","","When understood as a communication system, a musical work can be interpreted as data existing within three domains. In this interpretation an absolute domain is interposed as a communication channel between two programatic domains that act respectively as source and receiver. As a source, a programatic domain creates, evolves, organizes, and represents a musical work. When acting as a receiver it re-constitutes acoustic signals into unique auditory experience. The absolute domain transmits physical vibrations ranging from the stochastic structures of noise to the periodic waveforms of organized sound. Analysis of acoustic signals suggest recognition as a musical work requires signal periodicity to exceed some minimum. A methodological framework that satisfies recent definitions of sonification is outlined. This framework is proposed to extend to musification through incorporation of data features that represent more traditional elements of a musical work such as melody, harmony, and rhythm.","2016","2023-07-24 06:49:22","2023-07-24 06:49:22","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FPBE2F2C","journalArticle","2004","Ciardi, F. C.","sMax: A multimodal toolkit for stock market data sonification","","","","","","In this work, we present sMax a multimodal toolkit for stock market data sonification. Unlike most research focusing their effort primarily on the sonification of single stock information, sMax provides an auditory display for the user to monitor parallel distributed data. sMax uses a set of Java and Max modules to map real time stock market information into recognizable musical patterns. One of the main design goals of the toolkit is to allow low-latency controls over real-time data sonification. Because of its object-oriented architecture, sMax can be easily extended by the user when additional functionality is required. The project outcomes range from the creation of art installations to auditory display for mobile computing devices. We present the theoretical background, and the structure of the program.","2004","2023-07-24 06:49:22","2023-07-24 06:49:22","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YQJWDV6A","journalArticle","2018","Worrall, David","Sonification: A Prehistory","","","","","","","2018","2023-07-24 06:49:22","2023-07-24 06:49:22","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AIUE55BY","book","2009","Beilharz, Kirsty; Ferguson, Sam","An interface and framework design for interactive aesthetic sonification","","","","","","This paper describes the interface design of our AeSon (Aesthetic Sonification) Toolkit motivated by user-centred customisation of the aesthetic representation and scope of the data. The interface design is developed from 3 premises that distinguish our approach from more ubiquitous sonification methodologies. Firstly, we prioritise interaction both from the perspective of changing scale, scope and presentation of the data and the user's ability to reconfigure spatial panning, modality, pitch distribution, critical thresholds and granularity of data examined. The user, for the majority of parameters, determines their own listening experience for real-time data sonification, even to the extent that the interface can be used for live data-driven performance, as well as traditional information analysis and examination. Secondly, we have explored the theories of Tufte, Fry and other visualization and information design experts to find ways in which principles that are successful in the field of information visualization may be translated to the domain of sonification. Thirdly, we prioritise aesthetic variables and controls in the interface, derived from musical practice, aesthetics in information design and responses to experimental user evaluations to inform the design of the sounds and display. In addition to using notions of meter, beat, key or modality and emphasis drawn from music, we draw on our experiments that evaluated the effects of spatial separation in multivariate data presentations.","2009","2023-07-24 06:49:23","2023-07-24 06:49:23","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TWUXTUWG","journalArticle","2019","Seica, Mariana; Martins, Pedro; Roque, Licinio; Cardoso, F. Amilcar","A sonification experience to portray the sounds of portuguese consumption habits","","","","","","The stimuli for consumption is present in everyday life, where major retail companies play a role in providing a large range of products every single day. Using sonification techniques, we present a listening experiment of Portuguese consumption habits in the course of ten days, gathered from a Portuguese retail company. We focused on how to represent this time-series data as a musical piece that would engage the listenerﾒs attention and promote an active listening attitude, exploring the influence of aesthetics in the perception of auditory displays. Through a phenomenological approach, ten participants were interviewed to gather perceptions evoked by the piece, and how the consumption variations were understood. The tested composition revealed relevant associations about the data, with the consumption context indirectly present throughout the emerging themes: from the idea of everyday life, routine and consumption peaks to aesthetic aspects as the passage of time, frenzy and consumerism. Documentary, movie imagery and soundtrack were also perceived. Several musical aspects were also mentioned, as the constant, steady rhythm and the repetitive nature of the composition, and sensations such as pleasantness, satisfaction, annoyance, boredom and anxiety. These collected topics convey the incessant feeling and consumption needs which portray our present society, offering new paths for comprehending musical sound perception and consequent exploration.","2019","2023-07-24 06:49:23","2023-07-24 06:49:23","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5H3WGXFN","journalArticle","2011","Nguyen, Vinh Xuan","Tonal DisCo: Dissonance and Consonance in a Gaming Engine","","","","","","Whilst there are several existing toolkits specifically designed for sonification, there has been little investigation into the utilization of computer game engines for sonification. This paper will demonstrate the implementation of a real time game engine for the purpose of sonification and discuss the opportunities and limitations. An important aspect which is lacking in existing sonification toolkits is the ability to sonify streaming data in real-time. Gaming engines not only offer the potential to do this but also offer the ability to visualize data in 3D and in real-time. The sound design of an art exhibition is used as a case study to demonstrate the potential of a computer game editor/sandbox used for visualization and sonification. For the exhibition real-world objects were tracked inside a gallery space and represented in the virtual environment of a computer game, which was displayed on a projector screen. Their movement was sonified into musical form to convey their steady/consistent movement as consonant and their agitated/inconsistent movement as dissonant. Tonal “DisCo” is used to describe their dissonance and consonance rating in both musical tone and visual color. Although the sonification of data into musical structure distorts the accuracy of absolute data values, it does maintain the relationship between data values. This loss of resolution is counteracted by an increase in clarity of data relationships. This case study appropriates the single ratio scale of pitch into both an interval scale of tone and an ordinal scale of octaves in order to express interrelationships.","2011","2023-07-24 06:49:23","2023-07-24 06:49:23","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"URPECMA7","journalArticle","2017","Tsuchiya, Takahiko; Freeman, Jason","Spectral Parameter Encoding: Towards a Framework for Functional-Aesthetic Sonification","","","","","","Auditory-display research has had a largely unsolved challenge of balancing functional and aesthetic considerations. While functional designs tend to reduce musical expressivity for the fidelity of data, aesthetic or musical sound organization arguably has a potential for representing multi-dimensional or hierarchical data structure with enhanced perceptibility. Existing musical designs, however, generally employ nonlinear or interpretive mappings that hinder the assessment of functionality. The authors propose a framework for designing expressive and complex sonification using small timescale musical hierarchies, such as the harmony and timbral structures, while maintaining data integrity by ensuring a close-to-the-original recovery of the encoded data utilizing descriptive analysis by a machine listener.","2017","2023-07-24 06:49:23","2023-07-24 06:49:23","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JI9FAQJH","journalArticle","2008","Bologna, Guido; Deville, Benoit; Pun, Thierry","Pairing Colored Socks and Following a Red Serpentine With Sounds of Musical Instruments","","","","","","The See ColOr interface transforms a small portion of a colored video image into sound sources represented by spatialized musical instruments. This interface aims at providing visually impaired people with a capability of perception of the environment. As a first step of this on-going project, the purpose is to verify the hypothesis that it is possible to use sounds from musical instruments to replace color. Compared to state of the art devices, a quality of the See ColOr interface is that it allows the user to receive a feed-back auditory signal from the environment and its colors, promptly. Two experiments based on a head mounted camera have been performed. The first experiment pertaining to object manipulation is based on the pairing of colored socks, while the second experiment is related to outdoor navigation with the goal of following a colored serpentine. The “socks” experiment demonstrated that seven blindfolded individuals were able to accurately match pairs of colored socks. The same participants successfully followed a red serpentine for more than 80 meters.","2008","2023-07-24 06:49:23","2023-07-24 06:49:23","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9YSHWCJZ","journalArticle","2002","Upson, R.","Educational sonification exercises: Pathways for mathematics and musical achievement","","","","","","This paper reports developments in the use of sonifications and sonification software for educational purposes. Adolescent subjects received training in Cartesian graphing over several sessions with sonification software and a sonification-enhanced curriculum. The project attracted students with low linguistic and logical-mathematical capabilities. Students were engaged by musical composition activities, but they remained anxious about traditional mathematics activities. Though students' mathematical abilities improved only slightly according to a traditional mathematical assessment, this project demonstrated the students' increased comfort level with the subject of mathematics and an increased understanding of the concepts within their own set of linguistics.","2002","2023-07-24 06:49:24","2023-07-24 06:49:24","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2LS5TK5I","book","2009","Vogt, Katharina; Pirro, David; Kobenz, Ingo; Höldrich, Robert; Eckel, Gerhard","Physiosonic - movement sonification as auditory feedback","","","","","","We detect human body movement interactively via a tracking sys- tem. This data is used to synthesize sound and transform sound files (music or text). A subject triggers and controls sound param- eters with his or her movement within a pre-set range of motion. The resulting acoustic feedback enhances new modalities of per- ception and the awareness of the body movements. It is ideal for application in physiotherapy and other training contexts. The sounds we use depend on the context and aesthetic pref- erences of the subject. On the one hand, metaphorical sounds are used to indicate the leaving of the range of motion or to make un- intended movements aware. On the other hand, sound material like music or speech is played as intuitive means and motivating feedback to address humans. The sound material is transformed in order to indicate deviations from the target movement. With this sonification approach, subjects perceive the sounds they have cho- sen themselves in undistorted playback as long as they perform the training task appropriately. Our main premises are a simple map- ping of movement to sound and common sense metaphors, that both enhance the understanding for the subject.","2009","2023-07-24 06:49:24","2023-07-24 06:49:24","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PBJJEJP4","journalArticle","2013","Vogt, Katharina; Goudarzi, Visda; Parncutt, Richard","Empirical Aesthetic Evaluation Of Sonifications","","","","","","This paper discusses three experiments on the aesthetic evaluation of different sonifications. The effects of training and understanding of the auditory display on its aesthetic appealing were tested. Results showed no significant effect, but a trend towards less acceptance due to longer exposure to the sounds in general. Furthermore, there might be effects of musical ability and gender that should be further explored.","2013","2023-07-24 06:49:24","2023-07-24 06:49:24","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UBSHCWUI","journalArticle","2017","Taylor, Stephen","From Program Music to Sonification: Representation and the Evolution of Music and Language","","","","","","Research into the origins of music and language can shed new light on musical representation, including program music and more recent incarnations such as data sonification. Although sonification and program music have different aims—one scientific explication, the other artistic expression—similar techniques, relying on human and animal biology, cognition, and culture, underlie both. Examples include Western composers such as Beethoven and Berlioz, to more recent figures like Messiaen, Stockhausen and Tom Johnson, as well as music theory, semiotics, biology, and data sonifications by myself and others. The common thread connecting these diverse examples is the use of human musicality, in the biomusicological sense, for representation. Links between musicality and representation—dimensions like high/low, long/short, near/far, etc., bridging the real and abstract—can prove useful for researchers, sound designers, and composers.","2017","2023-07-24 06:49:24","2023-07-24 06:49:24","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6DWIH6AT","journalArticle","2019","Rönnberg, Niklas; Lowgren, Jonas","Traces of modal synergy: studying interactive musical sonification of images in general-audience use","","","","","","Photone is an interactive installation combining color images with musical sonification. The musical expression is generated based on the syntactic (as opposed to semantic) features of an image as it is explored by the userﾒs pointing device, intending to catalyze a holistic user experience we refer to as modal synergy where visual and auditory modalities multiply rather than add. We collected and analyzed two months' worth of data from visitorsﾒ interactions with Photone in a public exhibition at a science center. Our results show that a small proportion of visitors engaged in sustained interaction with Photone, as indicated by session times. Among the most deeply engaged visitors, a majority of the interaction was devoted to visually salient objects, i.e., semantic features of the images. However, the data also contains instances of interactive behavior that are best explained by exploration of the syntactic features of an image, and thus may suggest the emergence of modal synergy.","2019","2023-07-24 06:49:24","2023-07-24 06:49:24","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QVIR5WJ6","journalArticle","2004","Walker, Bruce N.; Mauney, Lisa M.","Individual differences, cognitive abilities, and the interpretation of auditory graphs","","","","","","Auditory graphs exploit pattern recognition in the auditory system, but questions remain about the relationship between cognitive abilities, demographics, and sonification interpretation. Subjects completed a magnitude estimation task relating sound dimensions to data dimensions. Subjects also completed a working memory task (2-back task) and a spatial reasoning task (Raven's Progressive Matrices) to assess cognitive abilities. Demographics, such as gender, age, handedness, and musical experience, were also reported and included in the analysis. A stepwise multiple regression analysis was performed to determine the relationship between the independent (cognitive abilities and demographics) and dependent (individual slopes and R-squared values) variables. The regression analysis indicates some support for most of the predictor variables, especially predicting R-squared values. The 2-back task does not seem to contribute significantly to the interpretation of sonifications and auditory graphs. However, Raven's and many of the demographic variables do show predictive value for interpretation of auditory graphs.","2004","2023-07-24 06:49:25","2023-07-24 06:49:25","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MUZWNGPD","journalArticle","2019","Falk, Courtney; Dykstra, Josiah","Sonification with music for cybersecurity situational awareness","","","","","","Cyber defenders work in stressful, information-rich, and highstakes environments. While other researchers have considered sonification for security operations centers (SOCs), the mappings of network events to sound parameters have produced aesthetically unpleasing results. This paper proposes a novel sonification process for transforming data about computer network traffic into music. The musical cues relate to notable network events in such a way as to minimize the amount of training time a human listener would need in order to make sense of the cues. We demonstrate our technique on a dataset of 708 million authentication events over nine continuous months from an enterprise network. We illustrate a volume-centric approach in relation to the amplitude of the input data, and also a volumetric approach mapping the input data signal into the number of notes played. The resulting music prioritizes aesthetics over bandwidth to balance performance with adoption.","2019","2023-07-24 06:49:25","2023-07-24 06:49:25","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TWYW4Z7I","journalArticle","2022","Kalonaris, Stefano","Tōkyō kion-on: Query-based generative sonification of atmospheric data","","","","","","Amid growing environmental concerns, interactive displays of data constitute an important tool for exploring and understanding the impact of climate change on the planet’s ecosystemic integrity. This paper presents Tokyo kion-on, a query-based sonification model of Tokyo’s air temperature from 1876 to 2021. The system uses a recurrent neural network architecture known as LSTM with attention trained on a small dataset of Japanese melodies and conditioned upon said atmospheric data. After describing the model’s implementation, a brief comparative illustration of the musical results is presented, along with a discussion on how the exposed hyper-parameters can promote active and non-linear exploration of the data.","2022","2023-07-24 06:49:25","2023-07-24 06:49:25","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4ZK5U5JE","journalArticle","2021","Kantan, Prithvi Ravi; Spaich, Erika G.; Dahl, Sofia","A metaphor-based technical framework for musical sonification in movement rehabilitation","","","","","","Interactive sonification has increasingly shown potential as a means of biofeedback to aid motor learning in movement rehabilitation. However, this application domain faces challenges related to the design of meaningful, task-relevant mappings as well as aesthetic qualities of the sonic feedback. A recent mapping design approach is that of using conceptual metaphors based on image schemata and embodied music cognition. In this work, we developed a framework to facilitate the design and real-time exploration of rehabilitation-tailored mappings rooted in a specific set of music-based conceptual metaphors. The outcome was a prototype system integrating wireless inertial measurement, flexible real-time mapping control and physical modelling-based musical sonification. We focus on the technical details of the system, and demonstrate mappings that we created through it for two exercises. These will be iteratively honed and evaluated in upcoming usercentered studies. We believe our framework can be a useful tool in musical sonification design for motor learning applications.","2021","2023-07-24 06:49:25","2023-07-24 06:49:25","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XDJE5NEN","journalArticle","2012","Giot, Rudi; Courbe, Yohan","InteNtion – Interactive Network Sonification","","","","","","This paper presents an innovative approach in monitoring network traffic by adding a new dimension: the sound. InteNtion (Interactive Network Sonification) is a project aimed at mapping network activity to musical aesthetic. The network traffic analysis is made with the SharpPCap library (a port of WinPCap to C# environment). From this analysis, the collected data are converted into MIDI (Musical Instrument Digital Interface) messages and sent to dedicated synthesizers, which generate sounds dynamically mixed together. The whole process results in an interactive soundscape. This novel approach will initiate two opportunities for technological development. It allows users to actively take part in an interactive exhibition system through simple actions involving network access, including streaming radio over the Internet, sharing music on Twitter, downloading mp3 files and others. This project initiates also a new dimension in monitoring the network by helping the administrator in detecting efficiently the hacking and abuse of the infrastructure.","2012","2023-07-24 06:49:25","2023-07-24 06:49:25","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N69SQYD8","journalArticle","2017","Worrall, David","Computational Designing for Auditory Environments","","","","","","This paper is a call for sonification designers to adapt their representational practices from that of designing objects for auditory engagement to the construction of systems of formally described relationships that define the ‘state space’ from which streams of such objects can be drawn. This shift from the crafting individual sonic objects and streams to defining dynamical space of design possibilities we call ‘computational designing’. Such sonification model spaces are inaudible, heard only through its instances, or the manifestations of particular trajectories through the space. Approaching the design of auditory displays as computational tasks poses both considerable challenges and opportunities. These challenges are often understood to be technical, requiring scripting or programming skills, however the main challenge lies in computational design thinking which is not best understood as the extension of established designing processes. The intellectual foundations of computational designing rest at the confluence of multiple fields ranging from mathematics, computer science and systems science to biology, psychophysical and cognitive perception, social science, music theory and philosophy. This paper outlines the fundamental concepts of computational design thinking based on seminal ideas from these fields and explores how they it might be applied to the construction of models for synthesized auditory environments.","2017","2023-07-24 06:49:26","2023-07-24 06:49:26","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5XUFB399","journalArticle","2021","Yang, Jing; Roth, Andreas","Musical features modification for less intrusive delivery of popular notification sounds","","","","","","Less intrusive information delivery has been a popular research topic for auditory displays. While most research has addressed this issue by creating new notification cues such as rendering ambient soundscapes or modifying background music, we present a novel method to gently deliver artificial notification sounds that have been commonly used in digital devices and for popular applications. We propose to play a notification sound by embedding it into the music that a user is listening to, after changing the musical timbre, amplitude, tempo, and octave of the notification to match these features of the music. To implement this concept, we extend a melody extraction algorithm for notification timbre transfer, and we present a pipeline that algorithmically selects a proper time spot and harmoniously embeds the notification into music. To validate our design concept, we present a user study comparing our method with the standard method of playing notification sounds on digital devices. Through an extensive analysis of 96 tasks performed by 32 participants, we demonstrate that our method can deliver notification sounds in a less intrusive but adequately noticeable manner and is preferred by most participants.","2021","2023-07-24 06:49:26","2023-07-24 06:49:26","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4ESVEPH4","book","2015","Schmele, Timothy; Romero, Juan Alzate; Troge, Thomas A.; Ruiter, Nicole; Zapf, Michael","Sonifying multichannel ultrasound data for periphonic loudspeaker array","","","","","","The following paper describes several transformations of ultrasonic data into musical material for hemispheric loudspeaker setup. This data comes from a medical prototype for the purpose of early breast cancer detection via 3D multimodal imaging. The data is acquired in a semi-ellipsoid mesh of thousands of ultrasonic emitters and receivers surrounding the measurement object with water as medium. A hemispheric loudspeaker array can reflect this aperture design and offers the possibility of projecting this data in a direct fashion for auditory display. The ultrasounds in the megahertz range are inaudible and need to be transformed into the audible range. We here describe our investigations and methods to transfer medical ultrasound data in an artistic context and report on our insights using different sonification strategies to gain audible, musical material.","2015","2023-07-24 06:49:26","2023-07-24 06:49:26","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V2RU5Q6X","journalArticle","2018","Snook, Kelly; Barri, Tarik; Goßmann, Joachim; Potts, Jason; Schedel, Margaret; Warm, Hartmut","Kepler Concordia: Designing an immersive modular musical and scientific instrument using novel blockchain and sonification technologies in XR","","","","","","This paper describes the first steps in the creation of a new scientific and musical instrument to be released in 2019 for the 400th anniversary of Johannes Kepler's Harmonies of the World, which laid out his three laws of planetary motion and launched the field of modern astronomy. Concordia is a musical instrument that is modularly extensible, with its first software and hardware modules and underlying framework under construction now. The instrument is being designed in an immersive extended-reality (XR) environment with scientifically accurate visualizations and datatransparent sonifications of planetary movements rooted in the musical and mathematical concepts of Johannes Kepler [1], extrapolated into visualizations by Hartmut Warm [2], and sonified. Principles of game design, data sonification/visualization optimization, and digital and analog music synthesis are used in the 3D presentation of information, the user interfaces (UX), and the controls of the instrument, with an optional DIY hardware “cockpit” interface. The instrument hardware and software are both designed to be modular and open source; Concordia can be played virtually without the DIY cockpit on a mobile platform, or users can build or customize their own interfaces, such as traditional keyboards, button grids, or gestural controllers with haptic feedback to interact with the system. It is designed to enable and reward practice and virtuosity through learning levels borrowed from game design, gradually building listening skills for decoding sonified information. The frameworks for uploading, verifying, and accessing the data; programming and verifying hardware and software module builds; tracking of instrument usage; and managing the instrument's economic ecosystem are being built using a combination of distributed computational technologies and peer-to-peer networks, including blockchain and the Interplanetary Filesystem (IPFS). Participants in Concordia fall into three general categories, listed here in decreasing degrees of agency: 1) Contributors; 2) Players; and 3) Observers. This paper lays out the broad structure of Concordia, describes progress on the first software module, and explores the creative, social, economic, and educational potential of Concordia as a new type of creative ecosystem.","2018","2023-07-24 06:49:26","2023-07-24 06:49:26","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N7CHT7JK","journalArticle","2006","Orzessek, B.; Falkner, M.","Sonification of autonomic rhythms in the frequency spectrum of heart rate variability","","","","","","This poster presents some of the work currently being done at the Paracelsus Clinic in Switzerland on heart rate variability biofeedback with a real time auditory display. Heart rate variability biofeedback is an important diagnostic and therapeutic tool in the work with a wide variety of chronic disorders. We use a proprietary building-block type laboratory computer program that is linked via MIDI to a software sequencer with a VST virtual instrument library. Beyond the sonification of RR intervals as discrete numbers, the development of new techniques became necessary in order to be able to sonify the dynamic, wave-like structure of autonomic rhythms in the frequency spectrum of HRV, what we call ”heartmusic”. The fact that patients can hear their inner autonomic activity as music in real time and so work with elements of their own autonomous rhythmic oscillations, may also add an important new dimension to this field in the future.","2006","2023-07-24 06:49:26","2023-07-24 06:49:26","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DWNR58YZ","journalArticle","2019","King, Rob","‘Music of the people': Music from data as social commentary","","","","","","Data-music reflects the ubiquity of data in modern society. Composers have not engaged widely with the opportunities opened up by this, despite the chance to overcome a gulf between academic art music and social engagement. Their reluctance might be traced to the challenge of reconciling abstract data and concrete sound, in political implications, and in technological barriers in computer music. The present paper argues that socially relevant music composition for the 21st century can adopt a programme of sonification grounded in politically acute data. As examples of such practice, two compositions are discussed founded upon US and UK social data sets, and realised via the SuperCollider programming language. The consequences for the composer of new music are further discussed from political and musicological angles, with the ﾑpurposeﾒ of writing such music analysed from the perspective of various commentators.","2019","2023-07-24 06:49:27","2023-07-24 06:49:27","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5FS7L98A","journalArticle","2019","Nadri, Chihab; Anaya, Chairunisa; Yuan, Shan; Jeon, Myounghoon","Preliminary guidelines on the sonification of visual artworks: Linking music, sonification & visual arts","","","","","","Sonification and data processing algorithms have advanced over the years to reach practical applications in our everyday life. Similarly, image processing techniques have improved over time. While a number of image sonification methods have already been developed, few have delved into potential synergies through the combined use of multiple data and image processing techniques. Additionally, little has been done on the use of image sonification for artworks, as most research has been focused on the transcription of visual data for people with visual impairments. Our goal is to sonify paintings reflecting their art style and genre to improve the experience of both sighted and visually impaired individuals. To this end, we have designed initial sonifications for paintings of abstractionism and realism, and conducted interviews with visual and auditory experts to improve our mappings. We believe the recommendations and design directions we have received will help develop a multidimensional sonification algorithm that can better transcribe visual art into appropriate music.","2019","2023-07-24 06:49:27","2023-07-24 06:49:27","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D4XS3C77","journalArticle","2002","Childs, E.","Achorripsis: A sonification of probability distributions","","","","","","The 1957 musical composition Achorripsis by Iannis Xenakis was composed using four different probability distributions, applied over three different organizational domains, during the course of the 7 minute piece. While Xenakis did not have sonification in mind, his artistic choices in rendering mathematical formulations into musical events (time, space, timbre, glissando speed) provide useful contributions to the “mapping problem” in three significant ways: 1. He pushes the limit of loading the ear with multiple formulations simultaneously. 2. His mapping of “velocity” to string glissando speed provides a useful method of working with a vector quantity with magnitude and direction. 3. His artistic renderings, ie. “musifications” of these distributions, invite the question, in general, as to whether musical/ artistic sonifications are more intelligible to the human ear than sonifications prepared without any musical “filtering” or constraints (e.g. that they could be notated and performed by musicians).","2002","2023-07-24 06:49:27","2023-07-24 06:49:27","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZU8PDXR8","journalArticle","2003","England, David; Salces, Fausto J. Sainz; Vickers, Paul","Household appliances control device for the elderly","","","","","","An evaluation of musical earcons was carried out to see whether they are an effective and efficient method of delivering information about household appliances to elderly people. A test was carried out to explore the ability of the elderly subjects in remembering and learning the musical earcons. This test indicated a poor rate of recognition of the earcons. A second test that included the presentation of information in three modes (audio, visual and multimodal) was performed to determine which modality was preferred to deliver certain types of information among this group. We hypothesized that the multimodal interface would be the best in terms of speed and accuracy of response, and this was supported by the data. The results showed the need for a redesign of the earcons.","2003","2023-07-24 06:49:28","2023-07-24 06:49:28","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZCFAQ839","journalArticle","2000","Sturm, Bob L.","Sonification of particle systems via de Broglie's Hypothesis","","","","","","Quantum mechanics states a particle can behave as either a particle or a wave. Thus systems of particles might be likened to a complex superposition of dynamic waves. Motivated by this, the author develops methods for the sonification of particle systems in a logical manner. Many systems and physical phenomena have thus far been simulated, producing a wide range of unique sonic events. The applications that have been explored are for algorithmic sound synthesis and music composition. Of critical importance is addressing the issue of latencies, caused by large complex numerical operations at audio sampling rates. This becomes painfully clear when particles interact with each other. Further applications of this system include scientific sonification, with an appropriate integration of psychoacoustic principles; creating an application for physics and music students to extend and enrich their comprehension of both topics; and inspiring philosophical dialogue regarding the similarities, intersections, and interdependence of Art and Science. Future work aims to produce a real-time application for simulated and real systems, and a deeper integration of quantum mechanics into these techniques.","2000","2023-07-24 06:49:28","2023-07-24 06:49:28","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2DZT7NAV","journalArticle","2013","Kikukawa, Yuya; Kato, Megumi; Baba, Tetsuaki; Kushiyama, Kumiko","Hakoniwa: A sonification art installation consists of sand and woodblocks","","","","","","In this research we present an interactive tabletop installation ÒHakoniwaÓ. It consists of a wooden box, white sand and painted woodblocks. In this system, corresponding to the arrangement of woodblocks, a ceiling-mounted projector shows visual effects on the sand surface. At the same time, generative music is composed in a computer corresponding to the arrangement of woodblocks and modeling of the sand. This is an attempt of sonification of miniature garden. We studied Sandtray therapy, one of a famous form of art therapy, as a motif of the installation. We directed our attention to tactile sensation of sand and woodblocks, and tried to extend sandtray using computer vision processing and multi-media output. In this paper we describe details of the interactive system and discuss the possibility of supporting primitive play using such interactive systems.","2013","2023-07-24 06:49:28","2023-07-24 06:49:28","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WDC4JBHN","journalArticle","2021","Fiedler, Brett L.; Walker, Bruce N.; Moore, Emily B.","To sonify or not to sonify? Educator perceptions of auditory display in interactive simulations","","","","","","With the growing presence of auditory display in popular learning tools, it is beneficial to researchers to consider not only the perceptions of the students who use the tools, but the educators who include the tools in their curriculum. We surveyed over 4000 educators to investigate educator perceptions and preferences across four interactive physics simulations for the presence and qualities of non-speech auditory display, as well as surveying users' selfrated musical sophistication as potentially predictive of auditory display preference. We find that the majority of teachers preferred the simulations with auditory display and consistently rated aspects of the experience using simulations with sound positively over the without-sound variants. We also identify simulation design features that align with trends in educator ratings. We did not find the measured musical sophistication to be a predictor of auditory display preference.","2021","2023-07-24 06:49:28","2023-07-24 06:49:28","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3MANVZ4K","journalArticle","2014","Zareei, Mo H.; McKinnon, Dugal; Kapur, Ajay; Carnegie, Dale A.","Complex: Physical Re-sonification of Urban Noise","","","","","","This paper explores the aesthetic and social values of the noises of modern urban soundscapes and discusses some strategies for boosting the accessibility and appreciation of works of sound art and experimental music that employ them. A proposed audiovisual installation––entitled complex––is outlined as a practical application of techniques designed to reveal the sonic aesthetics of urban technological noise, primarily through resonification and visualization. This will be achieved sonically and physically, by mapping sonic data collected from New York City soundscape (using the Citygram project) onto custom-designed mechatronic soundsculptures.","2014","2023-07-24 06:49:29","2023-07-24 06:49:29","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IFLUYCSP","journalArticle","2002","Johannsen, G.","Auditory display of directions and states for mobile systems","","","","","","Auditory displays for mobile systems, such as service robots, have been developed. The design of directional sounds and of additional sounds for robot states (e.g., Heavy Load), as well as the design of more complicated robot sound tracks are explained. Basic musical elements and robot movement sounds have been combined. Two experimental studies, on the understandability of the directional sounds and the robot state sounds as well as on the auditory perception of intended robot trajectories in a simulated supermarket scenario, are described. Subjective evaluations of sound characteristics such as urgency, expressiveness, and annoyance have been performed by non-musicians and musicians. These experimental results are compared with the diagrams which have been computed with two wavelet techniques for time-frequency analyses.","2002","2023-07-24 06:49:29","2023-07-24 06:49:29","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DYP9RJMT","journalArticle","2018","Khan, Ridwan Ahmed; Jeon, Myounghoon; Yoon, Tejin","“Musical Exercise” for people with visual impairments: A preliminary study with the blindfolded","","","","","","Performing independent physical exercise is critical to maintain one's good health, but it is specifically hard for people with visual impairments. To address this problem, we have developed a Musical Exercise platform for people with visual impairments so that they can perform exercise in a good form consistently. We designed six different conditions, including blindfolded or visual without audio conditions, and blindfolded or visual with two different types of audio feedback (continuous vs. discrete) conditions. Eighteen sighted participants participated in the experiment, by doing two exercises - squat and wall sit with all six conditions. The results show that Musical Exercise is a usable exercise assistance system without any adverse effect on exercise completion time or perceived workload. Also, the results show that with a specific sound design (i.e., discrete), participants in the blindfolded condition can do exercise as consistently as participants in the non-blindfolded condition. This implies that not all sounds equally work and thus, care is required to refine auditory displays. Potentials and limitations of Musical Exercise and future works are discussed with the results.","2018","2023-07-24 06:49:29","2023-07-24 06:49:29","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F5SGFX5Y","book","2015","Rutz, Hanns Holger; Vogt, Katharina; Höldrich, Robert","The SysSon platform: A computer music perspective of sonification","","","","","","We introduce SysSon, a platform for the development and application of sonification. SysSon aims to be an integrative system that serves different types of users, from domain scientists to sonification researchers to composers and sound artists. It therefore has an open nature capable of addressing different usage scenarios. We have used SysSon both in workshops with climatologists and sonification researchers and as the engine to run a real-time sound installation based on climate data. The paper outlines the architecture and design decisions made, showing how a sonification system can be conceived as a collection of specialised abstractions that sit atop a general computer music environment. We report on our experience with SysSon so far and make suggestions about future improvements.","2015","2023-07-24 06:49:30","2023-07-24 06:49:30","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"427IRQZP","journalArticle","2016","Zhang, Ruimin; Barnes, Jaclyn; Ryan, Joseph; Jeon, Myounghoon; Park, Chung Hyuk; Howard, Ayanna M.","Musical Robots For Children With ASD Using A Client-Server Architecture","","","","","","People with Autistic Spectrum Disorders (ASD) are known to have difficulty recognizing and expressing emotions, which affects their social integration. Leveraging the recent advances in interactive robot and music therapy approaches, and integrating both, we have designed musical robots that can facilitate social and emotional interactions of children with ASD. Robots communicate with children with ASD while detecting their emotional states and physical activities and then, make real-time sonification based on the interaction data. Given that we envision the use of multiple robots with children, we have adopted a client-server architecture. Each robot and sensing device plays a role as a terminal, while the sonification server processes all the data and generates harmonized sonification. After describing our goals for the use of sonification, we detail the system architecture and on-going research scenarios. We believe that the present paper offers a new perspective on the sonification application for assistive technologies.","2016","2023-07-24 06:49:30","2023-07-24 06:49:30","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RICKIP6R","journalArticle","2017","Newbold, Joseph W.; Bianchi-Berthouze, Nadia; Gold, Nicolas E.","Musical Expectancy in Squat Sonification For People Who Struggle With Physical Activity","","","","","","Physical activity is important for a healthy lifestyle. However, it can be hard to stay engaged with exercise and this can often lead to avoidance. Sonification has been used to support physical activity through the optimisation/correction of movement. Though previous work has shown how sonification can improve movement execution and motivation, the specific mechanisms of motivation have yet to be investigated in the context of challenging exercises. We investigate the role of music expectancy as a way to leverage people’s implicit and embodied understanding of music within movement sonification to provide information on technique while also motivating continuation of movement and rewarding its completion. The paper presents two studies showing how this musically informed sonification can be used to support the squat movement. The results show how musical expectancy impacted people’s perception of their own movement, in terms of reward, motivation and movement behaviour and the way in which they moved.","2017","2023-07-24 06:49:30","2023-07-24 06:49:30","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TXYGFYZY","journalArticle","2018","Tsuchiya, Takahiko; Freeman, Jason","A study of exploratory analysis in melodic sonification with structural and durational time scales","","","","","","Melodic sonification is one of the most common methods of sonification: data modulates the pitch of an audio synthesizer over time. This simple sonification, however, still raises questions about how we listen to a melody and perceive the motions and patterns characterized by the underlying data. We argue that analytical listening to such melodies may focus on different ranges of the melody at different times and discover the pitch (and data) relationships gradually over time and after repeated listening. To examine such behaviors in real-time listening to a melodic sonification, we conducted a user study employing interactive time and pitch resolution controls for the user. The study also examines the relationships of these changing time and pitch resolutions to perceived musicality. The results indicate a stronger general relationship between the time progression and the use of time-resolution control to analyze data characteristics, while the pitch resolution controls tend to have more correlation with subjective perceptions of musicality.","2018","2023-07-24 06:49:30","2023-07-24 06:49:30","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JDCXCR5K","journalArticle","2004","Roeber, N.; Masuch, M.","Interacting with sound: An interaction paradigm for virtual auditory worlds","","","","","","The visual and the auditory field of perception respond on different input signals from our environment. Thus, interacting with worlds solely trough sound is a very challenging task. This paper discusses methods and techniques for sonification and interaction in virtual auditory worlds. In particular, it describes auditory elements such as speech, sound and music and discusses their application in diverse auditory situations, as well as interaction techniques for assisted sonification. The work is motivated by the development of a framework for the interactive exploration of auditory environments which will be used to evaluate the later discussed techniques. The main focus for the design of this framework is the use in narrative environments for auditory games, but also for general purpose auditory user interfaces and communication processes.","2004","2023-07-24 06:49:30","2023-07-24 06:49:30","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8DV6QDHV","journalArticle","2001","Quinn, Marty","Research set to music: The climate symphony and other sonifications of ice core, radar, DNA, seismic and solar wind data","","","","","","The Climate Symphony and Other Sonifications of Ice Core, Radar, DNA, Seismic and Solar Wind Data is a one-hour performance/presentation of sonification research by Marty Quinn of Design Rhythmics Sonification Research Lab and BAE Systems. It was presented in November 2000 at the National Science Foundation at the invitation of the Director's Office of Public Affairs and the Office of Polar Programs and was warmly received. This paper describes the Climate Symphony portion of the presentation in detail.","2001","2023-07-24 06:49:30","2023-07-24 06:49:30","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MTLK3WZ8","journalArticle","2004","Coyle, Eugene; Cullen, C.","Orchestration within the sonification of basic data sets","","","","","","The use of sonification as a means of representing and analysing data has become a growing field of research in recent years and as such has become a far more accepted means of working with data. Existing work carried out as part of this research has focused primarily on the sonification of DNA/RNA sequences and their subsequent protein structures for the purposes of analysis. This sonification work raised many questions as regards the need for sequences to be set to music in a standard manner so that different strands could be analysed by comparison, and hence the orchestration and instrumentation used became of great importance. The basic principles of sonification can be rapidly extended to include many different data elements within a single rendering, and thus the importance of orchestration grows accordingly. Existing work on the use of rhythmic parsing within a sonification had suggested that far more information could be represented when orchestrated in a rhythmic manner than when simply reconstituted in single musical block. The principle was further extended to include the allocation specific instruments and pitches within rhythmic patterns so that each sonic event would convey the data it was intended to represent. To this end a fictional database of employees in a company was created as a means of developing the principles required for more effective sonification through orchestration. The employee database was intended as a means of using a straightforward data set to analyse the effect of basic changes in instrumentation and orchestration rather than the data itself. The allocation of chord intervals or melodies to different data elements allowed the data to be represented in different ways at output in order that these differences would eventually highlight some form of framework for effective sonification of data sets with multiple elements.","2004","2023-07-24 06:49:30","2023-07-24 06:49:30","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SCF4PR5P","book","2015","Lindborg, PerMagnus; Yuning, David Liu","Locust wrath: An iOS audience participatory auditory display","","","","","","Mobile devices have been used in soundscape installations and performances over the past decade or longer, often to emphasize social interaction. Multichannel sonification has been found to successfully represent data describing kinematic phenomena. However, there are few if any examples where these two approaches are combined. The Locust Wrath project has evolved in stages: first, as surround sonifications of climate data for a multimedia dance performance; then, as a frontal display sound installation and as material in a live performance of ‘musical’ interactive sonification; and recently, as an audience participator work. We developed a system for spatialized sonification of data using a server-client model with iOS devices. In two multimedia performances, the audience members’ iPhones were employed ad hoc to constitute a large auditory display. This paper describes the artistic background to the project, outlines the stages, and focuses on the design and implementation of the Locust Wrath client app.","2015","2023-07-24 06:49:30","2023-07-24 06:49:30","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IZN73J7L","book","2015","Tsuchiya, Takahiko; Freeman, Jason; Lerner, Lee W.","Data-to-music API: Real-time data-agnostic sonification with musical structure models","","","","","","In sonification methodologies that aim to represent the underlying data accurately, musical or artistic approaches are often dismissed as being not transparent, likely to distort the data, not generalizable, or not reusable for different data types. Scientific applications for sonification have been, therefore, hesitant to use approaches guided by artistic aesthetics and musical expressivity. All sonifications, however, may have musical effects on listeners, as our trained ears with daily exposure to music tend to naturally distinguish musical and non-musical sound relationships, such as harmony, rhythmic stability, or timbral balance. This study proposes to take advantage of the musical effects of sonification in a systematic manner. Data may be mapped to high-level musical parameters rather than to one-to-one low-level audio parameters. An approach to create models that encapsulate modulatable musical structures is proposed in the context of the new DataTo- Music JavaScript API. The API provides an environment for rapid development of data-agnostic sonification applications in a web browser, with a model-based modular musical structure system. The proposed model system is compared to existing sonification frameworks as well as music theory and composition models. Also, issues regarding the distortion of original data, transparency, and reusability of musical models are discussed.","2015","2023-07-24 06:49:30","2023-07-24 06:49:30","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S4AJTSX2","journalArticle","2017","Nees, Michael A.; Harris, Joanna; Leong, Peri","How Do People Think They Remember Melodies and Timbres? Phenomenological Reports of Memory for Nonverbal Sounds","","","","","","Memory for nonverbal sounds such as those used in sonifications has been recognized as a priority for cognitiveperceptual research in the field of auditory display. Yet memory processes for nonverbal sounds are not well understood, and existing theory and research have not provided a consensus on a mechanism of memory for nonverbal sounds. We report a new analysis of a qualitative question that asked participants to report the strategy they used to retain nonverbal sounds—both melodies and sounds discriminable primarily by timbre. The question was originally posed as part of the debriefing procedure for three separate memory experiments whose primary findings are reported elsewhere. Results of this new analysis suggested that auditory memory strategies— remembering acoustic properties of sounds—were common across both types of sounds but were more commonly reported for remembering melodies. Motor strategies were also more frequently reported for remembering melodies. Both verbal labeling of sounds and associative strategies—linking the sounds to existing information in memory—were more commonly reported as strategies for remembering sounds discriminable primarily by timbre. Implications for theory and future research are discussed.","2017","2023-07-24 06:49:31","2023-07-24 06:49:31","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QWANMI27","journalArticle","2019","Morawitz, Falk","Multilayered narration in electroacoustic music composition using nuclear magnetic resonance data sonification and acousmatic storytelling","","","","","","Nuclear magnetic resonance (NMR) spectroscopy is an analytical tool to determine the structure of chemical compounds. Unlike other spectroscopic methods, signals recorded using NMR spectrometers are frequently in a range of zero to 20000 Hz, making direct playback possible. As each type of molecule has, based on its structural features, distinct and predictable features in its NMR spectra, NMR data sonification can be used to create auditory ﾑfingerprintsﾒ of molecules. This paper describes the methodology of NMR data sonification of the nuclei nitrogen, phosphorous, and oxygen and analyses the sonification products of DNA and protein NMR data. The paper introduces On the Extinction of a Species, an acousmatic music composition combining NMR data sonification and voice narration. Ideas developed in electroacoustic composition, such as acousmatic storytelling and sound-based narration are presented and investigated for their use in sonification-based creative works.","2019","2023-07-24 06:49:31","2023-07-24 06:49:31","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AZNEFQN6","journalArticle","2021","Spence, Heather Ruth; Ballora, Mark","Layers of meaning: The ocean's natural acoustics and the music of its datasets","","","","","","The transdisciplinary National Academies Keck Futures Initiative (NAKFI) conference on the Deep Blue Sea sparked a collaboration between sonification expert Mark Ballora and marine biologist and sound artist Heather Spence. Research involving long-term Marine Passive Acoustic Monitoring (MPAM) of the MesoAmerican Reef system forms the basis for a gradient of audio products: 1) layering a tour guide acoustic instrument over raw and manipulated soundscape recordings; 2) layering of multiple acoustic instruments over duty cycle interpretation sampling; and 3) layering of data sonification over the original data, with additional acoustic instrument layers. The audio products are designed to promote data exploration and understanding by researchers and students, as well as an emotional impact musically with conservation themes. Presentations have included live and virtual performances and workshops. Next steps include sonification of other correlated environmental data with the original sound data in raw, manipulated, and sonified forms.","2021","2023-07-24 06:49:31","2023-07-24 06:49:31","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M6YIG59F","journalArticle","2001","Childs, Edward","The sonification of numerical fluid flow simulations","","","","","","Computational Fluid Dynamics (CFD) software simulates fluid, air flow and heat transfer by solving the Navier-Stokes (N-S) equations numerically. Realistic 3-D engineering simulations typically yield the values of 7 or more variables (e.g. fluid component velocities and temperatures) at hundreds of thousands of points in space, all as a function of time. It has been noted that solutions of the N-S equations sometimes yield highly complex, non-linear flow fields which can be aesthetically interesting from a purely visual standpoint. The analysis of CFD results may benefit substantially from sonification, to depict convergence behavior, scan large amounts of data with low activity, or codify global events in the flow field. As a corollary to this interest in developing CFD sonification techniques, we can explore its unusual potential as a tool for algorithmic musical composition. This paper will report the results of an initial implementation of the author's port of the two-dimensional, steady, laminar CFD code TEACH-L on a JAVA platform, in which the numerical output is linked in real time to the JSyn digital audio synthesis package. The sonification of steady, laminar, developing flow in a two dimensional duct will be described in detail.","2001","2023-07-24 06:49:31","2023-07-24 06:49:31","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5397ISMV","journalArticle","2014","Fan, Jianyu; Topel, Spencer","SonicTaiji: A Mobile Instrument for Taiji Performance","","","","","","SonicTaiji is a mobile instrument designed for the Android Platform. It utilizes accelerometer detection, sound synthesis, and data communication techniques to achieve real-time Taiji sonification. Taiji is an inner-strength martial art aimed at inducing meditative states. In this mobile music application, Taiji movements are sonified via gesture detection, connecting listening and movement. This instrument is a tool for practitioners to enhance the meditative experience of performing Taiji. We describe the implementation of gesture position selection, real-time synthesis, and data mapping. We describe outcomes of subjective tests of the user experience.","2014","2023-07-24 06:49:31","2023-07-24 06:49:31","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XWS3ZI78","journalArticle","2005","Lemmens, Paul M. C.","Using the major and minor mode to create affectively-charged earcons","","","","","","The importance of the structured fabrication of auditory (feedback) signals like earcons is common knowledge in the ICAD community. To create such structured families of earcons musical transformations like rhythm or pitch (and many others) are usually employed. However, one important transformation in Western tonal music, that of the distinction between major and minor mode, to our knowledge, has not been exploited, despite the fact that the affective connotation of the major and minor mode might be useful for research into auditory signals for affective human– computer interfaces. The present study investigated whether the transformation to major or minor mode can be used to create affectively–charged earcons for use in affective– computing research [1]. The affective–congruency effect that we obtained provides evidence that the processing of affective information can interfere with making rational, cognitive decisions. We argue that the transformation to the major or minor mode is suitable to create affectively–charged earcons and that it is important to ensure affective correspondence in computer interfaces to be able to realize optimal performance levels.","2005","2023-07-24 06:49:31","2023-07-24 06:49:31","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XQNCJK2S","journalArticle","2006","Palomaki, H.","Meanings conveyed by simple auditory rhythms","","","","","","In this article we concentrate on perception of non-musical rhythm. The purpose of this study has been to find possible meanings related to simple auditory rhythms. Meanings were examined using semantic scales. 26 subjects rated nine different rhythm samples according to adjective pair scales. We also identify some preliminary design suggestions as to how rhythm can be used in sonification and discuss duration limitation when composing earcons.","2006","2023-07-24 06:49:31","2023-07-24 06:49:31","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XKCGD2EM","journalArticle","2014","Ballora, Mark","Sonification Strategies for the Film Rhythms of the Universe","","","","","","Design strategies are discussed for sonifications that were created for the short film Rhythms of the Universe, which was conceived by a cosmologist and a musician, with multi-media contributions from a number of artists and scientists. Sonification functions as an engagement factor in this scientific outreach project, along with narration, music, and visualization. This paper describes how the sonifications were created from datasets describing pulsars, the planetary orbits, gravitational waves, nodal patterns in the sun’s surface, solar winds, extragalactic background light, and cosmic microwave background radiation.","2014","2023-07-24 06:49:31","2023-07-24 06:49:31","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BJ4BNZQR","journalArticle","2019","Savery, Richard; Ayyagari, Madhukesh; May, Keenan; Walker, Bruce N.","Soccer sonification: Enhancing viewer experience","","","","","","We present multiple approaches to soccer sonification, focusing on enhancing the experience for a general audience. For this work, we developed our own soccer data set through computer vision analysis of footage from a tactical overhead camera. This data-set included X, Y, coordinates for the ball and players throughout, as well as passes, steals and goals. After a divergent creation process, we developed four main methods of sports sonification for entertainment. For the Tempo Variation and Pitch Variation methods, tempo or pitch is operationalized to demonstrate ball and player movement data. The Key Moments method features only pass, steal and goal data, while the Musical Moments method takes existing music and attempts to align the track with important data points. Evaluation was done using a combination of qualitative focus groups and quantitative surveys, with 36 participants completing hour long sessions. Results indicated an overall preference for the Pitch Variation and Musical Moments methods, and revealed a robust trade-off between usability and enjoyability.","2019","2023-07-24 06:49:31","2023-07-24 06:49:31","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HFLCUXH7","book","2015","Perez-Lopez, Andres","3DJ: A supercollider framework for real-time sound spatialization","","","","","","The field of real time sound spatizalization is recently receiving much attention, as suggested by the large number of proposals appeared in last years - both from software spatialization frameworks and from hardware spatialization interfaces. However, most of the proposed works do not take into account the existing knowledge in Human Computer Interaction Design, which causes them to remain in a simplified approach. We propose a theoretical basis for real-time spatialization design from a holistic perspective, based on the Digital Musical Instruments theory, and use it to provide a comparative review of recent proposals. Furthermore, we develop our own state-of-the-art software spatialization system, 3Dj, which may help in the task of design and evaluation of new proposals for real-time sound spatialization in the fields of interactive performance, data sonification or virtual environments.","2015","2023-07-24 06:49:32","2023-07-24 06:49:32","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YD3CEBC8","journalArticle","2019","García Riber, Adrian","Sonifigrapher: Sonified light curve synthesizer","","","","","","In an attempt to contribute to the constant feedback existing between science and music, this work describes the design strategies used in the development of the virtual synthesizer prototype called Sonifigrapher. Trying to achieve new ways of creating experimental music through the exploration of exoplanet data sonifications, this software provides an easy-touse graph-to-sound quadraphonic converter, designed for the sonification of the light curves from NASAﾒs publiclyavailable exoplanet archive. Based on some features of the first analog tape recorder samplers, the prototype allows end-users to load a light curve from the archive and create controlled audio spectra making use of additive synthesis sonification. It is expected to be useful in creative, educational and informational contexts as part of an experimental and interdisciplinary development project for sonification tools, oriented to both non-specialized and specialized audiences.","2019","2023-07-24 06:49:32","2023-07-24 06:49:32","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XBN9Y6GW","journalArticle","2018","Riber, Adrián García","Planethesizer: Approaching exoplanet sonification","","","","","","The creation of simulations, sounds and images based on information related to an object of investigation is currently a real tool used in multiple areas to bring the non-specialized public closer to scientific achievements and discoveries. Under this context of multimodal representations and simulations developed for educational and informational purposes, this work intends to build a bridge between virtual musical instruments’ development and physical models, using the gravitation laws of the seven planets orbiting around the Trappist-1 star. The following is a case study of an interdisciplinary conversion algorithm design that relates musical software synthesis to exoplanets’ astronomical data - measured from the observed flux variations in the light curves of their star- and that tries to suggest a systematic and reproducible method, useful for any other planetary system or model-based virtual instrument design. As a result, the Virtual Interactive Synthesizer prototype Planethesizer is presented, whose default configurations display a multimodal Trappist-1, Kepler-444 and K2-72 planetary systems simulation.","2018","2023-07-24 06:49:32","2023-07-24 06:49:32","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KTNV9B8D","journalArticle","2018","Boren, Braxton; Genovese, Andrea","Acoustics of virtually coupled performance spaces","","","","","","Many different musical applications, including remote sonification, sound installation, augmented reality, and distributed/ telematic music performance, make use of high speed Internet connections between different performance spaces. Most of the technical literature on this subject focuses on system latency, but there are also significant contributions from the acoustics of all rooms connected: specifically, smaller auxiliary rooms will tend to introduce spectral coloration, and the “main” larger volume will send more reverberation to the off-site performers. Measurements taken in two linked networked sites used in telematic performance show that both of these issues are present. Some improvements are suggested, including physical room alterations and equalization methods using signal processing.","2018","2023-07-24 06:49:32","2023-07-24 06:49:32","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9KLRYL69","book","2015","Barrett, Natasha; Nymoen, Kristian","Investigations in coarticulated performance gestures using interactive parameter-mapping 3D sonification","","","","","","Spatial imagery is one focus of electroacoustic music, more recently advanced by 3D audio furnishing new avenues for exploring spatio-musical structures and addressing what can be called a tangible acousmatic experience. In this paper we present new insights into spatial, temporal and sounding coarticulated (contextually smeared) gestures by applying interactive parameter-mapping sonification in three-dimensional highorder ambisonics, numerical analysis and spatial composition. 3D motion gestures and audio performance data are captured and then explored in sonification. Spatial motion combined with spatial sound is then numerically analyzed to isolate gestural objects and smaller coarticulated atoms in time, space and sound. The results are then used to explore the acousmatic coarticulated image and as building blocks for a composed dataset embodying the original gestural performance. This new data is then interactively sonified in 3D to create acousmatic compositions embodying tangible gestural imagery.","2015","2023-07-24 06:49:32","2023-07-24 06:49:32","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6S3MTVRF","journalArticle","1998","Fernstrom, Mikael; McNamara, Caolan","After direct manipulation - direct sonification","","","","","","The effectiveness of providing multiple-stream audio to support browsing on a computer was investigated through the iterative development and evaluation of a series of sonic browser prototypes. The data set used was a database containing music. Interactive sonification1 was provided in conjunction with simplified human-computer interaction sequences. It was investigated to what extent interactive sonification with multiple-stream audio could enhance browsing tasks, compared to interactive sonification with single-stream audio support. With ten users it was found that with interactive multiple-stream audio the users could accurately complete the browsing tasks significantly faster than those who had single-stream audio support.","1998","2023-07-24 06:49:32","2023-07-24 06:49:32","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SRYZ45X3","journalArticle","2012","McLachlan, Ross; McGee-Lennon, Marilyn; Brewster, Stephen","The sound of musicons: investigating the design of musically derived audio cues","","","","","","Musicons (brief samples of well-known music used in auditory interface design) have been shown to be memorable and easy to learn. However, little is known about what actually makes a good Musicon and how they can be created. This paper reports on an empirical user study (N=15) to explore the recognition rate and preference ratings for a set of Musicons that were created by allowing users to self-select 5 second sections from (a) a selection of their own music and (b) a set of control tracks. It was observed that sampling a 0.5 second Musicon from a 5-second musical section resulted in easily identifiable and well liked Musicons. Qualitative analysis highlighted some of the underlying properties of the musical sections that resulted in ‘good’ Musicons. A preliminary set of guidelines is presented that provides a greater understanding of how to create effective and identifiable Musicons for future auditory interfaces.","2012","2023-07-24 06:49:32","2023-07-24 06:49:32","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A9Z5YAR4","journalArticle","2002","Sturm, B. L.","Surf music: Sonification of ocean buoy spectral data","","","","","","The Coastal Data Information Program (CDIP) has been collecting data on ocean wave conditions since late 1975, first using arrays of pressure sensors, and more recently directional buoys. Fourier analysis of the data reveals the spectral and directional content of the wave-driven motions measured by the buoy. Shifting the spectrum to an audible range and synthesizing a time-domain signal creates an aurally interesting and illuminating sonification of ocean wave dynamics. The work done so far has been guided by artistic curiosity; but input from a senior oceanographer has given guidance toward interpretation and elaboration of the methodology. Examples of ocean buoy spectral data sonification are presented, each illustrating important aspects of physical oceanography. Three forms of the data are sonified, from the least detailed to the most. The obvious sonic events are the effects of energy from storms, both local and far away. From the sonification one can estimate the energy of the storm, and the distance it originated. Entire years of data have been sonified in one to thirty minute durations for buoys in different regions, which demonstrate dramatic seasonal and regional differences. Also displayed are the time-lags of South moving storm energies at three distantly separated points on the West Coast of the United States.","2002","2023-07-24 06:49:33","2023-07-24 06:49:33","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VRSGB3NK","journalArticle","2017","Mathew, Marlene; Cetinkaya, Mert; Roginska, Agnieszka","BSONIQ: A 3-D EEG Sound Installation","","","","","","Brain Computer Interface (BCI) methods have received a lot of attention in the past several decades, owing to the exciting possibility of computer-aided communication with the outside world. Most BCIs allow users to control an external entity such as games, prosthetics, musical output etc. or are used for offline medical diagnosis processing. Most BCIs that provide neurofeedback, usually categorize the brainwaves into mental states for the user to interact with. Raw brainwave interaction by the user is not usually a feature that is readily available for a lot of popular BCIs. If there is, the user has to pay for or go through an additional process for raw brain wave data access and interaction. BSoniq is a multi-channel interactive neurofeedback installation which, allows for real-time sonification and visualization of electroencephalogram (EEG) data. This EEG data provides multivariate information about human brain activity. Here, a multivariate event-based sonification is proposed using 3D spatial location to provide cues about these particular events. With BSoniq, users can listen to the various sounds (raw brain waves) emitted from their brain or parts of their brain and perceive their own brainwave activities in a 3D spatialized surrounding giving them a sense that they are inside their own heads.","2017","2023-07-24 06:49:33","2023-07-24 06:49:33","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R7N87SZM","journalArticle","2018","Cowden, Patrick; Dosiek, Luke","Auditory displays of electric power grids","","","","","","This paper presents auditory displays of power grid voltage. Due to the constantly changing energy demands experienced by a power system, the voltage varies slightly about nominal, e.g., 120±2 V at 60±0.04 Hz. These variations are small enough that any audible effects, such as transformer hum, appear to have constant volume and pitch. Here, an audification technique is derived that amplifies the voltage variations and shifts the nominal frequency from 60 Hz to a common musical note. Sonification techniques are presented that map the voltage magnitude and frequency to MIDI velocity and pitch, and create a sampler trigger from frequency deviation. Several examples, including audio samples, are given under a variety of power system conditions. These results culminate in a multi-instrument track generated from the sonification of time-synchronized geographically widespread power grid measurements. In addition, an inexpensive Arduino-based device is detailed that allows for real-time sonification of wall outlet voltage.","2018","2023-07-24 06:49:33","2023-07-24 06:49:33","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DIYMPTG7","journalArticle","2019","Joo, Woohun","Sonifyd: A graphical approach for sound synthesis and synesthetic visual expression","","","","","","This paper describes Sonifyd, a sonification driven multimedia and audiovisual environment based on color-sound conversion for real-time manipulation. Sonifyd scans graphics horizontally or vertically from a scan line, generates sound and determines timbre according to its own additive synthesis based color-to-sound mapping. Color and sound relationships are fixed as default, but they can be organic for more tonal flexibility. Within this ecosystem, flexible timbre changes will be discovered by Sonifyd. The scan line is invisible, but Sonifyd provides another display that represents the scanning process in the form of dynamic imagery representation. The primary goal of this project is to be a functioning tool for a new kind of visual music, graphic sonification research and to further provide a synesthetic metaphor for audiences/users in the context of an art installation and audiovisual performance. The later section is a discussion about limitations that I have encountered: using an additive synthesis and frequency modulation technique with the line scanning method. In addition, it discusses potential possibilities for the future direction of development in relation to graphic expression and sound design context.","2019","2023-07-24 06:49:33","2023-07-24 06:49:33","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QUXRPQ3B","book","2010","Berger, Jonathan; Wang, Ge; Chang, Mindy","Sonification and Visualization of Neural Data","","","","","","This paper describes a method for integrating audio and visual displays to explore the activity of neurons in the brain. The motivation is twofold: to help understand how populations of neurons respond during cognitive tasks and in turn explore how signals from the brain might be used to create musical sounds. Experimental data was drawn from electrophysiological recordings of individual neurons in awake behaving monkeys, and an interface was designed to allow the user to step through a visual task as seen by the monkey along with concurrent sonification and visualization of activity from a population of recorded neurons. Data from two experimental paradigms illustrating different functional properties of neurons in the prefrontal cortex during attention and decisionmaking tasks are presented. The current system provides an accessible way to learn about how neural activity underlies cognitive functions and serves as a preliminary framework to explore both analytical and aesthetic dimensions of audiovisual representations of the data.","2010","2023-07-24 06:49:33","2023-07-24 06:49:33","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V7IQ24IA","journalArticle","2016","McGee, Ryan; Rogers, David E.","Musification of Seismic Data","","","","","","Seismic events are physical vibrations induced in the earth's crust which follow the general wave equation, making seismic data naturally conducive to audification. Simply increasing the playback rates of seismic recordings and rescaling the amplitude values to match those of digital audio samples (straight audification) can produce eerily realistic door slamming and explosion sounds. While others have produced a plethora of such audifications for international seismic events (i.e. earthquakes), the resulting sounds, while distinct to the trained auditory scientist, often lack enough variety to produce multiple instrumental timbres for the creation of engaging music for the public. This paper discusses approaches of sonification processing towards eventual musification of seismic data, beginning with straight audification and resulting in several musical compositions and new-media installations containing a variety of seismically derived timbres.","2016","2023-07-24 06:49:33","2023-07-24 06:49:33","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MRH5EDPN","journalArticle","1996","Back, Maribeth; Des, D.","Micro-narratives in sound design: Context, character, and caricature in waveform manipulation","","","","","","This paper reviews sound design techniques used in professional audio for music and theater and proposes a conceptual approach to the construction of audio based in narrative structure. The sound designer does not attempt to replicate ""real"" sounds; the task is rather to create the impression of a real sound in a listener's mind. In this attempt to create a sound in the listener's mind, the sound designer is aided by user expectations based upon cultural experience as well as physical experience. Practical sound manipulation techniques are discussed in view of their usefulness in matching a listener's mental model of a sound. Narrative aspects of audio design in computational environments are also delineated. Some keywords involved in this paper are sound design, auditory display, multimodal interaction, interface design, narrative, sonic narrative, micro-narrative, and audio.","1996","2023-07-24 06:49:33","2023-07-24 06:49:33","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PA8ZCJN6","journalArticle","2021","Frid, Emma; Orini, Michele; Martinelli, Giampaolo; Chew, Elaine","Mapping inter-cardiovascular time-frequency coherence to harmonic tension in sonification of ensemble interaction between a COVID-19 patient and the medical team","","","","","","This paper presents exploratory work on sonic and visual representations of heartbeats of a COVID-19 patient and a medical team. The aim of this work is to sonify heart signals to reflect how a medical team comes together during a COVID-19 treatment, i.e. to highlight other aspects of the COVID-19 pandemic than those usually portrayed through sonification, which often focuses on the number of cases. The proposed framework highlights synergies between sound and heart signals through mapping between timefrequency coherence (TFC) of heart signals and harmonic tension and dissonance in music. Results from a listening experiment suggested that the proposed mapping between TFC and harmonic tension was successful in terms of communicating low versus high coherence between heart signals, with an overall accuracy of 69%, which was significantly higher than chance. In the light of the performed work, we discuss how links between heart- and sound signals can be further explored through sonification to promote understanding of aspects related to cardiovascular health.","2021","2023-07-24 06:49:34","2023-07-24 06:49:34","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8P2SYCWD","journalArticle","2005","Cullen, Charlie; Coyle, Eugene","TrioSon: A graphical user interface for pattern sonification","","","","","","The TrioSon software allows users to map musical patterns to input data variables via a graphical user interface (GUI). The application is a Java routine designed to take input files of standard Comma Separated Values (CSV) format and output Standard Midi Files (SMF) using the internal Java Sound API. TrioSon renders output Sonifications from input data files for up to 3 user-defined parameters, allocated as bass, chord and melody instruments for the purposes of arrangement. In this manner each parameter concerned is distinguished by its individual instrumental timbre, with the option of rendering any combination of 1 to 3 parameters as required. The software parses indexed input data relating to individual variables for each user-defined parameter, and provides the means to allocate musical patterns to each variable for Sonification using drag and drop functionality. Control over the Rhythmic Parsing of the Sonification is provided, alongside individual control of the volume, panning, muting and timbre of each instrument in the trio. Sonifications can be rendered as full output files of the entire data, or can also be auditioned by index as required. This feature is designed to allow the user complete control over the data they are sonifying- either on an individual or collective basis. Context for each output Sonification is provided by Midi events defined by the index of the input data, which are mapped to percussive timbres in the final SMF (via track 10). Java development provides the added advantage of portability, with the final application being small enough (200kb) to attach in an email document. It is hoped that the compact and intuitive nature of the application will make it a straightforward means of investigating the Sonification of data sets.","2005","2023-07-24 06:49:34","2023-07-24 06:49:34","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"36BWG9M2","journalArticle","2012","Winton, Riley J.; Gable, Thomas M.; Schuett, Jonathan; Walker, Bruce N.","A sonification of Kepler space telescope star data","","","","","","A performing artist group interested in including a sonification of star data from NASA’s Kepler space telescope in their next album release approached the Georgia Tech Sonification Lab for assistance in the process. The artists had few constraints for the authors other than wanting the end product to be true to the data, and a musically appealing “heavenly” sound. Several sonifications of the data were created using various techniques, each resulting in a different sounding representation of the Kepler data. The details of this process are discussed in this poster. Ultimately, the researchers were able to produce the desired sounds via sound synthesis, and the artists plan to incorporate them into their next album release.","2012","2023-07-24 06:49:34","2023-07-24 06:49:34","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"36389KC3","journalArticle","2021","Falkenberg, Kjetil; Frid, Emma; Eriksson, Martin Ljungdahl; Otterbring, Tobias; Daunfeldt, Sven-Olov","Auditory notification of customer actions in a virtual retail environment: Sound design, awareness and attention","","","","","","In this paper, we introduce sonification as a less intrusive method for preventing shoplifting. Music and audible alerts are common in retail, and auditory monitoring of a store can aid clerks and reduce losses. Despite these potential advantages, sonification of interaction with goods in retail is an undeveloped field. We conducted an experiment focusing on peripheral auditory notifications in a virtual retail environment, evaluating aspects such as awareness and attention, sound design and noticeability, and localization of event sounds. Results highlighted behavioral differences depending on whether users were informed about the presence of auditory notification sounds or not. The alerts did not cause distraction or annoyance and we suggest that the findings give a promising starting point for future studies and investigations focused on improving the auditory environments in retail.","2021","2023-07-24 06:49:34","2023-07-24 06:49:34","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TZ5YS9AH","journalArticle","2014","Rouat, Jean; Lescal, Damien; Wood, Sean","Handheld Device for Substitution From Vision to Audition","","","","","","Sensorial substitution has great potential in rehabilitation, education, games, and in the creation of music and art. Current technologies allow us to develop sensorial substitution and sonification systems that would not have been imaginable two decades ago. It is desirable to let a large audience use and test sonification systems to provide feedback and improve their design. Handheld devices like smartphones or tablets include network connectivity (WIFI and/or Cellular radio) that can be used to transmit anonymous information about the configuration and strategies adopted by users. It is now feasible to obtain feedback from any user of substitution and sonification technology and not only from a limited number of subjects in the laboratory. Testing in the field with a large number of users is now possible thanks to telecommunication networks and machine learning tools to analyze big data. This work presents a handheld implementation of a simple video sonification system designed to test the acceptability of vision to audition substitution systems and in the near future to provide feedback from users. A first beta version was publicly released in November 2013 as an iOS application for large scale testing. The extended abstract introduces the interface and the underlying technology.","2014","2023-07-24 06:49:34","2023-07-24 06:49:34","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MT9RF9GQ","journalArticle","2013","Andreopoulou, Areti; Rogińska, Agnieszka; Mohanraj, Hariharan","A Database Of Repeated Head-Related Transfer Function Measurements","","","","","","This paper describes a new HRTF collection, measured at the Music and Audio Research Laboratory (MARL), at NYU. This collection of 40 datasets, consists of repeated HRTF measurements on 4 subjects (10 HRTF sets per subject). Analysis of the data offers an understanding of the expected degree of variation in HRTF sets, which, when supported by subjective evaluation, can provide deeper insight for perceptually detected differences between binaural filters. Such knowledge has applications in various research fields and related signal processing tasks, such as: binaural auditory displays, HRTF modeling, and HRTF interpolation.","2013","2023-07-24 06:49:34","2023-07-24 06:49:34","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NF4PRHZW","book","2010","Pun, Thierry; Deville, Benoit; Bologna, Guido","Sonification of Colour and Depth in a Mobility Aid for Blind People","","","","","","The See Color interface transforms a small portion of a colored video image into sound sources represented by spatialized musical instruments. Basically, the conversion of colors into sounds is achieved by quantization of the HSL color system. Our purpose is to provide visually impaired individuals with a capability of perception of the environment in real time. In this work the novelty is the simultaneous sonification of color and depth, depth being coded by sound rhythm. Our sonification model is illustrated by several experiments, such as: (1) detecting an open door in order to go out from the office; (2) walking in a hallway and looking for a blue cabinet; (3) walking in a hallway and looking for a red tee shirt; (4) moving outside and avoiding a parked car. Videos with sounds of experiments are available on http://www.youtube.com/guidobologna.","2010","2023-07-24 06:49:35","2023-07-24 06:49:35","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3G7KGUEI","journalArticle","2013","Wersényi, György; Répás, József","Practical Recommendations For Using Sound Transducers With Glass Memberane As Auditory Display Based On Measurements And Simulations","","","","","","Newly designed vibrating transducers can be used coupled directly to solid surfaces such as wood or glass as plane wave short-distance loudspeakers. Our former analysis evaluated the SolidDrive transducer glued on to glass surfaces of different sizes and forms. Further investigations concluded that these parameters do not influence the transmission and quality of the coupled system significantly. The second part of this investigation included only one size and geometry for further numerical simulations using COMSOL FEM, as well as for acoustic measurements of the transfer function at selected frequencies. Examination of different wall-fixing and mounting methods were also included. Concluding results show that the overall sound quality is inferior to regular loudspeakers, however, using a supplementary subwoofer results in enjoyable music transmission for a short distance. Furthermore, placement of the transducer in the middle of the membrane and having mounting points at the corners are recommended in practical applications","2013","2023-07-24 06:49:35","2023-07-24 06:49:35","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5ITCJX7C","journalArticle","2021","Morales, Esteban; James, Kedrick; Horst, Rachel; Takeda, Yuya; Yung, Effiam","The sound of our words: Singling, a textual sonification software","","","","","","As visualization struggles to grasp the intricate and temporal networks of meaning found in textual data, sonification emerges as a creative and effective way of representing language. Accordingly, this paper seeks to introduce Singling, a textual sonification software that allows users to create and manipulate auditory representations of a text's lexicogrammatical properties. To achieve this, we first present Singling's main features and interface. We then discuss an example of using this sonification software to explore—both analytically and aesthetically—three different poems. Overall, this paper seeks to introduce researchers, educators, and artists to the many possibilities of Singling and the practice of textual sonification, which includes data analysis, multimodal and collaborative narrative creation, and musical performance to name a few.","2021","2023-07-24 06:49:35","2023-07-24 06:49:35","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C9LQPIYA","journalArticle","2019","Shafer, Seth; Larson, Timothy; diFalco, Elaine","The sonification of solar harmonics (SOSH) project","","","","","","The Sun is a resonant cavity for very low frequency acoustic waves, and just like a musical instrument, it supports a number of oscillation modes, also commonly known as harmonics. We are able to observe these harmonics by looking at how the Sun's surface oscillates in response to them. Although this data has been studied scientifically for decades, it has only rarely been sonified. The Sonification of Solar Harmonics (SoSH) Project seeks to sonify data related to the field of helioseismology and distribute tools for others to do the same. Creative applications of this research by the authors include musical compositions, installation artwork, a short documentary, and a full-dome planetarium experience.","2019","2023-07-24 06:49:35","2023-07-24 06:49:35","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YFFTWM9I","book","2009","Gygi, Brian; Shafiro, Valeriy","From signal to substance and back: Insights from environmental sound research to auditory display design","","","","","","A persistent concern in the field of auditory display design has been how to effectively use environmental sounds, which are naturally occurring familiar non-speech, non-musical sounds. Environmental sounds represent physical events in the everyday world, and thus they have a semantic content that enables learning and recognition. However, unless used appropriately, their functions in auditory displays may cause problems. One of the main considerations in using environmental sounds as auditory icons is how to ensure the identifiability of the sound sources. The identifiability of an auditory icon depends on both the intrinsic acoustic properties of the sound it represents, and on the semantic fit of the sound to its context, i.e., whether the context is one in which the sound naturally occurs or would be unlikely to occur. Relatively recent research has yielded some insights into both of these factors. A second major consideration is how to use the source properties to represent events in the auditory display. This entails parameterizing the environmental sounds so the acoustics will both relate to source properties familiar to the user and convey meaningful new information to the user. Finally, particular considerations come into play when designing auditory displays for special populations, such as hearing impaired listeners who may not have access to all the acoustic information available to a normal hearing listener, or to elderly or other individuals whose cognitive resources may be diminished. Some guidelines for designing displays for these populations will be outlined.","2009","2023-07-24 06:49:35","2023-07-24 06:49:35","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CSZ7YDJT","book","2015","Parker, J. no e","Sonification as art: Developing praxis for the audification of compost","","","","","","This paper introduces compost as a rich site for creative exploration and expression via the medium of sonification art in the context of Composing [De]Composition, a large-scale audiovisual installation/performance work to be presented at University of California Riverside’s Culver Center for the Arts from June-October 2015. Here, the author non-reductively describes the multiagential and poly-temporal nature of compost through detailing the evolution of an artistic praxis involving: the observation, audification, and sonification of compost temperatures; the development new sensing methods for data-collection; and sound-mapping strategies. The main observable driving the project is incalescence—the heat generated by the composting process. Audification of this biological process brings a perceivably silent activity into the tangible reach of human hearing. The collection and real-time audification of temperature data using a custom interface to route sensor data to MAX/MSP enables listeners to better understand the complex ecology of a heterogeneous mass that is simultaneously decomposing, supporting a myriad of life forms while also enabling the bioavailability of macronutrients to the soil. In addition, the recontexualization of temporally-based temperature data into sound creates fertile ground for exploration in the compositional realm, as the collection of data over time depicts inherent patterns occurring in the systems analyzed, while the basis of music also builds upon the use of patterns (pitch based, rhythmic) through time. Sonification of these patterns enables the composer/sound artist to create compositions in partnership with her subject/phenomenon of study.","2015","2023-07-24 06:49:35","2023-07-24 06:49:35","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8INSLFYI","journalArticle","2007","Davison, Benjamin K.; Walker, Bruce N.","Sonification Sandbox Reconstruction: Software Standard for Auditory Graphs","","","","","","We report on an overhaul to the Sonification Sandbox. The Sonification Sandbox provides a cross-platform, flexible tool for converting tabular information into a descriptive auditory graph. It is implemented in Java, using the Java Sound API to generate MIDI output. An improved modular code structure provides a strong user interface and model framework for auditory graph representation and manipulation. A researcher can integrate part or the entire program into a different experimental implementation. The upgraded Sonification Sandbox provides a rich description of the auditory graph representation that can be saved or exported into various file formats. This description includes data representations of pitch, timbre, polarity, pan, and volume, along with graph contexts analogous to visual graph axes. Applications for the Sonification Sandbox include experimentation with various sonification techniques, data analytics beyond visualization, science education, auditory display for the blind, and musical interpretation of data.","2007","2023-07-24 06:49:35","2023-07-24 06:49:35","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9CWZXAXT","journalArticle","2000","Hankinson, John CK; Edwards, Alistair DN","Musical phrase-structured audio communication","","","","","","It has previously been shown that musical grammars can impose structural constraints upon the design of earcons, thereby providing a grammatical basis to earcon combinations. In this paper, more complex structural combinations are explored, based upon linguistic phrases. By mapping between a musical grammar and a linguistic grammar, musical phrases can be generated which correspond to linguistic sentences. A large number of unique meanings can be presented in this way based upon a simple musical vocabulary. This is of great value to auditory designers. A user study has been undertaken which reveals that users can recognize these complex auditory phrases after a small amount of training.","2000","2023-07-24 06:49:36","2023-07-24 06:49:36","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R2NAAH6M","journalArticle","2021","Elmquist, Elias; Ejdbo, Malin; Bock, Alexander; Rönnberg, Niklas","OpenSpace sonification: complementing visualization of the solar system with sound","","","","","","Data visualization software is commonly used to explore outer space in a planetarium environment, where the visuals of the software is typically accompanied with a narrator and supplementary background music. By letting sound take a bigger role in these kinds of presentations, a more informative and immersive experience can be achieved. The aim of the present study was to explore how sonification can be used as a complement to the visualization software OpenSpace to convey information about the Solar System, as well as increasing the perceived immersiveness for the audience in a planetarium environment. This was investigated by implementing a sonification that conveyed planetary properties, such as the size and orbital period of a planet, by mapping this data to sonification parameters. With a user-centered approach, the sonification was designed iteratively and evaluated in both an online and planetarium environment. The results of the evaluations show that the participants found the sonification informative and interesting, which suggest that sonification can be beneficially used as a complement to visualization in a planetarium environment.","2021","2023-07-24 06:49:36","2023-07-24 06:49:36","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MT9N38LI","book","2010","Barri, Tarik; Snook, Kelly","Versum: Data Sonification and Visualization in 3D","","","","","","Versum is an advanced, interactive 3D audiovisual composition environment which is augmented with a hardware and software front-end system that maps data into the environment for the purposes of exploratory scientific analysis. Originally intended as an audiovisual sequencer for real-time or automatable music and video performance, Versum also provides a unique environment for systematically investigating new data mappings for optimized human cognition of complex datasets.","2010","2023-07-24 06:49:36","2023-07-24 06:49:36","","","","","","","","","","","","","Georgia Institute of Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KMAE4N5F","journalArticle","2002","McGregor, I.; Crerar, A.; Benyon, D.; Macaulay, C.","Soundfields and soundscapes: Reifying auditory communities","","","","","","This paper reports progress towards mapping workplace soundscapes. In order to design auditory interfaces that integrate effectively with workplace environments, we need a detailed understanding of the way in which end users inhabit these environments, and in particular, how they interact with the existing auditory environment. Our work concentrates first on mapping the physical soundfield, then overlaying this with a representation of the soundscape as experienced by its active participants. The ultimate aim of this work is to develop an interactive soundscape-mapping tool, analogous to the modeling tools available to architects. Such a tool would be of use to designers of physical, augmented and virtual environments and usable without professional musical or acoustical expertise.","2002","2023-07-24 06:49:36","2023-07-24 06:49:36","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KKGQ8BJ3","journalArticle","2007","Palladino, Dianne K.; Walker, Bruce N.","Learning Rates for Auditory Menus Enhanced with Spearcons Versus Earcons","","","","","","Increasing the usability of menus on small electronic devices is essential due to their increasing proliferation and decreasing physical sizes in the marketplace. Auditory menus are being studied as an enhancement to the menus on these devices. This study compared the learning rates for earcons (hierarchical representations of menu locations using musical tones) and spearcons (compressed speech) as potential candidates for auditory menu enhancement. We found that spearcons outperformed earcons significantly in rate of learning. We also found evidence that spearcon comprehension was enhanced by a brief training cycle, and that participants considered the process of learning spearcons much easier than the same process using earcons. Since the efficiency of learning and the perceived ease of use of auditory menus will increase the likelihood they are embraced by those who need them, this paper presents compelling evidence that spearcons may be the superior choice for such applications.","2007","2023-07-24 06:49:36","2023-07-24 06:49:36","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DLGPJHHC","journalArticle","2001","Bonebright, Terri L.; Nees, Mike A.; Connerley, Tayla T.; McCain, Glenn R.","Testing the effectiveness of sonified graphs for education: A programmatic research project","","","","","","This programmatic research project builds on results from research on data sonification and from studies investigating comprehension of visual graphs. The purpose of the project is to explore the effectiveness of using sonified graphs of real data sets from disciplines to which students are exposed during academic courses. The primary question is whether sonified graphs can increase the comprehension of graphed data for students. The secondary question is whether stereo or monaural sonifications are most effective for graph comprehension. The third and final question of this project is whether sonified graphs with rhythm markers result in better comprehension than sonified graphs without them. The project consists of three laboratory experiments that explore whether students can match auditory representations with the correct visual graphs, whether they can comprehend graphed data sets more effectively by adding sonified components, and whether they can be trained to use sonified graphs better with practice. Results could provide new methods for teaching students with different learning styles quantitative skills in educational settings from kindergarten through college. They could also be extended to assist in teaching students with visual impairments about graphed data sets.","2001","2023-07-24 06:49:36","2023-07-24 06:49:36","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LYTA7GIS","journalArticle","2017","Broderick, James; Duggan, Jim; Redfern, Sam","Using Auditory Display Techniques to Enhance Decision Making And Perceive Changing Environmental Data Within a 3D Virtual Game Environment","","","","","","When it comes to understanding our environment, we use all our senses. Within the study and implementation of virtual environments and systems, huge advancements in the quality of visuals and graphics have been made, but when it comes to the audio in our environment, many people have been content with very basic sound information. Video games have strived towards powerful sound design, both for player immersion and information perception. Research exists showing how we can use audio sources and waypoints to navigate environments, and how we can perceive information from audio in our surroundings. This research explores using sonification of changing environmental data and environmental objects to improve user's perception of virtual spaces and navigation within simulated environments, with case studies looking at training and for remote operation of unmanned vehicles. This would also expand into how general awareness and perception of dynamic 3D environments can be improved. Our research is done using the Unity3D game engine to create a virtual environment, within which users navigate around water currents represented both visually and through sonification of their information using Csound, a C based programming language for sound and music creation.","2017","2023-07-24 06:49:37","2023-07-24 06:49:37","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V4TWQWIB","journalArticle","2011","Bujacz, Michal; Skulimowski, Piotr; Strumillo, Pawel","Sonification of 3D Scenes Using Personalized Spatial Audio to Aid Visually Impaired Persons","","","","","","The research presented concerns the development of a sonification algorithm for representation of 3D scenes for use in an electronic travel aid (ETA) for visually impaired persons. The proposed sonar-like algorithm utilizes segmented 3D scene images, personalized spatial audio and musical sound patterns. The use of segmented and parametrically described 3D scenes allowed to overcome the large sensory mismatch between visual and auditory perception. Utilization of individually measured head related transfer functions (HRTFs), enabled the application of illusions of virtual sound sources. The selection of sounds used was based on surveys with blind volunteers. A number of sonification schemes, dubbed sound codes, were proposed, assigning sound parameters to segmented object parameters. The sonification algorithm was tested in virtual reality using software simulation along with studies of virtual sound source localization accuracy. Afterwards, trials in controlled real environments were made using a portable ETA prototype, with participation of both blind and sighted volunteers. Successful trials demonstrated that it is possible to quickly learn and efficiently use the proposed sonification algorithm to aid spatial orientation and obstacle avoidance.","2011","2023-07-24 06:49:37","2023-07-24 06:49:37","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BR828YLT","journalArticle","2018","Quinton, Michael; McGregor, Iain; Benyon, David","Investigating effective methods of designing sonifications","","","","","","This study aims to provide an insight into effective sonification design. There are currently no standardized design methods, allowing a creative development approach. Sonifcation has been implemented in many different applications from scientific data representation to novel styles of musical expression. This means that methods of practice can vary a greatly. The indistinct line between art and science might be the reason why sonification is still sometimes deemed by scientists with a degree of scepticism. Some well established practitioners argue that it is poor design that renders sonifications meaningless, in-turn having an adverse effect on acceptance. To gain a deeper understanding about sonification research and development 11 practitioners were interviewed. They were asked about methods of sonification design and their insights. The findings present information about sonification research and development, and a variety of views regarding sonification design practice.","2018","2023-07-24 06:49:37","2023-07-24 06:49:37","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QRX6K3YJ","journalArticle","2007","Reissel, L. M.; Pai, Dinesh K.","High-Resolution Analysis and Resynthesis of Environmental Impact Sounds","","","","","","Impact sounds produced by everyday objects are an important source of information about contact interactions in virtual environments and auditory displays. Impact signals also provide a rich class of real and synthetic percussive musical sounds. However, their perceptually acceptable resynthesis and modification requires accurate estimation of mode parameters, which has proved difficult using traditional methods. In this paper we describe some of the problems posed by impact phenomena when applying standard methods, and present a phase-constrained high-resolution algorithm which allows more accurate estimation of modes and amplitudes for impact signals. The phase-constrained algorithm is based on least squares estimation, with initial estimates obtained from a modified ESPRIT algorithm, and it produces better resynthesis results than previously used methods. We give examples with everyday object impact sounds.","2007","2023-07-24 06:49:37","2023-07-24 06:49:37","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RKDTZA9V","journalArticle","2018","Lenzi, Sara; Gleria, Francesca","Humanising data through sound: Res Extensae and a user-centric approach to data sonification","","","","","","In this paper, starting from a case study (the mixed-media data sonification installation Res Extensae), we discuss a number of assumptions on the efficacy of sound as a means to represent and communicate numerical data. The discussion is supported by the results of a questionnaire aimed at validating our assumptions and conducted with fifteen of the participants to the experience. At the same time, we have the ambition to contribute to a wider debate on the value of data sonification. We introduce the first stage of a research on sonification as a design-driven, user-centred and multi-modal experience, in that closer to data design practices rather than to traditional composition and computer music. We describe the usage of physical objects to help users to put sounds and data into a wider context, improving the user experience and facilitating the comprehension and retention of the meaning of data.","2018","2023-07-24 06:49:37","2023-07-24 06:49:37","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VYM7M5GP","journalArticle","2016","Dyer, John; Stapleton, Paul; Rodger, Matthew","Sonification of Movement for Motor Skill Learning in a Novel Bimanual Task: Aesthetics and Retention Strategies","","","","","","Here we report early results from an experiment designed to investigate the use of sonification for the learning of a novel perceptual-motor skill. We find that sonification which employs melody is more effective than a strategy which provides only bare timing information. We additionally show that it might be possible to �refresh' learning after performance has waned following training - through passive listening to the sound that would be produced by perfect performance. Implications of these findings are discussed in terms of general motor performance enhancement and sonic feedback design.","2016","2023-07-24 06:49:37","2023-07-24 06:49:37","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W3XU2FXY","journalArticle","2006","Berman, L. I.; Gallagher, K. B.","Listening to program slices","","","","","","Comprehending a computer program can be a daunting task. There is much to understand, including the interaction among different portions of the code. Program slicing can help one to understand this interaction. Because present-day visual development environments tend to become cluttered, the authors have explored sonification of program slices in an attempt to determine if it is practical to offload some of the visual information. Three slice sonification techniques were developed, resulting in an understanding of how to sonify slices in a manner appropriate for the software developer undertaking program comprehension activities. The investigation has also produced a better understanding of sonification techniques that are musical yet non-melodic and non-harmonic. These techniques were demonstrated to a small set of developers, each reporting that the techniques are promising and useful.","2006","2023-07-24 06:49:37","2023-07-24 06:49:37","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"327LPMVC","journalArticle","2021","El Hajj, Tracey","Network sonification and the algorhythmics of everyday life","","","","","","Today, public concern with the extent to which they influence people's routines, and how much they affect cultures and societies, has grown substantially. This paper argues that, by listening to networks, people can begin to apprehend, and even comprehend, the complex, ostensibly ""magical"" nature of network communications. One problem is that listening semantically to networks is incredibly difficult, if not impossible. Networks are very noisy, and they do not, for instance, use alphabetic language for internal or external communication. For the purpose of interpreting networks, I propose ""tactical network sonification"" (TNS), a technique that focuses on making the materiality of networks sensibly accessible to the general public, especially people who are not technology experts. Using an electromagnetic transduction device — Shintaro Miyazaki and Martin Howse's Detektor — TNS results in crowded sound clips that represent the complexity of network infrastructure, through the many overlapping rhythms and layers of sound that each clip contains.","2021","2023-07-24 06:49:38","2023-07-24 06:49:38","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V6XF7JCN","journalArticle","2018","Aldana Blanco, Andrea Lorena; Grautoff, Steffen; Hermann, Thomas","CardioSounds: A portable system to sonify ECG rhythm disturbances in real-time","","","","","","CardioSounds is a portable system that allows users to measure and sonify their electrocardiogram signal in real-time. The ECG signal is acquired using the hardware platform BITalino and subsequently analyzed and sonified using a Raspberry Pi. Users can control basic features from the system (start recording, stop recording) using their smartphone. The system is meant to be used for diagnostic and monitoring of cardiac pathologies, providing users with the possibility to monitor a signal without occupying their visual attention. In this paper, we introduce a novel method, anticipatory mapping, to sonify rhythm disturbances such as Atrial Fibrillation, Atrial flutter and Ventricular Fibrillation. Anticipatory mapping enhances perception of rhythmic details without disrupting the direct perception of the actual heart beat rhythm. We test the method on selected pathological data involving three of the most known rhythm disturbances. A preliminary perception test to assess aesthetics of the sonifications and its possible use in medical scenarios shows that the anticipatory mapping method is regarded as informative discerning healthy and pathological states, however there is no agreement about a preferred sonification type.","2018","2023-07-24 06:49:38","2023-07-24 06:49:38","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F3WLSWIU","journalArticle","2004","Hermann, T.; Baier, G.","The sonification of rhythms in human electroencephalogram","","","","","","We use sonification of temporal information extracted from scalp EEG to characterize the dynamic properties of rhythms in certain frequency bands. Sonification proves particularly useful in the simultaneous monitoring of several EEG channels. Our results suggest sonification as an important tool in the analysis of multivariate data with subtle correlation differences.","2004","2023-07-24 06:49:38","2023-07-24 06:49:38","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""