"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"NAJ7H4VQ","conferencePaper","2021","Paroiu, Razvan; Trăuşan-Matu, Ştefan","A new approach for chat sonification","2021 23rd International Conference on Control Systems and Computer Science (CSCS)","","","10.1109/CSCS52396.2021.00080","","This paper presents a new approach of chat sonification based on a deep neural network for music generation. The advantage of chat sonification is that the feeling of something artificial in the generated music is less present than for other artificial intelligence approaches. The new method of sonification introduced in the paper is based on the polyphonic model theory, similarly to the idea used in the MusicXML Creator Platform, the novelty being that it uses sequence-to-sequence neural networks for an improved pitch generation. The duration generation algorithm remains the same as it was implemented in the MusicXML Creator. The results are evaluated by multiple participants","2021-05","2023-07-06 01:40:30","2023-07-06 01:40:30","","453-456","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2379-0482","","/Users/minsik/Zotero/storage/7ASDPA6Y/Paroiu and Trăuşan-Matu - 2021 - A new approach for chat sonification.pdf","","","Sonification; Music; sonification; artificial intelligence; Computational modeling; computer music generation; Computer science; Correlation; Deep learning; Neural networks; polyphonic model; recurrent neural networks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2021 23rd International Conference on Control Systems and Computer Science (CSCS)","","","","","","","","","","","","","","",""
"UJ43WM24","conferencePaper","2016","Hananoi, Shunsuke; Muraoka, Kazuki; Kiyoki, Yasushi","A music composition system with time-series data for sound design in next-generation sonification environment","2016 International Electronics Symposium (IES)","978-1-5090-1640-2","","10.1109/ELECSYM.2016.7861035","http://ieeexplore.ieee.org/document/7861035/","This research suggests an idea of synthesizing time series data and human sense. This software can help human to know environmental invisible changes that human cannot detect. Player can use time series data for music composition. We have already had a way of sonification as relating research. However, this research aims having more flexibility in composition. For the motivation, we set player flexibility as tonality editing and data conversion range control system. In our software, player can edit melody and its range of sampling. For example, if composer want to express data as bright atmosphere music, you can edit some settings in the tool and play it. We have tried to use this software to use foreign exchange data. In future work, we try to use this software in virtual reality. And we will test the listener recognition change.","2016-09","2023-07-06 01:41:55","2023-07-06 01:41:55","2023-07-06 01:41:55","380-384","","","","","","","","","","","IEEE","Denpasar, Indonesia","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/DENELJWL/Hananoi et al. - 2016 - A music composition system with time-series data f.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2016 International Electronics Symposium (IES)","","","","","","","","","","","","","","",""
"B9G4E9T8","conferencePaper","2021","Meytin, Sophia","A Novel Method for Protein-Protein Interface Analysis Using Sonification","2021 IEEE MIT Undergraduate Research Technology Conference (URTC)","978-1-66540-595-9","","10.1109/URTC54388.2021.9701622","https://ieeexplore.ieee.org/document/9701622/","Auditory inspection reduces analytical subjectivity through concrete musical parameters. Here, it is applied to oligomerically varied protein-protein interfaces (PPIs) in a harmony-based sonification method. Amino acids in the PPI bonded contacts for homo- and hetero- di/tri/tetramers were grouped using four qualitative systems. PPI hydrogen bonds and salt bridges were examined to determine groups’ bonded interaction frequencies, compared using the chi-squared test of independence. Highly significant p-values (<<0.05) for homo- and hetero-oligomers in all systems’ oligomeric states strongly suggested that bonded interactions’ frequencies differ. These frequencies were successfully sonified using variations in consonance, waveform, chord structure, and pitch.","2021-10-08","2023-07-06 01:41:59","2023-07-06 01:41:59","2023-07-06 01:41:59","1-5","","","","","","","","","","","IEEE","Cambridge, MA, USA","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/FYHTIR52/Meytin - 2021 - A Novel Method for Protein-Protein Interface Analy.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2021 IEEE MIT Undergraduate Research Technology Conference ((URTC))","","","","","","","","","","","","","","",""
"Z8AAWSN8","conferencePaper","2015","Huang, Chih-Fang; Nien, Wei-Po","A sonification system based on geographic and meteorologic data","2015 8th International Conference on Ubi-Media Computing (UMEDIA)","978-1-4673-8270-0","","10.1109/UMEDIA.2015.7297465","https://ieeexplore.ieee.org/document/7297465/","Most algorithmic composition system is implemented based on the input of music parameters, which needs professional music and technology training to complete the automated composition. This paper proposed an innovated way called Geographic and Meteorologic Data Sonification System (GMDSS), to perform the data mappings into music for various devices such as hand phone or other hand-hold devices, and the driver can retrieve the environment information in real time to let people concentrate on driving without the need of paying attention to the long-term complicated data visualization. The mapping relationship between geographic/meteorologic data and music features is discussed, and the result shows the implementation of GIS and weather data which can compose the proper correspondent music accordingly.","2015-08","2023-07-06 01:42:01","2023-07-06 01:42:01","2023-07-06 01:42:01","259-262","","","","","","","","","","","IEEE","Colombo","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/W3AAWVUA/Huang and Nien - 2015 - A sonification system based on geographic and mete.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2015 8th International Conference on Ubi-Media Computing (UMEDIA)","","","","","","","","","","","","","","",""
"IATE9PYG","conferencePaper","2008","Xiaoying Wu; Ze-Nian Li","A study of image-based music composition","2008 IEEE International Conference on Multimedia and Expo","978-1-4244-2570-9","","10.1109/ICME.2008.4607692","http://ieeexplore.ieee.org/document/4607692/","visual and auditory forms have some noticeable associations that can inspire similar cognitive and aesthetical experiences. this paper presents a study on the possibilities of applying low level visual auditory associations to music generation. a few methods are implemented to directly convert visual features extracted from images into musical elements (pitch, duration, and chord). some initial results suggest a high potential of composing interesting music along this direction. possible applications of this study include getting new ideas for music composers and generating accompanying music in various contexts.","2008-06","2023-07-06 01:42:04","2023-07-21 07:48:13","2023-07-06 01:42:04","1345-1348","","","","","","","","","","","IEEE","Hannover, Germany","","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/FZI4BDTN/Xiaoying Wu and Ze-Nian Li - 2008 - A study of image-based music composition.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2008 IEEE International Conference on Multimedia and Expo (ICME)","","","","","","","","","","","","","","",""
"JMBCYD58","conferencePaper","2020","Hall, Lawton","Leander: Navigating Musical Possibility Space Through Color Data Sonification","2020 IEEE VIS Arts Program (VISAP)","978-1-72818-553-8","","10.1109/VISAP51628.2020.00011","https://ieeexplore.ieee.org/document/9307993/","HDQGHU is an experimental film that sonifies color data to generate its musical soundtrack. The colors of Lake Michigan, captured in time lapse video, constitute ever-changing probability vectors that govern the behavior of musical sound-events over time. This VWRFKDVWLF, or probabilistic approach to data sonification imagines the musical experience as movement through a virtual possibility space, rather than the end result of a causal process. This pictorial describes how color data guides the various musical parameters at play in /HDQGHU through weighted chance.","2020-10","2023-07-06 01:42:05","2023-07-06 01:42:05","2023-07-06 01:42:05","45-60","","","","","","Leander","","","","","IEEE","Salt Lake City, UT, USA","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/BLG9SP57/Hall - 2020 - Leander Navigating Musical Possibility Space Thro.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020 IEEE VIS Arts Program (VISAP)","","","","","","","","","","","","","","",""
"U26VNG37","conferencePaper","2018","Morawitz, Falk","Quantum: An art-science case study on sonification and sound design in virtual reality","2018 IEEE 4th VR Workshop on Sonic Interactions for Virtual Environments (SIVE)","978-1-5386-5713-3","","10.1109/SIVE.2018.8577080","https://ieeexplore.ieee.org/document/8577080/","Molecular sonification is the transformation of chemical data into sound and has been used to gain insight into chemical systems and for the creation of contemporary music compositions. The combination of sonification with a virtual reality environment offers potential benefits such as providing a visual frame of reference, an increased sense of immersion, nuanced spatial information through binaural audio cues and ease of interactivity. To explore how strategies developed in sonification research and contemporary electroacoustic music composition can be adapted to virtual reality, the art-science installation ’Quantum’ was created. The multi-media work consists of computer-generated molecules in a virtual space producing sound created via the sonification of nuclear magnetic resonance data. Upon user interaction with different molecules, the overall composition and complexity of the sound world develop. The binaural sound material can migrate back and forth from the molecules to the non-binaural background composition and, depending on user input, develop in terms of timbre, spectral complexity, and gestural content. ‘Quantum’ is an exploration of the combination of sonification and virtual reality and offers first points of discussion that can be elaborated upon in future artworks, games or educational content.","2018-03","2023-07-06 01:42:08","2023-07-06 01:42:08","2023-07-06 01:42:08","1-5","","","","","","Quantum","","","","","IEEE","Reutlingen","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/9Z4MDJXN/Morawitz - 2018 - Quantum An art-science case study on sonification.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2018 IEEE 4th VR Workshop on Sonic Interactions for Virtual Environments (SIVE)","","","","","","","","","","","","","","",""
"W6P6G2Y3","conferencePaper","2020","Zahray, Lisa; Savery, Richard; Syrkett, Liana; Weinberg, Gil","Robot Gesture Sonification to Enhance Awareness of Robot Status and Enjoyment of Interaction","2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","978-1-72816-075-7","","10.1109/RO-MAN47096.2020.9223452","https://ieeexplore.ieee.org/document/9223452/","We present a divergent approach to robotic soniﬁcation with the goal of improving the quality and safety of human-robot interactions. Soniﬁcation (turning data into sound) has been underutilized in robotics, and has broad potential to convey robotic movement and intentions to users without requiring visual engagement. We design and evaluate six different soniﬁcations of movements for a robot with four degrees of freedom. Our soniﬁcation techniques include a direct mapping from each degree of freedom to pitch and timbre changes, emotion-based sound mappings, and velocity-based mappings using different types of sounds such as motors and music. We evaluate these soniﬁcations using metrics for ease of use, enjoyment/appeal, and conveyance of movement information. Based on our results, we make recommendations to inform decisions for future robot soniﬁcation design. We suggest that when using soniﬁcation to improve safety of human-robot collaboration, it is necessary not only to convey sufﬁcient information about movements, but also to convey that information in a pleasing and even social way to to enhance the human-robot relationship.","2020-08","2023-07-06 01:42:11","2023-07-06 01:42:11","2023-07-06 01:42:11","978-985","","","","","","","","","","","IEEE","Naples, Italy","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/FL22B9UN/Zahray et al. - 2020 - Robot Gesture Sonification to Enhance Awareness of.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","","","","","","","","","","","","","","",""
"TSFSV77Y","conferencePaper","2013","Sanchez, A.; Valderrama, M.","Sonification of EEG signals based on musical structures","2013 Pan American Health Care Exchanges (PAHCE)","978-1-4673-6257-3 978-1-4673-6254-2 978-1-4673-6256-6","","10.1109/PAHCE.2013.6568291","http://ieeexplore.ieee.org/document/6568291/","This short communication proposes a new method to translate human EEG recordings into music. The sonification method is based on the relation of primary concepts of musical composition to different time-frequency characteristics of electrical signals from the brain that give information about the mental states. In general terms, normalization and thresholding procedures were applied to wavelet transforms of analyzed signals in order to extract amplitude and frequency parameters that were mapped to several synthetic or natural sounds following basic musical composition structures. The method seeks to provide a new perspective about the brain activity by means of an auditory feedback which not only facilitates the long term monitoring in clinical contexts like polysomnographics studies or epilepsy but also offers a new tool for a self-understating of the different human body processes modulated by the brain.","2013-04","2023-07-06 01:42:13","2023-07-21 07:48:31","2023-07-06 01:42:13","1-1","","","","","","","","","","","IEEE","Medellin, Colombia","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/H2TP6URR/Sanchez and Valderrama - 2013 - Sonification of EEG signals based on musical struc.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2013 Pan American Health Care Exchanges (PAHCE)","","","","","","","","","","","","","","",""
"2JHPCUUE","journalArticle","2019","Colombo, R.; Raglio, A.; Panigazzi, M.; Mazzone, A.; Bazzini, G.; Imarisio, C.; Molteni, D.; Caltagirone, C.; Imbriani, M.","The SonicHand Protocol for Rehabilitation of Hand Motor Function: A Validation and Feasibility Study","IEEE Transactions on Neural Systems and Rehabilitation Engineering","","1534-4320, 1558-0210","10.1109/TNSRE.2019.2905076","https://ieeexplore.ieee.org/document/8667371/","Musical sonification therapy is a new technique that can reinforce conventional rehabilitation treatments by increasing therapy intensity and engagement through challenging and motivating exercises. The aim of this paper is to evaluate the feasibility and validity of the SonicHand protocol, a new training and assessment method for the rehabilitation of hand function. The study was conducted in 15 healthy individuals and 15 stroke patients. The feasibility of implementation of the training protocol was tested in stroke patients only, who practiced a series of exercises concurrently to music sequences produced by specific movements. The assessment protocol evaluated hand motor performance during pronation/supination, wrist horizontal flexion/extension, and hand grasp without sonification. From hand position data, 15 quantitative parameters were computed evaluating mean velocity, movement smoothness, and angular excursions of hand/fingers. We validated this assessment in terms of its ability to discriminate between patients and healthy subjects, test-retest reliability and concurrent validity with the upper limb section of the Fugl-Meyer scale (FM), the functional independence measure (FIM), and the Box and Block Test (BBT). All patients showed a good understanding of the assigned tasks and were able to correctly execute the proposed training protocol, confirming its feasibility. A moderateto-excellent intraclass correlation coefficient was found in 8/15 computed parameters. The moderate-to-strong correlation was found between the measured parameters and the clinical scales. The SonicHand training protocol is feasible and the assessment protocol showed good to excellent between-group discrimination ability, reliability, and concurrent validity, thus enabling the implementation of new personalized and motivating training programs employing sonification for the rehabilitation of hand function.","2019-04","2023-07-06 01:43:57","2023-07-21 07:48:56","2023-07-06 01:43:57","664-672","","4","27","","IEEE Trans. Neural Syst. Rehabil. Eng.","The SonicHand Protocol for Rehabilitation of Hand Motor Function","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/KS9WMCK8/Colombo et al. - 2019 - The SonicHand Protocol for Rehabilitation of Hand .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FYHKP2A8","conferencePaper","2011","Wu, Dan; Shi, Xun; Hu, Jun; Lei, Xu; Liu, Tiejun; Yao, De-Zhong","Listen to the song of the brain in real time: The Chengdu Brainwave Music","2011 8th International Symposium on Noninvasive Functional Source Imaging of the Brain and Heart and the 2011 8th International Conference on Bioelectromagnetism","","","10.1109/NFSI.2011.5936836","","Electroencephalogram (EEG) provides a window for the activity of the human brain. In this work, we propose a brainwave music display system in real time - the Chengdu Brainwave Music (CBM), which translates the event, amplitude and average power of EEG into musical parameters, and plays the generated music immediately. Real EEG data is utilized as examples for the system. The system can be a useful tool for EEG monitoring, neuro-feecback, and entertainment.","2011-05","2023-07-06 01:47:50","2023-07-06 01:47:50","","135-138","","","","","","Listen to the song of the brain in real time","","","","","","","","","","","","IEEE Xplore","","","","","","","Music; sonification; Brain; EEG; Electroencephalography; Humans; Monitoring; Multiple signal classification; music; real time system; Real time systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2011 8th International Symposium on Noninvasive Functional Source Imaging of the Brain and Heart and the 2011 8th International Conference on Bioelectromagnetism","","","","","","","","","","","","","","",""
"TT5RWEVQ","conferencePaper","2012","Dailly, Anabel Immoos; Sigrist, Roland; Kim, Yeongmi; Wolf, Peter; Erckens, Hendrik; Cerny, Joachim; Luft, Andreas; Gassert, Roger; Sulzer, James","Can simple error sonification in combination with music help improve accuracy in upper limb movements?","2012 4th IEEE RAS & EMBS International Conference on Biomedical Robotics and Biomechatronics (BioRob)","","","10.1109/BioRob.2012.6290908","","While repetitive training is widely regarded to be a useful rehabilitation strategy, such training requires motivation that may be lacking. In order to improve motivation in a potentially inexpensive and simple manner, we introduce in this proof-of-concept study a combination of error sonification and music for upper limb training. Twelve healthy participants trained a figure tracing task for the upper limb, six receiving feedback in terms of error sonification and music and six without receiving feedback in the control group. The error-sonified feedback group decreased its amount of error significantly compared to the control group. Thus this particular paradigm can help teach planar reaching movements. Eventually this paradigm may become simple and useful enough to enhance existing therapeutic intervention in stroke rehabilitation.","2012-06","2023-07-06 01:47:57","2023-07-06 01:47:57","","1423-1427","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2155-1782","","","","","Training; Visualization; Biological control systems; Current measurement; Measurement uncertainty; Medical treatment; Trajectory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2012 4th IEEE RAS & EMBS International Conference on Biomedical Robotics and Biomechatronics (BioRob)","","","","","","","","","","","","","","",""
"A4PLRBI3","conferencePaper","1996","Jameson, D.H.","Building real-time music tools visually with Sonnet","Proceedings Real-Time Technology and Applications","","","10.1109/RTTAS.1996.509518","","We are building a variety of interactive music tools using Sonnet, a visual programming language in use at our center. Originally designed for sonification experiments for monitoring and debugging programs, Sonnet has grown into a more general system with a focus on real-time event-driven applications. In this paper, we describe some of the features of Sonnet followed by some examples of how it is being used.","1996-06","2023-07-06 01:48:04","2023-07-06 01:48:04","","11-18","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","","","","Music; Multiple signal classification; Real time systems; Application software; Buildings; Circuits; Computer languages; Computerized monitoring; Debugging; System testing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings Real-Time Technology and Applications","","","","","","","","","","","","","","",""
"U43V7ZTS","conferencePaper","2008","O'Neill, Charles; Ng, Kia","Hearing Images: Interactive Sonification Interface for Images","2008 International Conference on Automated Solutions for Cross Media Content and Multi-Channel Distribution","","","10.1109/AXMEDIS.2008.42","","This paper describes research into methods for interactive sonification of 2D image data. The method utilizes an existing device (from computer game), Nintendopsilas wiimote controller to provide a means of interaction with 2D images for the exploration and triggering of sonic response. This paper details a selection of sonification algorithms which have been designed to provide auditory display of image structure through sonification of parameters which signify the users exploration within a segmented image environment.","2008-11","2023-07-06 01:48:14","2023-07-06 01:48:14","","25-31","","","","","","Hearing Images","","","","","","","","","","","","IEEE Xplore","","","","","","","sonification; Data analysis; Algorithm design and analysis; artistic; Auditory displays; Auditory system; Computer interfaces; Distributed computing; Feedback; Haptic interfaces; Image segmentation; interactive; Jitter; sensory; visual; wii","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2008 International Conference on Automated Solutions for Cross Media Content and Multi-Channel Distribution","","","","","","","","","","","","","","",""
"F8VKQ7ZM","conferencePaper","2018","Freitas, Tiago Abreu; Miranda, Hudson; Rabelo, Cassiano; Jorio, Ado","Instrumentation and Algorithms for the Sonification of Scanning Probe Microscopy Output","2018 3rd International Symposium on Instrumentation Systems, Circuits and Transducers (INSCIT)","","","10.1109/INSCIT.2018.8546705","","The present work reports on the development of scientific instrumentation in scanning probe microscopy (SPM), a field of wide application and versatility that allows exploring surface properties at the atomic and molecular levels. Usually, SPM systems deliver information to the user with images that represent the surface properties. Here we implemented a realtime data sonification system that enables visually impaired people to perform SPM experiments and provides other information that is not nicely captured by the human eye. Based on a Raspberry Pi 3B + board and Python algorithms, our system generates parameters based on an SPM scan for sonification that is tested here using the MIDI (Musical Instrument Digital Interface).","2018-08","2023-07-06 01:48:21","2023-07-06 01:48:21","","1-6","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","","","","Music; sonification; auditory feedback; Calibration; data sonification; microscopy; Probes; scanning probe microscopy; Surface morphology; Vibrations","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2018 3rd International Symposium on Instrumentation Systems, Circuits and Transducers (INSCIT)","","","","","","","","","","","","","","",""
"NJPT4UKQ","conferencePaper","2019","Giariskanis, Fotis; Parthenios, Panagiotis; Mania, Katerina","ARCHIMUSIC3D: Multimodal Playful Transformations between Music and Refined Urban Architectural Design","2019 11th International Conference on Virtual Worlds and Games for Serious Applications (VS-Games)","","","10.1109/VS-Games.2019.8864525","","The commonly used 3D architectural design tools for urban environments fail to capture aspects of an urban design which are aesthetic as well as functional. This paper describes an innovative multimodal user interface through which an urban designer can work on the music transcription of a specific urban environment applying music compositional rules and filters in order to identify discordant entities, highlight imbalanced parts and make design corrections. The proposed platform offers sonification of an Urban Virtual Environment (UVE), simulating a real-world cityscape, offering visual interpretation and musically playful modification of its soundscape. The system presented offers: The ability to view and convert an urban street to music (ready to play) based on a specific grammar of converting architectural elements to musical elements; secondly, the ability to transform this music in order to `harmonize' it based on musical rules as if composing and playing music; and finally, the prospect of converting back the aesthetically and harmonically “corrected” musical piece to a newly refined street or urban design, visualized in 3D. The presented platform comprises of three scenes, which compile the three main parts of the system's multimodal interface; e.g., the 3D scene, the Digital Audio Workstation (DAW) scene and the TouchOSC mobile controller. The purpose of this paper is to assist architects and urban designers in 1) identifying urban dissonances, 2) refining their design using musical rules and 3) interactively presenting the output both visually and acoustically.","2019-09","2023-07-06 01:48:26","2023-07-06 01:48:26","","1-4","","","","","","ARCHIMUSIC3D","","","","","","","","","","","","IEEE Xplore","","ISSN: 2474-0489","","","","","Music; Visualization; music; Buildings; 3D graphics; Grammar; Instruments; multimodal; Three-dimensional displays; Urban areas; urban design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2019 11th International Conference on Virtual Worlds and Games for Serious Applications (VS-Games)","","","","","","","","","","","","","","",""
"GM44B97K","conferencePaper","2011","Stewart, Rebecca; Sandler, Mark","The amblr: A mobile spatial audio music browser","2011 IEEE International Conference on Multimedia and Expo","","","10.1109/ICME.2011.6012203","","Music collections are often visualized in a two-dimensional space to show relationships between songs. Some user inter faces interacting with these two-dimensional maps of songs use spatial auditory display to allow easier access to the au dio content. A common auditory display is to have multiple songs playing simultaneously from differing spatial locations around the user. However, when using this style of interface the sonification of the collection needs to be limited to a lo cal subset of the collection, usually three to six songs. This paper presents the amblr, a spatial audio music browser that allows a user to auralize the collection. It combines effective design from previous work with new approaches to create a novel interface. This allows for a more intuitive navigation of a virtual space populated by a large collection songs without relying on textual metadata.","2011-07","2023-07-06 01:48:32","2023-07-06 01:48:32","","1-6","","","","","","The amblr","","","","","","","","","","","","IEEE Xplore","","ISSN: 1945-788X","","","","","Visualization; Auditory displays; auditory display; binaural; Graphical user interfaces; Mobile handsets; music information retrieval; Presses; Pressing; Servers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2011 IEEE International Conference on Multimedia and Expo","","","","","","","","","","","","","","",""
"7B3NYMAI","conferencePaper","2000","Van Scoy, F.L.","Sonification of remote sensing data: initial experiment","2000 IEEE Conference on Information Visualization. An International Conference on Computer Visualization and Graphics","","","10.1109/IV.2000.859796","","The author is generating music from a particular view of a multi-dimensional geographic information system (GIS) data set to alert a viewer to the existence of hidden clusters of data points. The author describes some results from an early experiment with generating music from a 2D slice of the data and testing whether these representations are easily understood by beginning users.","2000-07","2023-07-06 01:48:39","2023-07-06 01:48:39","","453-460","","","","","","Sonification of remote sensing data","","","","","","","","","","","","IEEE Xplore","","ISSN: 1093-9547","","","","","Music; Data visualization; Ear; Fingers; Frequency; Geographic Information Systems; Nose; Remote sensing; Testing; Virtual environment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2000 IEEE Conference on Information Visualization. An International Conference on Computer Visualization and Graphics","","","","","","","","","","","","","","",""
"IS3BJPEB","journalArticle","2021","Fernandes, Carlos M.; Migotina, Daria; Rosa, Agostinho C.","Brain's Night Symphony (BraiNSy): A Methodology for EEG Sonification","IEEE Transactions on Affective Computing","","1949-3045","10.1109/TAFFC.2018.2850008","","This paper describes a method for converting sleep Electroencephalogram (EEG) signals into music. For that purpose, a new segmentation procedure is used for extracting relevant information from the sleep EEG that is then translated into sequences of notes, chords, arpeggios and pauses, with a varying tempo that is defined by sleep stages. The final outcome is a direct time-domain conversion of the brain activity during sleep into sound. Since typical sleep EEGs vary with age and sleep disorders, different groups of subjects were used in the experiments: babies, sane adults and patients with air-flow limitation.","2021-01","2023-07-06 01:48:47","2023-07-06 01:48:47","","103-112","","1","12","","","Brain's Night Symphony (BraiNSy)","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Affective Computing","","","","","Sonification; Music; Electroencephalography; Multiple signal classification; Feature extraction; Frequency modulation; generative art; signal processing; Sleep; sleep EEG","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q5BF7RE4","conferencePaper","2017","O'Sullivan, Mark; Srbinovski, Bruno; Temko, Andriy; Popovici, Emanuel; McCarthy, Hugh","V2Hz: Music composition from wind turbine energy using a finite-state machine","2017 28th Irish Signals and Systems Conference (ISSC)","","","10.1109/ISSC.2017.7983637","","The study presents a multi-disciplinary application of the Internet of Things (IoT) benefiting both the engineering and music community. A music composition algorithm based on a finite-state machine was designed to receive and manipulate wind turbine voltage output data into a musically aesthetic composition. The algorithm adapts common western music theory and imposes these limitations on the wind turbine output voltage stream. The front-end of the system employs various transmission protocols. The data is streamed to a server on-site at the wind farm using transmission control protocol, and then received offsite via TCP/IP. The back-end of the system processes the digital signals, and finally constructs Musical Instrument Digital Interface messages, which can be routed to a host of various music synthesis software programs. The voltage output values of each wind turbine are directly represented in both the amplitude and frequency characteristics of the audio. Thus, the audio serves as an accurate real-time monitoring and maintenance tool for the wind farm data from an offsite location.","2017-06","2023-07-06 01:48:57","2023-07-06 01:48:57","","1-6","","","","","","V2Hz","","","","","","","","","","","","IEEE Xplore","","","","","","","Software; music; Automata; automated music synthesis; composition; energy control and optimisation; energy sonification; Internet of Things; maintenance and test; Protocols; Standards; wind farm; Wind farms; Wind turbines; Wireless communication","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2017 28th Irish Signals and Systems Conference (ISSC)","","","","","","","","","","","","","","",""
"EXJEG8Y8","journalArticle","2012","Nikolaidis, Ryan; Walker, Bruce; Weinberg, Gil","Generative Musical Tension Modeling and Its Application to Dynamic Sonification","Computer Music Journal","","0148-9267","10.1162/COMJ_a_00105","","","2012-03","2023-07-06 01:49:29","2023-07-06 01:49:29","","55-64","","1","36","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: Computer Music Journal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L9C5W22Z","conferencePaper","2012","Akti, Gizem; Goularas, Dionysis","Frequency component extraction from color images for specific sound transformation and analysis","2012 3rd International Conference on Image Processing Theory, Tools and Applications (IPTA)","","","10.1109/IPTA.2012.6469573","","This paper presents a method allowing the conversion of images into sound. Initially, a frequency component extraction is realized from the original image. At this stage, the image is divided into windows in order to represent consecutive different time periods using STFT. Then, the dominant frequencies of each window are mapped into corresponding sound frequencies through Fourier analysis. This procedure is applied twice and two series of sound frequency components are produced: The first is originated from the brightness of the image, the second from the dominant RGB layer. The connection between the visual impression of the image and the psychoacoustic effect of the sound mapping is done by using different musical scales according to the dominant color of the image. The results revealed that the melody extracted from this analysis produces a certain psychoacoustic impression, as it has reported by several volunteers. Despite the fact that volunteers could not always do the association between image and sound, they could hardly believe that the music was produced by an algorithmic procedure.","2012-10","2023-07-06 01:49:39","2023-07-06 01:49:39","","253-258","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2154-512X","","","","","Sonification; Music; sonification; Algorithm design and analysis; convolution; Fourier transforms; Image color analysis; Image processing; image to sound conversion; mapping; Psychoacoustics; Short Time Fourier Transform","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2012 3rd International Conference on Image Processing Theory, Tools and Applications (IPTA)","","","","","","","","","","","","","","",""
"L3FKEFII","journalArticle","2021","Su, Isabelle; Qin, Zhao; Saraceno, Tomás; Bisshop, Ally; Mühlethaler, Roland; Ziporyn, Evan; Buehler, Markus J.","Sonification of a 3-D Spider Web and Reconstitution for Musical Composition Using Granular Synthesis","Computer Music Journal","","0148-9267","10.1162/comj_a_00580","","Three-dimensional spider webs feature highly intricate fiber architectures, which can be represented via 3-D scanning and modeling. To allow novel interpretations of the key features of a 3-D Cyrtophora citricola spider web, we translate complex 3-D data from the original web model into music, using data sonification. We map the spider web data to audio parameters such as pitch, amplitude, and envelope. Paired with a visual representation, the resulting audio allows a unique and holistic immersion into the web that can describe features of the 3-D architecture (fiber distance, lengths, connectivity, and overall porosity of the structure) as a function of spatial location in the web. Using granular synthesis, we further develop a method to extract musical building blocks from the sonified web, transforming the original representation of the web data into new musical compositions. We build a new virtual, interactive musical instrument in which the physical 3-D web data are used to generate new variations in sound through exploration of different spatial locations and grain-processing parameters. The transformation of sound from grains to musical arrangements (variations of melody, rhythm, harmony, chords, etc.) is analogous to the natural bottom–up processing of proteins, resembling the design of sequence and higher-level hierarchical protein material organization from elementary chemical building blocks. The tools documented here open possibilities for creating virtual instruments based on spider webs for live performances and art installations, suggesting new possibilities for immersion into spider web data, and for exploring similarities between protein folding, on the one hand, and assembly and musical expression, on the other.","2021-12","2023-07-06 01:50:09","2023-07-06 01:50:09","","43-59","","4","44","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: Computer Music Journal","","/Users/minsik/Zotero/storage/UI9GVDYP/Su et al. - 2021 - Sonification of a 3-D Spider Web and Reconstitutio.pdf; /Users/minsik/Zotero/storage/IXTJRZZV/searchresult.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KJBKMVDZ","journalArticle","2015","Turchet, Luca; Bresin, Roberto","Effects of Interactive Sonification on Emotionally Expressive Walking Styles","IEEE Transactions on Affective Computing","","1949-3045","10.1109/TAFFC.2015.2416724","","This paper describes two experiments conducted to investigate the role of sonically simulated ground materials in modulating both production and recognition of walks performed with emotional intentions. The results of the first experiment showed that the involved auditory feedbacks affected the pattern of emotional walking in different ways, although such an influence manifested itself in more than one direction. The results of the second experiment showed the absence of an influence of the sound conditions on the recognition of the emotions from acoustic information alone. Similar results were found in both experiments for musically-trained and untrained participants. Our results suggest that tempo and sound level are two acoustical features important in both production and recognition of emotions in walking. In addition, the similarities of the presented results with those reported in the music performance domain, as well as the absence of an influence of musical expertise lend support to the “motor origin hypothesis of emotional expression in music” according to which a motor origin for the expression of emotions is common in all those domains of human activity that result in the generation of an acoustical signal.","2015-04","2023-07-06 01:50:09","2023-07-06 01:50:09","","152-164","","2","6","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Affective Computing","","/Users/minsik/Zotero/storage/WIXNRGGI/searchresult.html","","","Interactive sonification; Context; emotions; Legged locomotion; Microphones; Production; Snow; Solids; walking","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WJ3ID7VJ","conferencePaper","2022","Kaplan, Cyril; Husa, Pavel; Mikovec, Zdenek","Scale_it: Converting time series data into musical scales","2022 13th IEEE International Conference on Cognitive Infocommunications (CogInfoCom)","","","10.1109/CogInfoCom55841.2022.10081873","","This paper introduces open-source sonification software that represents a real-time stream of time-series data as notes on a musical scale. The software comprises a data processing unit and a signal-to-sound conversion unit. Before being expressed as a note on a musical scale of predefined length and character, each chunk of the incoming data stream is transformed into a parameter representing its relative position in the original population.","2022-09","2023-07-06 01:50:09","2023-07-06 01:50:09","","000021-000022","","","","","","Scale_it","","","","","","","","","","","","IEEE Xplore","","ISSN: 2380-7350","","/Users/minsik/Zotero/storage/RUEJNS8P/searchresult.html","","","Sociology; Sonification; Music; Time series analysis; Data processing; Real-time systems; Soft sensors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2022 13th IEEE International Conference on Cognitive Infocommunications (CogInfoCom)","","","","","","","","","","","","","","",""
"GRCALXJE","conferencePaper","2003","Westermann, M.","Interactive sound player (ISP): enabling interactive sound in digital media","Proceedings Third International Conference on WEB Delivering of Music","","","10.1109/WDM.2003.1233891","","Digitalklang unites the world of digital (IT knowledge) and the world of Klang (the German word for sound) into one integrated customer experience. At digitalklang, highly-qualified sound designers and programmers develop individual solutions for adding sound to the Internet and other digital media, while significantly enhancing user experience. The interactive sound player (ISP), digitalklang's groundbreaking product, adds highly scalable and customizable interactive audio to Websites and CD-ROMs.","2003-09","2023-07-06 01:50:09","2023-07-06 01:50:09","","154-","","","","","","Interactive sound player (ISP)","","","","","","","","","","","","IEEE Xplore","","","","/Users/minsik/Zotero/storage/2EMBGANY/searchresult.html","","","Application software; Production; Acoustic sensors; Aerodynamics; Internet; Manuals; Portals; Programming profession; Streaming media; XML","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings Third International Conference on WEB Delivering of Music","","","","","","","","","","","","","","",""
"VUQYKZCN","conferencePaper","2022","Husa, Pavel; Kaplan, Cyril; Mikovec, Zdenek","Towards modular sonic EEG neurofeedback interface","2022 13th IEEE International Conference on Cognitive Infocommunications (CogInfoCom)","","","10.1109/CogInfoCom55841.2022.10081815","","Abstract This paper introduces an electroencephalogram (EEG) to the sound transformation tool written in Python and Pure Data using Faust. The proposed signal chain performs a real-time sonification representing the distance of the level from the threshold. We present four different music games implemented in Pure Data using our custom object Scale_it in a neurofeedback application. These games were tested and evaluated by users. The subjective impression of the participants (N=53) of gaining control of the game after the session is overall 3.59 on a five-level Likert scale.","2022-09","2023-07-06 01:50:09","2023-07-06 01:50:09","","000019-000020","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2380-7350","","/Users/minsik/Zotero/storage/XIKFFQ24/searchresult.html","","","Sonification; Electroencephalography; Real-time systems; Atmospheric measurements; Games; Neurofeedback; Particle measurements","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2022 13th IEEE International Conference on Cognitive Infocommunications (CogInfoCom)","","","","","","","","","","","","","","",""
"VSILSHPG","conferencePaper","2019","Ley-Flores, Judith; Bevilacqua, Frédéric; Bianchi-Berthouze, Nadia; Taiadura-Jiménez, Ana","Altering body perception and emotion in physically inactive people through movement sonification","2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)","","","10.1109/ACII.2019.8925432","","Physical inactivity is an increasing problem. It has been linked to psychological and emotional barriers related to the perception of one's body, such as physical capabilities. It remains a challenge to design technologies to increase physical activity in inactive people. We propose the use of a sound interactive system where inputs from movement sensors integrated in shoes are transformed into sounds that evoke body sensations at a metaphorical level. Our user study investigates the effects of various gesture-sound mappings on the perception of one's body and its movement qualities (e.g. being flexible or agile), the related emotional state and movement patterns, when people performed two exercises, walking and thigh stretch. The results confirm the effect of the “metaphor” conditions vs. the control conditions in feelings of body weight; feeling less tired and more in control; or being more comfortable, motivated, and happier. These changes linked to changes in affective state and body movement. We discuss the results in terms of how acting upon body perception and affective states through sensory feedback may in turn enhance physical activity, and the opportunities opened by our findings for the design of wearable technologies and interventions in inactive populations.","2019-09","2023-07-06 01:50:09","2023-07-06 01:50:09","","1-7","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2156-8111","","/Users/minsik/Zotero/storage/U43W3DT5/Ley-Flores et al. - 2019 - Altering body perception and emotion in physically.pdf; /Users/minsik/Zotero/storage/NPUTTBK9/searchresult.html","","","Sonification; sonification; Psychology; Legged locomotion; Accelerometers; body perception; emotion; multisensory feedback; physical activity; Prototypes; self-care technologies; Sensors; Thigh; wearables","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)","","","","","","","","","","","","","","",""
"ICQYP6P7","conferencePaper","2003","Franklin, K.M.; Roberts, J.C.","Pie chart sonification","Proceedings on Seventh International Conference on Information Visualization, 2003. IV 2003.","","","10.1109/IV.2003.1217949","","Different acoustic variables such as pitch, volume, timbre and position can be used to represent quantitative, qualitative and categorical aspects of the information. Such sonifications are particularly useful for those with visual impairments; they are also beneficial in circumstances where visual representations would be impossible to use or to enrich a graphical realization. We demonstrate methods of representing an audible pie chart representation such that the hearer understands the information through an equivalent representation. We implement and evaluate five designs. In each, the user is positioned at the center of the chart and perceives the information through positional sound sources.","2003-07","2023-07-06 01:50:09","2023-07-06 01:50:09","","4-9","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","/Users/minsik/Zotero/storage/REYMDM6A/searchresult.html","","","Music; Computer interfaces; Data visualization; Acoustical engineering; Computer errors; Condition monitoring; Laboratories; Petroleum; Sorting; Timbre","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings on Seventh International Conference on Information Visualization, 2003. IV 2003.","","","","","","","","","","","","","","",""
"N578QCZ3","journalArticle","2020","Ramírez, Andrea Valenzuela; Hornero, Gemma; Royo, Daniel; Aguilar, Angel; Casas, Oscar","Assessment of Emotional States Through Physiological Signals and Its Application in Music Therapy for Disabled People","IEEE Access","","2169-3536","10.1109/ACCESS.2020.3008269","","Disabled people have more difficulties for expression and interaction on a non-verbal level because of the physical, sensory or communication difficulties that they usually present. Regarding music therapy, the existing adapted interfaces target disabled people with controlled motor abilities, which still suggests a barrier for the most affected users presenting truly little or uncontrolled movements. In this work, we have developed a device to build an adapted musical instrument through physiological signals. The instrument uses the electrocardiogram (ECG), the electrical activity of the skin (EDA), the respiration signal and the movement of the user to generate music via sonification of the characteristic features of each physiological signal. Furthermore, the ECG and EDA signals have been used to assess the emotional state of the person to provide to the music therapist objective information in real-time about the adaptation of the user to the techniques and interfaces used in their sessions. The adapted instrument has been tested by people with cerebral palsy showing its high degree of adaptability to the user. In four months, an increase in participation of the most affected users in their music therapy sessions has been achieved. In concrete, the results show that the considered users have achieved six of the so-called aids to analysis which are commonly used to evaluate such practices. The results of the assessment of emotional states indicate that the state of a person can be extracted from the ECG in periods of ten seconds while the evolution of the EDA reveals if the person is relaxed or excited. The device has generated a lot of interest among educators since it outperforms the state-of-the-art techniques allowing the integration of the most affected users by eliminating the aforementioned barrier because of non-controlled movements while assessing the emotional state of the person when facing the activity.","2020","2023-07-06 01:50:09","2023-07-06 01:50:09","","127659-127671","","","8","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Access","","/Users/minsik/Zotero/storage/Y2WKVEB9/Ramírez et al. - 2020 - Assessment of Emotional States Through Physiologic.pdf; /Users/minsik/Zotero/storage/63GZ6ECA/searchresult.html","","","Music; Multiple signal classification; Medical treatment; Instruments; Real-time systems; adaptability; biometic signals; disabilities; ECG; EDA; Electrocardiography; Music therapy; physiology; Physiology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W7QAKPF2","conferencePaper","2004","Matta, S.; Kumar, D.K.; Yu, Xinghuo; Burry, M.","An approach for image sonification","First International Symposium on Control, Communications and Signal Processing, 2004.","","","10.1109/ISCCSP.2004.1296321","","This paper presents a new approach for image to sound mapping. The proposed method utilizes the music parameters such as pitch and rhythm to support translation of images into sounds. Many people have tried image-to-sound mapping or data-to-sound mapping and failed to prove the useful results and many people haven't followed the principles of psychoacoustics in implementing image to sound conversion methods. The important bottleneck in these kinds of experiments is that humans can't remember the normal sounds as compared to music. A method is developed to overcome this bottleneck by utilizing musical parameters. Most of the available tools have been tested on the participants and it has been discovered that the technology available to convert data streams into sounds was not sufficient and needed an improvement.","2004-03","2023-07-06 01:50:37","2023-07-06 01:50:37","","431-434","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","","","","Music; Humans; Auditory displays; Ear; Frequency; Streaming media; Acoustical engineering; Australia; Image converters; Rhythm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","First International Symposium on Control, Communications and Signal Processing, 2004.","","","","","","","","","","","","","","",""
"SSTN3UWA","journalArticle","2022","Kantan, Prithvi; Spaich, Erika G.; Dahl, Sofia","A Technical Framework for Musical Biofeedback in Stroke Rehabilitation","IEEE Transactions on Human-Machine Systems","","2168-2305","10.1109/THMS.2021.3137013","","In this article, we present a technical framework aimed at facilitating musical biofeedback research in poststroke movement rehabilitation. The framework comprises wireless wearable inertial sensors and software built with inexpensive and open-source tools. The software enables layered and adjustable music synthesis and has a generic movement–music mapping module. Using this, we designed digital musical interactions for balance, sit-to-stand, and gait training. Preliminary trials with subacute stroke patients indicated that the interactions were clinically feasible. Expert interviews with a focus group of clinicians were also conducted, where these interactions were deemed as meaningful and relevant to clinical protocols, with comprehensible feedback (albeit sometimes unpleasant or disturbing) for several patient types. We carried out system benchmarking, finding that the system has sufficiently short loop delays (\sim90 ms) and a healthy sensing range (>9 m) and is computationally efficient (11.1% peak CPU usage on a quad-core processor). Future studies will focus on using this framework with patients to both develop the interactions further and measure their effects on motor learning, performance retention, and psychological factors to help gauge their true clinical potential.","2022-04","2023-07-06 01:50:55","2023-07-06 01:50:55","","220-231","","2","52","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Human-Machine Systems","","/Users/minsik/Zotero/storage/PRBZFNH8/searchresult.html; /Users/minsik/Zotero/storage/IKJ3R2Q4/Kantan et al. - 2022 - A Technical Framework for Musical Biofeedback in S.pdf","","","Music; Training; interactive sonification; Software; Biological control systems; Real-time systems; Sensors; Balance; biofeedback; gait; music intervention; neurorehabilitation; stroke; Task analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T7BTKZEP","conferencePaper","2000","Ballora, M.; Pennycook, B.; Ivanov, P.Ch.; Goldberger, A.; Glass, L.","Detection of obstructive sleep apnea through auditory display of heart rate variability","Computers in Cardiology 2000. Vol.27 (Cat. 00CH37163)","","","10.1109/CIC.2000.898630","","A data set of interbeat interval times is read into a music software synthesis program to become the basis of a ""soundtrack"" or auditory display. Here the authors present examples of the sonifications, and discuss potential advantages in listening to, as opposed to viewing, heart rate variability data. This method treats the diagnosis of obstructive sleep apnea as a problem of orchestration and melody.","2000-09","2023-07-06 01:50:55","2023-07-06 01:50:55","","739-740","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 0276-6547","","/Users/minsik/Zotero/storage/P4WIFNR4/searchresult.html","","","Physics; Computer languages; Auditory displays; Auditory system; Graphical user interfaces; Cardiology; Heart rate detection; Heart rate variability; Polymers; Sleep apnea","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Computers in Cardiology 2000. Vol.27 (Cat. 00CH37163)","","","","","","","","","","","","","","",""
"ZSEI2BUM","conferencePaper","2004","Cassidy, R.J.; Berger, J.; Lee, Kyogu; Maggioni, M.; Coifman, R.R.","Analysis of hyperspectral colon tissue images using vocal synthesis models","Conference Record of the Thirty-Eighth Asilomar Conference on Signals, Systems and Computers, 2004.","","","10.1109/ACSSC.2004.1399429","","In prior work, we examined the possibility of sound generation from colon tissue scan data using vocal synthesis models. In this work, we review key results and present extensions to the prior work. Sonification entails the mapping of data values to sound synthesis parameters such that informative sounds are produced by the chosen sound synthesis model. We review the physical equations and technical highlights of a vocal synthesis model developed by Cook. Next we present the colon tissue scan data gathered, and discuss processing steps applied to the data. Finally, we review preliminary results from a simple sonification map. New findings regarding perceptual distance of vowel sounds are presented.","2004-11","2023-07-06 01:50:55","2023-07-06 01:50:55","","1611-1615 Vol.2","","","2","","","","","","","","","","","","","","","IEEE Xplore","","","","/Users/minsik/Zotero/storage/QNU7X5YR/searchresult.html; /Users/minsik/Zotero/storage/PZ768ULA/Cassidy et al. - 2004 - Analysis of hyperspectral colon tissue images usin.pdf","","","Music; Acoustics; Colon; Colonography; Computed tomography; Equations; Hyperspectral imaging; Image analysis; Mathematical model; Mathematics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Conference Record of the Thirty-Eighth Asilomar Conference on Signals, Systems and Computers, 2004.","","","","","","","","","","","","","","",""
"ND6I6TUG","conferencePaper","2009","Ng, Kia","Interactive Multimedia Interface and Multimodal Analysis for Technology-Enhanced Learning and Performance Preservation","2009 Fifth International Joint Conference on INC, IMS and IDC","","","10.1109/NCM.2009.107","","This paper presents one of the interactive multimedia tools resulted from a research and development project in the context of technology-enhanced learning for music, and the usage of the multimodal interface for performing arts preservation. Firstly, the paper briefly introduces the i-Maestro project (see www.i-maestro.org). With particular focuses on the interactive multimedia interface for 3D motion visualization, the paper discusses the applications and usages of this gestural interface for technology-enhanced learning for string instruments. The interface utilizes online and offline multimodal data analysis and provide interactive feedback using 3D graphics and sonification. It provides additional level of details and data for stylistic and performance analysis. Next, the paper presents an ongoing research project which is working on digital preservation of interactive music performance using this interface.","2009-08","2023-07-06 01:50:55","2023-07-06 01:50:55","","1401-1405","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","/Users/minsik/Zotero/storage/7BGZPGVS/searchresult.html","","","Visualization; Data analysis; Computer interfaces; Feedback; interactive; Instruments; multimodal; Testing; education; Educational technology; Graphics; motion capture; multimedia; Multimedia computing; Performance analysis; preservation; sensor; technology-enahnced learning; visualisation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2009 Fifth International Joint Conference on INC, IMS and IDC","","","","","","","","","","","","","","",""
"7BWWUMJI","conferencePaper","2021","Li, Jiabao; Galvin, Cooper","Glacier’s Lament","2021 IEEE VIS Arts Program (VISAP)","","","10.1109/VISAP52981.2021.00010","","Glaciers are sentinels of climate change. They are the most visible evidence of global warming today. This series of works embodies the stunning beauty, rapid change, fragility, destructive power, and magnificence of glaciers. At the same time, they challenge the audience with the dramatic, irreversible ecological damages from climate change. In Glacier’s Lament, we used data from glacier melting in the past 60 years to compose music and dance with local musicians who have witnessed the recession of the Mendenhall glacier over their lifetimes in Juneau, Alaska. Each note is one season in a year. In the winter, the glacier is frozen, so the pitch is low. In the summer, the melting rate rises, so the pitch is high. Towards the end, the melting overflows into spring and autumn, and the melting in the summer becomes faster. We filmed the artists performing the piece on the glacier, collaborating with the glacier’s own sounds. There are four color cards in PANTONE for glacier blue. However, in real glaciers, this blue color is variable and dynamic. As glaciers are disappearing, this unique blue is also disappearing. We sampled and blended the blue color from glaciers in Alaska and hung them in recycled glass vials. When one glacier calving happened, one color vial fell down. At the end of the exhibition, all 60 vials fell down, forming a painting on the canvas beneath.","2021-10","2023-07-06 01:50:55","2023-07-06 01:50:55","","35-36","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2767-7028","","/Users/minsik/Zotero/storage/MD2GCXTQ/searchresult.html","","","Sonification; Climate change; Data visualization; climate-change; data-art; data-physicalization; Data-sonification; data-visualization; glacier; information-design; installation; performance; video-art","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2021 IEEE VIS Arts Program (VISAP)","","","","","","","","","","","","","","",""
"NC9YJ8MF","conferencePaper","2022","Wang, Yichen; Gardner, Henry; Martin, Charles; Adcock, Matt","Augmenting Sculpture with Immersive Sonification","2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)","","","10.1109/VRW55335.2022.00164","","We present an artistic Mixed Reality (MR) system that remixes a sculptural element of a building and its aesthetic context to provide an on-site augmented art experience. Mainstream MR systems, particularly art-related, focus on the use of visuals in presenting additional information, whereas the use of audio as the main information channel has rarely been considered. In this work, we explore two different versions of a sonic experience for walking through an artistic staircase to enhance its public's engagement. Our user evaluation reveals the effectiveness of sonic design for a rewarding MR experience. With this, we emphasise the importance of sonic design in MR applications.","2022-03","2023-07-06 01:50:55","2023-07-06 01:50:55","","626-627","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","/Users/minsik/Zotero/storage/DS5CDIIK/searchresult.html","","","Visualization; Buildings; Three-dimensional displays; Legged locomotion; Applied computing—Arts and humanities—Sound and music computing; Art; Conferences; Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Mixed/augmented reality; Mixed reality","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)","","","","","","","","","","","","","","",""
"SU58WBST","journalArticle","2022","Zhang, Brian J.; Sigafoos, Noel; Moffit, Rabecka; Syed, Ibrahim; Adams, Lili S.; Fick, Jason; Fitter, Naomi T.","SonifyIt: Towards Transformative Sound for All Robots","IEEE Robotics and Automation Letters","","2377-3766","10.1109/LRA.2022.3193228","","Transformative robot sound yields perceptual, functional, and social benefits in human-robot interactions, but broader research and implementation related to this topic is impeded by the lack of a common sound generation system for robots. Such a system could enable a wide array of situated robot sound studies, smoother collaborations with sound designers than current state of the art methods, and broader adoption of transformative robot sound. Based on other successful open-source projects in the robotics community, we integrated Robot Operating System, a popular robotics middleware, and Pure Data, a visual programming language for multimedia, to enable live sound synthesis and sample playback for robots. This sound generation system synthesized sound in an in-the-wild pilot study with positive qualitative results. Furthermore, an online within-subjects survey study with N=96 showed that the proposed sound system made the robot seem warmer, happier, and more energetic. This work benefits robotics researchers by providing the current sound system as a validated artifact and demonstrating its potential impact on broader robotics applications. We plan to develop this software into an open-source package: SonifyIt.","2022-10","2023-07-06 01:50:55","2023-07-06 01:50:55","","10566-10572","","4","7","","","SonifyIt","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Robotics and Automation Letters","","/Users/minsik/Zotero/storage/VAHSA6MT/searchresult.html","","","Sonification; Music; sonification; Visualization; Computer languages; Audio user interfaces; human-robot interaction; open source software; Open source software; Programming; Robots","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JZLAIUGN","conferencePaper","2004","Malandrino, D.; Meo, P.; Palmieri, G.; Scarano, V.","AMIFAST: an architecture for MIDI flows as sonification tools","Proceedings. Eighth International Conference on Information Visualisation, 2004. IV 2004.","","","10.1109/IV.2004.1320215","","We describe a framework in Java to create sonification applications with minimum effort from the programmer and musician. Our tool, AMIFAST, offers a set of modules that can be easily assembled to produce sonification of off-line as well as on-line (i.e. real-time) applications. Moreover, the programmer can easily add new functionalities In AMIFaST, we included a sonification technique that we introduce here, Markov Chain Perturbation.","2004-07","2023-07-06 01:50:55","2023-07-06 01:50:55","","677-682","","","","","","AMIFAST","","","","","","","","","","","","IEEE Xplore","","ISSN: 1093-9547","","/Users/minsik/Zotero/storage/5WYAVPRU/searchresult.html","","","Data analysis; Application software; Computerized monitoring; Programming profession; Assembly; Java; Meteorology; Proposals; Transfer functions; Web server","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings. Eighth International Conference on Information Visualisation, 2004. IV 2004.","","","","","","","","","","","","","","",""
"WRAZAIHP","conferencePaper","2019","Ahmetovic, Dragan; Avanzini, Federico; Baratè, Adriano; Bernareggi, Cristian; Galimberti, Gabriele; Ludovico, Luca A.; Mascetti, Sergio; Presti, Giorgio","Sonification of Rotation Instructions to Support Navigation of People with Visual Impairment","2019 IEEE International Conference on Pervasive Computing and Communications (PerCom","","","10.1109/PERCOM.2019.8767407","","Indoor navigation services for people with visual impairment are being researched in academia, and working systems have already been deployed in public places. While previous research mainly focuses on computing the user’s position with high accuracy, providing non-visual navigation instructions is also a challenge and naive approaches can fail in helping users reach their target destination or even expose them to hazards.In this paper we investigate the problem of guiding users to rotate towards a target direction. We propose three different sonification techniques that provide continuous guidance during rotation, and we compare them to a single-impulse baseline, used in previous works. We also explore three variations that reinforce the proposed techniques by combining them with the baseline. A preliminary study with 10 blind participants highlights two dominant techniques, which we analyze through a follow-up study with 18 participants, from 2 groups with very distant cultural backgrounds. While stark differences emerge in the performance from the two groups, we highlight two clear results common to both: 1) one of the proposed techniques significantly outperforms the baseline, reducing the average rotation error by a factor of 3.5 (from 11° to 3°); 2) the interaction speed of this technique, generally slower than the baseline, significantly improves when combined with the baseline technique.","2019-03","2023-07-06 01:50:55","2023-07-06 01:50:55","","1-10","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2474-249X","","/Users/minsik/Zotero/storage/4KCWPXXX/searchresult.html","","","Music; sonification; Visualization; Mobile handsets; Sensors; Global Positioning System; mobility; Turning; visual impairment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2019 IEEE International Conference on Pervasive Computing and Communications (PerCom","","","","","","","","","","","","","","",""
"UYYJCE49","conferencePaper","2018","Maçãs, Catarina; Martins, Pedro; Machado, Penousal","Consumption as a Rhythm: A Multimodal Experiment on the Representation of Time-Series","2018 22nd International Conference Information Visualisation (IV)","","","10.1109/iV.2018.00093","","Through Data Visualisation and Sonification models, we present a study of multimodal representations to characterise the Portuguese consumption patterns, which were gathered from Portuguese hypermarkets and supermarkets over the course of two years. We focus on the rhythmic nature of the data to create and discuss audio and visual representations that highlight disruptions and sudden changes in the normal consumption patterns. For this study, we present two distinct visual and audio representations and discuss their strengths and limitations.","2018-07","2023-07-06 01:51:25","2023-07-06 01:51:25","","504-509","","","","","","Consumption as a Rhythm","","","","","","","","","","","","IEEE Xplore","","ISSN: 2375-0138","","","","","Sonification; Visualization; Instruments; Data visualization; Rhythm; Atmospheric modeling; Consumption rhythm; Multimodal representation; Shape; Time-series; Visualisation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2018 22nd International Conference Information Visualisation (IV)","","","","","","","","","","","","","","",""
"TXHRFF29","conferencePaper","2009","Hussein, Khaled; Tilevich, Eli; Bukvic, Ivica Ico; Kim, SooBeen","Sonification design guidelines to enhance program comprehension","2009 IEEE 17th International Conference on Program Comprehension","","","10.1109/ICPC.2009.5090035","","Faced with the challenges of understanding the source code of a program, software developers are assisted by a wealth of software visualization research. This work explores how visualization can be supplemented by sonification as a cognitive tool for code comprehension. By engaging the programmer's auditory senses, sonification can improve the utility of program comprehension tools. This paper reports on our experiences of creating and evaluating a program comprehension prototype tool that employs sonification to assist program understanding by rendering sonic cues. Our empirical evaluation of the efficacy of information sonification indicates that this cognitive aid can effectively complement visualization when trying to understand an unfamiliar code base. Based on our experiences, we then propose a set of guidelines for the design of a new generation of tools that increase their information utility by combining visualization and sonification.","2009-05","2023-07-06 01:51:37","2023-07-06 01:51:37","","120-129","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 1092-8138","","/Users/minsik/Zotero/storage/8D8TP9JP/Hussein et al. - 2009 - Sonification design guidelines to enhance program .pdf","","","Visualization; Computer science; Auditory displays; Prototypes; Programming; Educational institutions; Guidelines; Software maintenance; Software prototyping; Software systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2009 IEEE 17th International Conference on Program Comprehension","","","","","","","","","","","","","","",""
"CKDG8ANE","conferencePaper","2018","Broderick, James; Duggan, Jim; Redfern, Sam","The Importance of Spatial Audio in Modern Games and Virtual Environments","2018 IEEE Games, Entertainment, Media Conference (GEM)","","","10.1109/GEM.2018.8516445","","In Virtual Reality (VR), virtual environments, and gaming, audio greatly impacts the user, yet it is an area that has been under researched quite recently. While music and environmental sounds have been used to great effect throughout gaming history to craft a more immersive experience, spatial audio doesn't see as much focus. Not only does well made spatial audio allow a user to become more immersed in their virtual experience, it is an important channel for information about their environment. With the advent of VR, games are putting more work into spatial audio and audio design, and now the results are becoming available for both research and game development. This paper will look at not only why spatial audio can greatly improve virtual experiences, but also some of the new technologies that have become available in recent times which make the implementation of spatial audio far easier and of a higher quality than ever before. It will also look at some planned research to examine the advantages of spatial audio in sonified virtual environments using Unity 3D.","2018-08","2023-07-06 01:52:03","2023-07-06 01:52:03","","1-9","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","/Users/minsik/Zotero/storage/59BR9IL6/searchresult.html","","","sonification; Three-dimensional displays; auditory display; Games; Task analysis; Engines; game engine; Headphones; virtual environments; Virtual environments; virtual reality","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2018 IEEE Games, Entertainment, Media Conference (GEM)","","","","","","","","","","","","","","",""
"A6Q7Q3CX","conferencePaper","2015","Yang, Jiajun; Hermann, Thomas","A Zen Garden interface for the interactive control of sonic ambiences in smart environment","2015 6th IEEE International Conference on Cognitive Infocommunications (CogInfoCom)","","","10.1109/CogInfoCom.2015.7390648","","In this paper we introduce SoZen, a novel interface for parameterizing aspects of ambient soundscapes including ambient music and peripheral sonifications using a Zen Garden. The SoZen system is both a stylish and aesthetic artifact in daily living environments and a computer-vision-based tangible and malleable representation for sound features. Due to the immediate correspondence between the arrangement of stones and the shape of sand with sonic features it also becomes a persistent visualization of the current sonic ambience. Thus it offers a conceptually different and `calm' way of reviewing and specifying sounds. The paper focuses on the conceptual ideas and showcases our current system. We demonstrate some initial ambiences and conclude with current ideas on how to embed and evaluate SoZen in the context of a smart apartment environment.","2015-10","2023-07-06 01:52:03","2023-07-06 01:52:03","","523-524","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","/Users/minsik/Zotero/storage/73BZY9EH/searchresult.html; /Users/minsik/Zotero/storage/XHH88KUP/Yang and Hermann - 2015 - A Zen Garden interface for the interactive control.pdf","","","ambient soundscape; Cross-modal interaction; image feature extraction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2015 6th IEEE International Conference on Cognitive Infocommunications (CogInfoCom)","","","","","","","","","","","","","","",""
"RH5UWAF7","journalArticle","1999","Jovanov, E.; Wegner, K.; Radivojevic, V.; Starcevic, D.; Quinn, M.S.; Karron, D.B.","Tactical audio and acoustic rendering in biomedical applications","IEEE Transactions on Information Technology in Biomedicine","","1558-0032","10.1109/4233.767086","","Complexity of biomedical data requires novel sophisticated analysis and presentation methods. Sonification is used as a new information display in augmented reality systems to overcome problems of existing human-computer interfaces (e.g., opaque or heavy head-mounted displays, slow computer graphics, etc.). A novel taxonomy of sonification methods and techniques is introduced. We present our experience with tactical audio and acoustic rendering in biomedical applications. Tactical audio as an audio feedback is used as support for precise manual positioning of a surgical instrument in the operating room. Acoustic rendering is applied as an additional information channel and/or warning signal in biomedical signal analysis and data presentation.","1999-06","2023-07-06 01:52:03","2023-07-06 01:52:03","","109-118","","2","3","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Information Technology in Biomedicine","","/Users/minsik/Zotero/storage/XYLURSRP/searchresult.html","","","Augmented reality; Application software; Auditory displays; Acoustic applications; Bioinformatics; Biomedical acoustics; Computer displays; Computer graphics; Rendering (computer graphics); Taxonomy","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P9DES4HJ","conferencePaper","2006","Kori, H.; Tezuka, T.; Tanaka, K.","Ranking of Regional Blogs by Suitability for Sonification","22nd International Conference on Data Engineering Workshops (ICDEW'06)","","","10.1109/ICDEW.2006.125","","Media content presented to a vehicle driver is mainly auditory, since visual content is distracting and viewing it increases the risk of an accident. Music and radio are thus commonly listened to while driving. However, these types of content rarely reflect regional characteristics and are therefore not well suited for tourists who want to get information about the region they are visiting. We have developed the Blog Car Radio system that presents blog entries in auditory style using sonification (speech synthesis). Blog entries are obtained from blog search engines, selected by distances from the vehicle’s current location, and ranked based on their suitability for sonification and relevance to the userspecified category. By using Blog Car Radio, a driver can obtain local information with only a small amount of distraction. In this paper, we particularly discussed the method to rank text contents which is suitable for sonification.","2006-04","2023-07-06 01:52:03","2023-07-06 01:52:03","","x132-x132","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","/Users/minsik/Zotero/storage/IFR3CULH/searchresult.html","","","Geographic Information Systems; Accidents; Blogs; Filtering; Informatics; Information systems; Search engines; Speech synthesis; User interfaces; Vehicle driving","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","22nd International Conference on Data Engineering Workshops (ICDEW'06)","","","","","","","","","","","","","","",""
"XGFHRWWQ","conferencePaper","2013","Milsap, Griffin; Fifer, Matthew; Crone, Nathan; Thakor, Nitish","Listening to the music of the brain: Live analysis of ECoG recordings using digital audio workstation software","2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)","","","10.1109/NER.2013.6696026","","A process is presented for analyzing electrocorticographic (ECoG) recordings and prototyping brain computer interfaces in which complex signal processing chains are able to be rapidly developed and iterated in digital audio workstation (DAW) software. DAW software includes many built-in “drag and drop” blocks that perform common, low-level signal processing algorithms such as filtering and envelope extraction. In addition to being optimized for real-time performance, DAW software also produces audio output, allowing for listening to raw and processed signals. Hearing these sonifications can impart new insights that may not be apparent in purely visual representations. A simple functional mapping analysis is performed in a DAW called Pure Data and compared to the results from a more traditional spatiotemporal analysis in MATLAB. Channels exhibiting qualitative activation in the resulting functional maps were further analyzed in another DAW called Renoise, wherein several high frequency (i.e., >400 Hz) features were observed. This study demonstrates an example use of DAW software, which we suggest is an easy-to-use and intuitive environment for real-time exploratory analyses and sophisticated sonification of ECoG recordings.","2013-11","2023-07-06 01:52:03","2023-07-06 01:52:03","","682-685","","","","","","Listening to the music of the brain","","","","","","","","","","","","IEEE Xplore","","ISSN: 1948-3554","","/Users/minsik/Zotero/storage/Q9TX4T8N/searchresult.html","","","Production; Real-time systems; MATLAB; Microelectrodes; Speech","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)","","","","","","","","","","","","","","",""
"WVYT3MFA","conferencePaper","1995","Minghim, R.; Forrest, A.R.","An illustrated analysis of sonification for scientific visualisation","Proceedings Visualization '95","","","10.1109/VISUAL.1995.480802","","This paper presents an analysis of progress in the use of sound as a tool in support of visualisation and gives an insight into its development and future needs. Special emphasis is given to the use of sound in scientific and engineering applications. A system developed to support surface data presentation and interaction by using sound is presented and discussed.","1995-10","2023-07-06 01:52:03","2023-07-06 01:52:03","","110-117","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 1070-2385","","/Users/minsik/Zotero/storage/EBXMURHG/searchresult.html","","","Music; Monitoring; Data visualization; Acoustical engineering; Graphics; Information systems; Displays; Information analysis; Redundancy; Sonar applications","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings Visualization '95","","","","","","","","","","","","","","",""
"KADFJD3N","journalArticle","2014","Jeon, Myounghoon; Walker, Bruce N.; Gable, Thomas M.","Anger Effects on Driver Situation Awareness and Driving Performance","Presence","","1054-7460","10.1162/PRES_a_00169","","Abstract Research has suggested that emotional states have critical effects on various cognitive processes, which are important components of situation awareness (Endsley, 1995b). Evidence from driving studies has also emphasized the importance of driver situation awareness for performance and safety. However, to date, little research has investigated the relationship between emotional effects and driver situation awareness. In our experiment, 30 undergraduates drove in a simulator after induction of either anger or neutral affect. Results showed that an induced angry state can degrade driver situation awareness as well as driving performance as compared to a neutral state. However, the angry state did not have an impact on participants' subjective judgment or perceived workload, which might imply that the effects of anger occurred below their level of conscious awareness. One of the reasons participants showed a lack of compensation for their deficits in performance might be that they were not aware of severe impacts of emotional effects on driving performance.","2014-02","2023-07-06 01:52:03","2023-07-06 01:52:03","","71-89","","1","23","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: Presence","","/Users/minsik/Zotero/storage/4GNIG27V/searchresult.html; /Users/minsik/Zotero/storage/93N93FLQ/Jeon et al. - 2014 - Anger Effects on Driver Situation Awareness and Dr.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3IMUP975","conferencePaper","2015","Serafin, Stefania; Nordahl, Rolf; Erkut, Cumhur; Geronazzo, Michele; Avanzini, Federico; de Götzen, Amalia","Sonic interaction in virtual environments","2015 IEEE 2nd VR Workshop on Sonic Interactions for Virtual Environments (SIVE)","","","10.1109/SIVE.2015.7361283","","This paper summarizes the main research topics addressed at the 2nd workshop on Sonic Interaction in Virtual Environments (SIVE) that took place in Arles, France in March 2015. The workshop is part of the annual IEEE Virtual Reality Conference.","2015-03","2023-07-06 01:52:03","2023-07-06 01:52:03","","1-2","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","/Users/minsik/Zotero/storage/PK5347GR/searchresult.html","","","Sonification; Music; Games; Conferences; Virtual environments; virtual reality; 3D sound; multimodal interaction; Solid modeling; Sonic interaction design; tangible interfaces","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2015 IEEE 2nd VR Workshop on Sonic Interactions for Virtual Environments (SIVE)","","","","","","","","","","","","","","",""
"C66S6AU3","conferencePaper","2014","McIntosh, Shane; Legere, Katie; Hassan, Ahmed E.","Orchestrating change: An artistic representation of software evolution","2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering (CSMR-WCRE)","","","10.1109/CSMR-WCRE.2014.6747192","","Several visualization tools have been proposed to highlight interesting software evolution phenomena. These tools help practitioners to navigate large and complex software systems, and also support researchers in studying software evolution. However, little work has explored the use of sound in the context of software evolution. In this paper, we propose the use of musical interpretation to support exploration of software evolution data. In order to generate music inspired by software evolution, we use parameter-based sonification, i.e., a mapping of dataset characteristics to sound. Our approach yields musical scores that can be played synthetically or by a symphony orchestra. In designing our approach, we address three challenges: (1) the generated music must be aesthetically pleasing, (2) the generated music must accurately reflect the changes that have occurred, and (3) a small group of musicians must be able to impersonate a large development team. We assess the feasibility of our approach using historical data from Eclipse, which yields promising results.","2014-02","2023-07-06 01:53:04","2023-07-06 01:53:04","","348-352","","","","","","Orchestrating change","","","","","","","","","","","","IEEE Xplore","","","","","","","Software; Instruments; Timbre; History; Maintenance engineering; Software engineering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering (CSMR-WCRE)","","","","","","","","","","","","","","",""
"5MDG6EXQ","conferencePaper","2018","Gal, Zoltan; Korteby, Mohamed Amine; AlDabbas, Ashraf","Estimation of the Wireless Sensor Network Performance using Fractal Behavior of the Generated Cognitive Harmonic Waves","2018 9th IEEE International Conference on Cognitive Infocommunications (CogInfoCom)","","","10.1109/CogInfoCom.2018.8639902","","In several domains like geometry and music, a classic kind of discrete pattern comes from similar systems, where the elements of the set are connected in a specific structure. Types of the similarity feature are symmetry with respect to the axis or to the origin, rotation, translation, and scaling. A special group of structure types with similarity character in different natural and artificial systems are fractals. They are set of objects that look similar on different scales. A musical fractal is a piece where the theme harmonizes with a slow down version of itself, that way the small-scale structure of the piece is repeated at a larger scale. Human matter of taste prefers fractal structure in music and is tight sensible to the similarity alteration. This phenomenon is used to evaluate the operating performance of the Wireless Sensor Network (WSN). We present a new method of sensing the WSN behavior based on cognitive generation mechanism of a musical sound signal and we give special features of this algorithm.","2018-08","2023-07-06 01:53:37","2023-07-06 01:53:37","","000057-000062","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2380-7350","","/Users/minsik/Zotero/storage/TRNB5DW3/searchresult.html","","","Music; sonification; Conferences; Harmonic analysis; box-counting dimension; cognitive harmonic wave; Estimation; fractal; Fractals; similarity; wireless sensor network; Wireless sensor networks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2018 9th IEEE International Conference on Cognitive Infocommunications (CogInfoCom)","","","","","","","","","","","","","","",""
"SUT6UCJI","journalArticle","2016","Barrett, Natasha","Interactive Spatial Sonification of Multidimensional Data for Composition and Auditory Display","Computer Music Journal","","0148-9267","10.1162/COMJ_a_00358","","This article presents a new approach to interactive spatial sonification of multidimensional data as a tool for spatial sound synthesis, for composing temporal–spatial musical materials, and as an auditory display for scientists to analyze multidimensional data sets in time and space. The approach applies parameter-mapping sonification and is currently implemented in an application called Cheddar, which was programmed in Max/MSP. Cheddar sonifies data in real time, where the user can modify a wide variety of temporal, spatial, and sonic parameters during the listening process, and thus more easily uncover patterns and processes in the data than when applying non-real-time, noninteractive techniques. The design draws on existing literature concerning perception and acoustics, and it applies the author's practical experience in acousmatic composition, spectromorphology, and sound semantics, while addressing accuracy, flexibility, and ease of use. Although previous sonification applications have addressed some degree of real-time control and spatialization, this approach integrates space and sound in an interactive framework. Spatial information is sonified in high-order 3-D ambisonics, where the user can interactively move the virtual listening position to reveal details easily missed from fixed or noninteractive spatial views. Sounds used as input to the sonification take advantage of the rich spectra and extramusical attributes of acoustic sources, which, although previously theorized, are investigated here in a practical context thoroughly tested alongside acoustic and psychoacoustic considerations. Furthermore, when using Cheddar, no specialized knowledge of programming, acoustics, or psychoacoustics is required. These approaches position Cheddar at the junction between science and art. With one application serving both disciplines, the patterns and processes of science are more fluently appropriated into music or sound art, and vice versa for scientific research, science public outreach, and education.","2016-06","2023-07-06 01:53:37","2023-07-06 01:53:37","","47-69","","2","40","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: Computer Music Journal","","/Users/minsik/Zotero/storage/LNBSNY6H/searchresult.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9R7I2HE2","journalArticle","2015","Kirke, Alexis; Freeman, Samuel; Miranda, Eduardo Reck","Wireless Interactive Sonification of Large Water Waves to Demonstrate the Facilities of a Large-Scale Research Wave Tank","Computer Music Journal","","0148-9267","10.1162/COMJ_a_00315","","Interactive sonification can provide a platform for demonstration and education as well as for monitoring and investigation. We present a system designed to demonstrate the facilities of the UK's most advanced large-scale research wave tank. The interactive sonification of water waves in the “ocean basin” wave tank at Plymouth University consisted of a number of elements: generation of ocean waves, acquisition and sonification of ocean-wave measurement data, and gesture-controlled pitch and amplitude of sonifications. The generated water waves were linked in real time to sonic features via depth monitors and motion tracking of a floating buoy. Types of water-wave patterns, varying in shape and size, were selected and triggered using wireless motion detectors attached to the demonstrator's arms. The system was implemented on a network of five computers utilizing Max/MSP alongside specialist marine research software, and was demonstrated live in a public performance for the formal opening of the Marine Institute building.","2015-09","2023-07-06 01:53:37","2023-07-06 01:53:37","","59-70","","3","39","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: Computer Music Journal","","/Users/minsik/Zotero/storage/LXV2R97F/searchresult.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G2XJZVAL","conferencePaper","2005","Baier, G.; Hermann, T.; Muller, M.","Polyrhythmic organization of coupled nonlinear oscillators","Ninth International Conference on Information Visualisation (IV'05)","","","10.1109/IV.2005.99","","We study the rhythmic organization of coupled nonlinear oscillators. If oscillators with non-identical internal frequency are coupled, they generate a great variety of periodic and chaotic rhythmic patterns. Sonification of these patterns suggests their characterization in terms of polyrhythms: each oscillatory unit subdivides ""measures"" of equal or varying length differently. For the case of two coupled oscillators, the organization of these polyrhythms is exemplified as a function of the internal frequency ratio and the coupling strength. Some sonification strategies are presented which aid to detect complex rhythmic relationships between oscillators. The results may be of importance for the analysis of complex multivariate time series like human EEG.","2005-07","2023-07-06 01:53:37","2023-07-06 01:53:37","","5-10","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2375-0138","","/Users/minsik/Zotero/storage/QFKKRYUE/searchresult.html; /Users/minsik/Zotero/storage/2U9W4YQK/Baier et al. - 2005 - Polyrhythmic organization of coupled nonlinear osc.pdf","","","Time series analysis; Frequency; Prototypes; Rhythm; Couplings; Biological system modeling; Biological systems; Differential equations; Nonlinear dynamical systems; Oscillators","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Ninth International Conference on Information Visualisation (IV'05)","","","","","","","","","","","","","","",""
"EVJ85PNA","journalArticle","2004","Hermann, T.; Ritter, H.","Sound and meaning in auditory data display","Proceedings of the IEEE","","1558-2256","10.1109/JPROC.2004.825904","","Auditory data display is an interdisciplinary field linking auditory perception research, sound engineering, data mining, and human-computer interaction in order to make semantic contents of data perceptually accessible in the form of (nonverbal) audible sound. For this goal it is important to understand the different ways in which sound can encode meaning. We discuss this issue from the perspectives of language, music, functionality, listening modes, and physics, and point out some limitations of current techniques for auditory data display, in particular when targeting high-dimensional data sets. As a promising, potentially very widely applicable approach, we discuss the method of model-based sonification (MBS) introduced recently by the authors and point out how its natural semantic grounding in the physics of a sound generation process supports the design of sonifications that are accessible even to untrained, everyday listening. We then proceed to show that MBS also facilitates the design of an intuitive, active navigation through ""acoustic aspects"", somewhat analogous to the use of successive two-dimensional views in three-dimensional visualization. Finally, we illustrate the concept with a first prototype of a ""tangible"" sonification interface which allows us to ""perceptually map"" sonification responses into active exploratory hand motions of a user, and give an outlook on some planned extensions.","2004-04","2023-07-06 01:53:37","2023-07-06 01:53:37","","730-741","","4","92","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: Proceedings of the IEEE","","/Users/minsik/Zotero/storage/89ZLM5VE/Hermann and Ritter - 2004 - Sound and meaning in auditory data display.pdf; /Users/minsik/Zotero/storage/JMZ5ALXA/searchresult.html","","","Data mining; Physics; Auditory displays; Acoustical engineering; Data engineering; Ground support; Grounding; Joining processes; Navigation; Process design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WRNXLXAD","conferencePaper","2003","Osmanovic, N.; Hrustemovic, N.; Myler, H.R.","A testbed for auralization of graphic art","Annual Technical Conference IEEE Region 5, 2003","","","10.1109/REG5.2003.1199709","","Sonification is the use of non-speech audio to convey information and auralization, a similar but distinct area, is an aural metaphor for visualization - the process by which objects and scenes are interpreted by the human visual system. A fundamental goal of this study is to produce a mechanism by which the visually impaired can experience the artistic content of images using an alternate modality, hearing. In this project, the auralization of images using music is explored. A software algorithm to map pixel data from images to corresponding sound sequences is presented. This conversion is based on a frequency relationship that exists between color and sound in human perception. For example, the frequency of the G tone at 392 Hz, 40 times redoubled (octaved), delivers the frequency of the color red (431 /spl times/ 10/sup 12/ Hz). In addition, the volume of the mapped sounds is used to represent color intensity.","2003-04","2023-07-06 01:53:37","2023-07-06 01:53:37","","45-49","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","/Users/minsik/Zotero/storage/I2HRLK67/searchresult.html","","","Visualization; Humans; Auditory system; Frequency; Testing; Graphics; Art; Layout; Software algorithms; Visual system","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Annual Technical Conference IEEE Region 5, 2003","","","","","","","","","","","","","","",""
"XAKVJZF6","conferencePaper","2007","Dodiya, Janki; Alexandrov, Vassil N.","Perspectives on Potential of Sound in Virtual Environments","2007 IEEE International Workshop on Haptic, Audio and Visual Environments and Games","","","10.1109/HAVE.2007.4371579","","Sound/3D Sound has been accepted as a significant cue for relaying information in the field of Human-Computer Interaction, multimedia and virtual reality (VR). The paper provides significant perspectives on the potential of sound in the field of virtual environments (VEs)/virtual reality (VR), which unlike other multimedia and visualization technologies aims to completely immerse a user inside a synthetic environment. The paper briefly describes the motivation for using sound in virtual environments and further discusses the applications of sound in VEs in relation to the functions it is expected to perform in the VEs.. Recent related research and solutions offered in using sound in VR are reviewed. Furthermore, challenges and shortcomings in realizing the usefulness of sound are addressed. Views on future directions and areas requiring further research are presented. This literature survey provides a useful source of information for prospective students and researchers interested in the area of sound applied in VEs.","2007-10","2023-07-06 01:53:37","2023-07-06 01:53:37","","15-20","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","/Users/minsik/Zotero/storage/UNTCLHFY/searchresult.html","","","Sonification; Music; Virtual reality; Haptic interfaces; Virtual environment; Conferences; Rendering (computer graphics); User interfaces; 3D Sound/Spatial Sound; Electronic mail; Haptics; Information resources; Multi-Sensory/Muti-modal; Psychoacoustic models; Sound; Virtual Environments; Virtual Reality","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2007 IEEE International Workshop on Haptic, Audio and Visual Environments and Games","","","","","","","","","","","","","","",""
"AWGKWBM5","conferencePaper","2015","Boyer, Eric O.; Vandervoorde, Lucyle; Bevilacqua, Frederic; Hanneton, Sylvain","Touching sounds: Perception of the curvature of auditory virtual surfaces","2015 IEEE Virtual Reality (VR)","","","10.1109/VR.2015.7223341","","In this study, we investigated the ability of blindfolded adults to discriminate between concave and convex auditory virtual surfaces. We used a Leap MotionTM device to measure the movements of the hand and fingers. Participants were asked to explore the space above the device with the palm of one hand and an auditory feedback was produced only when the palm was moving into the boundaries of the surface. In order to demonstrate that curvature direction was correctly perceived by our participants, we estimated their discrimination thresholds with a psychophysical staircase procedure. Two groups of participants were fed with two different sonification of the surface. Results showed that most of the participants were able to learn the task. The best results were obtained with an auditory feedback related to the component of the hand velocity tangential to the virtual surface. This work proposes a contribution to the introduction in virtual reality of auditory virtual objects.","2015-03","2023-07-06 01:53:37","2023-07-06 01:53:37","","153-154","","","","","","Touching sounds","","","","","","","","","","","","IEEE Xplore","","ISSN: 2375-5334","","/Users/minsik/Zotero/storage/LVBFQJQV/searchresult.html","","","Haptic interfaces; Real-time systems; Atmospheric measurements; Shape; Information systems; Estimation; H.5.1 [Information Systems]: Information Interfaces and Presentation — Multimedia Information Systems; H.5.2 [Information Systems]: Information Interfaces and Presentation — User Interface; H.5.5 [Information Systems]: Information Interfaces and Presentation — Sound and Music Computing; Space exploration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2015 IEEE Virtual Reality (VR)","","","","","","","","","","","","","","",""
"CV4PRRRE","conferencePaper","2019","Grond, Florian; Cascio, M. Ariel; Motta-Ochoa, Rossio; Tembeck, Tamar; Veen, Dan Ten; Blain-Moraes, Stefanie","Participatory design of biomusic with users on the autism spectrum","2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)","","","10.1109/ACII.2019.8925484","","Assistive technologies that incorporate affective computing methods may be useful for increasing inclusion across the autism spectrum. The involvement of this target population during development and design is key to ensuring that the technology is beneficial and has the potential to be adopted. This article reports on a participatory design process used for the development of an affective technology intended for users on the autism spectrum. We first briefly recapitulate concepts and theories about emotions to illuminate emotion-specific challenges faced in the design process. We then introduce biomusic, an affective technology that communicates emotional states by translating physiological signals into auditory output. The necessity of stakeholder involvement during the development of biomusic is discussed, with a focus specifically on including individuals with autism spectrum condition (ASC). Finally, we present a concrete example of a participatory sound design workshop focusing on the development of biomusic's auditory display in collaboration with adolescents with ASC. From this experience, challenges of using participatory design for the development of affective technologies with people with ASC are presented and discussed.","2019-09","2023-07-06 01:54:01","2023-07-06 01:54:01","","1-7","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2156-8111","","","","","Medical treatment; Auditory displays; auditory display; Production; Real-time systems; Physiology; Conferences; affective technologies; Autism; biomusic; inclusion; participatory design; physiological signal sonification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)","","","","","","","","","","","","","","",""
"YJR2GYQG","journalArticle","2019","Lopes, Phil; Liapis, Antonios; Yannakakis, Georgios N.","Modelling Affect for Horror Soundscapes","IEEE Transactions on Affective Computing","","1949-3045","10.1109/TAFFC.2017.2695460","","The feeling of horror within movies or games relies on the audience's perception of a tense atmosphere-often achieved through sound accompanied by the on-screen drama-guiding its emotional experience throughout the scene or game-play sequence. These progressions are often crafted through an a priori knowledge of how a scene or game-play sequence will playout, and the intended emotional patterns a game director wants to transmit. The appropriate design of sound becomes even more challenging once the scenery and the general context is autonomously generated by an algorithm. Towards realizing sound-based affective interaction in games this paper explores the creation of computational models capable of ranking short audio pieces based on crowdsourced annotations of tension, arousal and valence. Affect models are trained via preference learning on over a thousand annotations with the use of support vector machines, whose inputs are low-level features extracted from the audio assets of a comprehensive sound library. The models constructed in this work are able to predict the tension, arousal and valence elicited by sound, respectively, with an accuracy of approximately 65%, 66% and 72%.","2019-04","2023-07-06 01:54:15","2023-07-06 01:54:15","","209-222","","2","10","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Affective Computing","","/Users/minsik/Zotero/storage/MMEGFDVU/7903629.html; /Users/minsik/Zotero/storage/K6ZXPLY3/Lopes et al. - 2019 - Modelling Affect for Horror Soundscapes.pdf","","","Music; sonification; Visualization; Computational modeling; Games; crowdsourcing; Crowdsourcing; Horror; Libraries; Predictive models; preference learning; rank annotations; tension","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z74P5CH2","conferencePaper","2021","Robinson, Frederic Anthony; Velonaki, Mari; Bown, Oliver","Smooth Operator: Tuning Robot Perception Through Artificial Movement Sound","2021 16th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","","","","","Can we influence how a robot is perceived by designing the sound of its movement? Drawing from practices in film sound, we overlaid a video depicting a robot’s movement routine with three types of artificial movement sound. In a between-subject study design, participants saw either one of the three designs or a quiet control condition and rated the robot’s movement quality, safety, capability, and attractiveness. We found that, compared to our control, the sound designs both increased and decreased perceived movement quality. Coupling the same robotic movement with different sounds lead to the motions being rated as more or less precise, elegant, jerky, or uncontrolled, among others. We further found that the sound conditions decreased perceived safety, and did not affect perceived capability and attractiveness. More unrealistic sound conditions led to larger differences in ratings, while the subtle addition of harmonic material was not rated differently to the control condition in any of the measures. Based on these findings, we discuss the challenges and opportunities regarding the use of artificial movement sound as an implicit channel of communication that may eventually be able to selectively target specific characteristics, helping designers in creating more refined and nuanced human-robot interactions.CCS CONCEPTS• Human-centered computing \rightarrowInterface design prototyping; Auditory feedback.","2021-03","2023-07-06 05:39:30","2023-07-06 05:39:30","","53-62","","","","","","Smooth Operator","","","","","","","","","","","","IEEE Xplore","","ISSN: 2167-2148","","/Users/minsik/Zotero/storage/TDBYEQJ7/10045122.html; /Users/minsik/Zotero/storage/3EQNYHR4/Robinson et al. - 2021 - Smooth Operator Tuning Robot Perception Through A.pdf","","","Sonification; Music; Human computer interaction; human-robot interaction; Couplings; Harmonic analysis; Human-robot interaction; movement sonification; Robot motion; sonic interaction design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2021 16th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","","","","","","","","","","","","","","",""