"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"IJEGIIBF","journalArticle","2022","Destexhe, Alain; Foubert, Luc","A method to convert neural signals into sound sequences","The Journal of the Acoustical Society of America","","0001-4966","10.1121/10.0011549","https://doi.org/10.1121/10.0011549","We present a method to convert neural signals into sound sequences, with the constraint that the sound sequences precisely reflect the sequences of events in the neural signal. The method consists in quantifying the wave motifs in the signal and using these parameters to generate sound envelopes. We illustrate the procedure for sleep delta waves in the human electro-encephalogram (EEG), which are converted into sound sequences that encode the time structure of the original EEG waves. This procedure can be applied to synthesize personalized sound sequences specific to the EEG of a given subject.","2022-06-02","2023-07-10 06:09:18","2023-07-10 06:09:18","2023-07-10 06:09:18","3685-3689","","6","151","","The Journal of the Acoustical Society of America","","","","","","","","","","","","","Silverchair","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CYUZP3JX","journalArticle","2022","Foley, Liam; Schlesinger, Joseph; Schutz, Michael","More detectable, less annoying: Temporal variation in amplitude envelope and spectral content improves auditory interface efficacy","The Journal of the Acoustical Society of America","","0001-4966","10.1121/10.0010447","https://doi.org/10.1121/10.0010447","Auditory interfaces, such as auditory alarms, are useful tools for human computer interaction. Unfortunately, poor detectability and annoyance inhibit the efficacy of many interface sounds. Here, it is shown in two ways how moving beyond the traditional simplistic temporal structures of normative interface sounds can significantly improve auditory interface efficacy. First, participants rated tones with percussive amplitude envelopes as significantly less annoying than tones with flat amplitude envelopes. Crucially, this annoyance reduction did not come with a detection cost as percussive tones were detected more often than flat tones—particularly, at relatively low listening levels. Second, it was found that reductions in the duration of a tone's harmonics significantly lowered its annoyance without a commensurate reduction in detection. Together, these findings help inform our theoretical understanding of detection and annoyance of sound. In addition, they offer promising original design considerations for auditory interfaces.","2022-05-12","2023-07-10 06:09:30","2023-07-10 06:09:30","2023-07-10 06:09:30","3189-3196","","5","151","","The Journal of the Acoustical Society of America","More detectable, less annoying","","","","","","","","","","","","Silverchair","","","","/Users/minsik/Zotero/storage/R4DFNGQJ/Foley et al. - 2022 - More detectable, less annoying Temporal variation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y89UI66V","journalArticle","2021","Engel, Isaac; Henry, Craig; Amengual Garí, Sebastià V.; Robinson, Philip W.; Picinali, Lorenzo","Perceptual implications of different Ambisonics-based methods for binaural reverberation","The Journal of the Acoustical Society of America","","0001-4966","10.1121/10.0003437","https://doi.org/10.1121/10.0003437","Reverberation is essential for the realistic auralisation of enclosed spaces. However, it can be computationally expensive to render with high fidelity and, in practice, simplified models are typically used to lower costs while preserving perceived quality. Ambisonics-based methods may be employed to this purpose as they allow us to render a reverberant sound field more efficiently by limiting its spatial resolution. The present study explores the perceptual impact of two simplifications of Ambisonics-based binaural reverberation that aim to improve efficiency. First, a “hybrid Ambisonics” approach is proposed in which the direct sound path is generated by convolution with a spatially dense head related impulse response set, separately from reverberation. Second, the reverberant virtual loudspeaker method (RVL) is presented as a computationally efficient approach to dynamically render binaural reverberation for multiple sources with the potential limitation of inaccurately simulating listener's head rotations. Numerical and perceptual evaluations suggest that the perceived quality of hybrid Ambisonics auralisations of two measured rooms ceased to improve beyond the third order, which is a lower threshold than what was found by previous studies in which the direct sound path was not processed separately. Additionally, RVL is shown to produce auralisations with comparable perceived quality to Ambisonics renderings.","2021-02-04","2023-07-10 06:09:44","2023-07-10 06:09:44","2023-07-10 06:09:44","895-910","","2","149","","The Journal of the Acoustical Society of America","","","","","","","","","","","","","Silverchair","","","","/Users/minsik/Zotero/storage/4EDDAJZL/Engel et al. - 2021 - Perceptual implications of different Ambisonics-ba.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SDVFJDNI","journalArticle","2019","Fishbein, Adam R.; Lawson, Shelby L.; Dooling, Robert J.; Ball, Gregory F.","How canaries listen to their song: Species-specific shape of auditory perception","The Journal of the Acoustical Society of America","","0001-4966","10.1121/1.5087692","https://doi.org/10.1121/1.5087692","The melodic, rolling songs of canaries have entertained humans for centuries and have been studied for decades by researchers interested in vocal learning, but relatively little is known about how the birds listen to their songs. Here, it is investigated how discriminable the general acoustic features of conspecific songs are to canaries, and their discrimination abilities are compared with a small parrot species, the budgerigar. Past experiments have shown that female canaries are more sexually responsive to a particular song element—the “special” syllables—and consistent with those observations, it was found that special syllables are perceptually distinctive for canaries. It is also shown that canaries discriminate the subtle differences among syllables and phrases using spectral, envelope, and temporal fine structure cues. Yet, while canaries can hear these fine details of the acoustic structure of their song, the evidence overall suggests that they listen at a more global, phrase by phrase level, rather than an analytic, syllable by syllable level, except when attending to some features of special syllables. These results depict the species-specific shape of auditory perception in canaries and lay the groundwork for future studies examining how song perception changes seasonally and according to hormonal state.","2019-01-31","2023-07-10 06:10:34","2023-07-10 06:10:34","2023-07-10 06:10:34","562-574","","1","145","","The Journal of the Acoustical Society of America","How canaries listen to their song","","","","","","","","","","","","Silverchair","","","","/Users/minsik/Zotero/storage/Y9DV54JA/Fishbein et al. - 2019 - How canaries listen to their song Species-specifi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UMWV5DSP","journalArticle","2018","Lembke, Sven-Amin","Hearing triangles: Perceptual clarity, opacity, and symmetry of spectrotemporal sound shapes","The Journal of the Acoustical Society of America","","0001-4966","10.1121/1.5048130","https://doi.org/10.1121/1.5048130","In electroacoustic music, the spectromorphological approach commonly employs analogies to non-sonic phenomena like shapes, gestures, or textures. In acoustical terms, sound shapes can concern simple geometries on the spectrotemporal plane, for instance, a triangle that widens in frequency over time. To test the auditory relevance of such triangular sound shapes, two psychoacoustic experiments assessed if and how these shapes are perceived. Triangular sound-shape stimuli, created through granular synthesis, varied across the factors grain density, frequency and amplitude scales, and widening vs narrowing orientations. The perceptual investigation focused on three auditory qualities, derived in analogy to the visual description of a triangle: the clarity of the triangular outline, the opacity of the area enclosed by the outline, and the symmetry along the vertical dimension. These morphological qualities seemed to capture distinct perceptual aspects, each linked to different acoustical factors. Clarity of shape was conveyed even for sparse grain densities, while also exhibiting a perceptual bias for widening orientations. Opacity varied as a function of grain texture, whereas symmetry strongly depended on frequency and amplitude scales. The perception of sound shapes could relate to common perceptual cross-modal correspondences and share the same principles of perceptual grouping with vision.","2018-08-06","2023-07-10 06:10:46","2023-07-10 06:10:46","2023-07-10 06:10:46","608-619","","2","144","","The Journal of the Acoustical Society of America","Hearing triangles","","","","","","","","","","","","Silverchair","","","","/Users/minsik/Zotero/storage/AIHAPMU2/Lembke - 2018 - Hearing triangles Perceptual clarity, opacity, an.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T3K8F6WK","journalArticle","2017","Lee, Doheon; van Dorp Schuitman, Jasper; Cabrera, Densil; Qiu, Xiaojun; Burnett, Ian","Comparison of psychoacoustic-based reverberance parameters","The Journal of the Acoustical Society of America","","0001-4966","10.1121/1.5005508","https://doi.org/10.1121/1.5005508","This study compared psychoacoustic reverberance parameters to each other, as well as to reverberation time (RT) and early decay time (EDT) under various acoustic conditions. The psychoacoustic parameters were loudness-based RT (TN), loudness-based EDT [EDTN; Lee, Cabrera, and Martens, J. Acoust. Soc. Am. 131, 1194–1205 (2012a)], and parameter for reverberance [PREV; van Dorp Schuitman, de Vries, and Lindau., J. Acoust. Soc. Am. 133, 1572–1585 (2013)]. For the comparisons, a wide range of sound pressure levels (SPLs) from 20 dB to 100 dB and RTs from 0.5 s to 5.0 s were evaluated, and two sets of subjective data from the previous studies were used for the cross-validation and comparison. Results of the comparisons show that the psychoacoustic reverberance parameters provided better matches to reverberance than RT and EDT; however, the performance of these psychoacoustic reverberance parameters varied with the SPL range, the type of audio sample, and the reverberation conditions. This study reveals that PREV is the most relevant for estimating a relative change in reverberance between samples when the SPL range is small, while EDTN is useful in estimating the absolute reverberance. This study also suggests the use of PREV and EDTN for speech and music samples, respectively.","2017-10-05","2023-07-10 06:10:56","2023-07-10 06:10:56","2023-07-10 06:10:56","1832-1840","","4","142","","The Journal of the Acoustical Society of America","","","","","","","","","","","","","Silverchair","","","","/Users/minsik/Zotero/storage/5FUQJJ45/Lee et al. - 2017 - Comparison of psychoacoustic-based reverberance pa.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8ARJI7QW","journalArticle","2017","Farbood, Morwaread M.; Price, Khen C.","The contribution of timbre attributes to musical tensiona)","The Journal of the Acoustical Society of America","","0001-4966","10.1121/1.4973568","https://doi.org/10.1121/1.4973568","Timbre is an auditory feature that has received relatively little attention in empirical work examining musical tension. In order to address this gap, an experiment was conducted to explore the contribution of several specific timbre attributes—inharmonicity, roughness, spectral centroid, spectral deviation, and spectral flatness—to the perception of tension. Listeners compared pairs of sounds representing low and high degrees of each attribute and indicated which sound was more tense. Although the response profiles showed that the high states corresponded with increased tension for all attributes, further analysis revealed that some attributes were strongly correlated with others. When qualitative factors, attribute correlations, and listener responses were all taken into account, there was fairly strong evidence that higher degrees of roughness, inharmonicity, and spectral flatness elicited higher tension. On the other hand, evidence that higher spectral centroid and spectral deviation corresponded to increases in tension was ambiguous.","2017-01-20","2023-07-10 06:11:10","2023-07-10 06:11:10","2023-07-10 06:11:10","419-427","","1","141","","The Journal of the Acoustical Society of America","","","","","","","","","","","","","Silverchair","","","","/Users/minsik/Zotero/storage/MZT2WAQ2/Farbood and Price - 2017 - The contribution of timbre attributes to musical t.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZWTKBWIR","journalArticle","2017","Paté, Arthur; Boschi, Lapo; Dubois, Danièle; Le Carrou, Jean-Loïc; Holtzman, Benjamin","Auditory display of seismic data: On the use of experts' categorizations and verbal descriptions as heuristics for geoscience","The Journal of the Acoustical Society of America","","0001-4966","10.1121/1.4978441","https://doi.org/10.1121/1.4978441","Auditory display can complement visual representations in order to better interpret scientific data. A previous article showed that the free categorization of “audified seismic signals” operated by listeners can be explained by various geophysical parameters. The present article confirms this result and shows that cognitive representations of listeners can be used as heuristics for the characterization of seismic signals. Free sorting tests are conducted with audified seismic signals, with the earthquake/seismometer relative location, playback audification speed, and earthquake magnitude as controlled variables. The analysis is built on partitions (categories) and verbal comments (categorization criteria). Participants from different backgrounds (acousticians or geoscientists) are contrasted in order to investigate the role of the participants' expertise. Sounds resulting from different earthquake/station distances or azimuths, crustal structure and topography along the path of the seismic wave, earthquake magnitude, are found to (a) be sorted into different categories, (b) elicit different verbal descriptions mainly focused on the perceived number of events, frequency content, and background noise level. Building on these perceptual results, acoustic descriptors are computed and geophysical interpretations are proposed in order to match the verbal descriptions. Another result is the robustness of the categories with respect to the audification speed factor.","2017-03-27","2023-07-10 06:11:23","2023-07-10 06:11:23","2023-07-10 06:11:23","2143-2162","","3","141","","The Journal of the Acoustical Society of America","Auditory display of seismic data","","","","","","","","","","","","Silverchair","","","","/Users/minsik/Zotero/storage/HZ2N492N/Paté et al. - 2017 - Auditory display of seismic data On the use of ex.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7FTHP8HB","journalArticle","2007","Cavaco, Sofia; Lewicki, Michael S.","Statistical modeling of intrinsic structures in impacts sounds","The Journal of the Acoustical Society of America","","0001-4966","10.1121/1.2729368","https://doi.org/10.1121/1.2729368","This paper presents a statistical data-driven method for learning intrinsic structures of impact sounds. The method applies principal and independent component analysis to learn low-dimensional representations that model the distribution of both the time-varying spectral and amplitude structure. As a result, the method is able to decompose sounds into a small number of underlying features that characterize acoustic properties such as ringing, resonance, sustain, decay, and onsets. The method is highly flexible and makes no a priori assumptions about the physics, acoustics, or dynamics of the objects. In addition, by modeling the underlying distribution, the method can capture the natural variability of ensembles of related impact sounds.","2007-06-01","2023-07-10 06:11:41","2023-07-10 06:11:41","2023-07-10 06:11:41","3558-3568","","6","121","","The Journal of the Acoustical Society of America","","","","","","","","","","","","","Silverchair","","","","/Users/minsik/Zotero/storage/Z2CJGRX3/Cavaco and Lewicki - 2007 - Statistical modeling of intrinsic structures in im.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GSQG9DH5","journalArticle","2006","Semal, Catherine; Demany, Laurent","Individual differences in the sensitivity to pitch direction","The Journal of the Acoustical Society of America","","0001-4966","10.1121/1.2357708","https://doi.org/10.1121/1.2357708","It is commonly assumed that one can always assign a direction—upward or downward—to a percept of pitch change. The present study shows that this is true for some, but not all, listeners. Frequency difference limens (FDLs, in cents) for pure tones roved in frequency were measured in two conditions. In one condition, the task was to detect frequency changes; in the other condition, the task was to identify the direction of frequency changes. For three listeners, the identification FDL was about 1.5 times smaller than the detection FDL, as predicted (counterintuitively) by signal detection theory under the assumption that performance in the two conditions was limited by one and the same internal noise. For three other listeners, however, the identification FDL was much larger than the detection FDL. The latter listeners had relatively high detection FDLs. They had no difficulty in identifying the direction of just-detectable changes in intensity, or in the frequency of amplitude modulation. Their difficulty in perceiving the direction of small frequency/pitch changes showed up not only when the task required absolute judgments of direction, but also when the directions of two successive frequency changes had to be judged as identical or different.","2006-12-01","2023-07-10 06:11:55","2023-07-10 06:11:55","2023-07-10 06:11:55","3907-3915","","6","120","","The Journal of the Acoustical Society of America","","","","","","","","","","","","","Silverchair","","","","/Users/minsik/Zotero/storage/JHGQLG9E/Semal and Demany - 2006 - Individual differences in the sensitivity to pitch.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M288S9L7","journalArticle","2001","Krishnan, Sridhar; Rangayyan, Rangaraj M.; Bell, G. Douglas; Frank, Cyril B.","Auditory display of knee-joint vibration signals","The Journal of the Acoustical Society of America","","0001-4966","10.1121/1.1413995","https://doi.org/10.1121/1.1413995","Sounds generated due to rubbing of knee-joint surfaces may lead to a potential tool for noninvasive assessment of articular cartilage degeneration. In the work reported in the present paper, an attempt is made to perform computer-assisted auscultation of knee joints by auditory display (AD) of vibration signals (also known as vibroarthrographic or VAG signals) emitted during active movement of the leg. Two types of AD methods are considered: audification and sonification. In audification, the VAG signals are scaled in time and frequency using a time-frequency distribution to facilitate aural analysis. In sonification, the instantaneous mean frequency and envelope of the VAG signals are derived and used to synthesize sounds that are expected to facilitate more accurate diagnosis than the original signals by improving their aural quality. Auditory classification experiments were performed by two orthopedic surgeons with 37 VAG signals including 19 normal and 18 abnormal cases. Sensitivity values (correct detection of abnormality) of 31%, 44%, and 83%, and overall classification accuracies of 53%, 40%, and 57% were obtained with the direct playback, audification, and sonification methods, respectively. The corresponding d′ scores were estimated to be 1.10, −0.36, and 0.55. The high sensitivity of the sonification method indicates that the technique could lead to improved detection of knee-joint abnormalities; however, additional work is required to improve its specificity and achieve better overall performance.","2001-12-01","2023-07-10 06:12:07","2023-07-10 06:12:07","2023-07-10 06:12:07","3292-3304","","6","110","","The Journal of the Acoustical Society of America","","","","","","","","","","","","","Silverchair","","","","/Users/minsik/Zotero/storage/I2I9GI2U/Auditory-display-of-knee-joint-vibration-signals.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2NNDT6TM","journalArticle","2000","Jin, Craig; Schenkel, Markus; Carlile, Simon","Neural system identification model of human sound localization","The Journal of the Acoustical Society of America","","0001-4966","10.1121/1.1288411","https://doi.org/10.1121/1.1288411","This paper examines the role of biological constraints in the human auditory localization process. A psychophysical and neural system modeling approach was undertaken in which performance comparisons between competing models and a human subject explore the relevant biologically plausible “realism constraints.” The directional acoustical cues, upon which sound localization is based, were derived from the human subject’s head-related transfer functions (HRTFs). Sound stimuli were generated by convolving bandpass noise with the HRTFs and were presented to both the subject and the model. The input stimuli to the model were processed using the Auditory Image Model of cochlear processing. The cochlear data were then analyzed by a time-delay neural network which integrated temporal and spectral information to determine the spatial location of the sound source. The combined cochlear model and neural network provided a system model of the sound localization process. Aspects of humanlike localization performance were qualitatively achieved for broadband and bandpass stimuli when the model architecture incorporated frequency division (i.e., the progressive integration of information across the different frequency channels) and was trained using variable bandwidth and center-frequency sounds. Results indicate that both issues are relevant to human sound localization performance.","2000-09-01","2023-07-10 06:13:12","2023-07-10 06:13:12","2023-07-10 06:13:12","1215-1235","","3","108","","The Journal of the Acoustical Society of America","","","","","","","","","","","","","Silverchair","","","","/Users/minsik/Zotero/storage/LT5YKCG4/Jin et al. - 2000 - Neural system identification model of human sound .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""