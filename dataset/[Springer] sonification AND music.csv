"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"2RLFZ48I","journalArticle","2019","Sanyal, Shankha; Nag, Sayan; Banerjee, Archi; Sengupta, Ranjan; Ghosh, Dipak","Music of brain and music on brain: a novel EEG sonification approach","Cognitive Neurodynamics","","1871-4080, 1871-4099","10.1007/s11571-018-9502-4","http://link.springer.com/10.1007/s11571-018-9502-4","Can we hear the sound of our brain? Is there any technique which can enable us to hear the neuro-electrical impulses originating from the different lobes of brain? The answer to all these questions is YES. In this paper we present a novel method with which we can sonify the electroencephalogram (EEG) data recorded in ‘‘control’’ state as well as under the influence of a simple acoustical stimuli—a tanpura drone. The tanpura has a very simple construction yet the tanpura drone exhibits very complex acoustic features, which is generally used for creation of an ambience during a musical performance. Hence, for this pilot project we chose to study the nonlinear correlations between musical stimulus (tanpura drone as well as music clips) and sonified EEG data. Till date, there have been no study which deals with the direct correlation between a bio-signal and its acoustic counterpart and also tries to see how that correlation varies under the influence of different types of stimuli. This study tries to bridge this gap and looks for a direct correlation between music signal and EEG data using a robust mathematical microscope called Multifractal Detrended Cross Correlation Analysis (MFDXA). For this, we took EEG data of 10 participants in 2 min ‘‘control condition’’ (i.e. with white noise) and in 2 min ‘tanpura drone’ (musical stimulus) listening condition. The same experimental paradigm was repeated for two emotional music, ‘‘Chayanat’’ and ‘‘Darbari Kanada’’. These are well known Hindustani classical ragas which conventionally portray contrast emotional attributes, also verified from human response data. Next, the EEG signals from different electrodes were sonified and MFDXA technique was used to assess the degree of correlation (or the cross correlation coefficient cx) between the EEG signals and the music clips. The variation of cx for different lobes of brain during the course of the experiment provides interesting new information regarding the extraordinary ability of music stimuli to engage several areas of the brain significantly unlike any other stimuli (which engages specific domains only).","2019-02","2023-07-05 06:08:32","2023-07-19 23:45:33","2023-07-05 06:08:32","13-31","","1","13","","Cogn Neurodyn","Music of brain and music on brain","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/RQA9A9RR/Sanyal et al. - 2019 - Music of brain and music on brain a novel EEG son.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MZZKTPAL","bookSection","2018","Véron-Delor, Lauriane; Pinto, Serge; Eusebio, Alexandre; Velay, Jean-Luc; Danna, Jérémy","Music and Musical Sonification for the Rehabilitation of Parkinsonian Dysgraphia: Conceptual Framework","Music Technology with Swing","978-3-030-01691-3 978-3-030-01692-0","","","http://link.springer.com/10.1007/978-3-030-01692-0_21","Music has been shown to enhance motor control in patients with Parkinson’s disease (PD). Notably, musical rhythm is perceived as an external auditory cue that helps PD patients to better control movements. The rationale of such effects is that motor control based on auditory guidance would activate a compensatory brain network that minimizes the recruitment of the defective pathway involving the basal ganglia. Would associating music to movement improve its perception and control in PD? Musical sonification consists in modifying in real-time the playback of a preselected music according to some movement parameters. The validation of such a method is underway for handwriting in PD patients. When confirmed, this study will strengthen the clinical interest of musical sonification in motor control and (re)learning in PD.","2018","2023-07-05 06:08:32","2023-07-21 04:35:01","2023-07-05 06:08:32","312-326","","","11265","","","Music and Musical Sonification for the Rehabilitation of Parkinsonian Dysgraphia","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-01692-0_21","","/Users/minsik/Zotero/storage/9NVLLCUD/Véron-Delor et al. - 2018 - Music and Musical Sonification for the Rehabilitat.pdf","","","","Aramaki, Mitsuko; Davies, Matthew E. P.; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S3CRMK8N","journalArticle","2016","Walus, Bartlomiej P.; Pauletto, Sandra; Mason-Jones, Amanda","Sonification and music as support to the communication of alcohol-related health risks to young people: Study design and results","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-016-0220-0","http://link.springer.com/10.1007/s12193-016-0220-0","Excessive consumption of alcohol has been recognised as a significant risk factor impacting the health of young people. Effective communication of such risk is considered to be one key step to improve behaviour. We evaluated an innovative multimedia intervention that utilised audio (sonification—using sound to display data—and music) and interactivity to support the visual communication of alcohol health risk data. A 3-arm pilot experiment was undertaken. The trial measures included health knowledge, alcohol risk perception and user experience of the intervention. Ninetysix subjects participated in the experiment. At 1 month follow-up, alcohol knowledge and alcohol risk perception improved significantly in the whole sample. However, there was no difference between the intervention groups that experienced (1) visual presentation with interactivity (VI-Exp group) and, (2) visual presentation with audio (sonification and music) and interactivity (VAI-Exp group), when compared to the control group which experienced a (3) visual only presentation (V-Cont group). Participants reported enjoying the presentations and found them educational. The majority of participants indicated that the audio, music and sonification helped to convey the information well, and, although a larger sample size is needed to fully establish the effectiveness of the different interventions, this study provides a useful model for future similar studies.","2016-09","2023-07-05 06:08:32","2023-07-20 07:05:38","2023-07-05 06:08:32","235-246","","3","10","","J Multimodal User Interfaces","Sonification and music as support to the communication of alcohol-related health risks to young people","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/ILKJCIEL/Walus et al. - 2016 - Sonification and music as support to the communica.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4RHR52H3","journalArticle","2012","Huang, Chih-Fang; Lu, Hsiang-Pin; Ren, Jenny","Algorithmic approach to sonification of classical Chinese poetry","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-011-0856-4","http://link.springer.com/10.1007/s11042-011-0856-4","The classical Chinese poetry is a remarkable form of art in traditional Chinese character. However, it is difficult for people who are unfamiliar with ancient Chinese to experience the artistic content of the poetry. In this study, a sonification scheme, Tx2Ms (Text-to-Music), is proposed to extract the poetry features between lines in verses; moreover, dynamics and interval relations are modeled to map those features to the movement of multi-dimensional musical elements such as durations. This conversion is based on poetry intonation and acoustic analysis of the pronunciations of poems; and a stochastic compositional algorithm is created by applying a Markov chain. In addition, the best pentatonic mode for a specific poem is recommended according to the formants analysis. Therefore, the sonification of classical Chinese poetry not only provides a novel way for people to appreciate Chinese poetry but also enriches the state of mind and imagery in the delivery process, and the experiment results show that the proposed system is successfully accepted by most people.","2012-11","2023-07-05 06:08:32","2023-07-21 04:32:33","2023-07-05 06:08:32","489-518","","2","61","","Multimed Tools Appl","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/3PVKLA2M/Huang et al. - 2012 - Algorithmic approach to sonification of classical .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2QHW8T6Q","journalArticle","2020","Newbold, Joseph; Gold, Nicolas E.; Bianchi-Berthouze, Nadia","Movement sonification expectancy model: leveraging musical expectancy theory to create movement-altering sonifications","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-020-00322-2","http://link.springer.com/10.1007/s12193-020-00322-2","Abstract             When designing movement sonifications, their effect on people’s movement must be considered. Recent work has shown how real-time sonification can be designed to alter the way people move. However, the mechanisms through which these sonifications alter people’s expectations of their movement is not well explained. This is especially important when considering musical sonifications, to which people bring their own associations and musical expectation, and which can, in turn, alter their perception of the sonification. This paper presents a Movement Expectation Sonification Model, based on theories of motor-feedback and expectation, to explore how musical sonification can impact the way people perceive their movement. Secondly, we present a study that validates the predictions of this model by exploring how harmonic stability within sonification interacts with contextual cues in the environment to impact movement behaviour and perceptions. We show how musical expectancy can be built to either reward or encourage movement, and how such an effect is mediated through the presence of additional cues. This model offers a way for sonification designers to create movement sonifications that not only inform movement but can be used to encourage progress and reward successes.","2020-06","2023-07-05 06:08:32","2023-07-05 06:08:32","2023-07-05 06:08:32","153-166","","2","14","","J Multimodal User Interfaces","Movement sonification expectancy model","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/KKA2QX8L/Newbold et al. - 2020 - Movement sonification expectancy model leveraging.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MUYT32DV","bookSection","2008","Chemseddine, Maher; Noirhomme-Fraiture, Monique","Complex and Dynamic Data Representation by Sonification","Human-Computer Interaction Symposium","978-0-387-09677-3 978-0-387-09678-0","","","http://link.springer.com/10.1007/978-0-387-09678-0_18","So far, data representation has been based on visuals. The huge size of data verges on overuse of the visual capability. Thus, there is a need to reduce the almost exclusive use of visual techniques to represent data in order to increase our perception bandwidth. In this research, we aim to solve this problem by integrating the audio component. More precisely, we are interested in representing data using musical melodies. This paper presents a model for music elements based on human emotion, to express alert messages displayed by computer network monitoring.","2008","2023-07-05 06:08:32","2023-07-20 06:29:33","2023-07-05 06:08:32","195-200","","","272","","","","","","","","Springer US","Boston, MA","en","","","","","DOI.org (Crossref)","","ISSN: 1571-5736 Series Title: IFIP International Federation for Information Processing DOI: 10.1007/978-0-387-09678-0_18","","/Users/minsik/Zotero/storage/TDSDARHQ/Chemseddine and Noirhomme-Fraiture - 2008 - Complex and Dynamic Data Representation by Sonific.pdf","","","","Forbrig, Peter; Paternò, Fabio; Pejtersen, Annelise Mark","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IZLQIGIA","journalArticle","2012","Diniz, Nuno; Coussement, Pieter; Deweppe, Alexander; Demey, Michiel; Leman, Marc","An embodied music cognition approach to multilevel interactive sonification","Journal on Multimodal User Interfaces","","1783-8738","10.1007/s12193-011-0084-2","https://doi.org/10.1007/s12193-011-0084-2","In this paper, a new conceptual framework and related implementation for interactive sonification is introduced. The conceptual framework consists of a combination of three components, namely, gestalt-based electroacoustic composition techniques (sound), user and body-centered spatial exploration (body), and corporeal mediation technology (tools), which are brought together within an existing paradigm of embodied music cognition. The implementation of the conceptual framework is based on an iterative process that involves the development of several use cases. Through this methodology, it is possible to investigate new approaches for structuring and to interactively explore multivariable data through sound.","2012-05-01","2023-07-05 06:10:04","2023-07-05 06:10:04","2023-07-05 06:10:04","211-219","","3","5","","J Multimodal User Interfaces","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/KWERYVAV/Diniz et al. - 2012 - An embodied music cognition approach to multilevel.pdf","","","Sonification; Electroacoustic; Embodiment; Framework; Interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RSZG7DPL","journalArticle","2012","Gena, Peter","Apropos sonification: a broad view of data as music and sound","AI & SOCIETY","","1435-5655","10.1007/s00146-011-0339-1","https://doi.org/10.1007/s00146-011-0339-1","Numbers have been identified with symbolic data forever. The profound association of both with acoustics, music, and sonic art from Pythagoras to current work is beyond reproach. Recently, sonification looks for ways to realize symbolic data (representing results or measurements) as well as “raw” data (signals, impulses, images, etc.) into compositions. In the strictest sense, everything in a computer is symbolic, that is, represented by 0s and 1s. In the arts, the digital age has broadened and enhanced the conceptual landscape not simply through its servitude to the creative process, but as its partner. However, there is a rich history of the use of data that no doubt has paved the way for many of today’s experiments including my own.","2012-05-01","2023-07-05 06:10:33","2023-07-05 06:10:33","2023-07-05 06:10:33","197-205","","2","27","","AI & Soc","Apropos sonification","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/3AWTXPUP/Gena - 2012 - Apropos sonification a broad view of data as musi.pdf","","","Algorithmic composition; DNA music; Music; Music history; Sonic art","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DABWYJ52","conferencePaper","2021","Polaczyk, Jakub; Croft, Katelyn; Cai, Yang","Compositional Sonification of Cybersecurity Data in a Baroque Style","Advances in Artificial Intelligence, Software and Systems Engineering","978-3-030-80624-8","","10.1007/978-3-030-80624-8_38","","Compositional sonification is a musical mapping algorithm that transforms a dataset into a soundscape for humans to hear numbers within a musical structure. In this study, we used three methods to sonify data: 1) a Baroque-style gestural mapping algorithm that emphasized timing and rhythm using an analog method of handwritten composition, 2) an electronic online pitch sequencer, and 3) the classical instrument Harp performing notated music. We investigated related sound mapping algorithms and the sensitivity of pattern representation of cybersecurity data, including the malware distribution network datasets. Our preliminary experiments showed that a Baroque-inspired timing element based on gestures plays a critical role in aural sonification. Using a sequencer with visualizers helped listeners grasp the patterns more effectively since they were able to watch and hear the results simultaneously. Performing the sonified data live on the harp helped make the compositions more relatable and thus connected with a broader audience. The harp also added a visual element which helped listeners identify patterns. We found that listeners were able to accurately identify the sonified data patterns in both the baroque-style analog handwritten compositions performed on harp and the compositions produced by the Online Sequencer. While non-musicians were able to answer questions about the data patterns they heard with a high percentage of accuracy, their results improved when visual elements were added.","2021","2023-07-05 06:12:24","2023-07-05 06:12:24","","304-312","","","","","","","Lecture Notes in Networks and Systems","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/7HF8V3K5/Polaczyk et al. - 2021 - Compositional Sonification of Cybersecurity Data i.pdf","","","Sonification; AI; Augmented reality; Baroque; Classical instruments; Classical music; Composition; Computer music; Computer virus; Creativity; Harp; Malware; Network; Sequencer","Ahram, Tareq Z.; Karwowski, Waldemar; Kalra, Jay","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7JXEFZDG","journalArticle","2012","Eacott, John","Flood Tide: sonification as musical performance—an audience perspective","AI & SOCIETY","","1435-5655","10.1007/s00146-011-0338-2","https://doi.org/10.1007/s00146-011-0338-2","The number of events and artifacts described as sonification has increased considerably in recent years with some works making a bridge between the representation of data and artistic expression. FloodTide which sonifies the flow of tidal water is such a work and has achieved a relatively high profile attracting good audiences for its 10 performances to date. It is not entirely obvious however what it is that attracts audiences and whether it is effective at representing the data being sonified. This paper aims to address these issues and is based on a discussion group in which these and other questions are considered.","2012-05-01","2023-07-05 06:12:56","2023-07-05 06:12:56","2023-07-05 06:12:56","189-195","","2","27","","AI & Soc","Flood Tide","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/HGWTT38M/Eacott - 2012 - Flood Tide sonification as musical performance—an.pdf","","","Sonification; Algorithmic composition; Live notation; Tide","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DPAWS4FJ","journalArticle","2012","Fabiani, Marco; Bresin, Roberto; Dubus, Gaël","Interactive sonification of expressive hand gestures on a handheld device","Journal on Multimodal User Interfaces","","1783-8738","10.1007/s12193-011-0076-2","https://doi.org/10.1007/s12193-011-0076-2","We present here a mobile phone application called MoodifierLive which aims at using expressive music performances for the sonification of expressive gestures through the mapping of the phone’s accelerometer data to the performance parameters (i.e. tempo, sound level, and articulation). The application, and in particular the sonification principle, is described in detail. An experiment was carried out to evaluate the perceived matching between the gesture and the music performance that it produced, using two distinct mappings between gestures and performance. The results show that the application produces consistent performances, and that the mapping based on data collected from real gestures works better than one defined a priori by the authors.","2012-07-01","2023-07-05 06:13:38","2023-07-05 06:13:38","2023-07-05 06:13:38","49-57","","1","6","","J Multimodal User Interfaces","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/4DT98QXF/Fabiani et al. - 2012 - Interactive sonification of expressive hand gestur.pdf","","","Sonification; Automatic music performance; Emotional hand gestures; Mobile phone","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SRQSK2G9","journalArticle","2012","Varni, Giovanna; Dubus, Gaël; Oksanen, Sami; Volpe, Gualtiero; Fabiani, Marco; Bresin, Roberto; Kleimola, Jari; Välimäki, Vesa; Camurri, Antonio","Interactive sonification of synchronisation of motoric behaviour in social active listening to music with mobile devices","Journal on Multimodal User Interfaces","","1783-8738","10.1007/s12193-011-0079-z","https://doi.org/10.1007/s12193-011-0079-z","This paper evaluates three different interactive sonifications of dyadic coordinated human rhythmic activity. An index of phase synchronisation of gestures was chosen as coordination metric. The sonifications are implemented as three prototype applications exploiting mobile devices: Sync’n’Moog, Sync’n’Move, and Sync’n’Mood. Sync’n’Moog sonifies the phase synchronisation index by acting directly on the audio signal and applying a nonlinear time-varying filtering technique. Sync’n’Move intervenes on the multi-track music content by making the single instruments emerge and hide. Sync’n’Mood manipulates the affective features of the music performance. The three sonifications were also tested against a condition without sonification.","2012-05-01","2023-07-05 06:14:05","2023-07-05 06:14:05","2023-07-05 06:14:05","157-173","","3","5","","J Multimodal User Interfaces","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/HLWJPW9P/Varni et al. - 2012 - Interactive sonification of synchronisation of mot.pdf","","","Active music listening; Audio systems; Interactive sonification; Interactive systems; Sound and music computing; Synchronisation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P9QLLAFP","journalArticle","2019","Polo, Antonio; Sevillano, Xavier","Musical Vision: an interactive bio-inspired sonification tool to convert images into music","Journal on Multimodal User Interfaces","","1783-8738","10.1007/s12193-018-0280-4","https://doi.org/10.1007/s12193-018-0280-4","Musical Vision is a highly flexible, interactive and bio-inspired sonification tool that translates color images into harmonic polyphonic music by mimicking the human visual system in terms of its field of vision and photosensitive sensors. Putting the user at the center of the sonification process, Musical Vision allows the interactive design of fully configurable mappings between the color space and the MIDI instruments and audio pitch spaces to tailor the music rendering results to the application needs. Moreover, Musical Vision incorporates a harmonizer capable of introducing the necessary modifications to create melodies using harmonic chords. Above all else, Musical Vision is an extremely flexible system that the user can interactively configure to convert an image into either a few seconds or a several minutes long musical piece. Thus, it can be used, for instance, with trans-artistic purposes like the conversion of a painting into music, for augmenting vision with music, or for learning musical skills such as sol-fa. To evaluate the proposed sonification tool, we conducted a pilot user study, in which twelve volunteers were tested to interpret images containing geometric patterns from music rendered by Musical Vision. Results show that even those users with no musical education background were able to achieve nearly 70% accuracy in multiple choice tests after less than 25 min training. Moreover, users with some musical education were capable of accurately “drawing by ear” the images from no other stimuli than the sonifications.","2019-09-01","2023-07-05 06:14:31","2023-07-05 06:14:31","2023-07-05 06:14:31","231-243","","3","13","","J Multimodal User Interfaces","Musical Vision","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/MX9P5ZU3/Polo and Sevillano - 2019 - Musical Vision an interactive bio-inspired sonifi.pdf","","","Auditory user interface; MIDI; User-controlled sonification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7ZLIAE2D","journalArticle","2012","Gresham-Lancaster, Scot","Relationships of sonification to music and sound art","AI & SOCIETY","","1435-5655","10.1007/s00146-011-0337-3","https://doi.org/10.1007/s00146-011-0337-3","The definition of sonification has been reframed in recent years but remains somewhat in flux; the basic concepts and procedural flows have remained relatively unchanged. Recent definitions have focused on the objective the important uses of sonification in terms of scientific method. The full realization of the potential of the field must also include the craft and art of music composition. The author proposes examining techniques of sonification in a two-order framework: direct and procedural. The impact of new technologies and historical roots of that work argues that framing this broad topic should be in terms inclusive of scientific method and craftsmanship and art. The expressive use of sonic time-based data flows needs to be refined and expanded. The unexamined territory of how a broad-based population of listeners on a subjective, as well as objective level needs, have to be included in this new field.","2012-05-01","2023-07-05 06:14:59","2023-07-05 06:14:59","2023-07-05 06:14:59","207-212","","2","27","","AI & Soc","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/WYTPE9Z8/Gresham-Lancaster - 2012 - Relationships of sonification to music and sound a.pdf","","","Sonification; Composition; Sound Art","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EVFATR9U","journalArticle","2012","Polli, Andrea","Soundscape, sonification, and sound activism","AI & SOCIETY","","1435-5655","10.1007/s00146-011-0345-3","https://doi.org/10.1007/s00146-011-0345-3","In this article, the author will argue that the act of listening through public soundwalks and other formal and informal exercises builds environmental and social awareness and promotes changes in social and cultural practices. By examining the act of listening as an alternative pathway and comparing the research, writings, and creative work of leaders of the acoustic ecology movement (i.e., R. Murray Schafer, Hildegard Westerkamp, and Bernie Krause), the author hopes to shed light on these potentials. For purposes of comparison, projects that explore the sonification and audification of inaudible signals will be examined, including the work of Christina Kubisch. The process of audification and sonification of these signals will be examined in comparison to soundscape experiences in order to develop a theory of data sonification based on the soundscape. In order to build a community around the urban soundscape, in 2003, the author co-founded the New York Society of Acoustic Ecology. Through this endeavor, she co-created the ongoing NYSoundmap and Sound Seeker projects, which provide some practical research for this article. Thus, by comparing and contrasting theoretical writings with leading listening exercises, public soundwalks, soundscape-related brainstorming sessions, and presenting field recordings in various settings, new methodologies will be documented.","2012-05-01","2023-07-05 06:15:18","2023-07-05 06:15:18","2023-07-05 06:15:18","257-268","","2","27","","AI & Soc","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/6CWKMIEA/Polli - 2012 - Soundscape, sonification, and sound activism.pdf","","","Sonification; Acoustic ecology; Audification; Field recording; Soundscape; Soundwalks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WFFR33QR","journalArticle","2019","Maes, Pieter-Jan; Lorenzoni, Valerio; Six, Joren","The SoundBike: musical sonification strategies to enhance cyclists’ spontaneous synchronization to external music","Journal on Multimodal User Interfaces","","1783-8738","10.1007/s12193-018-0279-x","https://doi.org/10.1007/s12193-018-0279-x","The spontaneous tendency of people to synchronize their movements to music is a powerful mechanism useful for the development of strategies for tempo adaptation of simple repetitive movements. In the current article, we contribute to such strategies—applied to cycling—by introducing a new strategy based on the sonification of cyclists’ motor rhythm. For that purpose, we developed the SoundBike, a stationary bike equipped with sensors that allows interactive sonification of cyclists’ motor rhythm using two distinct but compatible sonification methods. One is based on the principle of step sequencers, which are frequently used for electronic music production. The other is based on the Kuramoto model, allowing automatic and continuous phase alignment of beat-annotated music pieces to cyclists’ motor rhythm, i.e., pedal cadence. Apart from an in-depth presentation of the technical aspects of the SoundBike, we present an experimental study in which we investigated whether the SoundBike could enhance spontaneous synchronization of cyclists to external music. The results of this experiment suggest that sonification of cyclists’ motor rhythm may increase their tendency to synchronize to external music, and helps to keep a more stable pedal cadence, compared to the condition of having external music only (without sonification). Although the results are preliminary and should be followed-up by additional experiments to become more conclusive, SoundBike seems anyhow a promising interactive sonification device to assist motor learning and adaptation in the field of sports and motor rehabilitation.","2019-09-01","2023-07-05 06:15:42","2023-07-05 06:15:42","2023-07-05 06:15:42","155-166","","3","13","","J Multimodal User Interfaces","The SoundBike","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/49V7UDG6/Maes et al. - 2019 - The SoundBike musical sonification strategies to .pdf","","","Movement tempo adaptation; Musical biofeedback; Reinforcement learning; Reward; Sensorimotor synchronization; Sonification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QMVQHAIG","journalArticle","2012","Joy, Jerome","What NMSAT says about sonification","AI & SOCIETY","","1435-5655","10.1007/s00146-011-0343-5","https://doi.org/10.1007/s00146-011-0343-5","This article presents a sample of references issuing directly from the existing NMSAT database. The method employed—that of systematically probing the database—reveals forms of sonification, but also hypothetical premises of sonification, covering the period from ancient times to the beginning of the twentieth century. The following are some of the categories of sonification that have emerged as a result of this search: Natural phenomenon & meteorology to sound (autophones); Image to sound; Text & communication to sound; Human & machine activities to sound (auditing); Localisation to sound (sonar); Architecture & geometry & abstract proportions to sound (scalization, transcription, & spatialization); Energy to sound; Human body to sound; Distance to sound (distance listening); Movement to sound (holophony, kynophony); and Interpreted observations to sound (naturalist music, transpositions & analogies, paraphrasing). The search also uncovered other principals and practices in the vicinity of sonification including: audification, auditing, auscultation, auralization, soniculation, transduction, mapping, earcons, auditory icons, sympathy, echometry, etc. It has been decided to summarise the results of « What NMSAT Says About Sonification » in this special issue of AI&Society, access to the unabridged version of article is available here: http://www.locusonus.org/sonification/.","2012-05-01","2023-07-05 06:16:59","2023-07-05 06:16:59","2023-07-05 06:16:59","233-244","","2","27","","AI & Soc","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/94RABVZJ/Joy - 2012 - What NMSAT says about sonification.pdf","","","Audio art; Database; Distance listening; Networked music; Networks; Sonification history; Timeline","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G9ATNJ9D","conferencePaper","2014","Zhang, Yuxi; Huang, Yifeng; Yue, Junwei; Zhang, Liqing","Sonification for EEG Frequency Spectrum and EEG-Based Emotion Features","Neural Information Processing","978-3-319-12643-2","","10.1007/978-3-319-12643-2_6","","Sonification is the use of representations of data through sound to convey information. It is particularly meaningful if the data are involved in time. This paper present a hybrid sonification method and aims to directly expressed the emotion hidden in the EEG signal through sound. The hybrid method mainly consists of two parts: (1) Frequency Mapping Representation (FMR) and (2) Emotion Feature Representation (EFR).","2014","2023-07-05 06:21:51","2023-07-05 06:21:51","","42-49","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/7EC4J4IN/Zhang et al. - 2014 - Sonification for EEG Frequency Spectrum and EEG-Ba.pdf","","","Emotion Feature; Motor Imagery; Music Generation; Music Piece; Note Generation Function","Loo, Chu Kiong; Yap, Keem Siah; Wong, Kok Wai; Beng Jin, Andrew Teoh; Huang, Kaizhu","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QMBKF2I3","journalArticle","2019","Wolf, KatieAnna E.; Fiebrink, Rebecca","Personalised interactive sonification of musical performance data","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-019-00294-y","http://link.springer.com/10.1007/s12193-019-00294-y","In this article, we describe methods and consequences for giving audience members interactive control over the real-time sonification of performer movement data in electronic music performance. We first briefly describe how to technically implement a musical performance in which each audience member can interactively construct and change their own individual sonification of performers’ movements, heard through headphones on a personal WiFi-enabled device, while also maintaining delay-free synchronization between performer movements and sound. Then, we describe two studies we conducted in the context of live musical performances with this technology. These studies have allowed us to examine how providing audience members with the ability to interactively sonify performer actions impacted their experiences, including their perceptions of their own role and engagement with the performance. These studies also allowed us to explore how audience members with different levels of expertise with sonification and sound, and different motivations for interacting, could be supported and influenced by different sonification interfaces. This work contributes to a better understanding of how providing interactive control over sonification may alter listeners’ experiences, of how to support everyday people in designing and using bespoke sonifications, and of new possibilities for musical performance and participation.","2019-09","2023-07-05 06:23:48","2023-07-20 07:05:49","2023-07-05 06:23:48","245-265","","3","13","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/4KCLJBR8/Wolf and Fiebrink - 2019 - Personalised interactive sonification of musical p.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JGDH3YRH","journalArticle","2022","Frid, Emma; Bresin, Roberto","Perceptual Evaluation of Blended Sonification of Mechanical Robot Sounds Produced by Emotionally Expressive Gestures: Augmenting Consequential Sounds to Improve Non-verbal Robot Communication","International Journal of Social Robotics","","1875-4791, 1875-4805","10.1007/s12369-021-00788-4","https://link.springer.com/10.1007/s12369-021-00788-4","Abstract             This paper presents two experiments focusing on perception of mechanical sounds produced by expressive robot movement and blended sonifications thereof. In the first experiment, 31 participants evaluated emotions conveyed by robot sounds through free-form text descriptions. The sounds were inherently produced by the movements of a NAO robot and were not specifically designed for communicative purposes. Results suggested no strong coupling between the emotional expression of gestures and how sounds inherent to these movements were perceived by listeners; joyful gestures did not necessarily result in joyful sounds. A word that reoccurred in text descriptions of all sounds, regardless of the nature of the expressive gesture, was “stress”. In the second experiment, blended sonification was used to enhance and further clarify the emotional expression of the robot sounds evaluated in the first experiment. Analysis of quantitative ratings of 30 participants revealed that the blended sonification successfully contributed to enhancement of the emotional message for sound models designed to convey frustration and joy. Our findings suggest that blended sonification guided by perceptual research on emotion in speech and music can successfully improve communication of emotions through robot sounds in auditory-only conditions.","2022-03","2023-07-05 06:23:48","2023-07-05 06:23:48","2023-07-05 06:23:48","357-372","","2","14","","Int J of Soc Robotics","Perceptual Evaluation of Blended Sonification of Mechanical Robot Sounds Produced by Emotionally Expressive Gestures","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/NMVAXPXX/Frid and Bresin - 2022 - Perceptual Evaluation of Blended Sonification of M.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TIPBA3QJ","bookSection","2008","Salter, Christopher L.; Baalman, Marije A. J.; Moody-Grigsby, Daniel","Between Mapping, Sonification and Composition: Responsive Audio Environments in Live Performance","Computer Music Modeling and Retrieval. Sense of Sounds","978-3-540-85034-2 978-3-540-85035-9","","","http://link.springer.com/10.1007/978-3-540-85035-9_17","This paper describes recent work on a large-scale, interactive theater performance entitled Schwelle as a platform to pose critical questions around the conception, design and implementation of what is commonly labeled responsive audio environments. The authors first discuss some principal issues in the design of responsive audio environments specifically within the domain of stage performance, addressing existing human-computer interaction paradigms and discussing three key areas: sensing, mapping and data sonification. Next, we discuss larger questions of composition in relation to these key areas, suggesting that potential strategies cross three different domains: mapping within algorithmic composition, data sonification techniques, and time-based evolutionary processes emerging from dynamical systems theory. We then examine in detail the recent work on Schwelle, which employs real time, distributed sensor data to drive a continuous dynamical systembased composition engine. The project’s conceptual and technical challenges are discussed as well as audience evaluation and feedback from the first presentation in Berlin in February 2007. This presentation lead the authors to re-iterate the design and build an additional state-system layer into the dynamical system in order to generate more perceivable sonic structures on both the macro as well as meso levels for the audience/listener. Finally, we conclude with a set of issues that may act as a framework for future research focused on compositional strategies for larger scale, distributed, networked-based sensor environments.","2008","2023-07-05 06:23:48","2023-07-19 23:49:26","2023-07-05 06:23:48","246-262","","","4969","","","Between Mapping, Sonification and Composition","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-85035-9_17","","/Users/minsik/Zotero/storage/HJ7IU9V4/Salter et al. - 2008 - Between Mapping, Sonification and Composition Res.pdf","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LWUSDBPC","journalArticle","2020","Roddy, Stephen; Bridges, Brian","Mapping for meaning: the embodied sonification listening model and its implications for the mapping problem in sonic information design","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-020-00318-y","http://link.springer.com/10.1007/s12193-020-00318-y","","2020-06","2023-07-05 06:23:48","2023-07-05 06:23:48","2023-07-05 06:23:48","143-151","","2","14","","J Multimodal User Interfaces","Mapping for meaning","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/2HV96TED/Roddy and Bridges - 2020 - Mapping for meaning the embodied sonification lis.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F6SKWD8H","bookSection","2021","Bardelli, Sandro; Ferretti, Claudia; Ludovico, Luca Andrea; Presti, Giorgio; Rinaldi, Maurizio","A Sonification of the zCOSMOS Galaxy Dataset","Culture and Computing. Interactive Cultural Heritage and Arts","978-3-030-77410-3 978-3-030-77411-0","","","https://link.springer.com/10.1007/978-3-030-77411-0_12","This paper proposes a sonification for zCOSMOS, an astronomical dataset that contains information about 20,000 galaxies. The goals of such an initiative are multiple: providing a sound-based description of the dataset in order to make hidden features emerge, hybridizing science with art in a cross-domain framework, and treating scientific data as cultural heritage to be preserved and enhanced, thus breaking down the barriers between scientists and the general audience. In the paper, both technical and artistic aspects of the sonification will be addressed. Finally, some relevant excerpts from the resulting sonification will be presented and discussed.","2021","2023-07-05 06:23:48","2023-07-19 23:53:59","2023-07-05 06:23:48","171-188","","","12794","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-77411-0_12","","/Users/minsik/Zotero/storage/8MV4ARUM/Bardelli et al. - 2021 - A Sonification of the zCOSMOS Galaxy Dataset.pdf","","","","Rauterberg, Matthias","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UGJJXZ32","journalArticle","2020","Aldana Blanco, Andrea Lorena; Grautoff, Steffen; Hermann, Thomas","ECG sonification to support the diagnosis and monitoring of myocardial infarction","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-020-00319-x","http://link.springer.com/10.1007/s12193-020-00319-x","Abstract                            This paper presents the design and evaluation of four sonification methods to support monitoring and diagnosis in Electrocardiography (ECG). In particular we focus on an ECG abnormality called ST-elevation which is an important indicator of a myocardial infarction. Since myocardial infarction represents a life-threatening condition it is of essential value to detect an ST-elevation as early as possible. As part of the evaluated sound designs, we propose two novel sonifications: (i)               Polarity sonification               , a continuous parameter-mapping sonification using a formant synthesizer and (ii)               Stethoscope sonification               , a combination of the ECG signal and a stethoscope recording. The other two designs, (iii) the               water ambience sonification               and the (iv)               morph sonification               , were presented in our previous work about ECG sonification (Aldana Blanco AL, Steffen G, Thomas H (2016) In: Proceedings of Interactive Sonification Workshop (ISon). Bielefeld, Germany). The study evaluates three components across the proposed sonifications (1) detection performance, meaning if participants are able to detect a transition from healthy to unhealthy states, (2) classification accuracy, that evaluates if participants can accurately classify the severity of the pathology, and (3) aesthetics and usability (pleasantness, informativeness and long-term listening). The study results show that the               polarity               design had the highest accuracy rates in the detection task whereas the               stethoscope sonification               obtained the better score in the classification assignment. Concerning aesthetics, the               water ambience sonification               was regarded as the most pleasant. Furthermore, we found a significant difference between sound/music experts and non-experts in terms of the error rates obtained in the detection task using the               morph sonification               and also in the classification task using the               stethoscope sonification               . Overall, the group of experts obtained lower error rates than the group of non-experts, which means that further training could improve accuracy rates and, particularly for designs that rely mainly on pitch variations, additional training is needed in the non-experts group.","2020-06","2023-07-05 06:23:48","2023-07-05 06:23:48","2023-07-05 06:23:48","207-218","","2","14","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/MCSVDJ7A/Aldana Blanco et al. - 2020 - ECG sonification to support the diagnosis and moni.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"926KCWKM","journalArticle","2021","Raglio, Alfredo; Panigazzi, Monica; Colombo, Roberto; Tramontano, Marco; Iosa, Marco; Mastrogiacomo, Sara; Baiardi, Paola; Molteni, Daniele; Baldissarro, Eleonora; Imbriani, Chiara; Imarisio, Chiara; Eretti, Laura; Hamedani, Mehrnaz; Pistarini, Caterina; Imbriani, Marcello; Mancardi, Gian Luigi; Caltagirone, Carlo","Hand rehabilitation with sonification techniques in the subacute stage of stroke","Scientific Reports","","2045-2322","10.1038/s41598-021-86627-y","https://www.nature.com/articles/s41598-021-86627-y","After a stroke event, most survivors suffer from arm paresis, poor motor control and other disabilities that make activities of daily living difficult, severely affecting quality of life and personal independence. This randomized controlled trial aimed at evaluating the efficacy of a music-based sonification approach on upper limbs motor functions, quality of life and pain perceived during rehabilitation. The study involved 65 subacute stroke individuals during inpatient rehabilitation allocated into 2 groups which underwent usual care dayweek) respectively of standard upper extremity motor rehabilitation or upper extremity treatment with sonification techniques. The Fugl-Meyer Upper Extremity Scale, Box and Block Test and the Modified Ashworth Scale were used to perform motor assessment and the McGill Quality of Life-it and the Numerical Pain Rating Scale to assess quality of life and pain. The assessment was performed at baseline, after 2 weeks, at the end of treatment and at follow-up (1 month after the end of treatment). Total scores of the Fugl-Meyer Upper Extremity Scale (primary outcome measure) and hand and wrist sub scores, manual dexterity scores of the affected and unaffected limb in the Box and Block Test, pain scores of the Numerical Pain Rating Scale (secondary outcomes measures) significantly improved in the sonification group compared to the standard of care group (time*group interaction < 0.05). Our findings suggest that music-based sonification sessions can be considered an effective standardized intervention for the upper limb in subacute stroke rehabilitation.","2021-03-31","2023-07-05 06:24:16","2023-07-05 06:24:16","2023-07-05 06:24:16","7237","","1","11","","Sci Rep","","","","","","","","en","2021 The Author(s)","","","","www.nature.com","","Number: 1 Publisher: Nature Publishing Group","","/Users/minsik/Zotero/storage/YUTB3P86/Raglio et al. - 2021 - Hand rehabilitation with sonification techniques i.pdf","","","Health care; Neurology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SXET4A4H","journalArticle","2012","Grond, Florian; Hermann, Thomas","Aesthetic strategies in sonification","AI & SOCIETY","","1435-5655","10.1007/s00146-011-0341-7","https://doi.org/10.1007/s00146-011-0341-7","Sound can be listened to in various ways and with different intentions. Multiple factors influence how and what we perceive when listening to sound. Sonification, the acoustic representation of data, is in essence just sound. It functions as sonification only if we make sure to listen attentively in order to access the abstract information it contains. This is difficult to accomplish since sound always calls the listener’s attention to concrete—whether natural or musical—points of references. Important aspects determining how we listen to sonification are discussed in this paper: elicited sounds, repeated sounds, conceptual sounds, technologically mediated sounds, melodic sounds, familiar sounds, multimodal sounds and vocal sounds. We discuss how these aspects help the listener engage with the sound, but also how they can become points of reference in and of themselves. The various sonic qualities employed in sonification can potentially open but also risk closing doors to the accessibility and perceptibility of the sonified data.","2012-05-01","2023-07-05 06:25:37","2023-07-05 06:25:37","2023-07-05 06:25:37","213-222","","2","27","","AI & Soc","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/GF389U77/Grond and Hermann - 2012 - Aesthetic strategies in sonification.pdf","","","Sonification; Aesthetics; Historic context","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U43J5F85","conferencePaper","2010","Moroni, Artemis; Manzolli, Jônatas","From Evolutionary Composition to Robotic Sonification","Applications of Evolutionary Computation","978-3-642-12242-2","","10.1007/978-3-642-12242-2_41","","A new approach is presented which integrates evolutionary computation and real world devices such as mobile robots and an omnidirectional vision system. Starting with an evolutionary composition system named JaVOX, a hybrid environment named AURAL evolved. In the AURAL, the behavior of mobile robots in an arena is applied as a compositional strategy. It uses trajectories produced by mobile robots to modify the fitness function of a real time sonic composition. The model is described, its evolutionary design and how the interaction between the real world devices was implemented. This research is oriented towards the study of automatic and semi-automatic processes of artistic production in the sound domain.","2010","2023-07-05 06:26:26","2023-07-05 06:26:26","","401-410","","","","","","","Lecture Notes in Computer Science","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/NY27QJAP/Moroni and Manzolli - 2010 - From Evolutionary Composition to Robotic Sonificat.pdf","","","Algorithmic composition; evolutionary computation; robotics; sonification","Di Chio, Cecilia; Brabazon, Anthony; Di Caro, Gianni A.; Ebner, Marc; Farooq, Muddassar; Fink, Andreas; Grahl, Jörn; Greenfield, Gary; Machado, Penousal; O’Neill, Michael; Tarantino, Ernesto; Urquhart, Neil","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B2JYG72S","conferencePaper","2014","Caldwell, Barrett S.; Viraldo, Jacob E.","Improving Control Room State Awareness through Complex Sonification Interfaces","Human Interface and the Management of Information. Information and Knowledge in Applications and Services","978-3-319-07863-2","","10.1007/978-3-319-07863-2_31","","Across a number of complex control room settings, there are concerns regarding operator information overload and alarm flooding. The evolution of control room technological capabilities has accelerated in recent years, due to drastic improvements in computer processing power, speed, and sensor integration. However, aging infrastructure and retiring senior operators in legacy control room systems such as chemical and power generation plants have begun to create opportunities for once-per-generation improvements in control room interface capabilities. Additional facilities, including power grid interconnection centers and computer network security monitoring centers, have created new generations of network operations control centers (NOCs). The authors’ work is emphasizing the development and application of audification and parameter mapping techniques to generate engineering-based principles for presenting state-based auditory information to plant or NOC operators. There are no current systematic engineering-based principles used to apply sonification to control rooms or engineering system states in a clearly standardized way. Our current work in this domain examines the critical parameters that control room operators recognize and monitor in order to get a “sense of the plant” in nominal, degrading, or hazardous states. Principles and parameters for implementing these sonification techniques for power plant and NOC contexts are presented and discussed.","2014","2023-07-05 06:26:47","2023-07-05 06:26:47","","317-323","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/Y6NIA8BN/Caldwell and Viraldo - 2014 - Improving Control Room State Awareness through Com.pdf","","","Audio Signal; Control Room; Nuisance Alarm; Power Generation Plant; State Awareness","Yamamoto, Sakae","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9XUV8PSM","journalArticle","2012","Jones, Stuart","Now? Towards a phenomenology of real time sonification","AI & SOCIETY","","1435-5655","10.1007/s00146-011-0342-6","https://doi.org/10.1007/s00146-011-0342-6","The author examines concepts of real time and real-time in relation to notions of perception and processes of sonification. He explores these relationships in three case studies and suggests that sonification can offer a form of reconciliation between ontology and phenomenology, and between ourselves and the flux we are part of.","2012-05-01","2023-07-05 06:27:33","2023-07-05 06:27:33","2023-07-05 06:27:33","223-231","","2","27","","AI & Soc","Now?","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/DH6BEU34/Jones - 2012 - Now Towards a phenomenology of real time sonifica.pdf","","","Sonification; Attention; Ontology; Perception; Phenomenology; Real time; Real-time","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YRPY3NN9","conferencePaper","2010","Vogt, Katharina; Pirrò, David; Kobenz, Ingo; Höldrich, Robert; Eckel, Gerhard","PhysioSonic - Evaluated Movement Sonification as Auditory Feedback in Physiotherapy","Auditory Display","978-3-642-12439-6","","10.1007/978-3-642-12439-6_6","","We detect human body movement interactively via a tracking system. This data is used to synthesize sound and transform sound files (music or text). A subject triggers and controls sound parameters with his or her movement within a pre-set range of motion. The resulting acoustic feedback enhances new modalities of perception and the awareness of the body movements. It is ideal for application in physiotherapy and other training contexts.","2010","2023-07-05 06:27:56","2023-07-05 06:27:56","","103-120","","","","","","","Lecture Notes in Computer Science","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/8I37BQMK/Vogt et al. - 2010 - PhysioSonic - Evaluated Movement Sonification as A.pdf","","","Auditory Feedback; Augmented Reality; Glenohumeral Joint; Shoulder Abduction; Shoulder Joint","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6JMJPD2G","journalArticle","2022","Sekhavat, Yoones A.; Azadehfar, Mohammad Reza; Zarei, Hossein; Roohi, Samad","Sonification and interaction design in computer games for visually impaired individuals","Multimedia Tools and Applications","","1573-7721","10.1007/s11042-022-11984-3","https://doi.org/10.1007/s11042-022-11984-3","Video games are changing how we interact and communicate with each other. They can provide an authentic and collaborative platform for building new communities and connecting people. Since video games generally rely on visual elements that must be recognized by the players, most of them are not accessible by visually impaired people. In this research, we study the sonification and interaction issues in the design of computer games for visually impaired individuals. We have proposed an audio game called GrandEscape with the focus on the special needs of visually impaired people while playing. A comprehensive set of user studies has been performed to evaluate different interaction and sonification techniques in terms of providing a sense of presence and gaming experience.","2022-03-01","2023-07-05 06:28:23","2023-07-05 06:28:23","2023-07-05 06:28:23","7847-7871","","6","81","","Multimed Tools Appl","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/JXS5LX5S/Sekhavat et al. - 2022 - Sonification and interaction design in computer ga.pdf","","","Sonification; Computer games; Game experience; Interaction design; Visually impaired individuals","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I6DUFMJF","journalArticle","2012","Knees, Peter; Pohle, Tim; Widmer, Gerhard","Sound/tracks: artistic real-time sonification of train journeys","Journal on Multimodal User Interfaces","","1783-8738","10.1007/s12193-011-0089-x","https://doi.org/10.1007/s12193-011-0089-x","We present an application of sonification in an artistic context, namely to augment the visual impressions of train journeys. While sonification and auditory displays are typically used as means to present data and to inform the user, our project sound/tracks aims at enhancing the visual experience of looking out of a moving train’s window at the passing landscape by adding a sound dimension. This allows for reflecting upon the visual impressions and deepening the state of contemplation. To this end, sound/tracks translates the perceived movement of the scenery and other visual impressions, such as passing trains, into music. The continuously changing view outside the window is captured with a camera and transformed into instantaneously played music. The application can be run on mobile phones with a built-in camera and on laptops with a Web-cam. The paper proposes and discusses different sonification approaches and presents different application scenarios.","2012-07-01","2023-07-05 06:29:29","2023-07-05 06:29:29","2023-07-05 06:29:29","87-93","","1","6","","J Multimodal User Interfaces","Sound/tracks","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/FGX4CY5C/Knees et al. - 2012 - Soundtracks artistic real-time sonification of t.pdf","","","Sonification; Mobile music generation; Train journey","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6LTDC8FF","journalArticle","2012","Barrass, Stephen","The aesthetic turn in sonification towards a social and cultural medium","AI & SOCIETY","","1435-5655","10.1007/s00146-011-0335-5","https://doi.org/10.1007/s00146-011-0335-5","The public release of datasets on the internet by government agencies, environmental scientists, political groups and many other organizations has fostered a social practice of data visualization. The audiences have expectations of production values commensurate with their daily experience of professional visual media. At the same time, access to this data has allowed visual designers and artists to apply their skills to what was previously a field dominated by scientists and engineers. The ‘aesthetic turn’ in data visualization has sparked debates between the new wave and older more scientifically grounded schools of thought on the topic. Sonification is not as well known or commonly practiced as visualization. But sound is a naturally affective, aesthetic and cultural medium. The extension of the aesthetic turn to sonification could transform this field from a scientific curiosity and engineering instrument into a popular mass medium. This paper proposes that a design approach can facilitate an aesthetic turn in sonification that integrates aesthetics and functionality by dissolving divisions between scientific and artistic methods. The first section applies the design perspective to the definition of sonification by replacing the linguistic concept of representation with non-verbal concept of functionality. The next section describes applications of the TaDa design method that raised aesthetic issues particular to sonification practice. The final section proposes a pragmatic aesthetics that distinguishes sonification from the auditory sciences and sonic arts. A design perspective may lead to a future where the general public tunes into pop sonifications for listening enjoyment as well as useful information about the world.","2012-05-01","2023-07-05 06:29:51","2023-07-05 06:29:51","2023-07-05 06:29:51","177-181","","2","27","","AI & Soc","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/P5SFUM3Y/Barrass - 2012 - The aesthetic turn in sonification towards a socia.pdf","","","Sonification; Data aesthetics; Information design; Popular culture","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TWSEYJ4Y","conferencePaper","2014","Parkinson, Adam; Tanaka, Atau","Making Data Sing: Embodied Approaches to Sonification","Sound, Music, and Motion","978-3-319-12976-1","","10.1007/978-3-319-12976-1_9","","We report on our experiences of two recent sonification pieces we have been involved in; composing for Peter Sinclair’s RoadMusic project, and creating a multi-speaker installation with sound artist Kaffe Matthews which sonifies the movement of sharks around the Galapagos islands.","2014","2023-07-05 06:39:23","2023-07-05 06:39:23","","151-160","","","","","","Making Data Sing","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/VBQ35YA5/Parkinson and Tanaka - 2014 - Making Data Sing Embodied Approaches to Sonificat.pdf","","","Sonification; Embodiment; Affect; Enaction","Aramaki, Mitsuko; Derrien, Olivier; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2G3YFMP2","bookSection","2019","Sanchez, Eric; Sanchez, Théophile","Let’s Listen to the Data: Sonification for Learning Analytics","Advances in Quantitative Ethnography","978-3-030-33231-0 978-3-030-33232-7","","","http://link.springer.com/10.1007/978-3-030-33232-7_16","This paper falls in the field of playing analytics. It deals with an empirical work dedicated to explore the potential of data sonification (i.e. the conversion of data into sound that reflects their objective properties or relations). Data sonification is proposed as an alternative to data visualization. We applied data sonification for the analysis of gameplays and players’ strategies during a session dedicated to game-based learning. The data of our study (digital traces) was collected from 200 pre-service teachers who played Tamagocours, an online collaborative multiplayer game dedicated to learn the rules (ie. copyright) that comply with the policies for the use of digital resources in an educational context. For one typical individual (parangon) for each of the 5 categories of players, the collected digital traces were converted into an audio format so that the actions that they performed become listenable. A specific software, SOnification of DAta for Learning Analytics (SODA4LA), was developed for this purpose. The first results show that different features of the data can be recognized from data listening. These results also enable for the identification of different parameters that should be taken into account for the sonification of diachronic data. We consider that this study open new perspectives for playing analytics. Thus we advocate for new research aiming at exploring the potential of data sonification for the analysis of complex and diachronic datasets in the field of educational sciences.","2019","2023-07-05 06:41:23","2023-07-19 11:12:57","2023-07-05 06:41:23","189-198","","","1112","","","Let’s Listen to the Data","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Communications in Computer and Information Science DOI: 10.1007/978-3-030-33232-7_16","","/Users/minsik/Zotero/storage/2TCK385L/Sanchez and Sanchez - 2019 - Let’s Listen to the Data Sonification for Learnin.pdf","","","","Eagan, Brendan; Misfeldt, Morten; Siebert-Evenstone, Amanda","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UAUS8448","bookSection","2013","Elgendi, Mohamed; Rebsamen, Brice; Cichocki, Andrzej; Vialatte, Francois; Dauwels, Justin","Real-Time Wireless Sonification of Brain Signals","Advances in Cognitive Neurodynamics (III)","978-94-007-4791-3 978-94-007-4792-0","","","https://link.springer.com/10.1007/978-94-007-4792-0_24","In this paper, an alternative representation of EEG is investigated, in particular, translation of EEG into sound; patterns in the EEG then correspond to sequences of notes. The aim is to provide an alternative tool for analysing and exploring brain signals, e.g., for diagnosis of neurological diseases. Specifically, a system is proposed that transforms EEG signals, recorded by a wireless headset, into sounds in real-time. In order to assess the resulting representation of EEG as sounds, the proposed sonification system is applied to EEG signals of Alzheimer’s (AD) patients and healthy age-matched control subjects (recorded by a high-quality wired EEG system). Fifteen volunteers were asked to classify the sounds generated from the EEG of 5 AD patients and 5 healthy subjects; the volunteers labeled most sounds correctly, in particular, an overall sensitivity and specificity of 93.3% and 97.3% respectively was obtained, suggesting that the sound sequences generated by the sonification system contain relevant information about EEG signals and underlying brain activity.","2013","2023-07-05 06:41:23","2023-07-19 11:10:15","2023-07-05 06:41:23","175-181","","","","","","","","","","","Springer Netherlands","Dordrecht","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-94-007-4792-0_24","","/Users/minsik/Zotero/storage/M7WRZR8D/Elgendi et al. - 2013 - Real-Time Wireless Sonification of Brain Signals.pdf","","","","Yamaguchi, Yoko","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LK36ZABP","journalArticle","2017","Dyer, J. F.; Stapleton, P.; Rodger, M. W. M.","Advantages of melodic over rhythmic movement sonification in bimanual motor skill learning","Experimental Brain Research","","0014-4819, 1432-1106","10.1007/s00221-017-5047-8","http://link.springer.com/10.1007/s00221-017-5047-8","n important question for skill acquisition is whether and how augmented feedback can be designed to improve the learning of complex skills. Auditory information triggered by learners’ actions, movement sonification, can enhance learning of a complex bimanual coordination skill, specifically polyrhythmic bimanual shape tracing. However, it is not clear whether the coordination of polyrhythmic sequenced movements is enhanced by auditory-specified timing information alone or whether more complex sound mappings, such as melodic sonification, are necessary. Furthermore, while short-term retention of bimanual coordination performance has been shown with movement sonification training, longer term retention has yet to be demonstrated. In the present experiment, participants learned to trace a diamond shape with one hand while simultaneously tracing a triangle with the other to produce a sequenced 4:3 polyrhythmic timing pattern. Two groups of participants received real-time auditory feedback during training: melodic sonification (individual movements triggered a separate note of a melody) and rhythmic sonification (each movement triggered a percussive sound), while a third control group received no augmented feedback. Task acquisition and performance in immediate retention were superior in the melodic sonification group as compared to the rhythmic sonification and control group. In a 24-h retention phase, a decline in performance in the melodic sonification group was reversed by brief playback of the target pattern melody. These results show that melodic sonification of movement can provide advantages over augmented feedback which only provides timing information by better structuring the sequencing of timed actions, and also allow recovery of complex target patterns of movement after training. These findings have important implications for understanding the role of augmented perceptual information in skill learning, as well as its application to real-world training or rehabilitation scenarios.","2017-10","2023-07-05 06:41:23","2023-07-20 00:05:08","2023-07-05 06:41:23","3129-3140","","10","235","","Exp Brain Res","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/4J6NBL9Q/Dyer et al. - 2017 - Advantages of melodic over rhythmic movement sonif.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3BV5ABI5","bookSection","2014","Jeon, Myounghoon; Smith, Michael T.; Walker, James W.; Kuhl, Scott A.","Constructing the Immersive Interactive Sonification Platform (iISoP)","Distributed, Ambient, and Pervasive Interactions","978-3-319-07787-1 978-3-319-07788-8","","","http://link.springer.com/10.1007/978-3-319-07788-8_32","For decades, researchers have spurred research on sonification, the use of non-speech audio to convey information [1]. With ‘interaction’ and ‘user experience’ being pervasive, interactive sonification [2], an emerging interdisciplinary area, has been introduced and its role and importance have rapidly increased in the auditory display community. From this background, we have devised a novel platform, “iISoP” (immersive Interactive Sonification Platform) for location, movement, and gesture-based interactive sonification research, by leveraging the existing Immersive Visualization Studio (IVS) at Michigan Tech. Projects in each developmental phase and planned research are discussed with a focus on “design research” and “interactivity”.","2014","2023-07-05 06:41:23","2023-07-19 23:58:01","2023-07-05 06:41:23","337-348","","","8530","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-07788-8_32","","/Users/minsik/Zotero/storage/Z3ZR3FLB/Jeon et al. - 2014 - Constructing the Immersive Interactive Sonificatio.pdf","","","","Streitz, Norbert; Markopoulos, Panos","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Kobsa, Alfred; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Doug; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y63M8222","journalArticle","2019","Frid, Emma; Elblaus, Ludvig; Bresin, Roberto","Interactive sonification of a fluid dance movement: an exploratory study","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-018-0278-y","http://link.springer.com/10.1007/s12193-018-0278-y","In this paper we present three different experiments designed to explore sound properties associated with fluid movement: (1) an experiment in which participants adjusted parameters of a sonification model developed for a fluid dance movement, (2) a vocal sketching experiment in which participants sketched sounds portraying fluid versus nonfluid movements, and (3) a workshop in which participants discussed and selected fluid versus nonfluid sounds. Consistent findings from the three experiments indicated that sounds expressing fluidity generally occupy a lower register and has less high frequency content, as well as a lower bandwidth, than sounds expressing nonfluidity. The ideal sound to express fluidity is continuous, calm, slow, pitched, reminiscent of wind, water or an acoustic musical instrument. The ideal sound to express nonfluidity is harsh, non-continuous, abrupt, dissonant, conceptually associated with metal or wood, unhuman and robotic. Findings presented in this paper can be used as design guidelines for future applications in which the movement property fluidity is to be conveyed through sonification.","2019-09","2023-07-05 06:41:23","2023-07-20 06:59:08","2023-07-05 06:41:23","181-189","","3","13","","J Multimodal User Interfaces","Interactive sonification of a fluid dance movement","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/L7SQJ5M6/Frid et al. - 2019 - Interactive sonification of a fluid dance movement.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8MX67HT3","bookSection","2009","Kildal, Johan","Aspects of Auditory Perception and Cognition for Usable Display Resolution in Data Sonification","Human-Computer Interaction – INTERACT 2009","978-3-642-03654-5 978-3-642-03655-2","","","http://link.springer.com/10.1007/978-3-642-03655-2_52","Sonification of data via the mapping of values to frequency of sound is an auditory data analysis technique commonly used to display graph information. The goal for any form of graph is to display numerical information with accuracy and neutrality while exploiting perceptual and cognitive processes. Conveying information in frequency of sound is subject to aspects of pitch perception, largely overlooked to date, that can influence these properties of auditory graphing. This paper identifies some of these aspects and describes potential design limitations and opportunities derived from the musical nature of auditory data representations.","2009","2023-07-05 06:41:23","2023-07-20 06:28:35","2023-07-05 06:41:23","467-470","","","5726","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-03655-2_52","","/Users/minsik/Zotero/storage/PMTUSZDX/Kildal - 2009 - Aspects of Auditory Perception and Cognition for U.pdf","","","","Gross, Tom; Gulliksen, Jan; Kotzé, Paula; Oestreicher, Lars; Palanque, Philippe; Prates, Raquel Oliveira; Winckler, Marco","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B9Y9JEGN","journalArticle","2021","Cibrian, Franceli L.; Ley-Flores, Judith; Newbold, Joseph W.; Singh, Aneesha; Bianchi-Berthouze, Nadia; Tentori, Monica","Interactive sonification to assist children with autism during motor therapeutic interventions","Personal and Ubiquitous Computing","","1617-4909, 1617-4917","10.1007/s00779-020-01479-z","http://link.springer.com/10.1007/s00779-020-01479-z","","2021-04","2023-07-05 06:41:23","2023-07-05 06:41:23","2023-07-05 06:41:23","391-410","","2","25","","Pers Ubiquit Comput","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/WQ7G2XJM/Cibrian et al. - 2021 - Interactive sonification to assist children with a.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U93G3DYK","journalArticle","2021","Martin, Edward J.; Meagher, Thomas R.; Barker, Daniel","Using sound to understand protein sequence data: new sonification algorithms for protein sequences and multiple sequence alignments","BMC Bioinformatics","","1471-2105","10.1186/s12859-021-04362-7","https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-021-04362-7","Abstract                            Background               The use of sound to represent sequence data—sonification—has great potential as an alternative and complement to visual representation, exploiting features of human psychoacoustic intuitions to convey nuance more effectively. We have created five parameter-mapping sonification algorithms that aim to improve knowledge discovery from protein sequences and small protein multiple sequence alignments. For two of these algorithms, we investigated their effectiveness at conveying information. To do this we focussed on subjective assessments of user experience. This entailed a focus group session and survey research by questionnaire of individuals engaged in bioinformatics research.                                         Results               For single protein sequences, the success of our sonifications for conveying features was supported by both the survey and focus group findings. For protein multiple sequence alignments, there was limited evidence that the sonifications successfully conveyed information. Additional work is required to identify effective algorithms to render multiple sequence alignment sonification useful to researchers. Feedback from both our survey and focus groups suggests future directions for sonification of multiple alignments: animated visualisation indicating the column in the multiple alignment as the sonification progresses, user control of sequence navigation, and customisation of the sound parameters.                                         Conclusions               Sonification approaches undertaken in this work have shown some success in conveying information from protein sequence data. Feedback points out future directions to build on the sonification approaches outlined in this paper. The effectiveness assessment process implemented in this work proved useful, giving detailed feedback and key approaches for improvement based on end-user input. The uptake of similar user experience focussed effectiveness assessments could also help with other areas of bioinformatics, for example in visualisation.","2021-12","2023-07-05 06:41:23","2023-07-05 06:41:23","2023-07-05 06:41:23","456","","1","22","","BMC Bioinformatics","Using sound to understand protein sequence data","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/ET57DLSC/Martin et al. - 2021 - Using sound to understand protein sequence data n.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I7XHBE4B","journalArticle","2017","Dyer, John; Stapleton, Paul; Rodger, Matthew","Transposing musical skill: sonification of movement as concurrent augmented feedback enhances learning in a bimanual task","Psychological Research","","0340-0727, 1430-2772","10.1007/s00426-016-0775-0","http://link.springer.com/10.1007/s00426-016-0775-0","Concurrent feedback provided during acquisition can enhance performance of novel tasks. The ‘guidance hypothesis’ predicts that feedback provision leads to dependence and poor performance in its absence. However, appropriately structured feedback information provided through sound (‘sonification’) may not be subject to this effect. We test this directly using a rhythmic bimanual shape-tracing task in which participants learned to move at a 4:3 timing ratio. Sonification of movement and demonstration was compared to two other learning conditions: (1) Sonification of task demonstration alone and (2) completely silent practice (control). Sonification of movement emerged as the most effective form of practice, reaching significantly lower error scores than control. Sonification of solely the demonstration, which was expected to benefit participants by perceptually unifying task requirements, did not lead to better performance than control. Good performance was maintained by participants in the Sonification condition in an immediate retention test without feedback, indicating that the use of this feedback can overcome the guidance effect. On a 24-h retention test, performance had declined and was equal between groups. We argue that this and similar findings in the feedback literature are best explained by an ecological approach to motor skill learning which places available perceptual information at the highest level of importance.","2017-07","2023-07-05 06:41:23","2023-07-21 04:55:56","2023-07-05 06:41:23","850-862","","4","81","","Psychological Research","Transposing musical skill","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/MCNFGW3Y/Dyer et al. - 2017 - Transposing musical skill sonification of movemen.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KIR6M7J3","journalArticle","2023","Toffa, O. K.; Mignotte, M.","Dataset and semantic based-approach for image sonification","Multimedia Tools and Applications","","1573-7721","10.1007/s11042-022-12914-z","https://doi.org/10.1007/s11042-022-12914-z","This paper presents an image-audio dataset and a mid-level image sonification system that strives to help visually impaired users understand the semantic content of an image and access visual information via a combination of semantic audio and an easily decodable audio generated in real time, both triggered by sliding, taping, holding actions when the users explore the image on a touch screen or with a pointer. Firstly, we segmented the original image using a label fusion model and based on the user position in the image, a sonified signal is generated using musical notes and meaningful visual information within the active region like the color and the luminance, then the gradient and the texture. Secondly, we integrated the semantic understanding of the image into our model using DeepLab semantic segmentation of the image and created a dataset of audio and images aligned on the 20 classes of the PASCAL VOC 2012 dataset. The dataset of images are organized based on color, gradient, texture for low-level sonification and on semantic content with sounds for mid-level sonification. Thirdly, in order to provide both types of information in a complementary way, the slide, tap and hold actions of a touch screen are incorporated in the model. The semantic audio providing a brief description of the visual object is played on slide action, the generated signal with color details of the object on the tap action, gradient and texture of the object on hold action. Finally, we validated our sonification model on the provided dataset during a pilot study and the subjects were generally able to identify the objects in the image, the color of the objects and even provide a general description of the scene of the image. Our system could be useful to visually impaired persons in a photo sharing application using a smartphone or for painting art description in a digital museum.","2023-01-01","2023-07-05 06:44:22","2023-07-05 06:44:22","2023-07-05 06:44:22","1505-1518","","1","82","","Multimed Tools Appl","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/3UCEFJA2/Toffa and Mignotte - 2023 - Dataset and semantic based-approach for image soni.pdf","","","Sonification; Auditory feedback; Image accessibility; Touch screen; Visually impaired","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BQRM58EN","conferencePaper","2019","Rogozinsky, Gleb G.; Lyzhinkin, Konstantin; Egorova, Anna; Podolsky, Dmitry","Distributed Software Hardware Solution for Complex Network Sonification","Language, Music and Computing","978-3-030-05594-3","","10.1007/978-3-030-05594-3_12","","The paper presents the hardware software solution for sonification of complex networks and systems. The sonification expands the possibilities of an analysis of complex information through using the human hearing. Auditory displays allow reducing the operator’s workload and better detection of specific features and patterns in the data. The proposed sonification complex consists of two main parts. Data source located in the LO ZNIIS generates data that describes current state of a network or complex system, accumulates and redirects it. The parametric sonification layer located in the SUT converts the information into forms suitable for creating new audio environment, and represents the data as relevant timbral classes.","2019","2023-07-05 06:44:45","2023-07-05 06:44:45","","149-160","","","","","","","Communications in Computer and Information Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","","","","Sonification; Big data; Csound","Eismont, Polina; Mitrenina, Olga; Pereltsvaig, Asya","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RQVQLT39","conferencePaper","2021","Goudarzi, Visda","Exploring a Taxonomy of Interaction in Interactive Sonification Systems","Human Interaction, Emerging Technologies and Future Applications III","978-3-030-55307-4","","10.1007/978-3-030-55307-4_22","","This paper explores a variety of existing interactive sonification systems in the context of interactive sound art. In design of interactive sonification from technological standpoint, the stress is put on studying the usability and functionality of the systems. We explore the focus towards creative aspects of interaction in both technology development and sound creation stages. In some artistic sonifications, the control is in the hand of the technology creators, in some others in the hand of the artists, and sometimes in the hand of the performers or the audience members. The numerous relations and interactions between performers, composers, technologists, data domain scientists, environment and audiences make it difficult to classify the complex phenomenon of interactive sonification. Some challenges in such systems are the ownership of technical and aesthetic components, balancing engagement and interaction among different stakeholders (domain scientist, designer, composer, spectator, etc.) and encouraging audience engagement.","2021","2023-07-05 06:45:57","2023-07-05 06:45:57","","140-145","","","","","","","Advances in Intelligent Systems and Computing","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","","","","Sonification; Interactive sonification; Auditory display; Human computer interaction; Parameter mapping","Ahram, Tareq; Taiar, Redha; Langlois, Karine; Choplin, Arnaud","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G8Y3KNXM","conferencePaper","2010","Grond, Florian; Hermann, Thomas; Verfaille, Vincent; Wanderley, Marcelo M.","Methods for Effective Sonification of Clarinetists’ Ancillary Gestures","Gesture in Embodied Communication and Human-Computer Interaction","978-3-642-12553-9","","10.1007/978-3-642-12553-9_15","","We present the implementation of two different sonifications methods of ancillary gestures from clarinetists. The sonifications are data driven from the clarinetist’s posture which is captured with a VICON motion tracking system. The first sonification method is based on the velocities of the tracking markers, the second method involves a principal component analysis as a data preprocessing step. Further we develop a simple complementary visual display with a similar information content to match the sonification. The effect of the two sonifications with respect to the movement perception is studied in an experiment where test subjects annotate the clarinetists performance represented by various combinations of the resulting uni- and multimodal displays.","2010","2023-07-05 06:46:23","2023-07-05 06:46:23","","171-181","","","","","","","Lecture Notes in Computer Science","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/NGTL3XIJ/Grond et al. - 2010 - Methods for Effective Sonification of Clarinetists.pdf","","","sonification; 3D movement data; ancillary gestures; multimodal displays","Kopp, Stefan; Wachsmuth, Ipke","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GPL5NHMU","conferencePaper","2010","Ramakrishnan, Chandrasekhar","Sonification and Information Theory","Auditory Display","978-3-642-12439-6","","10.1007/978-3-642-12439-6_7","","We apply tools from the mathematical theory of information to the problem of sonification design. This produces entropy sonification, a technique for sonification, as well as a framework for analyzing and understanding sonifications in general.","2010","2023-07-05 06:47:43","2023-07-05 06:47:43","","121-142","","","","","","","Lecture Notes in Computer Science","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/8W6DI7BS/Ramakrishnan - 2010 - Sonification and Information Theory.pdf","","","Granular Synthesis; Information Theory; Interaction Design; Sonification Design","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NT7FBCKS","conferencePaper","2022","Vishnevsky, Andrey; Abbas, Nadezda","Sonification of Information Security Incidents in an Organization Using a Multistep Cooperative Game Model","Information Systems and Technologies","978-3-031-04826-5","","10.1007/978-3-031-04826-5_30","","This work is devoted to the development of computer attacks detection tool with a sound interface. Information security tools transmit visual signals to characterize the behavior of violators, but various types of conflict processes could be expressed by the plot of musical compositions. This approach could be used to encode the interaction of a protective computer system with an attacker in a harmonious way. In presented work, as an example of the conflict situation, was used a stochastic multistep cooperative game between the units of the attacked organization. An attempt to express audibly the stability of the cooperative game is made. It was realized with the help of the musical harmony of several voices of musical instruments. The program code for calculating the positional consistency of the proposed game-theoretic model and fragments of musical compositions for voicing the state of the protected organization are also proposed. Combining the basics of musical composition with game-theoretic modeling could offer a number of new possibilities for creating an ergonomic auditory human-computer interfaces.","2022","2023-07-05 06:48:25","2023-07-05 06:48:25","","306-314","","","","","","","Lecture Notes in Networks and Systems","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/9KWZDGZ6/Vishnevsky and Abbas - 2022 - Sonification of Information Security Incidents in .pdf","","","Sonification; Music; Visually impaired; Assistive technologies; Computer attacks; Training","Rocha, Alvaro; Adeli, Hojjat; Dzemyda, Gintautas; Moreira, Fernando","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SUVYWF5N","conferencePaper","2006","Vialatte, François B.; Cichocki, Andrzej","Sparse Bump Sonification: A New Tool for Multichannel EEG Diagnosis of Mental Disorders; Application to the Detection of the Early Stage of Alzheimer’s Disease","Neural Information Processing","978-3-540-46485-3","","10.1007/11893295_11","","This paper investigates the use of sound and music as a means of representing and analyzing multichannel EEG recordings. Specific focus is given to applications in early detection and diagnosis of early stage of Alzheimer’s disease. We propose here a novel approach based on multi channel sonification, with a time-frequency representation and sparsification process using bump modeling. The fundamental question explored in this paper is whether clinically valuable information, not available from the conventional graphical EEG representation, might become apparent through an audio representation. Preliminary evaluation of the obtained music score – by sample entropy, number of notes, and synchronous activity – incurs promising results.","2006","2023-07-05 06:49:00","2023-07-05 06:49:00","","92-101","","","","","","Sparse Bump Sonification","Lecture Notes in Computer Science","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/Y86ZA4JD/Vialatte and Cichocki - 2006 - Sparse Bump Sonification A New Tool for Multichan.pdf","","","Blind Source Separation; Brain Computer Interface; Independent Component Analysis; Sample Entropy","King, Irwin; Wang, Jun; Chan, Lai-Wan; Wang, DeLiang","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CUJ2GM5H","conferencePaper","2015","Jeon, Myounghoon; Landry, Steven; Ryan, Joseph D.; Walker, James W.","Technologies Expand Aesthetic Dimensions: Visualization and Sonification of Embodied Penwald Drawings","Arts and Technology","978-3-319-18836-2","","10.1007/978-3-319-18836-2_9","","Even though defining art gets more and more difficult, reintegrating art and technology seems to be a clear trend. The present paper aims to show how technologies can expand aesthetic dimensions of art works. Michigan Tech researchers collaborated with a world-renowned artist, Tony Orrico in the immersive virtual environment. While he performed, multiple cameras tracked his body movements and physiological devices logged his biosignals (respiration, heart rate, etc.). Then, the system translated the data into visualization and sonification. Incremental aesthetic dimensions (representation-performance, 2d−3d, outside-inside) obtained based on this art-technology collaboration are discussed with research in progress.","2015","2023-07-05 06:49:19","2023-07-05 06:49:19","","69-76","","","","","","Technologies Expand Aesthetic Dimensions","Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","","","","Interactive sonification; Aesthetic computing; Digital aesthetics; Embodied drawing; Performing arts; Visualization","Brooks, Anthony Lewis; Ayiter, Elif; Yazicigil, Onur","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BVGPA37N","journalArticle","1999","Barrass, Stephen; Kramer, Gregory","Using sonification","Multimedia Systems","","1432-1882","10.1007/s005300050108","https://doi.org/10.1007/s005300050108","The idea behind sonification is that synthetic non-verbal sounds can represent numerical data and provide support for information processing activities of many different kinds. This article describes some of the ways that sonification has been used in assistive technologies, remote collaboration, engineering analyses, scientific visualisations, emergency services and aircraft cockpits. Approaches for designing sonifications are surveyed, and issues raised by the existing approaches and applications are outlined. Relations are drawn to other areas of knowledge where similar issues have also arisen, such as human-computer interaction, scientific visualisation, and computer music. At the end is a list of resources that will help you delve further into the topic.","1999-01-01","2023-07-05 06:50:03","2023-07-05 06:50:03","2023-07-05 06:50:03","23-31","","1","7","","Multimedia Systems","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/AR3CZMUM/Barrass and Kramer - 1999 - Using sonification.pdf","","","Key words:Sonification – Visualisation – Multi-modal – Multimedia – Perceptual display – Human-computer interaction – Information design – Auditory display","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D9QD8HZH","journalArticle","2019","Rönnberg, Niklas","Sonification supports perception of brightness contrast","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-019-00311-0","http://link.springer.com/10.1007/s12193-019-00311-0","In complex visual representations, there are several possible challenges for the visual perception that might be eased by adding sound as a second modality (i.e. sonification). It was hypothesized that sonification would support visual perception when facing challenges such as simultaneous brightness contrast or the Mach band phenomena. This hypothesis was investigated with an interactive sonification test, yielding objective measures (accuracy and response time) as well as subjective measures of sonification benefit. In the test, the participant’s task was to mark the vertical pixel line having the highest intensity level. This was done in a condition without sonification and in three conditions where the intensity level was mapped to different musical elements. The results showed that there was a benefit of sonification, with higher accuracy when sonification was used compared to no sonification. This result was also supported by the subjective measurement. The results also showed longer response times when sonification was used. This suggests that the use and processing of the additional information took more time, leading to longer response times but also higher accuracy. There were no differences between the three sonification conditions.","2019-12","2023-07-05 06:51:38","2023-07-20 06:56:56","2023-07-05 06:51:38","373-381","","4","13","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/D3CEBA5Y/Rönnberg - 2019 - Sonification supports perception of brightness con.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2VYJUSZX","bookSection","2015","Walker, James; Smith, Michael T.; Jeon, Myounghoon","Interactive Sonification Markup Language (ISML) for Efficient Motion-Sound Mappings","Human-Computer Interaction: Interaction Technologies","978-3-319-20915-9 978-3-319-20916-6","","","http://link.springer.com/10.1007/978-3-319-20916-6_36","Despite rapid growth of research on auditory display and sonification mapping per se, there has been little effort on efficiency or accessibility of the mapping process. In order to expedite variations on sonification research configurations, we have developed the Interactive Sonification Markup Language (ISML). ISML is designed within the context of the Immersive Interactive Sonification Platform (iISoP) at Michigan Technological University. We present an overview of the system, the motivation for developing ISML, and the time savings realized through its development. We then discuss the features of ISML and its accompanying graphical editor, and conclude by summarizing the system’s feature development and future plans for its further enhancement. ISML is expected to decrease repetitive development tasks for multiple research studies and to increase accessibility to diverse sonification researchers who do not have programming experience.","2015","2023-07-05 06:51:38","2023-07-20 06:30:06","2023-07-05 06:51:38","385-394","","","9170","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-20916-6_36","","/Users/minsik/Zotero/storage/ZUDWX2M3/Walker et al. - 2015 - Interactive Sonification Markup Language (ISML) fo.pdf","","","","Kurosu, Masaaki","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZQSPG7MZ","journalArticle","2021","Plaisier, Heleen; Meagher, Thomas R.; Barker, Daniel","DNA sonification for public engagement in bioinformatics","BMC Research Notes","","1756-0500","10.1186/s13104-021-05685-7","https://bmcresnotes.biomedcentral.com/articles/10.1186/s13104-021-05685-7","Abstract                            Objective               Visualisation methods, primarily color-coded representation of sequence data, have been a predominant means of representation of DNA data. Algorithmic conversion of DNA sequence data to sound—sonification—represents an alternative means of representation that uses a different range of human sensory perception. We propose that sonification has value for public engagement with DNA sequence information because it has potential to be entertaining as well as informative. We conduct preliminary work to explore the potential of DNA sequence sonification in public engagement with bioinformatics. We apply a simple sonification technique for DNA, in which each DNA base is represented by a specific note. Additionally, a beat may be added to indicate codon boundaries or for musical effect. We report a brief analysis from public engagement events we conducted that featured this method of sonification.                                         Results               We report on use of DNA sequence sonification at two public events. Sonification has potential in public engagement with bioinformatics, both as a means of data representation and as a means to attract audience to a drop-in stand. We also discuss further directions for research on integration of sonification into bioinformatics public engagement and education.","2021-12","2023-07-05 06:51:38","2023-07-05 06:51:38","2023-07-05 06:51:38","273","","1","14","","BMC Res Notes","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/FREXN75R/Plaisier et al. - 2021 - DNA sonification for public engagement in bioinfor.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LEYNHUM5","bookSection","2017","Sousa, Luís; Pinto, António","MuSec: Sonification of Alarms Generated by a SIEM","Ambient Intelligence– Software and Applications – 8th International Symposium on Ambient Intelligence (ISAmI 2017)","978-3-319-61117-4 978-3-319-61118-1","","","http://link.springer.com/10.1007/978-3-319-61118-1_5","The information generated by a network monitoring system is overwhelming. Monitoring is imperative but very difficult to accomplish due to several reasons. More so for the case of non tech-savvy home users. Security Information Event Management applications generate alarms that correlate multiple occurrences on the network. These events are classified accordingly to their risk. An application that allows the sonification of events generated by a Security Information Event Management can facilitate the security monitoring of a home network by a less tech-savvy user by allowing him to just listen to the result of the sonification of such events.","2017","2023-07-05 06:51:38","2023-07-19 11:28:40","2023-07-05 06:51:38","32-39","","","615","","","MuSec","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Advances in Intelligent Systems and Computing DOI: 10.1007/978-3-319-61118-1_5","","/Users/minsik/Zotero/storage/68FWBI8B/Sousa and Pinto - 2017 - MuSec Sonification of Alarms Generated by a SIEM.pdf","","","","De Paz, Juan F.; Julián, Vicente; Villarrubia, Gabriel; Marreiros, Goreti; Novais, Paulo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YDGXY444","bookSection","2021","Denjean, Sébastien; Kronland-Martinet, Richard; Roussarie, Vincent; Ystad, Sølvi","Zero-Emission Vehicles Sonification Strategy Based on Shepard-Risset Glissando","Perception, Representations, Image, Sound, Music","978-3-030-70209-0 978-3-030-70210-6","","","http://link.springer.com/10.1007/978-3-030-70210-6_46","In this paper we present a sonification strategy developed for electric vehicles aiming to synthetize a new engine sound to enhance the driver’s dynamic perception of his vehicle. We chose to mimic the internal combustion engine (ICE) noise by informing the driver through pitch variations. However, ICE noise pitch variations are correlated to the engine’s rotations per minute (RPM) and its dynamics is covered within a limited vehicle speed range. In order to inform the driver with a significant pitch variation throughout the full vehicle speed range, we based our sonification strategy on the Shepard-Risset glissando. These illusory infinite ascending/descending sounds enable to represent accelerations with significant pitch variations for an unlimited range of speeds. In a way, we stay within the metaphor of ICE noise with unheard gearshifts. We tested this sonification strategy in a perceptual test in a driving simulator and showed that the mapping of this acoustical feedback affects the drivers’ perception of vehicle dynamics.","2021","2023-07-05 06:51:38","2023-07-21 04:45:13","2023-07-05 06:51:38","709-724","","","12631","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-70210-6_46","","/Users/minsik/Zotero/storage/YMQKYYI7/Denjean et al. - 2021 - Zero-Emission Vehicles Sonification Strategy Based.pdf","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Aramaki, Mitsuko","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5DE4YYA5","journalArticle","2012","Kronland-Martinet, Richard; Ystad, Sølvi; Aramaki, Mitsuko","High-level control of sound synthesis for sonification processes","AI & SOCIETY","","0951-5666, 1435-5655","10.1007/s00146-011-0340-8","http://link.springer.com/10.1007/s00146-011-0340-8","Methods of sonification based on the design and control of sound synthesis is presented in this paper. The semiotics of isolated sounds was evidenced by performing fundamental studies using a combined acoustical and brain imaging (event-related potentials) approach. The perceptual cues (which are known as invariants) responsible for the evocations elicited by the sounds generated by impacts, moving sound sources, dynamic events and vehicles (car-door closing and car engine noise) were then identified based on physical and perceptual considerations. Lastly, some examples of the high-level control of a synthesis process simulating immersive 3-D auditory scenes, interacting objects and evoked dynamics are presented.","2012-05","2023-07-05 06:51:38","2023-07-19 11:22:28","2023-07-05 06:51:38","245-255","","2","27","","AI & Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TQECEVAU","journalArticle","2019","Niewiadomski, Radoslaw; Mancini, Maurizio; Cera, Andrea; Piana, Stefano; Canepa, Corrado; Camurri, Antonio","Does embodied training improve the recognition of mid-level expressive movement qualities sonification?","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-018-0284-0","http://link.springer.com/10.1007/s12193-018-0284-0","This research is a part of a broader project exploring how movement qualities can be recognized by means of the auditory channel: can we perceive an expressive full-body movement quality by means of its interactive sonification? The paper presents a sonification framework and an experiment to evaluate if embodied sonic training (i.e., experiencing interactive sonification of your own body movements) increases the recognition of such qualities through the auditory channel only, compared to a non-embodied sonic training condition. We focus on the sonification of two mid-level movement qualities: fragility and lightness. We base our sonification models, described in the first part, on the assumption that specific compounds of spectral features of a sound can contribute to the cross-modal perception of a specific movement quality. The experiment, described in the second part, involved 40 participants divided into two groups (embodied sonic training vs. no training). Participants were asked to report the level of lightness and fragility they perceived in 20 audio stimuli generated using the proposed sonification models. Results show that (1) both expressive qualities were correctly recognized from the audio stimuli, (2) a positive effect of embodied sonic training was observed for fragility but not for lightness. The paper is concluded by the description of the artistic performance that took place in 2017 in Genoa (Italy), in which the outcomes of the presented experiment were exploited.","2019-09","2023-07-05 06:51:38","2023-07-20 07:03:37","2023-07-05 06:51:38","191-203","","3","13","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/FPG6QPWL/Niewiadomski et al. - 2019 - Does embodied training improve the recognition of .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7VMSUXXG","journalArticle","2022","Misdariis, N.; Özcan, E.; Grassi, M.; Pauletto, S.; Barrass, S.; Bresin, R.; Susini, P.","Sound experts’ perspectives on astronomy sonification projects","Nature Astronomy","","2397-3366","10.1038/s41550-022-01821-w","https://www.nature.com/articles/s41550-022-01821-w","The Audible Universe project aims to create dialogue between two scientific domains investigating two distinct research objects: stars and sound. It has been instantiated within a collaborative workshop that began to mutually acculturate the two communities, by sharing and transmitting respective knowledge, skills and practices. One main outcome of this exchange was a global view on the astronomical data sonification paradigm for observing the diversity of tools, uses and users (including visually impaired people), but also the current limitations and potential methods of improvement. From this viewpoint, here we present basic elements gathered and contextualized by sound experts in their respective fields (sound perception/cognition, sound design, psychoacoustics, experimental psychology), to anchor sonification for astronomy in a more well informed, methodological and creative process.","2022-11","2023-07-05 06:51:55","2023-07-05 06:51:55","2023-07-05 06:51:55","1249-1255","","11","6","","Nat Astron","","","","","","","","en","2022 Springer Nature Limited","","","","www.nature.com","","Number: 11 Publisher: Nature Publishing Group","","/Users/minsik/Zotero/storage/KFDSCWPT/Misdariis et al. - 2022 - Sound experts’ perspectives on astronomy sonificat.pdf","","","Astronomy and astrophysics; Information theory and computation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GHMCKEBD","conferencePaper","2006","Hermann, Thomas; Höner, Oliver; Ritter, Helge","AcouMotion – An Interactive Sonification System for Acoustic Motion Control","Gesture in Human-Computer Interaction and Simulation","978-3-540-32625-0","","10.1007/11678816_35","","This paper introduces AcouMotion as a new hard-/software system for combining human body motion, tangible interfaces and sonification to a closed-loop human computer interface that allows non-visual motor control by using sonification (non-speech auditory displays) as major feedback channel. AcouMotion’s main components are (i) a sensor device for measuring motion parameters (ii) a computer simulation to represent the dynamical evolution of a model world, and (iii) a sonification engine which generates an auditory representation of objects and any interactions in the model world. The intended applications of AcouMotion range from new kinds of sport games that can be played without visual displays and therefore may be particularly interesting for people with visual impairment to further applications in data mining, physiotherapy and cognitive research. The first application of AcouMotion presented in this paper is Blindminton, a sport game similar to Badminton which is particularly adapted to the abilities of people with visual impairment. We describe our current system and its state of development, and we present first sound examples for interactive sonification using an early prototype. Finally, we discuss some interesting research directions based on the fact that AcouMotion binds auditory stimuli and body motion, and thus can represent a counterpart to the Eye-tracker device that exploits the binding of visual stimuli and eye-movement in cognitive research.","2006","2023-07-05 06:52:24","2023-07-05 06:52:24","","312-323","","","","","","","Lecture Notes in Computer Science","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/RYKEP468/Hermann et al. - 2006 - AcouMotion – An Interactive Sonification System fo.pdf","","","Auditory Information; Cognitive Research; Sensor Device; Tangible Interface; Visual Impairment","Gibet, Sylvie; Courty, Nicolas; Kamp, Jean-François","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TZP2G3XK","conferencePaper","2022","Baratè, Adriano; Ludovico, Luca A.; Motola, Alessia; Presti, Giorgio","Augmentation of a Virtual Exhibition of Paintings Through Sonification","The Future of Heritage Science and Technologies: ICT and Digital Heritage","978-3-031-20302-2","","10.1007/978-3-031-20302-2_28","","In this paper, we will take into consideration a virtual exhibition of paintings and focus on one of the possibilities offered by a virtual approach: the implementation of additional features aiming at the augmentation of users’ experience. Specifically, we will propose a framework to sonify paintings by introducing virtual audio sources in the pictures. The reproduction of spatialized sound during the vision of the painting is expected to induce sensory enhancement. Audio materials can be introduced either to reinforce the experience, as for the natural sounds of a landscape, or to deliver new meanings, thus aiming at Gesamtkunstwerk, i.e. an all-embracing art form. After presenting the state of the art about virtual exhibitions and the use of sonification in virtual reality, this paper will introduce a publicly available prototype of sound-augmented experience. The project, implemented in Unity, represents a pilot study to be further developed with a back-end area to let the user configure her/his own exhibition space, showcase selected paintings, and add sound sources in custom positions of the artworks.","2022","2023-07-05 06:52:46","2023-07-05 06:52:46","","380-392","","","","","","","Communications in Computer and Information Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/ZN4Q8SNX/Baratè et al. - 2022 - Augmentation of a Virtual Exhibition of Paintings .pdf","","","Sonification; Unity; Virtual reality; Visual arts","Furferi, Rocco; Governi, Lapo; Volpe, Yary; Seymour, Kate; Pelagotti, Anna; Gherardini, Francesco","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PW44C267","conferencePaper","2006","Rutkowski, Tomasz M.; Vialatte, Francois; Cichocki, Andrzej; Mandic, Danilo P.; Barros, Allan Kardec","Auditory Feedback for Brain Computer Interface Management – An EEG Data Sonification Approach","Knowledge-Based Intelligent Information and Engineering Systems","978-3-540-46544-7","","10.1007/11893011_156","","An auditory feedback for Brain Computer Interface (BCI) applications is proposed. This is achieved based on the so-called sonification of the mental states of humans, captured by Electro-Encephalogram (EEG) recordings. Two time-frequency signal decomposition techniques, the Bump Modelling and Empirical Mode Decomposition (EMD), are used to map the EEG recordings onto musical scores. This auditory feedback proves to have extremely high potential in the development of on-line BCI interfaces. Examples based on the responses from visual stimuli support the analysis.","2006","2023-07-05 06:53:04","2023-07-05 06:53:04","","1232-1239","","","","","","","Lecture Notes in Computer Science","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/T8IAS8R9/Rutkowski et al. - 2006 - Auditory Feedback for Brain Computer Interface Man.pdf","","","Auditory Feedback; Brain Computer Interface; Empirical Mode Decomposition; Instantaneous Frequency; Intrinsic Mode Function","Gabrys, Bogdan; Howlett, Robert J.; Jain, Lakhmi C.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TA9Q947Q","journalArticle","2015","Dubus, Gaël; Bresin, Roberto","Exploration and evaluation of a system for interactive sonification of elite rowing","Sports Engineering","","1460-2687","10.1007/s12283-014-0164-0","https://doi.org/10.1007/s12283-014-0164-0","In recent years, many solutions based on interactive sonification have been introduced for enhancing sport training. Few of them have been assessed in terms of efficiency or design. In a previous study, we performed a quantitative evaluation of four models for the sonification of elite rowing in a non-interactive context. For the present article, we conducted on-water experiments to investigate the effects of some of these models on two kinematic quantities: stroke rate value and fluctuations in boat velocity. To this end, elite rowers interacted with discrete and continuous auditory displays in two experiments. A method for computing an average rowing cycle is introduced, together with a measure of velocity fluctuations. Participants answered to questionnaires and interviews to assess the degree of acceptance of the different models and to reveal common trends and individual preferences. No significant effect of sonification could be determined in either of the two experiments. The measure of velocity fluctuations was found to depend linearly on stroke rate. Participants provided feedback about their aesthetic preferences and functional needs during interviews, allowing us to improve the models for future experiments to be conducted over longer periods.","2015-03-01","2023-07-05 06:53:39","2023-07-05 06:53:39","2023-07-05 06:53:39","29-41","","1","18","","Sports Eng","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/C4PX5WXW/Dubus and Bresin - 2015 - Exploration and evaluation of a system for interac.pdf","","","Sonification; Auditory display; Evaluation; Interactive; Rowing; Sonic interaction; Sport","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q3UC584J","conferencePaper","2018","Parseihian, Gaëtan; Aramaki, Mitsuko; Ystad, Sølvi; Kronland-Martinet, Richard","Exploration of Sonification Strategies for Guidance in a Blind Driving Game","Music Technology with Swing","978-3-030-01692-0","","10.1007/978-3-030-01692-0_27","","This paper explores the use of continuous auditory display for a dynamic guidance task. Through a driving game with blindfolded players, the success and the efficiency of a lane-keeping task in which no visual feedback is provided is observed. The results highlight the importance of the display information and reveal that a task-related rather than an error-related feedback should be used to enable the driver to finish the circuit. In terms of sound strategies, a first experiment explores the effect of two complex strategies (pitch and modulations) combined with a basic stereo strategy that informs the user about the distance and the direction to the target. The second experiment examines the influence of morphological sound attributes on the performance compared to the use of the spatial sound attributes alone. The results reveal the advantage of using morphological sound attributes for such kinds of applications.","2018","2023-07-05 06:54:17","2023-07-05 06:54:17","","413-428","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/MHM9XYTS/Parseihian et al. - 2018 - Exploration of Sonification Strategies for Guidanc.pdf","","","Sonification; Auditory display; Blind driving; Sound guidance","Aramaki, Mitsuko; Davies, Matthew E. P.; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GZYJIRSC","conferencePaper","2014","Danna, Jérémy; Paz-Villagrán, Vietminh; Capel, Annabelle; Pétroz, Céline; Gondre, Charles; Pinto, Serge; Thoret, Etienne; Aramaki, Mitsuko; Ystad, Sølvi; Kronland-Martinet, Richard; Velay, Jean-Luc","Movement Sonification for the Diagnosis and the Rehabilitation of Graphomotor Disorders","Sound, Music, and Motion","978-3-319-12976-1","","10.1007/978-3-319-12976-1_16","","The dynamic features of sounds make them particularly appropriate for assessing the spatiotemporal characteristics of movements. Furthermore, sounds can inform about the correctness of an ongoing movement without directly interfering with the visual and proprioceptive feedback. Finally, because of their playful characteristics, sounds are potentially effective for motivating writers in particular need of any writing assistance. By associating relevant sounds to the specific variables of handwriting movement, the present chapter aimed at reporting how supplementary auditory information allows an examiner (teacher or therapist) to assess the movement quality from his/her hearing. Furthermore, a writer could also improve his/her movement from this real-time auditory feedback. Sonification of some movement characteristics would be a relevant tool for the diagnosis and the rehabilitation of some developmental disabilities (e.g. dysgraphia) or acquired disorders (e.g. Parkinson’s disease).","2014","2023-07-05 06:55:14","2023-07-05 06:55:14","","246-255","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/KDDMRCVS/Danna et al. - 2014 - Movement Sonification for the Diagnosis and the Re.pdf","","","Auditory feedback; Dysgraphia; Kinematics; Motor control; Parkinson’s disease","Aramaki, Mitsuko; Derrien, Olivier; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B7R7ASMZ","journalArticle","2020","Burdick, Kendall J.; Jorgensen, Seiver K.; Combs, Taylor N.; Holmberg, Megan O.; Kultgen, Samantha P.; Schlesinger, Joseph J.","SAVIOR ICU: sonification and vibrotactile interface for the operating room and intensive care unit","Journal of Clinical Monitoring and Computing","","1573-2614","10.1007/s10877-019-00381-1","https://doi.org/10.1007/s10877-019-00381-1","Alarm fatigue is an issue for healthcare providers in the intensive care unit, and may result from desensitization of overbearing and under-informing alarms. To directly increase the overall identification of medical alarms and potentially contribute to a downstream decrease in the prevalence of alarm fatigue, we propose advancing alarm sonification by combining auditory and tactile stimuli to create a multisensory alarm. Participants completed four trials—two multisensory (auditory and tactile) and two unisensory (auditory). Analysis compared the unisensory trials to the multisensory trials based on the percentage of correctly identified point of change, direction of change and identity of three physiological parameters (indicated by different instruments): heart rate (drums), blood pressure (piano), blood oxygenation (guitar). A repeated-measures of ANOVA yielded a significant improvement in performance for the multisensory group compared to the unisensory group (p < 0.05). Specifically, the multisensory group had better performance in correctly identifying parameter (p < 0.05) and point of change (p < 0.05) compared to the unisensory group. Participants demonstrated a higher accuracy of identification with the use of multisensory alarms. Therefore, multisensory alarms may relieve the auditory burden of the medical environment and increase the overall quality of care and patient safety.","2020-08-01","2023-07-05 06:55:46","2023-07-05 06:55:46","2023-07-05 06:55:46","787-796","","4","34","","J Clin Monit Comput","SAVIOR ICU","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/ZPTNQATD/Burdick et al. - 2020 - SAVIOR ICU sonification and vibrotactile interfac.pdf","","","Alarm fatigue; Medical errors/prevention; Multisensory; Noise/adverse effects; Vibrotactile display","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U89TVI7Y","conferencePaper","2010","Constantinescu, Angela; Müller, Karin","Sonification of ASCII Circuit Diagrams","Computers Helping People with Special Needs","978-3-642-14097-6","","10.1007/978-3-642-14097-6_16","","Graphics and diagrams can be made available to blind users mainly in two ways: via speech descriptions or tactile printing. However, both approaches require help from a sighted, well instructed third party. We propose here a fast and inexpensive alternative to making graphics accessible to blind people, using sound and a corpus of ASCII graphics. The goal is to outline the challenges in sonifying ASCII diagrams in such a way that the semantics of the graphics is successfully brought to their users. Additionally, the users should have the possibility to perform the sonifications themselves. The concept is exemplified using circuit diagrams.","2010","2023-07-05 06:56:02","2023-07-05 06:56:02","","89-91","","","","","","","Lecture Notes in Computer Science","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/G478LWS9/Constantinescu and Müller - 2010 - Sonification of ASCII Circuit Diagrams.pdf","","","Sonification; accessibility; ASCII graphics; blind","Miesenberger, Klaus; Klaus, Joachim; Zagler, Wolfgang; Karshmer, Arthur","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A8RL7Y64","journalArticle","2023","Kacem, Amal; Zbiss, Khalil; Watta, Paul; Mohammadi, Alireza","Wave space sonification of the folding pathways of protein molecules modeled as hyper-redundant robotic mechanisms","Multimedia Tools and Applications","","1573-7721","10.1007/s11042-023-15385-y","https://doi.org/10.1007/s11042-023-15385-y","Investigation of the folding pathways of protein molecules plays a key role in studying diseases such as Alzheimer’s and designing viral drugs at the molecular level. Despite recent advances in visualization techniques, effective sonification (i.e., non-speech auditory representation) of large datasets associated with protein folding pathways is still an open question. This paper investigates the problem of sonification of protein folding pathway datasets by using the wave space sonification (WSS) framework due to Hermann (2018). In particular, this paper utilizes the powerful WSS framework to develop a sonification methodology for the dihedral angle folding trajectories of protein molecules, which are modeled as hyper-redundant robotic mechanisms with many rigid nano-linkages. As an example, the developed sonification methodology is applied to a protein molecule backbone chain with a dihedral angle space of dimension 82, where a canonical wave space function based on a sum-of-sinusoids with conformation-dependent frequencies and a sample-based wave space function based on Mozart’s Alla Turca are utilized for sonification of the folding trajectories of this peptide chain.","2023-05-30","2023-07-05 06:56:54","2023-07-05 06:56:54","2023-07-05 06:56:54","","","","","","Multimed Tools Appl","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/CXJHJMGR/Kacem et al. - 2023 - Wave space sonification of the folding pathways of.pdf","","","Sonification; Hyper-redundant robots; Protein folding; Wave Space Sonification (WSS)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HBT3QKLE","journalArticle","2012","Gresham-Lancaster, Scot","Waveguide synthesis for sonification of distributed sensor arrays","AI & SOCIETY","","1435-5655","10.1007/s00146-011-0357-z","https://doi.org/10.1007/s00146-011-0357-z","A decade of work is outlined based on the use of sensors on plants that are used to change the parameters of a fixed rotation of overlapping pitches. The use of waveguide, physical modeling synthesis, allows the repeated music figures to be changed in timbral space in real time in a discernable set of ongoing parameter mapping from a large data set being generated by various biological and atmospheric sensors.","2012-05-01","2023-07-05 06:57:14","2023-07-05 06:57:14","2023-07-05 06:57:14","289-292","","2","27","","AI & Soc","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/HNE4YATI/Gresham-Lancaster - 2012 - Waveguide synthesis for sonification of distribute.pdf","","","Sonification; Self-organization; Sensor network; Sound art; Waveguide","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7T2F42F2","bookSection","2018","Seiça, Mariana; Lopes, Rui; Martins, Pedro; Cardoso, F. Amílcar","Sonifying Twitter’s Emotions Through Music","Music Technology with Swing","978-3-030-01691-3 978-3-030-01692-0","","","http://link.springer.com/10.1007/978-3-030-01692-0_39","","2018","2023-07-05 06:59:22","2023-07-05 06:59:22","2023-07-05 06:59:22","586-608","","","11265","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-01692-0_39","","","","","","Aramaki, Mitsuko; Davies, Matthew E. P.; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6MB95IJ5","bookSection","2023","Torresan, Christian; Bernardes, Gilberto; Caetano, Elsa; Restivo, Teresa","The Singing Bridge: Sonification of a Stress-Ribbon Footbridge","ArtsIT, Interactivity and Game Creation","978-3-031-28992-7 978-3-031-28993-4","","","https://link.springer.com/10.1007/978-3-031-28993-4_25","Stress-ribbon footbridges are often prone to excessive vibrations induced by environmental phenomena (e.g., wind and rain) and human actions (e.g., walking and jumping) and their liveliness is strongly associated with their slenderness. In earlier studies, multiple dynamic responses of a stress-ribbon footbridge were observed on the campus of the Faculty of Engineering of the University of Porto (FEUP) in Portugal. Although extreme vibrations have never been reported, vertical oscillations are clearly perceptible under pedestrian excitement. While monitoring the bridge, the technology revealed physical phenomena that are invisible to humans. This project aims to adopt sonification techniques as a compositional tool to create a sonic manifestation that shows the dynamic response of the bridge. In this study, two different sonification techniques (audification and parameter mapping) were used to extrapolate the same phenomena using different strategies. For what concerns sound synthesis, the first technique used an FM synthesizer, while the second one used external VSTs and generative approaches to create a compelling musical sonification. In order to evaluate the proposed sonification techniques’ reliability, an online listening test was conducted to assess the three main dimensions of a collected dataset: the number of people crossing the bridge, their walking speed, and the steadiness of their pace. Respondents were required to complete both a blind test and one after a short training to assess the intuitiveness and reliability of both methods. According to the results, it is clear that the training significantly improves the participants’ accuracy in identifying the correct categories. In fact, almost all values have increased after the short training. Therefore, this suggests that the success of both sonification techniques could improve significantly with deeper training. Additionally, the overall trend shows parameter mapping sonification as a more intuitive and precise technique than audification.","2023","2023-07-05 06:59:22","2023-07-19 11:35:53","2023-07-05 06:59:22","359-373","","","479","","","The Singing Bridge","","","","","Springer Nature Switzerland","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering DOI: 10.1007/978-3-031-28993-4_25","","/Users/minsik/Zotero/storage/ILXQC5BD/Torresan et al. - 2023 - The Singing Bridge Sonification of a Stress-Ribbo.pdf","","","","Brooks, Anthony L.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B2K4HGKF","bookSection","2010","Saranti, Anna; Eckel, Gerhard; Pirrò, David","Quantum Harmonic Oscillator Sonification","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_10","","2010","2023-07-05 06:59:22","2023-07-05 06:59:22","2023-07-05 06:59:22","184-201","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_10","","/Users/minsik/Zotero/storage/C79X4IYL/Saranti et al. - 2010 - Quantum Harmonic Oscillator Sonification.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6U2QKPTA","bookSection","2007","Daunys, Gintautas; Lauruska, Vidas","Sonification System of Maps for Blind","Universal Access in Human-Computer Interaction. Ambient Interaction","978-3-540-73280-8 978-3-540-73281-5","","","http://link.springer.com/10.1007/978-3-540-73281-5_37","Presentation of graphical information is very important for blind. This information will help blind better understand surrounding world. The developed system is devoted for investigation of graphical information by blind user using a digitiser. SVG language with additional elements is used for describing of maps. Non-speech sounds are used to transfer information about colour. Alerting sound signal is issued near two regions boundary.","2007","2023-07-05 06:59:22","2023-07-21 05:08:34","2023-07-05 06:59:22","349-352","","","4555","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73281-5_37","","/Users/minsik/Zotero/storage/2NCAS8ZZ/Daunys and Lauruska - 2007 - Sonification System of Maps for Blind.pdf","","","","Stephanidis, Constantine","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6R73UGAK","journalArticle","2012","Dubus, Gaël","Evaluation of four models for the sonification of elite rowing","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0085-1","http://link.springer.com/10.1007/s12193-011-0085-1","Many aspects of sonification represent potential benefits for the practice of sports. Taking advantage of the characteristics of auditory perception, interactive sonification offers promising opportunities for enhancing the training of athletes. The efficient learning and memorizing abilities pertaining to the sense of hearing, together with the strong coupling between auditory and sensorimotor systems, make the use of sound a natural field of investigation in quest of efficiency optimization in individual sports at a high level. This study presents an application of sonification to elite rowing, introducing and evaluating four sonification models. The rapid development of mobile technology capable of efficiently handling numerical information offers new possibilities for interactive auditory display. Thus, these models have been developed under the specific constraints of a mobile platform, from data acquisition to the generation of a meaningful sound feedback. In order to evaluate the models, two listening experiments have then been carried out with elite rowers. Results show a good ability of the participants to efficiently extract basic characteristics of the sonified data, even in a non-interactive context. Qualitative assessment of the models highlights the need for a balance between function and esthetics in interactive sonification design. Consequently, particular attention on usability is required for future displays to become widespread.","2012-05","2023-07-05 06:59:22","2023-07-20 06:53:58","2023-07-05 06:59:22","143-156","","3-4","5","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/4HHYGB4L/Dubus - 2012 - Evaluation of four models for the sonification of .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GQRLQWIC","journalArticle","2018","Matinfar, Sasan; Nasseri, M. Ali; Eck, Ulrich; Kowalsky, Michael; Roodaki, Hessam; Navab, Navid; Lohmann, Chris P.; Maier, Mathias; Navab, Nassir","Surgical soundtracks: automatic acoustic augmentation of surgical procedures","International Journal of Computer Assisted Radiology and Surgery","","1861-6410, 1861-6429","10.1007/s11548-018-1827-2","http://link.springer.com/10.1007/s11548-018-1827-2","","2018-09","2023-07-05 06:59:22","2023-07-05 06:59:22","2023-07-05 06:59:22","1345-1355","","9","13","","Int J CARS","Surgical soundtracks","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"62PVCZ4I","bookSection","2009","Olivetti Belardinelli, Marta; Federici, Stefano; Delogu, Franco; Palmiero, Massimiliano","Sonification of Spatial Information: Audio-Tactile Exploration Strategies by Normal and Blind Subjects","Universal Access in Human-Computer Interaction. Intelligent and Ubiquitous Interaction Environments","978-3-642-02709-3 978-3-642-02710-9","","","http://link.springer.com/10.1007/978-3-642-02710-9_62","On the basis of a meta-analysis of existing literature about sonification technologies, new experimental results on audio-tactile exploration strategies of georeferenced sonificated data by sighted and blind subjects are presented, discussing: technology suitability, subjects’ performances, accessibility and usability in the user/technology interaction.","2009","2023-07-05 06:59:22","2023-07-21 05:10:23","2023-07-05 06:59:22","557-563","","","5615","","","Sonification of Spatial Information","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-02710-9_62","","/Users/minsik/Zotero/storage/DGA6GELV/Olivetti Belardinelli et al. - 2009 - Sonification of Spatial Information Audio-Tactile.pdf","","","","Stephanidis, Constantine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S3N6QJXU","bookSection","2007","Ag. Ibrahim, Ag. Asri; Hunt, Andy","An HCI Model for Usability of Sonification Applications","Task Models and Diagrams for Users Interface Design","978-3-540-70815-5 978-3-540-70816-2","","","http://link.springer.com/10.1007/978-3-540-70816-2_18","Sonification is a representation of data using sounds with the intention of communication and interpretation. The process and technique of converting the data into sound is called the sonification technique. One or more techniques might be required by a sonification application. However, sonification techniques are not always suitable for all kinds of data, and often custom techniques are used - where the design is tailored to the domain and nature of the data as well as the users’ required tasks within the application. Therefore, it is important to assure the usability of the technique for the specific domain application being developed. This paper describes a new HCI Model for usability of sonification applications. It consists of two other models, namely the Sonification Application (SA) Model and User Interpretation Construction (UIC) Model. The SA model will be used to explain the application from the designer’s point of view. The UIC Model will be used to explain what the user might perceive and understand.","2007","2023-07-05 06:59:22","2023-07-21 05:03:49","2023-07-05 06:59:22","245-258","","","4385","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-70816-2_18","","","","","","Coninx, Karin; Luyten, Kris; Schneider, Kevin A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q6MGP8H7","journalArticle","2019","Lorenzoni, Valerio; Van Den Berghe, Pieter; Maes, Pieter-Jan; De Bie, Tijl; De Clercq, Dirk; Leman, Marc","Design and validation of an auditory biofeedback system for modification of running parameters","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-018-0283-1","http://link.springer.com/10.1007/s12193-018-0283-1","Real-time auditory feedback during sports activities is becoming increasingly popular in view of opportunities for monitoring and movement (re)training in ecological environments. However, the design of an effective feedback strategy is difficult. In this paper, we present a methodical approach to the design of an auditory feedback strategy for running gait modification of recreational runners, using distortion of a musical baseline. First tests were conducted to select the best performing auditory distortion signal in terms of clarity and level perception, and to derive the relative perception curve. This was found to be pink noise with an exponential response curve. Further tests were carried out to determine the just noticeable difference of this signal in actual running conditions. Finally, validation tests were performed to examine if the real-time auditory biofeedback, combined with music, could alter the runner’s steps per minute (SPM) during treadmill-based running. The results show that our sonification strategy can alter the mean running SPM in a clear and non-disturbing way, and that our noise-based continuous feedback approach performs better than standard verbal instructions. Even though some of the participants did not respond effectively to the feedback, a large majority of the participants rated the feedback system as pleasant and indicated that they would use such system to improve their running style.","2019-09","2023-07-05 06:59:22","2023-07-20 07:01:36","2023-07-05 06:59:22","167-180","","3","13","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HJE4PQ5I","bookSection","2002","Hermann, Thomas; Nölker, Claudia; Ritter, Helge","Hand Postures for Sonification Control","Gesture and Sign Language in Human-Computer Interaction","978-3-540-43678-2 978-3-540-47873-7","","","http://link.springer.com/10.1007/3-540-47873-6_32","Sonification is a rather new technique in human-computer interaction which addresses auditory perception. In contrast to speech interfaces, sonification uses non-verbal sounds to present information. The most common sonification technique is parameter mapping where for each data point a sonic event is generated whose acoustic attributes are determined from data values by a mapping function. For acoustic data exploration, this mapping must be adjusted or manipulated by the user. We propose the use of hand postures as a particularly natural and intuitive means of parameter manipulation for this data exploration task. As a demonstration prototype we developed a hand posture recognition system for gestural controlling of sound. The presented implementation applies artificial neural networks for the identification of continuous hand postures from camera images and uses a real-time sound synthesis engine. In this paper, we present our system and first applications of the gestural control of sounds. Techniques to apply gestures to control sonification are proposed and sound examples are given.","2002","2023-07-05 06:59:22","2023-07-20 00:08:01","2023-07-05 06:59:22","307-316","","","2298","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-47873-6_32","","","","","","Wachsmuth, Ipke; Sowa, Timo","Goos, G.; Hartmanis, J.; Van Leeuwen, J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GKQEVKJ9","bookSection","2014","Lockton, Dan; Bowden, Flora; Brass, Clare; Gheerawo, Rama","Powerchord: Towards Ambient Appliance-Level Electricity Use Feedback through Real-Time Sonification","Ubiquitous Computing and Ambient Intelligence. Personalisation and User Adapted Services","978-3-319-13101-6 978-3-319-13102-3","","","http://link.springer.com/10.1007/978-3-319-13102-3_10","Feedback on energy use mainly uses visual, numerical interfaces. This paper introduces an alternative: energy sonification, turning real-time electricity use data from appliances into ambient sound. Powerchord, a work in progress prototype developed through co-creation with householders, is detailed.","2014","2023-07-05 06:59:22","2023-07-21 05:08:16","2023-07-05 06:59:22","48-51","","","8867","","","Powerchord","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-13102-3_10","","","","","","Hervás, Ramón; Lee, Sungyoung; Nugent, Chris; Bravo, José","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HSH8E3U8","journalArticle","2022","Su, Isabelle; Hattwick, Ian; Southworth, Christine; Ziporyn, Evan; Bisshop, Ally; Mühlethaler, Roland; Saraceno, Tomás; Buehler, Markus J.","Interactive exploration of a hierarchical spider web structure with sound","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-021-00375-x","https://link.springer.com/10.1007/s12193-021-00375-x","","2022-03","2023-07-05 06:59:22","2023-07-05 06:59:22","2023-07-05 06:59:22","71-85","","1","16","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/6DGTNUQA/Su et al. - 2022 - Interactive exploration of a hierarchical spider w.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z37LNKY9","bookSection","2010","Tünnermann, René; Kolbe, Lukas; Bovermann, Till; Hermann, Thomas","Surface Interactions for Interactive Sonification","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_9","","2010","2023-07-05 06:59:22","2023-07-05 06:59:22","2023-07-05 06:59:22","166-183","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_9","","/Users/minsik/Zotero/storage/CPJKUHBK/Tünnermann et al. - 2010 - Surface Interactions for Interactive Sonification.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IVE495T5","journalArticle","2017","Ferraro-Petrillo, Umberto","Using the audio of 8-bit video games to monitor web marketing campaigns","Multimedia Systems","","0942-4962, 1432-1882","10.1007/s00530-016-0509-6","http://link.springer.com/10.1007/s00530-016-0509-6","Monitoring the performance of a web marketing campaign is usually a long-lasting, low-effort but distracting task, where a user repeatedly glances at some sort of visual analytics tools to check whether the campaign is going well. In this paper, we explore an alternative approach for this task, where the performance of a web marketing campaign is monitored through sonification, using the soundset of popular 8-bit arcade video games. On one hand, sonification would allow a user to be constantly informed about the current state of the campaign without being distracted. On the other hand, the sound metaphors coming from popular 8-bit arcade video games would be able to convey information about the status of the campaign in a simple and effective way (i.e., if the sonification of a campaign resembles the audio of a successful game session, then the campaign is going well). We investigated this idea by developing a prototype system for the sonification of the behavior of a web server activity through a configurable set of sound metaphors. We then analyzed the effectiveness of our approach by conducting a simple experimental study. This was done, first, by sonifying the progress of a given web marketing campaign using the soundset of two popular 8-bit video games: Super Mario Bros and Bubble Bobble. The outcoming soundtrack was then used in a controlled setting to assess the performance of a group of 20 participants listening to our soundtrack under different work conditions.","2017-07","2023-07-05 06:59:22","2023-07-21 04:31:28","2023-07-05 06:59:22","469-484","","4","23","","Multimedia Systems","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PTMK8EDN","bookSection","2000","Soddell, Fran; Soddell, Jacques","Microbes and Music","PRICAI 2000 Topics in Artificial Intelligence","978-3-540-67925-7 978-3-540-44533-3","","","http://link.springer.com/10.1007/3-540-44533-1_76","L-systems are string rewriting mechanisms used to create images of complex organisms from a simple set of an axiom and production rules. They have also been used to create music. This study developed a Musical Instrument Digital Interface interpretation suitable for applying to strings generated by L-systems that had been previously developed to model the growth of filamentous microbes (fungi and bacteria). The resulting sound files helped distinguish between organisms with different growth rates, provided some insight into the temporal differences among stages of growth, and also resulted in interesting musical pieces.","2000","2023-07-05 06:59:22","2023-07-21 04:55:06","2023-07-05 06:59:22","767-777","","","1886","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-44533-1_76","","","","","","Mizoguchi, Riichiro; Slaney, John","Goos, G.; Hartmanis, J.; Van Leeuwen, J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LN7XK4RN","bookSection","2014","Braund, Edward; Miranda, Eduardo","Music with Unconventional Computing: A System for Physarum Polycephalum Sound Synthesis","Sound, Music, and Motion","978-3-319-12975-4 978-3-319-12976-1","","","https://link.springer.com/10.1007/978-3-319-12976-1_11","The field of computer music is evolving in tandem with advances in computer science. Our research is interested in how the developing field of unconventional computation may provide new pathways for music and music technologies. In this paper we present the development of a system for harnessing the biological computing substrate Physarum Polycephalum for sonification. Physarum Polycephalum is a large single cell with a myriad of diploid nuclei, which moves like a giant amoeba in its pursuit for food. The organism is amorphous, and although without a brain or any serving centre of control, can respond to the environmental conditions that surround it.","2014","2023-07-05 06:59:22","2023-07-21 05:01:17","2023-07-05 06:59:22","175-189","","","8905","","","Music with Unconventional Computing","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-12976-1_11","","","","","","Aramaki, Mitsuko; Derrien, Olivier; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GHY279NQ","journalArticle","2016","Stahl, Benjamin; Thoshkahna, Balaji","Design and evaluation of the effectiveness of a sonification technique for real time heart-rate data","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-016-0218-7","http://link.springer.com/10.1007/s12193-016-0218-7","This article is motivated by the question “Can a sonification system that provides continuous auditory heart rate feedback help stabilize an athlete’s heart rate at a given target heart rate while exercising?” The sonification system uses a Polar H7 heart rate sensor to measure the heart rate of the athlete and an iOS device for its processing and display. We implemented several sonification approaches, of which two were tested in both a unimodal and an audiovisual context in comparison to a purely visual feedback and to not having any feedback. The system’s objective performance and multiple subjective usability aspects were evaluated in an experiment with 16 subjects. The experiment has to be considered a pilot study because the exercising conditions were artificial. The subjects were exercising on an indoor cycle and could focus their visual sense on the visual display all the time. It was found that all of the feedback methods could convey information to the athlete and were therefore clearly superior to not having any feedback. The failure of showing a supremacy of the multimodal methods over the purely visual one can be reasoned by the fact that the testing conditions were artificial and could therefore not show the advantages of auditory/audiovisual feedback due to limited bandwidth of the visual channel. The conclusions we make about the design and evaluation of such sonification systems can be considered a useful starting point for further work in this field.","2016-09","2023-07-05 06:59:22","2023-07-20 07:04:22","2023-07-05 06:59:22","207-219","","3","10","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AYPIHIAJ","bookSection","2010","Schaffert, Nina; Mattes, Klaus; Effenberg, Alfred O.","A Sound Design for Acoustic Feedback in Elite Sports","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_8","Sound (acoustic information) is the naturally evocative, audible result of kinetic events. Humans interact with the world by the everyday experience of listening to perceive and interpret the environment. Elite athletes, especially, rely on sport specific sounds for feedback about successful (or unsuccessful) movements. Visualization plays the dominant role in technique analysis, but the limitations of visual observation (of time related events) compared with auditory perception, which represents information with a clearer time-resolution, mean that acoustic displays offer a promising alternative to visual displays. Sonification, as acoustic representation of information, offers an abundance of applications in elite sports for monitoring, observing movement and detecting changes therein. Appropriate sound is needed to represent specific movement patterns. This article presents conceptual considerations for a sound design to fulfill the specific purpose of movement optimization that would be acceptable to elite athletes, with first practical experience with elite athletes in rowing.","2010","2023-07-05 06:59:22","2023-07-19 11:38:26","2023-07-05 06:59:22","143-165","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_8","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R7ZACSXJ","conferencePaper","2023","Fink, Thomas; Akdag Salah, Alkim Almila","Extending the Visual Arts Experience: Sonifying Paintings with AI","Artificial Intelligence in Music, Sound, Art and Design","978-3-031-29956-8","","10.1007/978-3-031-29956-8_7","","Sonification of visual information is a relatively new research line that aims to create a new way to access and experience visual displays, especially for the visually impaired. When applied to artworks, sonification needs to translate the aesthetic experience as well. This is attempted via a handful studies in the literature, where most of the transformation and music generation is done manually, or only by using the low level visual features of artworks. In this paper, we present a sonification model that uses both low level and high level features such as color, edge information, saliency, object and scene detection to create a pleasant and descriptive sonification of artworks with the use of a fully automatic pipeline. The results of the model are tested via interviews done with experts in music theory and generative music models. We found a high agreement among experts for the evaluation of a small set of sonified paintings. Addition of high level features such as sounds extracted from the scene played a big role in this. Among the challenges observed during the interviews was the need to add emotion and mood information as well as semantic information to the sonification in order to create more descriptive melodies and sounds. The complexity and ambiguity of the visual information generated the most disagreement among experts both in their interpretation of the paintings as well as their sonifications.","2023","2023-07-05 07:02:24","2023-07-05 07:02:24","","100-116","","","","","","Extending the Visual Arts Experience","Lecture Notes in Computer Science","","","","Springer Nature Switzerland","Cham","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/P8MVVUII/Fink and Akdag Salah - 2023 - Extending the Visual Arts Experience Sonifying Pa.pdf","","","Artwork sonification; Generative music models; High level visual feature sonification; Low level visual feature sonification","Johnson, Colin; Rodríguez-Fernández, Nereida; Rebelo, Sérgio M.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XRMJ97BI","journalArticle","2023","Asonitis, Tasos; Allmendinger, Richard; Benatan, Matt; Climent, Ricardo","SonOpt: understanding the behaviour of bi-objective population-based optimisation algorithms through sound","Genetic Programming and Evolvable Machines","","1389-2576, 1573-7632","10.1007/s10710-023-09451-5","https://link.springer.com/10.1007/s10710-023-09451-5","Abstract                            We present an extension of SonOpt, the first ever openly available tool for the sonification of bi-objective population-based optimisation algorithms. SonOpt has already introduced benefits on the understanding of algorithmic behaviour by proposing the use of sound as a medium for the process monitoring of bi-objective optimisation algorithms. The first edition of SonOpt utilised two different sonification paths to provide information on convergence, population diversity, recurrence of objective values across consecutive generations and the shape of the approximation set. The present extension provides further insight through the introduction of a third sonification path, which involves hypervolume contributions to facilitate the understanding of the relative importance of non-dominated solutions. Using a different sound generation approach than the existing ones, this newly proposed sonification path utilizes pitch deviations to highlight the distribution of hypervolume contributions across the approximation set. To demonstrate the benefits of SonOpt we compare the sonic results obtained from two popular population-based multi-objective optimisation algorithms, Non-Dominated Sorting Genetic Algorithm (NSGA-II) and Multi-Objective Evolutionary Algorithm based on Decomposition (MOEA/D), and use a Multi-objective Random Search (MRS) approach as a baseline. The three algorithms are applied to numerous test problems and showcase how sonification can reveal various aspects of the optimisation process that may not be obvious from visualisation alone. SonOpt is available for download at               https://github.com/tasos-a/SonOpt-2.0               .","2023-06","2023-07-05 07:04:22","2023-07-05 07:04:22","2023-07-05 07:04:22","3","","1","24","","Genet Program Evolvable Mach","SonOpt","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/QESU3FU5/Asonitis et al. - 2023 - SonOpt understanding the behaviour of bi-objectiv.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E84DTQLY","journalArticle","2021","Iber, Michael; Lechner, Patrik; Jandl, Christian; Mader, Manuel; Reichmann, Michael","Auditory augmented process monitoring for cyber physical production systems","Personal and Ubiquitous Computing","","1617-4909, 1617-4917","10.1007/s00779-020-01394-3","https://link.springer.com/10.1007/s00779-020-01394-3","Abstract             We describe two proof-of-concept approaches on the sonification of estimated operation states and conditions focusing on two scenarios: a laboratory setup of a manipulated 3D printer and an industrial setup focusing on the operations of a punching machine. The results of these studies form the basis for the development of an “intelligent” noise protection headphone as part of Cyber Physical Production Systems which provides auditorily augmented information to machine operators and enables radio communication between them. Further application areas are implementations in control rooms (equipped with multi-channel loudspeaker systems) and utilization for training purposes. As a first proof-of-concept, the data stream of error probability estimations regarding partly manipulated 3D printing processes were mapped to three sonification models, providing evidence about momentary operation states. The neural network applied indicates a high accuracy (> 93%) of the error estimation distinguishing between normal and manipulated operation states. None of the manipulated states could be identified by listening. An auditory augmentation, or sonification of these error estimations, provides a considerable benefit to process monitoring. For a second proof-of-concept, setup operations of a punching machine were recorded. Since all operations were apparently flawlessly executed, and there were no errors to be reported, we focused on the identification of operation phases. Each phase of a punching process could be algorithmically distinguished at an estimated probability rate of > 94%. In the auditory display, these phases were represented by different instrumentations of a musical piece in order to allow users to differentiate between operations auditorily.","2021-08","2023-07-05 07:04:22","2023-07-05 07:04:22","2023-07-05 07:04:22","691-704","","4","25","","Pers Ubiquit Comput","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/WGEB3NRY/Iber et al. - 2021 - Auditory augmented process monitoring for cyber ph.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VXWMM39L","journalArticle","2017","Temple, Mark D.","An auditory display tool for DNA sequence analysis","BMC Bioinformatics","","1471-2105","10.1186/s12859-017-1632-x","http://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-017-1632-x","Background: DNA Sonification refers to the use of an auditory display to convey the information content of DNA sequence data. Six sonification algorithms are presented that each produce an auditory display. These algorithms are logically designed from the simple through to the more complex. Three of these parse individual nucleotides, nucleotide pairs or codons into musical notes to give rise to 4, 16 or 64 notes, respectively. Codons may also be parsed degenerately into 20 notes with respect to the genetic code. Lastly nucleotide pairs can be parsed as two separate frames or codons can be parsed as three reading frames giving rise to multiple streams of audio. Results: The most informative sonification algorithm reads the DNA sequence as codons in three reading frames to produce three concurrent streams of audio in an auditory display. This approach is advantageous since start and stop codons in either frame have a direct affect to start or stop the audio in that frame, leaving the other frames unaffected. Using these methods, DNA sequences such as open reading frames or repetitive DNA sequences can be distinguished from one another. These sonification tools are available through a webpage interface in which an input DNA sequence can be processed in real time to produce an auditory display playable directly within the browser. The potential of this approach as an analytical tool is discussed with reference to auditory displays derived from test sequences including simple nucleotide sequences, repetitive DNA sequences and coding or non-coding genes. Conclusion: This study presents a proof-of-concept that some properties of a DNA sequence can be identified through sonification alone and argues for their inclusion within the toolkit of DNA sequence browsers as an adjunct to existing visual and analytical tools.","2017-12","2023-07-05 07:04:22","2023-07-21 07:35:47","2023-07-05 07:04:22","221","","1","18","","BMC Bioinformatics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/U7FL4KIX/Temple - 2017 - An auditory display tool for DNA sequence analysis.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JQI847XD","journalArticle","2021","Datta, Prerit; Namin, Akbar Siami; Jones, Keith S.; Hewett, Rattikorn","Warning users about cyber threats through sounds","SN Applied Sciences","","2523-3963, 2523-3971","10.1007/s42452-021-04703-4","https://link.springer.com/10.1007/s42452-021-04703-4","Abstract               This paper reports a formative evaluation of auditory representations of cyber security threat indicators and cues, referred to as sonifications, to warn users about cyber threats. Most Internet browsers provide visual cues and textual warnings to help users identify when they are at risk. Although these alarming mechanisms are very effective in informing users, there are certain situations and circumstances where these alarming techniques are unsuccessful in drawing the user’s attention: (1) security warnings and features (e.g., blocking out malicious Websites) might overwhelm a typical Internet user and thus the users may overlook or ignore visual and textual warnings and, as a result, they might be targeted, (2) these visual cues are inaccessible to certain users such as those with visual impairments. This work is motivated by our previous work of the use of sonification of security warnings to users who are visually impaired. To investigate the usefulness of sonification in general security settings, this work uses real Websites instead of simulated Web applications with sighted participants. The study targets sonification for three different types of security threats: (1) phishing, (2) malware downloading, and (3) form filling. The results show that on average 58% of the participants were able to correctly remember what the sonification conveyed. Additionally, about 73% of the participants were able to correctly identify the threat that the sonification represented while performing tasks using real Websites. Furthermore, the paper introduces “CyberWarner”, a sonification sandbox that can be installed on the Google Chrome browser to enable auditory representations of certain security threats and cues that are designed based on several URL heuristics.                                         Article highlights                                                                        It is feasible to develop sonified cyber security threat indicators that users intuitively understand with minimal experience and training.                                                           Users are more cautious about malicious activities in general. However, when navigating real Websites, they are less informed. This might be due to the appearance of the navigating Websites or the overwhelming issues when performing tasks.                                                           Participants’ qualitative responses indicate that even when they did not remember what the sonification conveyed, the sonification was able to capture the user’s attention and take safe actions in response.","2021-07","2023-07-05 07:04:22","2023-07-05 07:04:22","2023-07-05 07:04:22","714","","7","3","","SN Appl. Sci.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/6R8LP8ZW/Datta et al. - 2021 - Warning users about cyber threats through sounds.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HBG9IBUQ","bookSection","2015","Braund, Edward; Miranda, Eduardo","Music with Unconventional Computing: Towards a Step Sequencer from Plasmodium of Physarum Polycephalum","Evolutionary and Biologically Inspired Music, Sound, Art and Design","978-3-319-16497-7 978-3-319-16498-4","","","https://link.springer.com/10.1007/978-3-319-16498-4_2","The field of computer music has evolved in tandem with advances made in computer science. We are interested in how the developing field of unconventional computation may provide new pathways for music and related technologies. In this paper, we outline our initial work into harnessing the behaviour of the biological computing substrate Physarum polycephalum for a musical step sequencer. The plasmodium of Physarum polycephalum is an amorphous unicellular organism, which moves like a giant amoeba as it navigates its environment for food. Our research manipulates the organism’s route-efficient propagation characteristics in order to create a growth environment for musical/sound arrangement. We experiment with this device in two different scenarios: sample triggering and MIDI note triggering using sonification techniques.","2015","2023-07-05 07:04:22","2023-07-20 00:02:31","2023-07-05 07:04:22","15-26","","","9027","","","Music with Unconventional Computing","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-16498-4_2","","","","","","Johnson, Colin; Carballal, Adrian; Correia, João","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ACAXDZY8","journalArticle","2019","Ezquerro, L.; Simón, J. L.","Geomusic as a New Pedagogical and Outreach Resource: Interpreting Geoheritage with All the Senses","Geoheritage","","1867-2477, 1867-2485","10.1007/s12371-019-00364-3","http://link.springer.com/10.1007/s12371-019-00364-3","The scientific, rational approach to the knowledge of Earth can be complemented and enhanced with an emotional approach by means of arts. Sonification of sedimentary series, by converting distinct lithology, facies or geochemical parameters into notes, and bed thickness into duration of sounds, provides a new viewpoint on both their sequential features and the cultural meaning of geoheritage. A total of 14 musical compositions have been achieved according to that procedure, based on successions of diverse ages and sedimentary environments within the Iberian Peninsula. Some of these successions exhibit cyclic features that have been analyzed by a number of authors. Cyclostratigraphy shows how certain sedimentary patterns can reveal climatic oscillations related to periodic variations of Earth orbital cycles. Geomusic elaborated from sonification of such sedimentary cycles could be therefore linked with Music of the Spheres postulated by Pythagoras in ancient Greece. Its hidden message deals with asking for a New Culture of Earth, for a renewed, friendly relationship with our planet. Its applied development could extend to soundtracks of scientific documentaries, background music at museums or geoparks, or performances at outreach events, or as a motivating factor in Earth Sciences learning.","2019-09","2023-07-05 07:04:22","2023-07-20 00:07:51","2023-07-05 07:04:22","1187-1198","","3","11","","Geoheritage","Geomusic as a New Pedagogical and Outreach Resource","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SJ8G3KCK","bookSection","2017","Matinfar, Sasan; Nasseri, M. Ali; Eck, Ulrich; Roodaki, Hessam; Navab, Navid; Lohmann, Chris P.; Maier, Mathias; Navab, Nassir","Surgical Soundtracks: Towards Automatic Musical Augmentation of Surgical Procedures","Medical Image Computing and Computer-Assisted Intervention − MICCAI 2017","978-3-319-66184-1 978-3-319-66185-8","","","https://link.springer.com/10.1007/978-3-319-66185-8_76","","2017","2023-07-05 07:04:22","2023-07-05 07:04:22","2023-07-05 07:04:22","673-681","","","10434","","","Surgical Soundtracks","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-66185-8_76","","","","","","Descoteaux, Maxime; Maier-Hein, Lena; Franz, Alfred; Jannin, Pierre; Collins, D. Louis; Duchesne, Simon","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7TTZ9UCZ","bookSection","2010","Worrall, David","Using Sound to Identify Correlations in Market Data","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_11","Despite intensive study, a comprehensive understanding of the structure of capital market trading data remains elusive. The one known application of audification to market price data reported in 1990 that it was difficult to interpret the results probably because the market does not resonate according to acoustic laws. This paper reports on some techniques for transforming the data so it does resonate; so audification can be used as a means of identifying autocorrelation in capital market trading data. Also reported are some experiments in which the data is sonified using a homomorphic modulation technique. The results obtained indicate that the technique may have a wider application to other similarly structured time-series data.","2010","2023-07-05 07:04:22","2023-07-19 11:39:55","2023-07-05 07:04:22","202-218","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_11","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9ZZNPJ3W","bookSection","2022","Asonitis, Tasos; Allmendinger, Richard; Benatan, Matt; Climent, Ricardo","SonOpt: Sonifying Bi-objective Population-Based Optimization Algorithms","Artificial Intelligence in Music, Sound, Art and Design","978-3-031-03788-7 978-3-031-03789-4","","","https://link.springer.com/10.1007/978-3-031-03789-4_1","","2022","2023-07-05 07:04:22","2023-07-05 07:04:22","2023-07-05 07:04:22","3-18","","","13221","","","SonOpt","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-031-03789-4_1","","/Users/minsik/Zotero/storage/ZL4PRIRG/Asonitis et al. - 2022 - SonOpt Sonifying Bi-objective Population-Based Op.pdf","","","","Martins, Tiago; Rodríguez-Fernández, Nereida; Rebelo, Sérgio M.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5CU9DKV3","journalArticle","2014","Wu, Dan; Li, Chao-yi; Liu, Jie; Lu, Jing; Yao, De-zhong","Scale-free brain ensemble modulated by phase synchronization","Journal of Zhejiang University SCIENCE C","","1869-1951, 1869-196X","10.1631/jzus.C1400199","http://link.springer.com/10.1631/jzus.C1400199","To listen to brain activity as a piece of music, we proposed the scale-free brainwave music (SFBM) technology, which could translate the scalp electroencephalogram (EEG) into music notes according to the power law of both EEG and music. In the current study, this methodology was further extended to a musical ensemble of two channels. First, EEG data from two selected channels are translated into musical instrument digital interface (MIDI) sequences, where the EEG parameters modulate the pitch, duration, and volume of each musical note. The phase synchronization index of the two channels is computed by a Hilbert transform. Then the two MIDI sequences are integrated into a chorus according to the phase synchronization index. The EEG with a high synchronization index is represented by more consonant musical intervals, while the low index is expressed by inconsonant musical intervals. The brain ensemble derived from real EEG segments illustrates differences in harmony and pitch distribution during the eyes-closed and eyes-open states. Furthermore, the scale-free phenomena exist in the brainwave ensemble. Therefore, the scale-free brain ensemble modulated by phase synchronization is a new attempt to express the EEG through an auditory and musical way, and it can be used for EEG monitoring and bio-feedback.","2014-10","2023-07-05 07:04:22","2023-07-20 06:50:20","2023-07-05 07:04:22","821-831","","10","15","","J. Zhejiang Univ. - Sci. C","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F774G35J","journalArticle","2023","Esau-Held, Margarita; Marsh, Andrew; Krauß, Veronika; Stevens, Gunnar","“Foggy sounds like nothing” — enriching the experience of voice assistants with sonic overlays","Personal and Ubiquitous Computing","","1617-4909, 1617-4917","10.1007/s00779-023-01722-3","https://link.springer.com/10.1007/s00779-023-01722-3","Abstract                            Although Voice Assistants are ubiquitously available for some years now, the interaction is still monotonous and utilitarian. Sound design offers conceptual and methodological research to design auditive interfaces. Our work aims to complement and supplement voice interaction with               sonic overlays               to enrich the user experience. Therefore, we followed a user-centered design process to develop a sound library for weather forecasts based on empirical results from a user survey of associative mapping. After analyzing the data, we created audio clips for seven weather conditions and evaluated the perceived combination of sound and speech with 15 participants in an interview study. Our findings show that supplementing speech with soundscapes is a promising concept that communicates information and induces emotions with a positive affect for the user experience of Voice Assistants. Besides a novel design approach and a collection of sound overlays, we provide four design implications to support voice interaction designers.","2023-06-06","2023-07-05 07:04:22","2023-07-05 07:04:22","2023-07-05 07:04:22","","","","","","Pers Ubiquit Comput","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/VPVGXWHF/Esau-Held et al. - 2023 - “Foggy sounds like nothing” — enriching the experi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UT8X597G","journalArticle","2023","Alfaro-Contreras, María; Iñesta, José M.; Calvo-Zaragoza, Jorge","Optical music recognition for homophonic scores with neural networks and synthetic music generation","International Journal of Multimedia Information Retrieval","","2192-6611, 2192-662X","10.1007/s13735-023-00278-5","https://link.springer.com/10.1007/s13735-023-00278-5","Abstract                            The recognition of patterns that have a time dependency is common in areas like speech recognition or natural language processing. The equivalent situation in image analysis is present in tasks like text or video recognition. Recently, Convolutional Recurrent Neural Networks (CRNN) have been broadly applied to solve these tasks in an end-to-end fashion with successful performance. However, its application to Optical Music Recognition (OMR) is not so straightforward due to the presence of different elements sharing the same horizontal position, disrupting the linear flow of the timeline. In this paper, we study the ability of the state-of-the-art CRNN approach to learn codes that represent this disruption in homophonic scores. In our experiments, we study the lower bounds in the recognition task of real scores when the models are trained with synthetic data. Two relevant conclusions are drawn: (1) Our serialized ways of encoding the music content are appropriate for CRNN-based OMR; (2) the learning process is possible with synthetic data, but there exists a               glass ceiling               when recognizing real sheet music.","2023-06","2023-07-05 07:04:22","2023-07-05 07:04:22","2023-07-05 07:04:22","12","","1","12","","Int J Multimed Info Retr","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/5VZKX4QY/Alfaro-Contreras et al. - 2023 - Optical music recognition for homophonic scores wi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BN8ACU4I","bookSection","2021","Buongiorno Nardelli, Marco","MUSICNTWRK: Data Tools for Music Theory, Analysis and Composition","Perception, Representations, Image, Sound, Music","978-3-030-70209-0 978-3-030-70210-6","","","http://link.springer.com/10.1007/978-3-030-70210-6_14","We present the API for MUSICNTWRK, a python library for pitch class set and rhythmic sequences classification and manipulation, the generation of networks in generalized music and sound spaces, deep learning algorithms for timbre recognition, and the sonification of arbitrary data. The software is freely available under GPL 3.0 and can be downloaded at www.musicntwrk.com.","2021","2023-07-05 07:04:22","2023-07-21 04:44:57","2023-07-05 07:04:22","190-215","","","12631","","","MUSICNTWRK","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-70210-6_14","","/Users/minsik/Zotero/storage/ESYUANFS/Buongiorno Nardelli - 2021 - MUSICNTWRK Data Tools for Music Theory, Analysis .pdf","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Aramaki, Mitsuko","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BB8Y5T8G","bookSection","2013","Wu, Dan; Yao, Dezhong","Chorus from the Two Brain hemispheres with Chinese Pentatonic Scale","World Congress on Medical Physics and Biomedical Engineering May 26-31, 2012, Beijing, China","978-3-642-29304-7 978-3-642-29305-4","","","https://link.springer.com/10.1007/978-3-642-29305-4_114","To listen to brain activity as a piece of music, we proposed a scale-free brainwave music (SFBM) technology, which translates the scalp EEG into music notes according to the power law of both EEG and music. In this paper, the methodology was further extended to chorus music of two channels from the two hemispheres. First, EEG data from two channels symmetrically located on the left and right hemispheres are translated into MIDI sequences by SFBM, respectively, where the EEG parameters modulate the pitch, duration and volume of each music note. Then, the two sequences are filtered into a chorus of the Chinese pentatonic scale or the Western major scale. The resulted Chinese and western music of different sleep stages illustrate distinct differences in harmony, and the music with Chinese pentatonic scale sounds more harmonious.","2013","2023-07-05 07:04:22","2023-07-21 05:15:24","2023-07-05 07:04:22","430-433","","","39","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: IFMBE Proceedings DOI: 10.1007/978-3-642-29305-4_114","","","","","","Long, Mian","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9LGN5W9E","bookSection","2023","Riber, Adrián García; Serradilla, Francisco","AI-rmonies of the Spheres","Artificial Intelligence in Music, Sound, Art and Design","978-3-031-29955-1 978-3-031-29956-8","","","https://link.springer.com/10.1007/978-3-031-29956-8_9","Thanks to the efforts and cooperation of the international community, nowadays it is possible to analyze astronomical data captured by the observatories and telescopes of major space agencies around the world from a personal computer. The development of virtual observatory technology (VO), and the standardization of the formats it uses, allow professional and amateur astronomers to access astronomical data and images through internet with relative ease. Immersed in this environment of global accessibility, this article presents an astronomical data-driven unsupervised music composition system based on Deep Learning, aimed at offering an automatic and objective review on the classical topic of the Harmonies of the Spheres. The system explores the MILES stellar library from the Spanish Virtual Observatory (SVO) using a variational autoencoder architecture to cross-match its stellar spectra via Pitch-Class Set Theory with a music score generated by a LSTM with attention neural network in the style of late-renaissance music.","2023","2023-07-05 07:04:22","2023-07-19 11:34:11","2023-07-05 07:04:22","132-147","","","13988","","","","","","","","Springer Nature Switzerland","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-031-29956-8_9","","","","","","Johnson, Colin; Rodríguez-Fernández, Nereida; Rebelo, Sérgio M.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MCKXWSH9","journalArticle","2012","Sinclair, Peter","Living with alarms: the audio environment in an intensive care unit","AI & SOCIETY","","0951-5666, 1435-5655","10.1007/s00146-011-0344-4","http://link.springer.com/10.1007/s00146-011-0344-4","This article treats the use of sonification in Percy Military Training Hospital’s intensive care unit, through an interview with Anaesthetist Professor Bruno Debien. It starts with a description of the environment completed by some technical information concerning the equipment. This is followed by a commented transcription of the interview with Bruno Debien and concludes with reflections on the nature of audio alarms and their relation to different modes of listening.","2012-05","2023-07-05 07:04:22","2023-07-19 11:23:55","2023-07-05 07:04:22","269-276","","2","27","","AI & Soc","Living with alarms","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/D26YF9TF/Sinclair - 2012 - Living with alarms the audio environment in an in.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D464JRQI","bookSection","2023","Ocampo, Rodolfo; Andres, Josh; Schmidt, Adrian; Pegram, Caroline; Shave, Justin; Hill, Charlton; Wright, Brendan; Bown, Oliver","Using GPT-3 to Achieve Semantically Relevant Data Sonificiation for an Art Installation","Artificial Intelligence in Music, Sound, Art and Design","978-3-031-29955-1 978-3-031-29956-8","","","https://link.springer.com/10.1007/978-3-031-29956-8_14","Large Language Models such as GPT-3 exhibit generative language capabilities with multiple potential applications in creative practice. In this paper, we present a method for data sonification that employs the GPT-3 model to create semantically relevant mappings between artificial intelligence-generated natural language descriptions of data, and human-generated descriptions of sounds. We implemented this method in a public art installation to generate a soundscape based on data from different systems. While common sonification approaches rely on arbitrary mappings between data values and sonic values, our approach explores the use of language models to achieve a mapping not via values but via meaning. We find our approach is a useful tool for musification practice and demonstrates a new application of generative language models in creative new media arts practice. We show how different prompts influence data to sound mappings, and highlight that matching the embeddings of texts of different lengths produces undesired behavior.","2023","2023-07-05 07:04:22","2023-07-19 11:33:47","2023-07-05 07:04:22","212-227","","","13988","","","","","","","","Springer Nature Switzerland","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-031-29956-8_14","","","","","","Johnson, Colin; Rodríguez-Fernández, Nereida; Rebelo, Sérgio M.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HBA5MMQF","bookSection","2018","O’Brien, Benjamin; Juhas, Brett; Bieńkiewicz, Marta; Pruvost, Laurent; Buloup, Frank; Bringnoux, Lionel; Bourdin, Christophe","Considerations for Developing Sound in Golf Putting Experiments","Music Technology with Swing","978-3-030-01691-3 978-3-030-01692-0","","","http://link.springer.com/10.1007/978-3-030-01692-0_23","This chapter presents the core interests and challenges of using sound for learning motor skills and describes the development of sonification techniques for three separate golf-putting experiments. These studies are part of the ANR SoniMove project, which aims to develop new Human Machine Interfaces (HMI) that provide gestural control of sound in the areas of sports and music. After a brief introduction to sonification and sound-movement studies, the following addresses the ideas and sound synthesis techniques developed for each experiment.","2018","2023-07-05 07:04:22","2023-07-21 04:34:09","2023-07-05 07:04:22","338-358","","","11265","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-01692-0_23","","","","","","Aramaki, Mitsuko; Davies, Matthew E. P.; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WQ8YIKCI","journalArticle","2013","Wu, Dan; Li, Chao-Yi; Yao, De-Zhong","An ensemble with the chinese pentatonic scale using electroencephalogram from both hemispheres","Neuroscience Bulletin","","1673-7067, 1995-8218","10.1007/s12264-013-1334-y","http://link.springer.com/10.1007/s12264-013-1334-y","To listen to brain activity as a piece of music, we previously proposed scale-free brainwave music (SFBM) technology, which translated the scalp electroencephalogram (EEG) into musical notes according to the power law of both the EEG and music. In this study, the methodology was further extended to ensemble music on two channels from the two hemispheres. EEG data from two channels symmetrically located on the left and right hemispheres were translated into MIDI sequences by SFBM, and the EEG parameters modulated the pitch, duration and volume of each note. Then, the two sequences were filtered into an ensemble with two voices: the pentatonic scale (traditional Chinese music) or the heptatonic scale (standard Western music). We demonstrated differences in harmony between the two scales generated at different sleep stages, with the pentatonic scale being more harmonious. The harmony intervals of this brain ensemble at various sleep stages followed the power law. Compared with the heptatonic scale, it was easier to distinguish the different stages using the pentatonic scale. These results suggested that the hemispheric ensemble can represent brain activity by variations in pitch, tempo and harmony. The ensemble with the pentatonic scale sounds more consonant, and partially reflects the relations of the two hemispheres. This can be used to distinguish the different states of brain activity and provide a new perspective on EEG analysis.","2013-10","2023-07-05 07:06:20","2023-07-21 04:38:41","2023-07-05 07:06:20","581-587","","5","29","","Neurosci. Bull.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/YEZKCMMI/Wu et al. - 2013 - An ensemble with the chinese pentatonic scale usin.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DMBJ4JWA","journalArticle","2014","McGregor, Iain","Comparing designers’ and listeners’ experiences","AI & SOCIETY","","0951-5666, 1435-5655","10.1007/s00146-013-0489-4","http://link.springer.com/10.1007/s00146-013-0489-4","","2014-11","2023-07-05 07:06:20","2023-07-05 07:06:20","2023-07-05 07:06:20","473-483","","4","29","","AI & Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UH7MIMDF","bookSection","2005","Beilharz, Kirsty","Responsive Sensate Environments: Past and Future Directions","Computer Aided Architectural Design Futures 2005","978-1-4020-3460-2","","","http://link.springer.com/10.1007/1-4020-3698-1_34","This paper looks at ways in which recent developments in sensing technologies and gestural control of data in 3D space provide opportunities to interact with information. Social and spatial data, the utilisation of space, flows of people and dense abstract data lend themselves to visual and auditory representation to enhance our understanding of socio-spatial patterns. Mapping information to visualisation and sonification leads to gestural interaction with information representation, dissolving the visibility and tangibility of traditional computational interfaces and hardware. The purpose of this integration of new technologies is to blur boundaries between computational and spatial interaction and to transform building spaces into responsive, intelligent interfaces for display and information access.","2005","2023-07-05 07:06:20","2023-07-19 23:47:41","2023-07-05 07:06:20","361-370","","","","","","Responsive Sensate Environments","","","","","Springer-Verlag","Berlin/Heidelberg","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/1-4020-3698-1_34","","","","","","Martens, Bob; Brown, Andre","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7MSM425N","bookSection","2012","Tulilaulu, Aurora; Paalasmaa, Joonas; Waris, Mikko; Toivonen, Hannu","Sleep Musicalization: Automatic Music Composition from Sleep Measurements","Advances in Intelligent Data Analysis XI","978-3-642-34155-7 978-3-642-34156-4","","","http://link.springer.com/10.1007/978-3-642-34156-4_36","We introduce data musicalization as a novel approach to aid analysis and understanding of sleep measurement data. Data musicalization is the process of automatically composing novel music, with given data used to guide the process. We present Sleep Musicalization, a methodology that reads a signal from state-of-the-art mattress sensor, uses highly non-trivial data analysis methods to measure sleep from the signal, and then composes music from the measurements. As a result, Sleep Musicalization produces music that reflects the user’s sleep during a night and complements visualizations of sleep measurements. The ultimate goal is to help users improve their sleep and well-being. For practical use and later evaluation of the methodology, we have built a public web service at http://sleepmusicalization.net for users of the sleep sensors.","2012","2023-07-05 07:06:20","2023-07-19 11:12:20","2023-07-05 07:06:20","392-403","","","7619","","","Sleep Musicalization","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-34156-4_36","","","","","","Hollmén, Jaakko; Klawonn, Frank; Tucker, Allan","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"79RZBEIW","journalArticle","2019","De Amicis, Raffaele; Riggio, Mariapaola; Shahbaz Badr, Arash; Fick, Jason; Sanchez, Christopher A.; Prather, Eric Andrew","Cross-reality environments in smart buildings to advance STEM cyberlearning","International Journal on Interactive Design and Manufacturing (IJIDeM)","","1955-2513, 1955-2505","10.1007/s12008-019-00546-x","http://link.springer.com/10.1007/s12008-019-00546-x","Real time data associated with the Building Information Model plays a critical role in the interpretation of the built environment, which is particularly relevant as an increasing number of education facilities and institutions promote sustainable engineering practices and monitoring data available to the public. However, it is challenging for non-technical audiences to fully comprehend or use information concealed in scientific data related to the performance of structures and materials. It is especially difficult for them to connect these concepts to physical contexts and phenomena. In this paper, we present how cross-reality paradigms in Architecture, Engineering, and Construction, coupled with multimodal representation techniques, enhance data literacy in both professionals and laypeople alike. In particular, we present the design of a learning environment where cutting-edge holographic interfaces and display technologies are combined with sonified and visual data to create a more immersive environment for data analysis and exploration, empowering users with situated data awareness and new ways of understanding real-time data.","2019-03","2023-07-05 07:06:20","2023-07-20 06:46:14","2023-07-05 07:06:20","331-348","","1","13","","Int J Interact Des Manuf","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VHIS7AN2","journalArticle","2022","Reh, Julia; Schmitz, Gerd; Hwang, Tong-Hun; Effenberg, Alfred O.","Loudness affects motion: asymmetric volume of auditory feedback results in asymmetric gait in healthy young adults","BMC Musculoskeletal Disorders","","1471-2474","10.1186/s12891-022-05503-6","https://bmcmusculoskeletdisord.biomedcentral.com/articles/10.1186/s12891-022-05503-6","Abstract                            Background               The potential of auditory feedback for motor learning in the rehabilitation of various diseases has become apparent in recent years. However, since the volume of auditory feedback has played a minor role so far and its influence has hardly been considered, we investigate the volume effect of auditory feedback on gait pattern and gait direction and its interaction with pitch.                                         Methods                                Thirty-two healthy young participants were randomly divided into two groups: Group 1 (                 n                  = 16) received a high pitch (150-250 Hz) auditory feedback; group 2 (                 n                  = 16) received a lower pitch (95-112 Hz) auditory feedback. The feedback consisted of a real-time sonification of the right and left foot ground contact. After an initial condition (no auditory feedback and full vision), both groups realized a 30-minute habituation period followed by a 30-minute asymmetry period. At any condition, the participants were asked to walk blindfolded and with auditory feedback towards a target at 15 m distance and were stopped 5 m before the target. Three different volume conditions were applied in random order during the habituation period: loud, normal, and quiet. In the subsequent asymmetry period, the three volume conditions baseline, right quiet and left quiet were applied in random order.                                                        Results               In the habituation phase, the step width from the loud to the quiet condition showed a significant interaction of volume*pitch with a decrease at high pitch (group 1) and an increase at lower pitch (group 2) (group 1: loud 1.02 ± 0.310, quiet 0.98 ± 0.301; group 2: loud 0.95 ± 0.229, quiet 1.11 ± 0.298). In the asymmetry period, a significantly increased ground contact time on the side with reduced volume could be found (right quiet: left foot 0.988 ± 0.033, right foot 1.003 ± 0.040, left quiet: left foot 1.004 ± 0.036, right foot 1.002 ± 0.033).                                         Conclusions               Our results suggest that modifying the volume of auditory feedback can be an effective way to improve gait symmetry. This could facilitate gait therapy and rehabilitation of hemiparetic and arthroplasty patients, in particular if gait improvement based on verbal corrections and conscious motor control is limited.","2022-12","2023-07-05 07:06:20","2023-07-05 07:06:20","2023-07-05 07:06:20","586","","1","23","","BMC Musculoskelet Disord","Loudness affects motion","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/BXW4RHS9/Reh et al. - 2022 - Loudness affects motion asymmetric volume of audi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"63YJ6T4F","bookSection","2005","Hepting, Daryl H.; Gerhard, David","Collaborative Computer-Aided Parameter Exploration for Music and Animation","Computer Music Modeling and Retrieval","978-3-540-24458-5 978-3-540-31807-1","","","http://link.springer.com/10.1007/978-3-540-31807-1_13","Although many artists have worked to create associations between music and animation, this has traditionally be done by developing one to suit the pre-existing other, as in visualization or sonification. The approach we employ in this work is to enable the simultaneous development of both music and sound from a common and rather generic central parameter variation, which may simply indicate a structure for periodic repetitions. This central parameter variation is then simultaneously mapped to appropriate musical and graphical variables by the musician and the animator, thereby contributing their own interpretations. The result of this mapping is then rendered in an intermediate form where music and animation are allowed to iteratively influence each other. The main piece of software in this development is the system which allows exploration of parameter mappings. The software interface allows both musician and animator to meaningfully experiment with the other’s mappings since the interface permits access in a common form, without requiring additional skills to interpret.","2005","2023-07-05 07:06:20","2023-07-19 23:48:23","2023-07-05 07:06:20","158-172","","","3310","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-31807-1_13","","/Users/minsik/Zotero/storage/YC8Q93MG/Hepting and Gerhard - 2005 - Collaborative Computer-Aided Parameter Exploration.pdf","","","","Wiil, Uffe Kock","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D5KFQTS3","journalArticle","2022","Rocchesso, Davide; Andolina, Salvatore; Ilardo, Giacomo; Palumbo, Salvatore Danilo; Galluzzo, Ylenia; Randazzo, Mario","A perceptual sound space for auditory displays based on sung-vowel synthesis","Scientific Reports","","2045-2322","10.1038/s41598-022-23736-2","https://www.nature.com/articles/s41598-022-23736-2","Abstract             When designing displays for the human senses, perceptual spaces are of great importance to give intuitive access to physical attributes. Similar to how perceptual spaces based on hue, saturation, and lightness were constructed for visual color, research has explored perceptual spaces for sounds of a given timbral family based on timbre, brightness, and pitch. To promote an embodied approach to the design of auditory displays, we introduce the Vowel–Type–Pitch (VTP) space, a cylindrical sound space based on human sung vowels, whose timbres can be synthesized by the composition of acoustic formants and can be categorically labeled. Vowels are arranged along the circular dimension, while voice type and pitch of the vowel correspond to the remaining two axes of the cylindrical VTP space. The decoupling and perceptual effectiveness of the three dimensions of the VTP space are tested through a vowel labeling experiment, whose results are visualized as maps on circular slices of the VTP cylinder. We discuss implications for the design of auditory and multi-sensory displays that account for human perceptual capabilities.","2022-11-12","2023-07-05 07:06:20","2023-07-05 07:06:20","2023-07-05 07:06:20","19370","","1","12","","Sci Rep","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/N3X64A6W/Rocchesso et al. - 2022 - A perceptual sound space for auditory displays bas.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MVHTF4XH","journalArticle","2022","Hamilton-Fletcher, Giles; Alvarez, James; Obrist, Marianna; Ward, Jamie","SoundSight: a mobile sensory substitution device that sonifies colour, distance, and temperature","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-021-00376-w","https://link.springer.com/10.1007/s12193-021-00376-w","Abstract                            Depth, colour, and thermal images contain practical and actionable information for the blind. Conveying this information through alternative modalities such as audition creates new interaction possibilities for users as well as opportunities to study neuroplasticity. The ‘SoundSight’ App (               www.SoundSight.co.uk               ) is a smartphone platform that allows 3D position, colour, and thermal information to directly control thousands of high-quality sounds in real-time to create completely unique and responsive soundscapes for the user. Users can select the specific sensor input and style of auditory output, which can be based on anything—tones, rainfall, speech, instruments, or even full musical tracks. Appropriate default settings for image-sonification are given by designers, but users still have a fine degree of control over the timing and selection of these sounds. Through utilising smartphone technology with a novel approach to sonification, the SoundSight App provides a cheap, widely accessible, scalable, and flexible sensory tool. In this paper we discuss common problems encountered with assistive sensory tools reaching long-term adoption, how our device seeks to address these problems, its theoretical background, its technical implementation, and finally we showcase both initial user experiences and a range of use case scenarios for scientists, artists, and the blind community.","2022-03","2023-07-05 07:06:20","2023-07-05 07:06:20","2023-07-05 07:06:20","107-123","","1","16","","J Multimodal User Interfaces","SoundSight","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/EQ7F6NWH/Hamilton-Fletcher et al. - 2022 - SoundSight a mobile sensory substitution device t.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PIY2463J","bookSection","2005","Graham, M. Ian; Karahalios, Karrie","ChatAmp: Talking with Music and Text","Human-Computer Interaction - INTERACT 2005","978-3-540-28943-2 978-3-540-31722-7","","","http://link.springer.com/10.1007/11555261_84","Current systems for synchronous, text-based communication offer more varied interactions than e-mail, but cannot easily convey non-verbal or emotional information in an unobtrusive and intuitive manner. In this report we introduce ChatAmp, a new chat system which incorporates music as a central part of social interaction. Music is used in order to create an unobtrusive ambient soundscape that gives information about conversational activity and emotion using changes to instrument behavior. This soundscape acts as a peripheral channel to let a multitasking user monitor the conversation while focused elsewhere without being interrupted by jarring alert sounds. By combining this with non-sequential visualization which groups all of a user's activity in his area of the screen, ChatAmp provides ""at-a-glance"" information through both auditory and visual channels. Informal user tests support the effectiveness of integrating music and conversation in achieving the goals above and suggest directions for further research.","2005","2023-07-05 07:06:20","2023-07-20 05:55:29","2023-07-05 07:06:20","978-981","","","3585","","","ChatAmp","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11555261_84","","/Users/minsik/Zotero/storage/QJDG9UD3/Graham and Karahalios - 2005 - ChatAmp Talking with Music and Text.pdf","","","","Costabile, Maria Francesca; Paternò, Fabio","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GCYZISXM","bookSection","2019","Mannone, Maria","Have Fun with Math and Music!","Mathematics and Computation in Music","978-3-030-21391-6 978-3-030-21392-3","","","http://link.springer.com/10.1007/978-3-030-21392-3_33","If abstraction makes mathematics strong, it often makes it also hard to learn, if not discouraging. If math pedagogy suffers from the lack of engaging strategies, the pedagogy of mathematical music theory must deal with the additional difficulty of double fields and double vocabulary. However, games and interdisciplinary references in a STEAM framework can help the learner break down complex concepts into essential ideas, and gain interest and motivation to approach advanced topics. Here we present some general considerations, followed by two examples which may be applied in a high-school or early college level course. The first is a musical application of a Rubik’s cube, the CubeHarmonic, to approach group theory and combinatorics jointly with musical chords; the second is an application of category theory to investigate simple musical variations together with transformations on a visual shape.","2019","2023-07-05 07:06:20","2023-07-21 04:28:28","2023-07-05 07:06:20","379-382","","","11502","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-21392-3_33","","","","","","Montiel, Mariana; Gomez-Martin, Francisco; Agustín-Aquino, Octavio A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N5KTKV6Q","bookSection","2015","Escalante, Juan Manuel","The Sound Digestive System: A Strategy for Music and Sound Composition","Evolutionary and Biologically Inspired Music, Sound, Art and Design","978-3-319-16497-7 978-3-319-16498-4","","","https://link.springer.com/10.1007/978-3-319-16498-4_7","Sound Digestive System is an audio visual project that uses the digestive system processes into algorithmic sound composition. This project proposes different strategies to bring bio-data, translations and interpretations of living processes into the sound domain, thus generating an artistic result based on scientific data.","2015","2023-07-05 07:06:20","2023-07-20 00:04:20","2023-07-05 07:06:20","71-77","","","9027","","","The Sound Digestive System","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-16498-4_7","","","","","","Johnson, Colin; Carballal, Adrian; Correia, João","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JBGUQNDT","bookSection","2017","Jeon, Myounghoon","Aesthetic Computing for Representation of the Computing Process and Expansion of Perceptual Dimensions: Cases for Art, Education, and Interfaces","Interactivity, Game Creation, Design, Learning, and Innovation","978-3-319-55833-2 978-3-319-55834-9","","","http://link.springer.com/10.1007/978-3-319-55834-9_36","","2017","2023-07-05 07:06:20","2023-07-05 07:06:20","2023-07-05 07:06:20","305-313","","","196","","","Aesthetic Computing for Representation of the Computing Process and Expansion of Perceptual Dimensions","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering DOI: 10.1007/978-3-319-55834-9_36","","","","","","Brooks, Anthony L.; Brooks, Eva","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KPFTANQT","journalArticle","2020","Jeon, Myounghoon; Andreopoulou, Areti; Katz, Brian F. G.","Auditory displays and auditory user interfaces: art, design, science, and research","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-020-00324-0","https://link.springer.com/10.1007/s12193-020-00324-0","","2020-06","2023-07-05 07:06:20","2023-07-05 07:06:20","2023-07-05 07:06:20","139-141","","2","14","","J Multimodal User Interfaces","Auditory displays and auditory user interfaces","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/N6HSP397/Jeon et al. - 2020 - Auditory displays and auditory user interfaces ar.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"55LW796Z","journalArticle","2023","Jeanne, Rudy; Piton, Timothy; Minjoz, Séphora; Bassan, Nicolas; Le Chenechal, Morgan; Semblat, Antoine; Hot, Pascal; Kibleur, Astrid; Pellissier, Sonia","Gut-Brain Coupling and Multilevel Physiological Response to Biofeedback Relaxation After a Stressful Task Under Virtual Reality Immersion: A Pilot Study","Applied Psychophysiology and Biofeedback","","1090-0586, 1573-3270","10.1007/s10484-022-09566-y","https://link.springer.com/10.1007/s10484-022-09566-y","Human physiological reactions to the environment are coordinated by the interactions between brain and viscera. In particular, the brain, heart, and gastrointestinal tract coordinate with each other to provide physiological equilibrium by involving the central, autonomic, and enteric nervous systems. Recent studies have demonstrated an electrophysiological coupling between the gastrointestinal tract and the brain (gut-brain axis) under resting-state conditions. As the gut-brain axis plays a key role in individual stress regulation, we aimed to examine modulation of gut-brain coupling through the use of an overwhelming and a relaxing module as a first step toward modeling of the underlying mechanisms. This study was performed in 12 participants who, under a virtual reality environment, performed a 9-min cognitive stressful task followed by a 9-min period of relaxation. Brain activity was captured by electroencephalography, autonomic activities by photoplethysmography, and electrodermal and gastric activities by electrogastrography. Results showed that compared with the stressful task, relaxation induced a significant decrease in both tonic and phasic sympathetic activity, with an increase in brain alpha power and a decrease in delta power. The intensity of gut-brain coupling, as assessed by the modulation index of the phase-amplitude coupling between the normogastric slow waves and the brain alpha waves, decreased under the relaxation relative to the stress condition. These results highlight the modulatory effect of biofeedback relaxation on gut-brain coupling and suggest noninvasive multilevel electrophysiology as a promising way to investigate the mechanisms underlying gut-brain coupling in physiological and pathological situations.","2023-03","2023-07-05 07:06:20","2023-07-19 11:31:56","2023-07-05 07:06:20","109-125","","1","48","","Appl Psychophysiol Biofeedback","Gut-Brain Coupling and Multilevel Physiological Response to Biofeedback Relaxation After a Stressful Task Under Virtual Reality Immersion","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GA53KICV","journalArticle","2017","Vets, T.; Nijs, L.; Lesaffre, M.; Moens, B.; Bressan, F.; Colpaert, P.; Lambert, P.; Van De Walle, R.; Leman, M.","Gamified music improvisation with BilliArT: a multimodal installation with balls","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-016-0224-9","http://link.springer.com/10.1007/s12193-016-0224-9","This paper presents the concept and the realisation of an interactive multimedia installation, called BilliArT, together with an explorative user study conducted on the data gathered during a public exhibition of the installation. The study concerns functional properties of the installation (e.g. usability, design quality) and subjective qualities of the sonic and visual feedback of the installation. The installation consists in a collaborative environment based on the carambole billiards game, which allows the users to engage in a user-driven machine-based jazz-inspired music improvisation, augmented with visual feedback. The installation is designed to promote the interaction among the users and the billiard game, stimulating the motivation to engage in the game by balancing predictable and unpredictable output, and reinforcing the feeling of reward, irrespective of their level of musical training. BilliArT introduces a new framework for expressive interaction related to the concepts of motivation and reward. The exploratory study proved the ability of the installation to activate the users’ sense of aesthetic reward, leading to a more active and satisfactory engagement in the game. Future studies may exploit these results to the advantage of the world of the arts, as well as of studies in human-computer interaction, interface design, and cultural heritage preservation.","2017-03","2023-07-05 07:06:20","2023-07-20 07:05:17","2023-07-05 07:06:20","25-38","","1","11","","J Multimodal User Interfaces","Gamified music improvisation with BilliArT","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6Y5HXUTF","journalArticle","2013","Schmitz, Gerd; Mohammadi, Bahram; Hammer, Anke; Heldmann, Marcus; Samii, Amir; Münte, Thomas F; Effenberg, Alfred O","Observation of sonified movements engages a basal ganglia frontocortical network","BMC Neuroscience","","1471-2202","10.1186/1471-2202-14-32","https://bmcneurosci.biomedcentral.com/articles/10.1186/1471-2202-14-32","Abstract                            Background               Producing sounds by a musical instrument can lead to audiomotor coupling, i.e. the joint activation of the auditory and motor system, even when only one modality is probed. The sonification of otherwise mute movements by sounds based on kinematic parameters of the movement has been shown to improve motor performance and perception of movements.                                         Results               Here we demonstrate in a group of healthy young non-athletes that congruently (sounds match visual movement kinematics) vs. incongruently (no match) sonified breaststroke movements of a human avatar lead to better perceptual judgement of small differences in movement velocity. Moreover, functional magnetic resonance imaging revealed enhanced activity in superior and medial posterior temporal regions including the superior temporal sulcus, known as an important multisensory integration site, as well as the insula bilaterally and the precentral gyrus on the right side. Functional connectivity analysis revealed pronounced connectivity of the STS with the basal ganglia and thalamus as well as frontal motor regions for the congruent stimuli. This was not seen to the same extent for the incongruent stimuli.                                         Conclusions               We conclude that sonification of movements amplifies the activity of the human action observation system including subcortical structures of the motor loop. Sonification may thus be an important method to enhance training and therapy effects in sports science and neurological rehabilitation.","2013-12","2023-07-05 07:06:20","2023-07-05 07:06:20","2023-07-05 07:06:20","32","","1","14","","BMC Neurosci","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/VPNL4XSX/Schmitz et al. - 2013 - Observation of sonified movements engages a basal .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZM86MAVQ","bookSection","2015","Al-Rifaie, Asmaa Majid; Al-Rifaie, Mohammad Majid","Generative Music with Stochastic Diffusion Search","Evolutionary and Biologically Inspired Music, Sound, Art and Design","978-3-319-16497-7 978-3-319-16498-4","","","https://link.springer.com/10.1007/978-3-319-16498-4_1","This paper introduces an approach for using a swarm intelligence algorithm, Stochastic Diffusion Search (SDS) – inspired by one species of ants, Leptothorax acervorum – in order to generate music from plain text. In this approach , SDS is adapted in such a way to vocalise the agents, to hear their “chit-chat” . While the generated music depends on the input text, the algorithm’s search capability in locating the words in the input text is reflected in the duration and dynamic of the resulting musical notes. In other words, the generated music depends on the behaviour of the algorithm and the communication between its agents. This novel approach, while staying loyal to the original input text, when run each time, ‘vocalises’ the input text in varying ‘flavours’.","2015","2023-07-05 07:06:20","2023-07-20 00:02:19","2023-07-05 07:06:20","1-14","","","9027","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-16498-4_1","","/Users/minsik/Zotero/storage/NA4IGSFI/Al-Rifaie and Al-Rifaie - 2015 - Generative Music with Stochastic Diffusion Search.pdf","","","","Johnson, Colin; Carballal, Adrian; Correia, João","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I63M742Y","journalArticle","2022","De Prisco, Roberto; Zaccagnino, Rocco","Creative DNA computing: splicing systems for music composition","Soft Computing","","1432-7643, 1433-7479","10.1007/s00500-022-06828-z","https://link.springer.com/10.1007/s00500-022-06828-z","Abstract                            Splicing systems are a form of DNA computing as they mimic the recombination process among DNA molecules. This work discusses the use of splicing systems to build automatic tools for reproducing human beings’               creativity               , in the context of               automatic music composition               . More specifically, this work describes three general splicing system approaches for automatic music composition, and their application to two specific cases, namely composing 4-voice music and composing Jazz solos in a given style. Examples of music composed by the systems are presented.","2022-09","2023-07-05 07:10:08","2023-07-05 07:10:08","2023-07-05 07:10:08","9689-9706","","18","26","","Soft Comput","Creative DNA computing","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/7C4YAS62/De Prisco and Zaccagnino - 2022 - Creative DNA computing splicing systems for music.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8QK42WLT","journalArticle","2018","Gupta, Ashish; Bhushan, Braj; Behera, Laxmidhar","Short-term enhancement of cognitive functions and music: A three-channel model","Scientific Reports","","2045-2322","10.1038/s41598-018-33618-1","https://www.nature.com/articles/s41598-018-33618-1","Abstract             Short-term effects of music stimulus on enhancement of cognitive functions in human brain are documented, however the underlying neural mechanisms in these cognitive effects are not well investigated. In this study, we have attempted to decipher the mechanisms involved in alterations of neural networks that lead to enhanced cognitive effects post-exposure to music. We have investigated the changes in Electroencephalography (EEG) power and functional connectivity of alpha band in resting state of the brain after exposure to Indian classical music. We have quantified the changes in functional connectivity by phase coherence, phase delay, and phase slope index analyses. Spatial mapping of functional connectivity dynamics thus obtained, on brain networks revealed reduced information flow in long-distance connections between frontal and parietal cortex, and between other cortical regions underpinning intelligence. Analyses also showed increased power in the prefrontal and occipital cortex. With these findings, we have developed a stimulus-mechanism-end effect based neuro-cognitive model that explains the music induced cognitive enhancement by a three-channel framework - (1) enhanced global efficiency of brain, (2) enhanced local neural efficiency at the prefrontal lobe, and (3) increased sustained attention. Results signify that music directly affects the cognitive system and leads to improved brain efficiency through well-defined mechanisms.","2018-10-19","2023-07-05 07:10:08","2023-07-05 07:10:08","2023-07-05 07:10:08","15528","","1","8","","Sci Rep","Short-term enhancement of cognitive functions and music","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/US8GZGCQ/Gupta et al. - 2018 - Short-term enhancement of cognitive functions and .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WEE2I62G","bookSection","2007","DeWitt, Anna; Bresin, Roberto","Sound Design for Affective Interaction","Affective Computing and Intelligent Interaction","978-3-540-74888-5 978-3-540-74889-2","","","http://link.springer.com/10.1007/978-3-540-74889-2_46","Different design approaches contributed to what we see today as the prevalent design paradigm for Human Computer Interaction; though they have been mostly applied to the visual aspect of interaction. In this paper we presented a proposal for sound design strategies that can be used in applications involving affective interaction. For testing our approach we propose the sonification of the Affective Diary, a digital diary with focus on emotions, affects, and bodily experience of the user. We applied results from studies in music and emotion to sonic interaction design. This is one of the first attempts introducing different physics-based models for the real-time complete sonification of an interactive user interface in portable devices.","2007","2023-07-05 07:10:08","2023-07-19 11:13:13","2023-07-05 07:10:08","523-533","","","4738","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-74889-2_46","","","","","","Paiva, Ana C. R.; Prada, Rui; Picard, Rosalind W.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MDK4XEQA","bookSection","2018","Effenberg, Alfred O.; Hwang, Tong-Hun; Ghai, Shashank; Schmitz, Gerd","Auditory Modulation of Multisensory Representations","Music Technology with Swing","978-3-030-01691-3 978-3-030-01692-0","","","http://link.springer.com/10.1007/978-3-030-01692-0_20","Motor control and motor learning as well as interpersonal coordination are based on motor perception and emergent perceptuomotor representations. At least in early stages motor learning and interpersonal coordination are emerging heavily on visual information in terms of observing others and transforming the information into internal representations to guide owns behavior. With progressing learning, also other perceptual modalities are added when a new motor pattern is established by repeated physical exercises. In contrast to the vast majority of publications on motor learning and interpersonal coordination referring to a certain perceptual modality here we look at the perceptual system as a unitary system coordinating and unifying the information of all involved perceptual modalities. The relation between perceptual streams of different modalities, the intermodal processing and multisensory integration of information as a basis for motor control and learning will be the main focus of this contribution. Multi-/intermodal processing of perceptual streams results in multimodal representations and opens up new approaches to support motor learning and interpersonal coordination: Creating an additional perceptual stream adequately auditory movement information can be generated suitable to be integrated with information of other modalities and thereby modulating the resulting perceptuomotor representations without the need of attention and higher cognition. Here, the concept of a movement defined real-time acoustics is used to serve the auditory system in terms of an additional movement-auditory stream. Before the computational approach of kinematic real-time sonification is finally described, a special focus is directed to the level of adaptation modules of the internal models. Furthermore, this concept is compared with different approaches of additional acoustic movement information. Moreover, a perspective of this approach is given in a broad spectrum of new applications of supporting motor control and learning in sports and motor rehabilitation as well as a broad spectrum of joint action and interpersonal coordination between humans but also concerning human-robotinteraction.","2018","2023-07-05 07:10:08","2023-07-21 04:33:53","2023-07-05 07:10:08","284-311","","","11265","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-01692-0_20","","/Users/minsik/Zotero/storage/AKCCYUSI/Effenberg et al. - 2018 - Auditory Modulation of Multisensory Representation.pdf","","","","Aramaki, Mitsuko; Davies, Matthew E. P.; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LDP95RLH","bookSection","2017","Parseihian, Gaëtan; Bourdin, Christophe; Bréjard, Vincent; Kronland-Martinet, Richard","Increasing Pleasantness and Security Using 3D-Sound Design in Public Transport","Bridging People and Sound","978-3-319-67737-8 978-3-319-67738-5","","","http://link.springer.com/10.1007/978-3-319-67738-5_9","A collaborative project aiming to improve the bus trip with auditory information is presented in this paper. This project took place in Marseille and involved several laboratories from Aix-Marseille University and the Marseille transit operator. This project consists in the study of three fundamental actions of the sound on the bus’ passengers: designing sound announcement to inform passengers of the next stop in a playful and intuitive way, brightening up the route with spatialized soundscapes to increase the trips pleasantness, and using sound to alert passengers of emergency braking. For that purpose, a high quality multi channel sound spatialization system was integrated in the bus and a sonification software based on geolocation was designed. The overall concepts of this project are first presented, then the integration of the sound spatialization system and the implementation of the sonification software are described. Finally, an evaluation method of passengers satisfaction is discussed and first results of a laboratory experiment are presented.","2017","2023-07-05 07:10:08","2023-07-19 23:37:01","2023-07-05 07:10:08","150-168","","","10525","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-67738-5_9","","","","","","Aramaki, Mitsuko; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EX348NLM","bookSection","2023","Hoy, Rory; Van Nort, Doug","Networked and Collaborative Musical Play Amongst Humans and Virtual Biological Agents in Locus Diffuse","Music in the AI Era","978-3-031-35381-9 978-3-031-35382-6","","","https://link.springer.com/10.1007/978-3-031-35382-6_9","Locus Diffuse is a networked multi-user instrument populated by a simulated slime mold and four human players. Mimicking the biological behavior of slime mold and establishing a virtual living network between player nodes, the system sonifies interaction along these connections. Participants use a browser based interface to play the multi-user instrument, and access an accompanying stream for audio and visual output of the system. Player responses from various play sessions are reported, and discussed relative to the concept of sonic ecosystems. These responses demonstrate distinct frames of focus employed by participants in regard to human/machine and inter-human collaboration, including perceived interaction of sound sources and agent behavior, perceived interaction through personal connection to agents, and differing perceptions of an aural vs visual understanding of the system.","2023","2023-07-05 07:10:08","2023-07-21 04:33:43","2023-07-05 07:10:08","94-110","","","13770","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-031-35382-6_9","","","","","","Aramaki, Mitsuko; Hirata, Keiji; Kitahara, Tetsuro; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9VK6AAP6","bookSection","2015","Maureira, Marcello A. Gómez; Rombout, Lisa E.","The Vocal Range of Movies - Sonifying Gender Representation in Film","Entertainment Computing - ICEC 2015","978-3-319-24588-1 978-3-319-24589-8","","","http://link.springer.com/10.1007/978-3-319-24589-8_54","Research has shown that in contemporary movies, male characters consistently outnumber female characters. In recent years, the number of speaking roles identified as female has declined or remained stable. Guidelines like the Bechdel and Mako Mori test have emerged as a method of evaluating gender representation in film. In this study, a more abstract and experiential form of evaluation is proposed. The per-segment sonification of the assigned gender of a character and the amount of lines they have in that segment of the script creates an audio file, showcasing the gender-representation in the movie dynamically. Two focus groups, one specifically consisting of young filmmakers, have expressed their interest in this form of movie-sonification. Expressed wishes for additional features and other suggested improvements are taken into consideration for the creation of the next prototype.","2015","2023-07-05 07:10:08","2023-07-19 23:59:47","2023-07-05 07:10:08","545-550","","","9353","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-24589-8_54","","/Users/minsik/Zotero/storage/AUWVMGL3/Maureira and Rombout - 2015 - The Vocal Range of Movies - Sonifying Gender Repre.pdf","","","","Chorianopoulos, Konstantinos; Divitini, Monica; Baalsrud Hauge, Jannicke; Jaccheri, Letizia; Malaka, Rainer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5IZCUKH3","bookSection","2007","Raś, Zbigniew W.; Zhang, Xin; Lewis, Rory","MIRAI: Multi-hierarchical, FS-Tree Based Music Information Retrieval System","Rough Sets and Intelligent Systems Paradigms","978-3-540-73450-5 978-3-540-73451-2","","","http://link.springer.com/10.1007/978-3-540-73451-2_10","With the fast booming of online music repositories, there is a need for content-based automatic indexing which will help users to find their favorite music objects in real time. Recently, numerous successful approaches on musical data feature extraction and selection have been proposed for instrument recognition in monophonic sounds. Unfortunately, none of these methods can be successfully applied to polyphonic sounds. Identification of music instruments in polyphonic sounds is still difficult and challenging, especially when harmonic partials are overlapping with each other. This has stimulated the research on music sound separation and new features development for content-based automatic music information retrieval. Our goal is to build a cooperative query answering system (QAS), for a musical database, retrieving from it all objects satisfying queries like ”find all musical pieces in pentatonic scale with a viola and piano where viola is playing for minimum 20 seconds and piano for minimum 10 seconds”. We use the database of musical sounds, containing almost 4000 sounds taken from the MUMs (McGill University Master Samples), as a vehicle to construct several classifiers for automatic instrument recognition. Classifiers showing the best performance are adopted for automatic indexing of musical pieces by instruments. Our musical database has an FS-tree (Frame Segment Tree) structure representation. The cooperativeness of QAS is driven by several hierarchical structures used for classifying musical instruments.","2007","2023-07-05 07:10:08","2023-07-21 04:59:19","2023-07-05 07:10:08","80-89","","","4585","","","MIRAI","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73451-2_10","","","","","","Kryszkiewicz, Marzena; Peters, James F.; Rybinski, Henryk; Skowron, Andrzej","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XH9Q5TYL","journalArticle","2013","Sigrist, Roland; Rauter, Georg; Riener, Robert; Wolf, Peter","Augmented visual, auditory, haptic, and multimodal feedback in motor learning: A review","Psychonomic Bulletin & Review","","1069-9384, 1531-5320","10.3758/s13423-012-0333-8","http://link.springer.com/10.3758/s13423-012-0333-8","It is generally accepted that augmented feedback, provided by a human expert or a technical display, effectively enhances motor learning. However, discussion of the way to most effectively provide augmented feedback has been controversial. Related studies have focused primarily on simple or artificial tasks enhanced by visual feedback. Recently, technical advances have made it possible also to investigate more complex, realistic motor tasks and to implement not only visual, but also auditory, haptic, or multimodal augmented feedback. The aim of this review is to address the potential of augmented unimodal and multimodal feedback in the framework of motor learning theories. The review addresses the reasons for the different impacts of feedback strategies within or between the visual, auditory, and haptic modalities and the challenges that need to be overcome to provide appropriate feedback in these modalities, either in isolation or in combination. Accordingly, the design criteria for successful visual, auditory, haptic, and multimodal feedback are elaborated.","2013-02","2023-07-05 07:10:08","2023-07-21 04:56:17","2023-07-05 07:10:08","21-53","","1","20","","Psychon Bull Rev","Augmented visual, auditory, haptic, and multimodal feedback in motor learning","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/VJ57FFJZ/Sigrist et al. - 2013 - Augmented visual, auditory, haptic, and multimodal.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N53XRLFZ","bookSection","2014","Mikami, Tomoya; Takano, Kosuke","A Music Search System for Expressive Music Performance Learning","Human Interface and the Management of Information. Information and Knowledge in Applications and Services","978-3-319-07862-5 978-3-319-07863-2","","","http://link.springer.com/10.1007/978-3-319-07863-2_9","In this paper, we present a music search system that focuses on performance style to cultivate a pupil’s expressive performance of music. The system allows pupils to learn the performance style to be mastered by obtaining both model and non-model content. By browsing non-model content that is similar to the quality of a pupil’s performance, the pupil can quickly identify his/her areas that require improvement. In addition, the pupil can improve his/her performance skill by repeatedly imitating the models. We evaluate the capabilities of our music search system regarding the extraction of performance style from a classical music source and the precision of the music search results for performance style.","2014","2023-07-05 07:10:08","2023-07-20 05:53:34","2023-07-05 07:10:08","80-89","","","8522","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-07863-2_9","","","","","","Yamamoto, Sakae","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Kobsa, Alfred; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Doug; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MGQRMCHP","journalArticle","2012","Grond, Florian; Hermann, Thomas","Singing function: Exploring auditory graphs with a vowel based sonification","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0068-2","http://link.springer.com/10.1007/s12193-011-0068-2","We present in this paper SingingFunction, a vowel-based sonification strategy for mathematical functions. Within the research field of auditory graphs as representation of scalar functions, we focus in SingingFunction on important aspects of sound design, which allow to better distinguish function shapes as auditory gestalts. SingingFunction features the first vowel-based synthesis for function sonification, and allows for a seamless integration of higher derivatives of the function into a single sound stream. We present further the results of a psycho physical experiment, where we compare the effectiveness of function sonifications based on either mapping only f′(x), or including hierarchically further information about the first derivatives f′(x), or the second derivative f″(x). Further we look at interactivity as an important factor and report interesting effects across all 3 sonification methods by comparing interactive explorations versus simple playback of sonified functions. Finally, we discuss SingingFunction within the context of existing function sonifications, and possible evaluation methods.","2012-05","2023-07-05 07:10:08","2023-07-20 06:59:17","2023-07-05 07:10:08","87-95","","3-4","5","","J Multimodal User Interfaces","Singing function","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T7FVVRS8","journalArticle","2016","Metatla, Oussama; Martin, Fiore; Parkinson, Adam; Bryan-Kinns, Nick; Stockman, Tony; Tanaka, Atau","Audio-haptic interfaces for digital audio workstations: A participatory design approach","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-016-0217-8","http://link.springer.com/10.1007/s12193-016-0217-8","We examine how auditory displays, sonification and haptic interaction design can support visually impaired sound engineers, musicians and audio production specialists access to digital audio workstation. We describe a user-centred approach that incorporates various participatory design techniques to help make the design process accessible to this population of users. We also outline the audio-haptic designs that results from this process and reflect on the benefits and challenges that we encountered when applying these techniques in the context of designing support for audio editing.","2016-09","2023-07-05 07:10:08","2023-07-20 07:02:45","2023-07-05 07:10:08","247-258","","3","10","","J Multimodal User Interfaces","Audio-haptic interfaces for digital audio workstations","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/HZCF4N3X/Metatla et al. - 2016 - Audio-haptic interfaces for digital audio workstat.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VHL52WWM","journalArticle","2020","Temple, Mark D.","Real-time audio and visual display of the Coronavirus genome","BMC Bioinformatics","","1471-2105","10.1186/s12859-020-03760-7","https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-03760-7","Abstract                            Background               This paper describes a web based tool that uses a combination of sonification and an animated display to inquire into the SARS-CoV-2 genome. The audio data is generated in real time from a variety of RNA motifs that are known to be important in the functioning of RNA. Additionally, metadata relating to RNA translation and transcription has been used to shape the auditory and visual displays. Together these tools provide a unique approach to further understand the metabolism of the viral RNA genome. This audio provides a further means to represent the function of the RNA in addition to traditional written and visual approaches.                                         Results               Sonification of the SARS-CoV-2 genomic RNA sequence results in a complex auditory stream composed of up to 12 individual audio tracks. Each auditory motive is derived from the actual RNA sequence or from metadata. This approach has been used to represent transcription or translation of the viral RNA genome. The display highlights the real-time interaction of functional RNA elements. The sonification of codons derived from all three reading frames of the viral RNA sequence in combination with sonified metadata provide the framework for this display. Functional RNA motifs such as transcription regulatory sequences and stem loop regions have also been sonified. Using the tool, audio can be generated in real-time from either genomic or sub-genomic representations of the RNA. Given the large size of the viral genome, a collection of interactive buttons has been provided to navigate to regions of interest, such as cleavage regions in the polyprotein, untranslated regions or each gene. These tools are available through an internet browser and the user can interact with the data display in real time.                                         Conclusion               The auditory display in combination with real-time animation of the process of translation and transcription provide a unique insight into the large body of evidence describing the metabolism of the RNA genome. Furthermore, the tool has been used as an algorithmic based audio generator. These audio tracks can be listened to by the general community without reference to the visual display to encourage further inquiry into the science.","2020-12","2023-07-05 07:10:08","2023-07-05 07:10:08","2023-07-05 07:10:08","431","","1","21","","BMC Bioinformatics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/XH6RRUA5/Temple - 2020 - Real-time audio and visual display of the Coronavi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DZ43YGE7","journalArticle","2023","Samara, Mohammed; Deriche, Mohamed; Al-Sadah, Jihad; Osais, Yahya","Design and Implementation of a Real-Time Color Recognition System for the Visually Impaired","Arabian Journal for Science and Engineering","","2193-567X, 2191-4281","10.1007/s13369-022-07506-w","https://link.springer.com/10.1007/s13369-022-07506-w","There is a growing interest in developing Computer-Based Assistive Technology (CAT) systems able to help the Visually Impaired (VI) in their daily needs and integrate well within society. One aspect that has not been well addressed is helping the visually impaired is the identification of colors for daily activities. Color recognition and perception is very important in interacting with the society and the surrounding environment. This paper presents a proof-of-concept design of a real-time embedded system that can help the visually impaired recognize colors, interact, and take decisions based on their perception of colors. Our approach is based on conveying color information, from the full color space, using a unique set of synthesized sound signals. The hardware part of the system is a pen-like device, which can detect color and generate a language-independent auditory signal representing the HSV values of the identified color. Numerous experiments have been performed using the new system with both the visually impaired and some blindfolded (BLD) participants. The system was proven to be very efficient in relation to training time and leads to high accuracy in color detection, classification, and matching tests. These experiments confirmed that the developed sonification scheme is effective yet simple in achieving color perception for the visually impaired. The proof-of-concept achieves about 93% recognition accuracy using off-the-shelf components, it is cheap to implement, robust, and requires a much shorter time for training when compared to existing systems.","2023-05","2023-07-05 07:10:08","2023-07-19 11:32:22","2023-07-05 07:10:08","6783-6796","","5","48","","Arab J Sci Eng","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P5A6EN86","journalArticle","2020","Abboud, Ralph; Tekli, Joe","Integration of nonparametric fuzzy classification with an evolutionary-developmental framework to perform music sentiment-based analysis and composition","Soft Computing","","1432-7643, 1433-7479","10.1007/s00500-019-04503-4","http://link.springer.com/10.1007/s00500-019-04503-4","Over the past years, several approaches have been developed to create algorithmic music composers. Most existing solutions focus on composing music that appears theoretically correct or interesting to the listener. However, few methods have targeted sentiment-based music composition: generating music that expresses human emotions. The few existing methods are restricted in the spectrum of emotions they can express (usually to two dimensions: valence and arousal) as well as the level of sophistication of the music they compose (usually monophonic, following translation-based, predefined templates or heuristic textures). In this paper, we introduce a new algorithmic framework for autonomous music sentiment-based expression and composition, titled MUSEC, that perceives an extensible set of six primary human emotions (e.g., anger, fear, joy, love, sadness, and surprise) expressed by a MIDI musical file and then composes (creates) new polyphonic (pseudo) thematic, and diversified musical pieces that express these emotions. Unlike existing solutions, MUSEC is: (i) a hybrid crossover between supervised learning (SL, to learn sentiments from music) and evolutionary computation (for music composition, MC), where SL serves at the fitness function of MC to compose music that expresses target sentiments, (ii) extensible in the panel of emotions it can convey, producing pieces that reflect a target crisp sentiment (e.g., love) or a collection of fuzzy sentiments (e.g., 65% happy, 20% sad, and 15% angry), compared with crisp-only or two-dimensional (valence/arousal) sentiment models used in existing solutions, (iii) adopts the evolutionary-developmental model, using an extensive set of specially designed music-theoretic mutation operators (trille, staccato, repeat, compress, etc.), stochastically orchestrated to add atomic (individual chord-level) and thematic (chord pattern-level) variability to the composed polyphonic pieces, compared with traditional evolutionary solutions producing monophonic and non-thematic music. We conducted a large battery of tests to evaluate MUSEC’s effectiveness and efficiency in both sentiment analysis and composition. It was trained on a specially constructed set of 120 MIDI pieces, including 70 sentiment-annotated pieces: the first significant dataset of sentiment-labeled MIDI music made available online as a benchmark for future research in this area. Results are encouraging and highlight the potential of our approach in different application domains, ranging over music information retrieval, music composition, assistive music therapy, and emotional intelligence.","2020-07","2023-07-05 07:10:08","2023-07-21 05:00:26","2023-07-05 07:10:08","9875-9925","","13","24","","Soft Comput","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XWSSB7L5","journalArticle","2022","Yang, Tiancheng; Nazir, Shah","A comprehensive overview of AI-enabled music classification and its influence in games","Soft Computing","","1432-7643, 1433-7479","10.1007/s00500-022-06734-4","https://link.springer.com/10.1007/s00500-022-06734-4","With the development and advancement of information technology, artificial intelligence (AI) and machine learning (ML) are applied in every sector of life. Among these applications, music is one of them which has gained attention in the last couple of years. The music industry is revolutionized with AIbased innovative and intelligent techniques. It is very convenient for composers to compose music of high quality using these technologies. Artificial intelligence and Music (AIM) is one of the emerging fields used to generate and manage sounds for different media like the Internet, games, etc. Sounds in the games are very effective and can be made more attractive by implementing AI approaches. The quality of sounds in the game directly impacts the productivity and experience of the player. With computer-assisted technologies, the game designers can create sounds for different scenarios or situations like horror and suspense and provide gamer information. The practical and productive audio of a game can guide visually impaired people during other events in the game. For the better creation and composition of music, good quality of knowledge about musicology is essential. Due to AIM, there are a lot of intelligent and interactive tools available for the efficiency and effective learning of music. The learners can be provided with a very reliable and interactive environment based on artificial intelligence. The current study has considered presenting a detailed overview of the literature available in the area of research. The study has demonstrated literature analysis from various perspectives, which will become evidence for researchers to devise novel solutions in the field.","2022-08","2023-07-05 07:10:08","2023-07-21 05:00:40","2023-07-05 07:10:08","7679-7693","","16","26","","Soft Comput","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/8GPIYF78/Yang and Nazir - 2022 - A comprehensive overview of AI-enabled music class.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A2VVJLD2","journalArticle","2007","Ardito, C.; Costabile, M. F.; De Angeli, A.; Pittarello, F.","Navigation help in 3D worlds: some empirical evidences on use of sound","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-006-0060-0","http://link.springer.com/10.1007/s11042-006-0060-0","The concept of Interaction Locus (IL) has been introduced to help the users to orient, navigate, and identify relevant interaction areas in 3D Virtual Environments (VEs). The IL is a multimodal concept: it adds to the 3D visual scene parallel information channels that are perceived by other senses. In particular, the IL emphasizes the role of music as a navigation aid in a VE. This paper reports three user-evaluations of different IL enriched virtual worlds, and in particular of the role of the IL auditory component. Results suggest that audio in 3D plays not only an aesthetic role, which the users greatly appreciate, but also a functional role simplifying navigation and helping the users to recognise scenes in the environment. Such a functional role however is subordinated to a proper understanding of the link between music and virtual space. While these experiments refer to desktop virtual reality environments, their findings are general enough to inform the design of navigational tools for other segments of the mixed reality domain.","2007-05","2023-07-05 07:10:08","2023-07-21 04:31:37","2023-07-05 07:10:08","201-216","","2","33","","Multimed Tools Appl","Navigation help in 3D worlds","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BIQ5VIF6","journalArticle","2023","Weaver, Alexis; Firmer, Genevieve; Motion, Alice; O’Regan, Jadey; O’Reilly, Chiara; Yeadon, Daniel","Sounding Out Science: the Sonaphor and Electronic Sound Design as a Learning Tool in Secondary Science","Postdigital Science and Education","","2524-485X, 2524-4868","10.1007/s42438-022-00321-4","https://link.springer.com/10.1007/s42438-022-00321-4","Abstract                            The divergent use of digital technologies provides an important opportunity for students to develop critical and postdigital approaches to learning. Despite the rising accessibility of music technology, creatively composed sound is a relatively underexplored educational tool compared to the musical elements of melody, rhythm, and lyrics. Sound’s ability to transfer spatial and temporal information renders it a transformative tool for teaching and learning. Embracing an interdisciplinary approach, our research explores the possibility of supplementing secondary science education with a sound-based learning tool which creatively interprets scientific concepts to increase comprehension and engagement. Building on the existing ways in which science is communicated through music and sound, we have developed the               Sonaphor               (abbreviated from ‘sonic metaphor’). This article will outline the capacity for experimental electronic sound design to increase engagement in contexts ranging from classrooms through to informal learning environments. We see potential for the               Sonaphor               as a learning tool that reignites wonder and curiosity in science; it combines learning and creativity in sound design and science, allowing learners to interact with, and create their own               Sonaphors               . Through exemplar               Sonaphors               , we highlight a proposed structure and discuss the importance of harmonious script, dialogue, and sound design. The flexibility of the digital medium and increasing ubiquity of sound recording and editing software presents an opportunity for               Sonaphors               to become ‘living’ digital objects that could be adapted by different narrators, sound designers, and artists for different cultures, languages, syllabi, and purposes that build inclusivity in science education and communication.","2023-04","2023-07-05 07:10:08","2023-07-05 07:10:08","2023-07-05 07:10:08","408-439","","2","5","","Postdigit Sci Educ","Sounding Out Science","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/V4WB6EB6/Weaver et al. - 2023 - Sounding Out Science the Sonaphor and Electronic .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IL6IJADS","journalArticle","2016","Katz, Brian F. G.; Marentakis, Georgios","Advances in auditory display research","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-016-0226-7","http://link.springer.com/10.1007/s12193-016-0226-7","","2016-09","2023-07-05 07:10:08","2023-07-05 07:10:08","2023-07-05 07:10:08","191-193","","3","10","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/4DHZSA6P/Katz and Marentakis - 2016 - Advances in auditory display research.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JQS9AY9J","journalArticle","2019","Ghai, Shashank; Ghai, Ishan","Effects of (music-based) rhythmic auditory cueing training on gait and posture post-stroke: A systematic review & dose-response meta-analysis","Scientific Reports","","2045-2322","10.1038/s41598-019-38723-3","https://www.nature.com/articles/s41598-019-38723-3","Gait dysfunctions are common post-stroke. Rhythmic auditory cueing has been widely used in gait rehabilitation for movement disorders. However, a consensus regarding its influence on gait and postural recovery post-stroke is still warranted. A systematic review and meta-analysis was performed to analyze the effects of auditory cueing on gait and postural stability post-stroke. Nine academic databases were searched according to PRISMA guidelines. The eligibility criteria for the studies were a) studies were randomized controlled trials or controlled clinical trials published in English, German, Hindi, Punjabi or Korean languages b) studies evaluated the effects of auditory cueing on spatiotemporal gait and/or postural stability parameters post-stroke c) studies scored ≥4 points on the PEDro scale. Out of 1,471 records, 38 studies involving 968 patients were included in this present review. The review and meta-analyses revealed beneficial effects of training with auditory cueing on gait and postural stability. A training dosage of 20–45 minutes session, for 3–5 times a week enhanced gait performance, dynamic postural stability i.e. velocity (Hedge’s g: 0.73), stride length (0.58), cadence (0.75) and timed-up and go test (−0.76). This review strongly recommends the incorporation of rhythmic auditory cueing based training in gait and postural rehabilitation, post-stroke.","2019-02-18","2023-07-05 07:10:23","2023-07-05 07:10:23","2023-07-05 07:10:23","2183","","1","9","","Sci Rep","Effects of (music-based) rhythmic auditory cueing training on gait and posture post-stroke","","","","","","","en","2019 The Author(s)","","","","www.nature.com","","Number: 1 Publisher: Nature Publishing Group","","/Users/minsik/Zotero/storage/C3CKX3LW/Ghai and Ghai - 2019 - Effects of (music-based) rhythmic auditory cueing .pdf","","","Rehabilitation; Stroke","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"64V9BRGX","bookSection","2014","Sterkenburg, Jason; Jeon, Myounghoon; Plummer, Christopher","Auditory Emoticons: Iterative Design and Acoustic Characteristics of Emotional Auditory Icons and Earcons","Human-Computer Interaction. Advanced Interaction Modalities and Techniques","978-3-319-07229-6 978-3-319-07230-2","","","http://link.springer.com/10.1007/978-3-319-07230-2_60","In recent decades there has been an increased interest in sonification research. Two commonly used sonification techniques, auditory icons and earcons, have been the subject of a lot of study. However, despite this there has been relatively little research investigating the relationship between these sonification techniques and emotions and affect. Additionally, despite their popularity, auditory icons and earcons are often treated separately and are rarely compared directly in studies. The current paper shows iterative design procedures to create emotional auditory icons and earcons. The ultimate goal of the study is to compare auditory icons and earcons in their ability to represent emotional states. The results show that there are some strong user preferences both within sonification categories and between sonfication categories. The implications and extensions of this work are discussed.","2014","2023-07-05 07:11:51","2023-07-20 06:30:48","2023-07-05 07:11:51","633-640","","","8511","","","Auditory Emoticons","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-07230-2_60","","","","","","Kurosu, Masaaki","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Kobsa, Alfred; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Doug; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JF5HR37I","bookSection","2010","Sobue, Shin-ichi; Araki, Hiroshi; Tazawa, Seiichi; Noda, Hirotomo; Kamiya, Izumi; Yamamoto, Aya; Fujita, Takeo; Higashiizumi, Ichiro; Okumura, Hayato","An Application of Lunar GIS with Visualized and Auditory Japan’s Lunar Explorer “Kaguya” Data","Advanced Techniques in Computing Sciences and Software Engineering","978-90-481-3659-9 978-90-481-3660-5","","","https://link.springer.com/10.1007/978-90-481-3660-5_27","This paper describes an application of a geographical information system with visualized and sonification lunar remote sensing data provided by Japan’s lunar explorer (SELENE “KAGUYA”). Web based GIS is a very powerful tool which lunar scientists can use to visualize and access remote sensing data with other geospatial information. We discuss enhancement of the pseudo-colored visual map presentation of lunar topographical altimetry data derived from LALT and the map of the data to several sound parameters (Interval, harmony, and tempo). This paper describes an overview of this GIS with a sonification system, called “Moonbell”.","2010","2023-07-05 07:11:51","2023-07-19 11:09:41","2023-07-05 07:11:51","159-163","","","","","","","","","","","Springer Netherlands","Dordrecht","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-90-481-3660-5_27","","","","","","Elleithy, Khaled","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AZ7X7EPY","journalArticle","2020","Groß-Vogt, Katharina; Frank, Matthias; Höldrich, Robert","Focused Audification and the optimization of its parameters","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-019-00317-8","http://link.springer.com/10.1007/s12193-019-00317-8","Abstract             We present a sonification method which we call Focused Audification (FA; previously: Augmented Audification) that allows to expand pure audification in a flexible way. It is based on a combination of single-side-band modulation and a pitch modulation of the original data stream. Based on two free parameters, the sonification’s frequency range is adjustable to the human hearing range and allows to interactively zoom into the data set at any scale. The parameters have been adjusted in a multimodal experiment on cardiac data by laypeople. Following from these results we suggest a procedure for parameter optimization to achieve an optimal listening range for any data set, adjusted to human speech.","2020-06","2023-07-05 07:11:51","2023-07-05 07:11:51","2023-07-05 07:11:51","187-198","","2","14","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/W2UIQ4QV/Groß-Vogt et al. - 2020 - Focused Audification and the optimization of its p.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8LEFTREU","bookSection","2014","Taibbi, Marzia; Bernareggi, Cristian; Gerino, Andrea; Ahmetovic, Dragan; Mascetti, Sergio","AudioFunctions: Eyes-Free Exploration of Mathematical Functions on Tablets","Computers Helping People with Special Needs","978-3-319-08595-1 978-3-319-08596-8","","","http://link.springer.com/10.1007/978-3-319-08596-8_84","It is well known that mathematics presents a number of hindrances to visually impaired students. In case of function graphs, for example, several assistive solutions have been proposed to enhance their accessibility. Unfortunately, both hardware tools (e.g., tactile paper) and existing software applications cannot guarantee, at the same time, a clear understanding of the graph and a full autonomous study. In this paper we present AudioFunctions, an iPad app that adopts three sonification techniques to convey information about the function graph. Our experimental evaluation, conducted with 7 blind people, clearly highlights that, by using AudioFunctions, students have a better understanding of the graph than with tactile paper and existing software solutions.","2014","2023-07-05 07:11:51","2023-07-19 23:53:17","2023-07-05 07:11:51","537-544","","","8547","","","AudioFunctions","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-08596-8_84","","","","","","Miesenberger, Klaus; Fels, Deborah; Archambault, Dominique; Peňáz, Petr; Zagler, Wolfgang","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2SN3MXP7","bookSection","2005","Cahen, Roland; Rodet, Xavier; Lambert, Jean-Philippe","Sound Navigation in PHASE Installation: Producing Music as Performing a Game Using Haptic Feedback","Virtual Storytelling. Using Virtual Reality Technologies for Storytelling","978-3-540-30511-8 978-3-540-32285-6","","","http://link.springer.com/10.1007/11590361_5","Sound Navigation consists in browsing through different sound objects and sound generators situated within a virtual world including virtual spatialized sound and visual scenes, to perform a musical trajectory and composition. In the PHASE Project installation, the 3D virtual world resembles the surface of a vinyl disk, magnified so that one can see the groove and move a “needle” (the “reading head”) in it and out of it to read the disk. Another such “needle” (the “writing head”) can “write” music in the groove. A part of the game is a pursuit between the writing head and the reading head handled by the player. Different musical devices have been implemented. Most of them have a haptic behavior. The scenario is fully related to the musical metaphor and aims to give equivalent pleasure to contemplative as well as to competitive players.","2005","2023-07-05 07:11:51","2023-07-21 05:15:07","2023-07-05 07:11:51","41-50","","","3805","","","Sound Navigation in PHASE Installation","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11590361_5","","","","","","Subsol, Gérard","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8KMT8ZD7","bookSection","2008","Young, Michael","NN Music: Improvising with a ‘Living’ Computer","Computer Music Modeling and Retrieval. Sense of Sounds","978-3-540-85034-2 978-3-540-85035-9","","","http://link.springer.com/10.1007/978-3-540-85035-9_23","This paper proposes attributes of a living computer music, the product of a live algorithm. It illustrates how these attributes can inform creative design with reference to a real-time system for solo performer-machine collaboration, Neural Network Music, and the PQƒ framework proposed for live algorithms. Improvisation is treated as a classification problem at a high level of musical behaviour which can be measured statistically and train a multilayer perceptron neural network. Network outputs shape a stochastic-based synthesis engine. Mappings are covertly assigned, revisited by both player and machine as a performance develops. As the timing and choice of mapping is unknown, both participants are invited to learn and adapt to a responsive sonic environment which is created afresh on each performance. This offers a novel real-time application of feed-forward neural networks and a challenging, creative technological platform for freely improvised music.","2008","2023-07-05 07:11:51","2023-07-19 23:50:06","2023-07-05 07:11:51","337-350","","","4969","","","NN Music","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-85035-9_23","","/Users/minsik/Zotero/storage/SNLRSZA3/Young - 2008 - NN Music Improvising with a ‘Living’ Computer.pdf","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T87CT5DS","bookSection","2008","Tanaka, Atau","Visceral Mobile Music Systems","Transdisciplinary Digital Art. Sound, Vision and the New Screen","978-3-540-79485-1 978-3-540-79486-8","","","http://link.springer.com/10.1007/978-3-540-79486-8_15","This paper describes a second-generation mobile music system that adds qualities of physical interaction to previous participative, networked, multi-user systems. We call upon traditions in interactive sensor music instrument building to inform this process. The resulting system underscores its dual personal/community context awareness with a technique of hybrid audio display. This allows the system to exhibit qualities of reflexive social translucence providing a view of the group all while giving each member of the group a responsive sense of agency. The visceral mobile music system was tested in a theatre environment with manageable location tracking and creative non-musical test subjects. The combination of musical practice and interaction design establish artistic creativity as an important component of the research process.","2008","2023-07-05 07:11:51","2023-07-21 05:07:39","2023-07-05 07:11:51","155-170","","","7","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Communications in Computer and Information Science DOI: 10.1007/978-3-540-79486-8_15","","","","","","Adams, Randy; Gibson, Steve; Arisona, Stefan Müller","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U2RIZDDA","bookSection","2016","Braund, Edward; Miranda, Eduardo R.","BioComputer Music: Generating Musical Responses with Physarum polycephalum-Based Memristors","Music, Mind, and Embodiment","978-3-319-46281-3 978-3-319-46282-0","","","http://link.springer.com/10.1007/978-3-319-46282-0_26","This paper introduces BioComputer Music, an experimental one piano duet between pianist and plasmodial slime mould Physarum polycephalum. This piece harnesses a system we have been developing, which we call BioComputer. BioComputer consists of an analogue circuit that encompasses components grown from the biological computing substrate Physarum polycephalum. Our system listens to the pianist and uses the memristive characteristics of Physarum polycephalum to generate a musical response that it plays through electromagnets placed on the strings of the piano. Such electromagnets set the strings into vibration, producing a distinctive timbre. Physarum polycephalum is an amorphous unicellular organism that has been discovered to exhibit memristive qualities. The memristor changes its resistance according to the amount of charge that has previously flown through. In this paper, we introduce the general concepts, technology and musical composition behind the BioComputer Music piece. We also discuss our rationale for using Physarum polycephalum .","2016","2023-07-05 07:11:51","2023-07-21 04:35:10","2023-07-05 07:11:51","405-419","","","9617","","","BioComputer Music","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-46282-0_26","","/Users/minsik/Zotero/storage/B7URZW7I/Braund and Miranda - 2016 - BioComputer Music Generating Musical Responses wi.pdf","","","","Kronland-Martinet, Richard; Aramaki, Mitsuko; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y9Q2AV7Y","bookSection","2013","Funk, Mathias; Hengeveld, Bart; Frens, Joep; Rauterberg, Matthias","Aesthetics and Design for Group Music Improvisation","Distributed, Ambient, and Pervasive Interactions","978-3-642-39350-1 978-3-642-39351-8","","","http://link.springer.com/10.1007/978-3-642-39351-8_40","Performing music as a group—improvised or from sheet music—is an intensive and immersive interaction activity that bears its own aesthetics. Players in such a setting are usually skilled in playing an instrument up to the level where they do not need to focus on the “operation” of the instrument, but can instead focus on higher-level feedback loops, e.g., between players in their section or the entire group. Novel technology can capitalize on these higherlevel feedback loops through the creation of interactive musical instruments that stimulate playing in groups (collaborative music rather than parallel music). However, making this experience accessible to fresh or novice players involves two challenges: how to design (1) musical instruments for such a setting and experience, and (2) instrument support that extends the interaction between players to their instruments. This allows to interact not only via their instrument with other human players, but directly with other instruments, producing a much richer and more intertwined musical experience. The paper shows results from a class of design students and reports on the lessons learned.","2013","2023-07-05 07:11:51","2023-07-19 23:57:50","2023-07-05 07:11:51","368-377","","","8028","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-39351-8_40","","/Users/minsik/Zotero/storage/XINN6B9Z/Funk et al. - 2013 - Aesthetics and Design for Group Music Improvisatio.pdf","","","","Streitz, Norbert; Stephanidis, Constantine","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AANSA4NP","journalArticle","2017","Raglio, Alfredo; Zaliani, Alberto; Baiardi, Paola; Bossi, Daniela; Sguazzin, Cinzia; Capodaglio, Edda; Imbriani, Chiara; Gontero, Giulia; Imbriani, Marcello","Active music therapy approach for stroke patients in the post-acute rehabilitation","Neurological Sciences","","1590-1874, 1590-3478","10.1007/s10072-017-2827-7","http://link.springer.com/10.1007/s10072-017-2827-7","Guidelines in stroke rehabilitation recommend the use of a multidisciplinary approach. Different approaches and techniques with music are used in the stroke rehabilitation to improve motor and cognitive functions but also psychological outcomes. In this randomized controlled pilot trial, relational active music therapy approaches were tested in the post-acute phase of disease. Thirty-eight hospitalized patients with ischemic and hemorrhagic stroke were recruited and allocated in two groups. The experimental group underwent the standard of care (physiotherapy and occupational therapy daily sessions) and relational active music therapy treatments. The control group underwent the standard of care only. Motor functions and psychological aspects were assessed before and after treatments. Music therapy process was also evaluated using a specific rating scale. All groups showed a positive trend in quality of life, functional and disability levels, and gross mobility. The experimental group showed a decrease of anxiety and, in particular, of depression (p = 0.016). In addition, the strength of non-dominant hand (grip) significantly increased in the experimental group (p = 0.041). Music therapy assessment showed a significant improvement over time of non-verbal and sonorous-music relationships. Future studies, including a greater number of patients and follow-up evaluations, are needed to confirm promising results of this study.","2017-05","2023-07-05 07:11:51","2023-07-21 04:37:34","2023-07-05 07:11:51","893-897","","5","38","","Neurol Sci","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W9LUS8N2","journalArticle","2020","Wilson, Rebekah","Aesthetic and technical strategies for networked music performance","AI & SOCIETY","","0951-5666, 1435-5655","10.1007/s00146-020-01099-4","http://link.springer.com/10.1007/s00146-020-01099-4","Networked music is no longer a future genre: the global quarantine event of 2020 launched the concept of performing together over the Internet into the mainstream. While the demand for performing at a distance may be a new imperative, musicians find themselves faced with technological and performative processes that do not appear to be suitable for performing music together online, due particularly to network latency which disrupts the ability for musicians to synchronize. The research presented in this paper investigates and challenges the reasons why networked music is not readily embraced by musicians and describes how that might change, by way of interviews with practitioners and an in-depth review of the technical constraints. Limitations that might cause frustration are in fact shown to have creative strategies that give rise to aesthetic approaches, distinct to the platform. By exploiting the constraints, in tandem with developing technology designed specifically for remote performance, aesthetic implications arise that not only accommodate the inconveniences of latency and acoustic feedback but can help us adapt and transform how we engage in real-time online, towards a future where we can imagine performing together over even more dramatic distances such as high-latency, low-bandwidth locations outside of urban areas—or even over galactic distances.","2020-11-18","2023-07-05 07:11:51","2023-07-19 11:28:29","2023-07-05 07:11:51","","","","","","AI & Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/KCQU4KW8/Wilson - 2020 - Aesthetic and technical strategies for networked m.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q8IKFXB2","bookSection","2013","Schubert, Emery; Ferguson, Sam; Farrar, Natasha; Taylor, David; McPherson, Gary E.","The Six Emotion-Face Clock as a Tool for Continuously Rating Discrete Emotional Responses to Music","From Sounds to Music and Emotions","978-3-642-41247-9 978-3-642-41248-6","","","http://link.springer.com/10.1007/978-3-642-41248-6_1","","2013","2023-07-05 07:11:51","2023-07-05 07:11:51","2023-07-05 07:11:51","1-18","","","7900","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-41248-6_1","","","","","","Aramaki, Mitsuko; Barthet, Mathieu; Kronland-Martinet, Richard; Ystad, Sølvi","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SM5CGKG8","bookSection","2005","Arifi, Vlora; Clausen, Michael; Kurth, Frank; Müller, Meinard","Score-PCM Music Synchronization Based on Extracted Score Parameters","Computer Music Modeling and Retrieval","978-3-540-24458-5 978-3-540-31807-1","","","http://link.springer.com/10.1007/978-3-540-31807-1_15","In this paper we present algorithms for the automatic time-synchronization of score-, MIDI- or PCM-data streams which represent the same polyphonic piano piece. In contrast to related approaches, we compute the actual alignment by using note parameters such as onset times and pitches. Working in a score-like domain has advantages in view of the efficiency and accuracy: due to the expressiveness of score-like parameters only a small number of such features is sufficient to solve the synchronization task. To obtain a score-like representation from the waveform-based PCM-data streams we use a preprocessing step to extract note parameters. In this we use the concept of novelty curves for onset detection and multirate filter banks in combination with note templates for pitch extraction. Also the data streams in MIDI- and score-format have to be suitably preprocessed. In particular, we suggest a data format which handles possible ambiguities such as trills or arpeggios by introducing the concept of fuzzy-notes. Further decisive ingredients of our approach are carefully designed cost functions in combination with an appropriate notion of alignment which is more flexible than the classical DTW concept. Our synchronization algorithms have been tested on a variety of classical polyphonic piano pieces recorded on MIDI- and standard acoustic pianos or taken from CD-recordings.","2005","2023-07-05 07:11:51","2023-07-19 23:48:04","2023-07-05 07:11:51","193-210","","","3310","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-31807-1_15","","","","","","Wiil, Uffe Kock","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SEZWGCYV","bookSection","2008","Amatriain, Xavier; Castellanos, Jorge; Höllerer, Tobias; Kuchera-Morin, JoAnn; Pope, Stephen T.; Wakefield, Graham; Wolcott, Will","Experiencing Audio and Music in a Fully Immersive Environment","Computer Music Modeling and Retrieval. Sense of Sounds","978-3-540-85034-2 978-3-540-85035-9","","","http://link.springer.com/10.1007/978-3-540-85035-9_27","The UCSB Allosphere is a 3-story-high spherical instrument in which virtual environments and performances can be experienced in full immersion. The space is now being equipped with high-resolution active stereo projectors, a 3D sound system with several hundred speakers, and with tracking and interaction mechanisms. The Allosphere is at the same time multimodal, multimedia, multi-user, immersive,andinteractive. This novel and unique instrument will be used for research into scientific visualization/auralization and data exploration, and as a research environment for behavioral and cognitive scientists. It will also serve as a research and performance space for artists exploring new forms of art. In particular, the Allosphere has been carefully designed to allow for immersive music and aural applications. In this paper, we give an overview of the instrument, focusing on the audio subsystem. We give the rationale behind some of the design decisions and explain the different techniques employed in making the Allosphere a truly generalpurpose immersive audiovisual lab and stage. Finally, we present first results and our experiences in developing and using the Allosphere in several prototype projects.","2008","2023-07-05 07:11:51","2023-07-19 23:48:58","2023-07-05 07:11:51","380-400","","","4969","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-85035-9_27","","/Users/minsik/Zotero/storage/CZCTJIB6/Amatriain et al. - 2008 - Experiencing Audio and Music in a Fully Immersive .pdf","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HVW5KR3M","bookSection","2023","Partesotti, Elena; Hebling, Eduardo D.; Moroni, Artemis S.; Antunes, Micael; Da Silva, César P.; Dezotti, Cássio G.; Manzolli, Jônatas","Analysis of Affective Behavior in the Artistic Installation Moviescape","ArtsIT, Interactivity and Game Creation","978-3-031-28992-7 978-3-031-28993-4","","","https://link.springer.com/10.1007/978-3-031-28993-4_23","The purpose of this paper is to study the correlation between attention and affective response when a user interacts with the artistic installation MovieScape - an Extended Immersive Digital Music Instrument. We analyzed the affective modulation in ten subjects. For that, we applied Affective Slider and audio-segmentation on the recorded performances of a Circumplex Space (a 2D Circumplex Model of Affect) to apply quantitative metrics and divide the interaction in Explorative and Contemplative states. In our paper we propose this as a unifying conceptual framework for analysing affective behavior within interactive installation such as MS. Overall, the analyses present an increase of Arousal after the interaction of the subjects within the environment, and a correlation between the Explorative interaction and the increment of Arousal which confirms the improvement in attention. Eventually, an inductive mechanism of expectancy occurred. These conclusions open up for potential in designing EIDMIs for therapeutic and pedagogical purposes. Also, these findings may also prove that subjects reached Creative Empowerment while becoming aware of MovieScape.","2023","2023-07-05 07:11:51","2023-07-19 11:35:32","2023-07-05 07:11:51","327-345","","","479","","","","","","","","Springer Nature Switzerland","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering DOI: 10.1007/978-3-031-28993-4_23","","","","","","Brooks, Anthony L.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6IBHBW7F","journalArticle","2011","Yuan, Bei; Folmer, Eelke; Harris, Frederick C.","Game accessibility: a survey","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-010-0189-5","http://link.springer.com/10.1007/s10209-010-0189-5","Over the last three decades, video games have evolved from a pastime into a force of change that is transforming the way people perceive, learn about, and interact with the world around them. In addition to entertainment, games are increasingly used for other purposes such as education or health. Despite this increased interest, a significant number of people encounter barriers when playing games due to a disability. Accessibility problems may include the following: (1) not being able to receive feedback; (2) not being able to determine in-game responses; (3) not being able to provide input using conventional input devices. This paper surveys the current state-of-the-art in research and practice in the accessibility of video games and points out relevant areas for future research. A generalized game interaction model shows how a disability affects ones ability to play games. Estimates are provided on the total number of people in the United States whose ability to play games is affected by a disability. A large number of accessible games are surveyed for different types of impairments, across several game genres, from which a number of high- and low-level accessibility strategies are distilled for game developers to inform their design.","2011-03","2023-07-05 07:11:51","2023-07-21 05:13:08","2023-07-05 07:11:51","81-100","","1","10","","Univ Access Inf Soc","Game accessibility","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WASBZ2GF","bookSection","2001","Van Scoy, Frances L.; Kawai, Takamitsu; Darrah, Marjorie; Rash, Connie","Haptic display of mathematical functions for teaching mathematics to students with vision disabilities: design and proof of concept","Haptic Human-Computer Interaction","978-3-540-42356-0 978-3-540-44589-0","","","http://link.springer.com/10.1007/3-540-44589-7_4","The design and initial implementation of a system for constructing a haptic model of a mathematical function for exploration using a PHANToM are described. A user types the mathematical function as a Fortran arithmetic expression and the system described here carves the trace of the function onto a virtual block of balsa wood. Preliminary work in generating music which describes the function has begun.","2001","2023-07-05 07:11:51","2023-07-20 00:16:57","2023-07-05 07:11:51","31-40","","","2058","","","Haptic display of mathematical functions for teaching mathematics to students with vision disabilities","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-44589-7_4","","","","","","Brewster, Stephen; Murray-Smith, Roderick","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZMIEFRNM","bookSection","2018","Manzolli, Jonatas; Moroni, Artemis; Valarini, Guilherme A.","SELFHOOD: An Evolutionary and Interactive Experience Synthesizing Images and Sounds","Music Technology with Swing","978-3-030-01691-3 978-3-030-01692-0","","","http://link.springer.com/10.1007/978-3-030-01692-0_41","The SELFHOOD installation was conceived aiming to instigate a reflection on the self through a practical and interactive experience. A representation of each participant is created in a form of a cloud of points and a sound drone, suggesting their selves. The dynamics of the visitors’ movements is sonified in such way that colours and sound textures are fused in a surrounding hexaphonic system. CromaCrono≈, the system for immersive improvisation that produces digitally synthesized sounds in real time, is described. Philosophical concepts concerning notions of the Self are presented. We propose that the notion of Presence can be induced by virtual and/or physical sources of stimulation governed by a number of principles that underlie human experience, creativity, and discovery. The methodological point of view is that the notion of Presence indicates that there are essential inputs for the construction of self-referral agents.","2018","2023-07-05 07:13:09","2023-07-21 04:34:01","2023-07-05 07:13:08","625-636","","","11265","","","SELFHOOD","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-01692-0_41","","","","","","Aramaki, Mitsuko; Davies, Matthew E. P.; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LFRIR8VP","bookSection","2018","Chamberlain, Alan; Bødker, Mads; De Roure, David; Willcox, Pip; Emsley, Iain; Malizia, Alessio","A Landscape of Design: Interaction, Interpretation and the Development of Experimental Expressive Interfaces","Human-Computer Interaction. Theories, Methods, and Human Issues","978-3-319-91237-0 978-3-319-91238-7","","","https://link.springer.com/10.1007/978-3-319-91238-7_3","This short paper presents the initial research insights of an ongoing research project that focuses upon understanding the role of landscape, its use as a resource for designing interfaces for musical expression, and as a tool for leveraging ethnographic understandings about space, place, design and musical expression. We briefly discuss the emerging research and reasoning behind our approach, the site that we are focusing on, our participatory methodology and conceptual designs. This innovative research is envisaged as something that can engage and interest the conference participants, encourage debate and act as an exploratory platform, which will in turn inform our research, practice and design.","2018","2023-07-05 07:13:09","2023-07-20 06:33:04","2023-07-05 07:13:08","24-34","","","10901","","","A Landscape of Design","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-91238-7_3","","/Users/minsik/Zotero/storage/A76A8PCZ/Chamberlain et al. - 2018 - A Landscape of Design Interaction, Interpretation.pdf","","","","Kurosu, Masaaki","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TNPNWR2W","journalArticle","2012","Ness, Steven; Reimer, Paul; Love, Justin; Schloss, W. Andrew; Tzanetakis, George","Sonophenology: A multimodal tangible interface for the sonification of phenological data at multiple time-scales","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0066-4","http://link.springer.com/10.1007/s12193-011-0066-4","","2012-05","2023-07-05 07:13:09","2023-07-05 07:13:09","2023-07-05 07:13:08","123-129","","3-4","5","","J Multimodal User Interfaces","Sonophenology","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M7IDTVCU","bookSection","2007","Korhonen, Hannu; Holm, Jukka; Heikkinen, Mikko","Utilizing Sound Effects in Mobile User Interface Design","Human-Computer Interaction – INTERACT 2007","978-3-540-74794-9 978-3-540-74796-3","","","http://link.springer.com/10.1007/978-3-540-74796-3_27","The current generation of mobile devices is capable of producing polyphonic sounds, has enough processing power for real-time signal processing, and much better sound quality than their predecessors. The importance of audio is increasing as we are moving towards multimodal user interfaces where audio is one of the major components. In this paper, we present new ways of using audio feedback more efficiently and intelligently in mobile user interfaces by utilizing real-time signal processing. To test the ideas in practice, a prototype calendar application was implemented. We arranged a one week field trial to validate the design ideas. The results indicate that sound effects are capable of passing information to the user in some extent, but they are more useful in impressing the user and making existing audio feedback sound better.","2007","2023-07-05 07:13:09","2023-07-20 05:55:58","2023-07-05 07:13:08","283-296","","","4662","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-74796-3_27","","/Users/minsik/Zotero/storage/INP5M5VW/Korhonen et al. - 2007 - Utilizing Sound Effects in Mobile User Interface D.pdf","","","","Baranauskas, Cécilia; Palanque, Philippe; Abascal, Julio; Barbosa, Simone Diniz Junqueira","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IBQECUE7","journalArticle","2022","Paté, Arthur; Farge, Gaspard; Holtzman, Benjamin K.; Barth, Anna C.; Poli, Piero; Boschi, Lapo; Karlstrom, Leif","Combining audio and visual displays to highlight temporal and spatial seismic patterns","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-021-00378-8","https://link.springer.com/10.1007/s12193-021-00378-8","","2022-03","2023-07-05 07:13:09","2023-07-05 07:13:09","2023-07-05 07:13:08","125-142","","1","16","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A6LYQH6E","journalArticle","2019","Ziemer, Tim; Schultheis, Holger","Psychoacoustic auditory display for navigation: an auditory assistance system for spatial orientation tasks","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-018-0282-2","http://link.springer.com/10.1007/s12193-018-0282-2","A psychoacoustic auditory display for navigation is presented. Interactive sonification guides users to an invisible target location in two-dimensional space. Orthogonal spatial dimensions are mapped to perceptual auditory qualities that are orthogonal as well. The psychoacoustic auditory display could serve as an alternative or complement to conventional assistance systems for vehicle or airplane control, or for minimally invasive surgery. The approach is evaluated by an experiment, which compares the performance of 18 participants approaching (i) a visually presented target (ii) an invisible target guided by sound. Results demonstrate that users are able to integrate the sonified information to find the right angle and distance, or to segregate both spatial axes and interpret one at a time. Auditory navigation takes significantly longer than visual navigation, but path lengths are not significantly different.","2019-09","2023-07-05 07:13:09","2023-07-20 07:06:16","2023-07-05 07:13:08","205-218","","3","13","","J Multimodal User Interfaces","Psychoacoustic auditory display for navigation","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UUBL5HN9","journalArticle","2009","Vézien, J. M.; Ménélas, B.; Nelson, J.; Picinali, L.; Bourdot, P.; Ammi, M.; Katz, B. F. G.; Burkhardt, J. M.; Pastur, L.; Lusseyran, F.","Multisensory VR exploration for computer fluid dynamics in the CoRSAIRe project","Virtual Reality","","1359-4338, 1434-9957","10.1007/s10055-009-0134-1","http://link.springer.com/10.1007/s10055-009-0134-1","In the last 30 years, the evolution of digital data processing in terms of processing power, storage capacity, and algorithmic efficiency in the simulation of physical phenomena has allowed the emergence of the discipline known as computational fluid dynamics or CFD. More recently, virtual reality (VR) systems have proven an interesting alternative to conventional user interfaces, in particular, when exploring complex and massive datasets, such as those encountered in scientific visualization applications. Unfortunately, all too often, VR technologies have proven unsatisfactory in providing a true added value compared to standard interfaces, mostly because insufficient attention was given to the activity and needs of the intended user audience. The present work focuses on the design of a multimodal VR environment dedicated to the analysis of non-stationary flows in CFD. Specifically, we report on the identification of relevant strategies of CFD exploration coupled to adapted VR data representation and interaction techniques. Three different contributions will be highlighted. First, we show how placing the CFD expert user at the heart of the system is accomplished through a formalized analysis of work activity and through system evaluation. Second, auditory outputs providing analysis of time-varying phenomena in a spatialized virtual environment are introduced and evaluated. Finally, specific haptic feedbacks are designed and evaluated to enhance classical visual data exploration of CFD simulations.","2009-12","2023-07-05 07:13:09","2023-07-21 05:14:46","2023-07-05 07:13:08","257-271","","4","13","","Virtual Reality","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KXDQN7YX","bookSection","2013","Carroll, Dustin; Chakraborty, Suranjan; Lazar, Jonathan","Designing Accessible Visualizations: The Case of Designing a Weather Map for Blind Users","Universal Access in Human-Computer Interaction. Design Methods, Tools, and Interaction Techniques for eInclusion","978-3-642-39187-3 978-3-642-39188-0","","","http://link.springer.com/10.1007/978-3-642-39188-0_47","Major strides have been made to improve the accessibility of textbased documents for blind users, however, visualizations still remain largely inaccessible. The AISP framework represents an attempt to streamline the design process by aligning the information seeking behaviors of a blind user with those of a sighted user utilizing auditory feedback. With the recent popularity of touch-based devices, and the overwhelming success of the talking tactile tablet, we therefore suggest that the AISP framework be extended to include the sense of touch. This research-in-progress paper proposes such an extended design framework, MISD. In addition, the article also presents the preliminary work done in designing an accessible weather map based on our theory-driven design. A discussion and an outline of future work conclude the manuscript.","2013","2023-07-05 07:13:09","2023-07-21 05:09:59","2023-07-05 07:13:08","436-445","","","8009","","","Designing Accessible Visualizations","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-39188-0_47","","/Users/minsik/Zotero/storage/6ULQ8FLP/Carroll et al. - 2013 - Designing Accessible Visualizations The Case of D.pdf","","","","Stephanidis, Constantine; Antona, Margherita","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6FZAYGZE","bookSection","2021","Villeneuve, Jérôme; Leonard, James; Tache, Olivier","Instruments and Sounds as Objects of Improvisation in Collective Computer Music Practice","Perception, Representations, Image, Sound, Music","978-3-030-70209-0 978-3-030-70210-6","","","http://link.springer.com/10.1007/978-3-030-70210-6_41","This paper presents the authors’ first attempt at a new (and unexpected) exercise: that of observing, contextualising and problematising their own collective Computer Music experiences. After two years practising emergent collective improvisation in private and public settings, which has led the authors to fundamentally reconsider both individual and collective musical creation, came the desire to methodologically deconstruct this process - one that they never anticipated and, until now, had never formalised. By starting from the very notions or performance and improvisation in the context of Computer Music, and crossing prolific literature on these topics with humble observations from their own experience, the authors then elaborate on what appears to them as the most enticing perspective of this creative context: the systematic improvisation of both their tools and sounds in an unique flow.","2021","2023-07-05 07:13:09","2023-07-21 04:45:41","2023-07-05 07:13:08","636-654","","","12631","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-70210-6_41","","/Users/minsik/Zotero/storage/38KARUHQ/Villeneuve et al. - 2021 - Instruments and Sounds as Objects of Improvisation.pdf","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Aramaki, Mitsuko","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"85EBJ78N","bookSection","2011","Castagna, Riccardo; Chiolerio, Alessandro; Margaria, Valentina","Music Translation of Tertiary Protein Structure: Auditory Patterns of the Protein Folding","Applications of Evolutionary Computation","978-3-642-20519-4 978-3-642-20520-0","","","http://link.springer.com/10.1007/978-3-642-20520-0_22","We have translated genome-encoded protein sequence into musical notes and created a polyphonic harmony taking in account its tertiary structure. We did not use a diatonic musical scale to obtain a pleasant sound, focusing instead on the spatial relationship between aminoacids closely placed in the 3-dimensional protein folding. In this way, the result is a musical translation of the real morphology of the protein, that opens the challenge to bring musical harmony rules into the proteomic research field.","2011","2023-07-05 07:13:09","2023-07-19 11:30:30","2023-07-05 07:13:08","214-222","","","6625","","","Music Translation of Tertiary Protein Structure","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-20520-0_22","","","","","","Di Chio, Cecilia; Brabazon, Anthony; Di Caro, Gianni A.; Drechsler, Rolf; Farooq, Muddassar; Grahl, Jörn; Greenfield, Gary; Prins, Christian; Romero, Juan; Squillero, Giovanni; Tarantino, Ernesto; Tettamanzi, Andrea G. B.; Urquhart, Neil; Uyar, A. Şima","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AYVKD65W","journalArticle","2021","Mandanici, Marcella; Di Filippo, Roberto; Delle Monache, Stefano","The Discovery of Interactive Spaces: Learning by Design in High School Music Technology Classes","Technology, Knowledge and Learning","","2211-1662, 2211-1670","10.1007/s10758-020-09464-4","https://link.springer.com/10.1007/s10758-020-09464-4","This paper describes an educational experience realized in the form of extracurricular workshops involving music technology students of the “V. Gambara” music high school in Brescia (Italy). By means of a participatory prototyping experience, the project aimed at fostering the students awareness and understanding of technological means and their utility . The Discovery of Interactive Spaces project focuses on motion tracking technologies in connection with sound and visual production, as means to provoke reflections on their cultural and societal impact on social utility and inclusion, and artistic expression. To this end, students proposed design concepts, and prototyped sonic interactive experiences. The Discovery of Interactive Spaces is framed within the broader themes of computational thinking and creativity, learning by design, and technology awareness. These themes represent the pillars of technological citizenship, which is considered crucial for the twenty-first century student.","2021-12","2023-07-05 07:13:09","2023-07-21 05:04:09","2023-07-05 07:13:08","1131-1151","","4","26","","Tech Know Learn","The Discovery of Interactive Spaces","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3CMFLQRP","journalArticle","2014","Thaut, Michael H.; McIntosh, Gerald C.","Neurologic Music Therapy in Stroke Rehabilitation","Current Physical Medicine and Rehabilitation Reports","","2167-4833","10.1007/s40141-014-0049-y","http://link.springer.com/10.1007/s40141-014-0049-y","Based on insights from brain research in music, neurologic music therapy (NMT) has been established as a new model for music in therapy and medicine. Standardized clinical interventions are based on clusters of research evidence and established learning principles in motor, speech/language, and cognitive training. The research support for NMT in stroke rehabilitation has been growing rapidly over the past 20 years. This paper will review research data and clinical applications for neurorehabilitation in the speech/language, cognitive and sensorimotor domains.","2014-06","2023-07-05 07:13:09","2023-07-19 23:54:34","2023-07-05 07:13:08","106-113","","2","2","","Curr Phys Med Rehabil Rep","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/XCPCPR5A/Thaut and McIntosh - 2014 - Neurologic Music Therapy in Stroke Rehabilitation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RNHZ7Y5D","bookSection","2006","Havryliv, Mark; Narushima, Terumi","Metris: A Game Environment for Music Performance","Computer Music Modeling and Retrieval","978-3-540-34027-0 978-3-540-34028-7","","","http://link.springer.com/10.1007/11751069_9","Metris is a version of the Tetris game that uses a player’s musical response to control game performance. The game is driven by two factors: traditional game design and the player’s individual sense of music and sound. Metris uses tuning principles to determine relationships between pitch and the timbre of the sounds produced. These relationships are represented as bells synchronised with significant events in the game. Key elements of the game design control a musical environment based on just intonation tuning. This presents a scenario where the game design is enhanced by a user's sense of sound and music. Conventional art music is subverted by responses to simple design elements in a popular game.","2006","2023-07-05 07:13:09","2023-07-19 23:48:15","2023-07-05 07:13:08","101-109","","","3902","","","Metris","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11751069_9","","/Users/minsik/Zotero/storage/92UAY6H9/Havryliv and Narushima - 2006 - Metris A Game Environment for Music Performance.pdf","","","","Kronland-Martinet, Richard; Voinier, Thierry; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HW97J364","journalArticle","2011","Van Der Vlist, Bram; Bartneck, Christoph; Mäueler, Sebastian","moBeat: Using Interactive Music to Guide and Motivate Users During Aerobic Exercising","Applied Psychophysiology and Biofeedback","","1090-0586, 1573-3270","10.1007/s10484-011-9149-y","http://link.springer.com/10.1007/s10484-011-9149-y","An increasing number of people are having trouble staying fit and maintaining a healthy bodyweight because of lack of physical activity. Getting people to exercise is crucial. However, many struggle with developing healthy exercising habits, due to hurdles like having to leave the house and the boring character of endurance exercising. In this paper, we report on a design project that explores the use of audio to motivate and provide feedback and guidance during exercising in a home environment. We developed moBeat, a system that provides intensity-based coaching while exercising, giving real-time feedback on training pace and intensity by means of interactive music. We conducted a within-subject comparison between our moBeat system and a commercially available heart rate watch. With moBeat, we achieved a comparable success rate: our system has a significant, positive influence on intrinsic motivation and attentional focus, but we did not see significant differences with regard to either perceived exertion or effectiveness. Although promising, future research is needed.","2011-06","2023-07-05 07:13:09","2023-07-19 11:32:14","2023-07-05 07:13:08","135-145","","2","36","","Appl Psychophysiol Biofeedback","moBeat","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/9IMYTPS6/Van Der Vlist et al. - 2011 - moBeat Using Interactive Music to Guide and Motiv.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y7CKLC5V","bookSection","2009","Kuuskankare, Mika; Laurson, Mikael","Strategies and Methods for Creating Symbolic Electroacoustic Scores in ENP","Computer Music Modeling and Retrieval. Genesis of Meaning in Sound and Music","978-3-642-02517-4 978-3-642-02518-1","","","http://link.springer.com/10.1007/978-3-642-02518-1_19","In this paper we explore the potential of Expressive Notation Package (ENP) in representing electroacoustic scores. We use the listening score by Rainer Wehinger–originally created for György Ligeti’s electronic piece Articulation–as reference material. Our objective is to recreate a small excerpt form the score using an ENP tool called Expression Designer (ED). ED allows the user to create new graphical expressions with programmable behavior. In this paper we aim to demonstrate how a collection of specific graphic symbols found in Articulation can be implemented using ED. We also discuss how this information can be later manipulated and accessed for the needs of a sonic realization, for example.","2009","2023-07-05 07:13:09","2023-07-19 23:48:47","2023-07-05 07:13:08","262-271","","","5493","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-02518-1_19","","","","","","Ystad, Sølvi; Kronland-Martinet, Richard; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MHLK7JK3","journalArticle","2010","Pettey, Gary; Bracken, Cheryl Campanella; Rubenking, Bridget; Buncher, Michael; Gress, Erika","Telepresence, soundscapes and technological expectation: putting the observer into the equation","Virtual Reality","","1359-4338, 1434-9957","10.1007/s10055-009-0148-8","http://link.springer.com/10.1007/s10055-009-0148-8","In an experiment exploring the impact of sound on sensations of telepresence, 126 participants watched a video clip using either headphones or speakers. The results illustrate that sound is an important factor in stimulating telepresence responses in audiences. Interactions between soundscape and screen size were also revealed. A traverse interaction between aural/visual congruency and sound­ scapes was evident. A second data set of 102 participants was collected to illuminate the effect of technological expectation that emerged in the first study. Expectations had been mentioned in other studies, and the data support the notion that people have an expectation of the techno­ logical quality of a presentation. The results suggest that examining expectations could assist in future conceptual­ izations of telepresence.","2010-03","2023-07-05 07:13:09","2023-07-21 05:14:37","2023-07-05 07:13:08","15-25","","1","14","","Virtual Reality","Telepresence, soundscapes and technological expectation","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/PZ6VZCUQ/Pettey et al. - 2010 - Telepresence, soundscapes and technological expect.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IV5P4DDX","journalArticle","2022","Ley-Flores, Judith; Alshami, Eslam; Singh, Aneesha; Bevilacqua, Frédéric; Bianchi-Berthouze, Nadia; Deroy, Ophelia; Tajadura-Jiménez, Ana","Effects of pitch and musical sounds on body-representations when moving with sound","Scientific Reports","","2045-2322","10.1038/s41598-022-06210-x","https://www.nature.com/articles/s41598-022-06210-x","Abstract             The effects of music on bodily movement and feelings, such as when people are dancing or engaged in physical activity, are well-documented—people may move in response to the sound cues, feel powerful, less tired. How sounds and bodily movements relate to create such effects? Here we deconstruct the problem and investigate how different auditory features affect people’s body-representation and feelings even when paired with the same movement. In three experiments, participants executed a simple arm raise synchronised with changing pitch in simple tones (Experiment 1), rich musical sounds (Experiment 2) and within different frequency ranges (Experiment 3), while we recorded indirect and direct measures on their movement, body-representations and feelings. Changes in pitch influenced people’s general emotional state as well as the various bodily dimensions investigated—movement, proprioceptive awareness and feelings about one’s body and movement. Adding harmonic content amplified the differences between ascending and descending sounds, while shifting the absolute frequency range had a general effect on movement amplitude, bodily feelings and emotional state. These results provide new insights in the role of auditory and musical features in dance and exercise, and have implications for the design of sound-based applications supporting movement expression, physical activity, or rehabilitation.","2022-02-17","2023-07-05 07:13:09","2023-07-05 07:13:09","2023-07-05 07:13:08","2676","","1","12","","Sci Rep","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/YA6BKVBA/Ley-Flores et al. - 2022 - Effects of pitch and musical sounds on body-repres.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QTFEESTG","bookSection","2009","Eslambochilar, Parisa; Buchanan, George; Loizides, Fernando","Hear It Is: Enhancing Rapid Document Browsing with Sound Cues","Research and Advanced Technology for Digital Libraries","978-3-642-04345-1 978-3-642-04346-8","","","http://link.springer.com/10.1007/978-3-642-04346-8_9","Document navigation has become increasingly commonplace as the use of electronic documents has grown. Speed–Dependent Automatic Zooming (SDAZ) is one popular method for providing rapid movement within a digital text. However, there is evidence that details of the document are overlooked as the pace of navigation rises. We produced a document reader software where sound is used to complement the visual cues that a user searches for visually. This software was then evaluated in a user study that provides strong supportive evidence that non-visual cues can improve user performance in visual seeking tasks.","2009","2023-07-05 07:13:09","2023-07-21 04:56:56","2023-07-05 07:13:08","75-86","","","5714","","","Hear It Is","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-04346-8_9","","/Users/minsik/Zotero/storage/8MM4HKL8/Eslambochilar et al. - 2009 - Hear It Is Enhancing Rapid Document Browsing with.pdf","","","","Agosti, Maristella; Borbinha, José; Kapidakis, Sarantos; Papatheodorou, Christos; Tsakonas, Giannis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3JEC6EQP","bookSection","2003","Tzanetakis, George","Musescape: A Tool for Changing Music Collections into Libraries","Research and Advanced Technology for Digital Libraries","978-3-540-40726-3 978-3-540-45175-4","","","http://link.springer.com/10.1007/978-3-540-45175-4_38","Increases in hard disk capacity and audio compression technology have enabled the storage of large collections of music on personal computers and portable devices. As an example a portable device with 20 Gigabytes of storage can hold up to 4000 songs in compressed audio format. Currently the only way of structuring these collections is using a file system hierarchy which allows very limited forms of searching and retrieval. These limitations are even more pronounced in the case of portable devices where there is less screen real estate and user attention is limited compared to a personal computer. Musescape is a prototype tool for organizing and interacting with large music collections in audio format with specific emphasis on portable devices. It provides a variety of automatic and manual ways to organize and interact with large music collections using a consistent continuous audio feedback user interface for browsing, searching and annotating. Using this system a user can convert an unstructured or partially structured collection of music with limited retrieval capabilities into a music library with enhanced functionality.","2003","2023-07-05 07:15:22","2023-07-21 04:57:06","2023-07-05 07:15:22","412-421","","","2769","","","Musescape","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-45175-4_38","","","","","","Koch, Traugott; Sølvberg, Ingeborg Torvik","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y5GP3YZ5","journalArticle","2023","Sørensen, Vibeke; Lansing, J. Stephen","Art, technology and the Internet of Living Things","AI & SOCIETY","","0951-5666, 1435-5655","10.1007/s00146-023-01667-4","https://link.springer.com/10.1007/s00146-023-01667-4","Intelligence augmentation was one of the original goals of computing. Artificial Intelligence (AI) inherits this project and is at the leading edge of computing today. Computing can be considered an extension of brain and body, with mathematical prowess and logic fundamental to the infrastructure of computing. Multimedia computing—sensing, analyzing, and translating data to and from visual images, animation, sound and music, touch and haptics, as well as smell—is based on our human senses and is now commonplace. We use data visualization and sonification, as well as data mining and analysis, to sort through the complexity and vast volume of data coming from the world inside and around us. It helps us ‘see’ in new ways. We can think of this capacity as a new kind of “digital glasses”. The Internet of Living Things (IOLT) is potentially an even more profound extension of ourselves to the world: a network of electronic devices embedded into objects, but now with subcutaneous, ingestible devices, and embedded sensors that include people and other living things. Like the Internet of Things (IOT), living things are connected; we call those connections “ecology”. As the IOT becomes increasingly synonymous with the IOLT, the question of ethics that is at the centre of aesthetics and the arts will move to the forefront of our experience of and regard for the world in and around us.","2023-05-16","2023-07-05 07:15:22","2023-07-19 11:27:01","2023-07-05 07:15:22","","","","","","AI & Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/N2JX3SUK/Sørensen and Lansing - 2023 - Art, technology and the Internet of Living Things.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L6XECBUF","bookSection","2021","Sørensen, Vibeke; Lansing, J. Stephen; Thummanapalli, Nagaraju","Voyages Along the Star Paths: Capturing Calendrical Cycles from Kauai to Bali","Culture and Computing. Interactive Cultural Heritage and Arts","978-3-030-77410-3 978-3-030-77411-0","","","https://link.springer.com/10.1007/978-3-030-77411-0_20","Systems for the representation of temporal cycles play a vital role in all cultures, but they seldom figure prominently in studies of heritage. Anthropologists are often frustrated by he lumping together and dismissal of nonwestern concepts of time as merely “cyclical time”, interpreted as changelessness or the absence of progress. Here we compare two of the most complex and sophisticated calendrical systems known to anthropology, from the islands of Bali and Hawaii. The sheer complexity of these concepts and their intimate relationship to astronomical phenomena make them very difficult to compare using the scholar’s traditional toolkit of text and images. But they are admirably suited to immersive digital media. As well as facilitating descriptive exposition, real-time computer animation, music programming and other digital technology opens new avenues for research on the relationship of the abstract structure of calendrical systems to polyrhythms in music and other aspects of the phenomenology of time consciousness.","2021","2023-07-05 07:15:22","2023-07-19 23:54:15","2023-07-05 07:15:22","296-317","","","12794","","","Voyages Along the Star Paths","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-77411-0_20","","","","","","Rauterberg, Matthias","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HRUH2SFX","journalArticle","2022","Robinson, Frederic Anthony; Bown, Oliver; Velonaki, Mari","Designing Sound for Social Robots: Candidate Design Principles","International Journal of Social Robotics","","1875-4791, 1875-4805","10.1007/s12369-022-00891-0","https://link.springer.com/10.1007/s12369-022-00891-0","Abstract             How can we use sound and music to create rich and engaging human-robot interactions? A growing body of HRI research explores the many ways in which sound affects human-robot interactions and although some studies conclude with tentative design recommendations, there are, to our knowledge, no generalised design recommendations for the robot sound design process. We address this gap by first investigating sound design frameworks in the domains of product sound design and film sound to see whether practices and concepts from these areas contain actionable insights for the creation of robot sound. We then present three case studies, detailed examinations of the sound design of commercial social robots Cozmo and Vector, Jibo, and Kuri, facilitated by expert interviews with the robots’ sound designers. Combining insights from the design frameworks and case studies, we propose nine candidate design principles for robot sound which provide (1) a design-oriented perspective on robot sound that may inform future research, and (2) actionable guidelines for designers, engineers and decision-makers aiming to use sound to create richer and more refined human-robot interactions.","2022-08","2023-07-05 07:15:22","2023-07-05 07:15:22","2023-07-05 07:15:22","1507-1525","","6","14","","Int J of Soc Robotics","Designing Sound for Social Robots","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/XGRH4S6L/Robinson et al. - 2022 - Designing Sound for Social Robots Candidate Desig.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4XM5U3Q8","bookSection","2014","Houix, Olivier; Misdariis, Nicolas; Susini, Patrick; Bevilacqua, Frédéric; Gutierrez, Florestan","Sonically Augmented Artifacts: Design Methodology Through Participatory Workshops","Sound, Music, and Motion","978-3-319-12975-4 978-3-319-12976-1","","","https://link.springer.com/10.1007/978-3-319-12976-1_2","Participatory workshops have been organized within the framework of the ANR project Legos that concerns gesture-sound interactive systems. These workshops addressed both theoretical issues and experimentation with prototypes. The first goal was to stimulate new ideas related to the control of everyday objects using sound feedback, and then, to create and experiment with new sonic augmented objects. The second aim was educational. We investigated how sonic interaction design can be introduced to people without backgrounds in sound and music. We present in this article an overview of three workshops. The first workshop focused on the analysis and the possible sonification of everyday objects. New usage scenarios were obtained and tested. The second workshop focused on sound metaphor, questioning the relationship between sound and gesture. The last one was a workshop organized during a summer school for students. During these workshops, we experimented a cycle of design process: analysis, creation and testing.","2014","2023-07-05 07:15:22","2023-07-21 05:01:46","2023-07-05 07:15:22","20-40","","","8905","","","Sonically Augmented Artifacts","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-12976-1_2","","","","","","Aramaki, Mitsuko; Derrien, Olivier; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WKFATE8M","bookSection","2021","Kariyado, Yuta; Arevalo, Camilo; Villegas, Julián","Auralization of Three-Dimensional Cellular Automata","Artificial Intelligence in Music, Sound, Art and Design","978-3-030-72913-4 978-3-030-72914-1","","","https://link.springer.com/10.1007/978-3-030-72914-1_11","","2021","2023-07-05 07:15:22","2023-07-05 07:15:22","2023-07-05 07:15:22","161-170","","","12693","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-72914-1_11","","","","","","Romero, Juan; Martins, Tiago; Rodríguez-Fernández, Nereida","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MBI93HXF","bookSection","2001","Harding, Chris; Kakadiaris, Ioannis A.; Casey, John F.; Bowen Loftin, R.","A Case Study in Multi-Sensory Investigation of Geoscientific Data","Data Visualization 2001","978-3-211-83674-3 978-3-7091-6215-6","","","http://link.springer.com/10.1007/978-3-7091-6215-6_2","In this paper, we report our ongoing research into multi-sensory investigation of geoscientific data. Our Geoscientific Data Investigation System (GDIS) integrates three-dimensional, interactive computer graphics, touch (haptics) and real-time sonification into a multi-sensory Virtual Environment. GDIS has been used to investigate geological structures on the high-resolution bathymetry data from the Mid-Atlantic Ridge. Haptic force feedback was used to precisely digitize line features on threedimensional morphology and to feel surface properties via varying friction settings; additional, overlapping data can be perceived via sound (sonification). We also report on the results of a psycho-acoustic study about the absolute recognition of sound signals, and on the actual feedback that we have received from a number of geoscientists during a recent major geoscience conference.","2001","2023-07-05 07:15:22","2023-07-19 23:54:45","2023-07-05 07:15:22","3-14","","","","","","","","","","","Springer Vienna","Vienna","en","","","","","DOI.org (Crossref)","","Series Title: Eurographics DOI: 10.1007/978-3-7091-6215-6_2","","","","","","Ebert, David S.; Favre, Jean M.; Peikert, Ronald","Hansmann, W.; Purgathofer, W.; Sillion, F.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R2HUFYQG","bookSection","2007","Ohyama, Kisa; Itoh, Takayuki","DIVA: An Automatic Music Arrangement Technique Based on Impression of Images","Smart Graphics","978-3-540-73213-6 978-3-540-73214-3","","","http://link.springer.com/10.1007/978-3-540-73214-3_16","This poster reports our first approach for automatic music arrangement based on the impression of input images. Given a digital image and keywords of objects shot in the image, the technique selects a rhythm pattern associated from the keywords and color distribution of the image. As a preprocessing, the technique first provides sample colors, images, and keywords to users, and then collects the feedback of selection of rhythm patterns associated from them. The technique then leads equations to calculate the association of rhythm pattern from arbitrary input images. Finally, the technique automatically calculates the association scores of all prepared rhythm patterns from the images, and provides music arranged applying the associated rhythm pattern.","2007","2023-07-05 07:15:22","2023-07-21 04:59:47","2023-07-05 07:15:22","178-181","","","4569","","","DIVA","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73214-3_16","","","","","","Butz, Andreas; Fisher, Brian; Krüger, Antonio; Olivier, Patrick; Owada, Shigeru","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ENFELAZA","bookSection","2020","Baird, Alice; Song, Meishu; Schuller, Björn","Interaction with the Soundscape: Exploring Emotional Audio Generation for Improved Individual Wellbeing","Artificial Intelligence in HCI","978-3-030-50333-8 978-3-030-50334-5","","","http://link.springer.com/10.1007/978-3-030-50334-5_15","Ourdailyinteractionwiththesoundscapeisinflux,and complexnaturalsoundcombinationshaveshowntohaveadverseimplicationsonuserexperience.Acomputationalapproachtostabilisethe sonicenvironment,tailoredtoauser’scurrentaffectivestatemayprove beneficialinavarietyofscenarios,includingworkplaceefficiency,and exercise.Herein,wepresentinitialperceptiontestresults,fromarudimentaryapproachforsoundscapeaugmentationutilisingchromaticfeaturesonification.Resultsshowthatarousalandvalancedimensionsof emotioncanbealteredthroughaugmentationofthreeclassesofnaturalsoundscape,namely‘mechanical’,‘nature’,and‘human’.Proceeding thisweoutlineapossibleapproachforanaffectiveaudio-basedrecognitionandgenerationsystem,inwhichusers(eitherindividuallyorasa groupwithinaspecificenvironment)areprovidedwithan augmentation oftheircurrentsoundscape,asameansofimprovingwellbeing.","2020","2023-07-05 07:15:22","2023-07-19 11:33:15","2023-07-05 07:15:22","229-242","","","12217","","","Interaction with the Soundscape","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-50334-5_15","","/Users/minsik/Zotero/storage/78LHSWVB/Baird et al. - 2020 - Interaction with the Soundscape Exploring Emotion.pdf","","","","Degen, Helmut; Reinerman-Jones, Lauren","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PGZ3IK5Y","journalArticle","2016","Hinterberger, Thilo; Fürnrohr, Elena","The Sensorium: Psychophysiological Evaluation of Responses to a Multimodal Neurofeedback Environment","Applied Psychophysiology and Biofeedback","","1090-0586, 1573-3270","10.1007/s10484-016-9332-2","http://link.springer.com/10.1007/s10484-016-9332-2","The Sensorium is a multimodal neurofeedback environment that reflects a person’s physiological state by presenting physiological signals via orchestral sounds from a speaker and multi-coloured lights projected onto a white surface. The software manages acquisition, real-time processing, storage, and sonification of various physiological signals such as the electroencephalogram (EEG) or electrocardiogram (ECG). Each of the 36 participants completed 6 interventional conditions consisting of three different Sensorium-phases with EEG and ECG feedback, a mindfulness meditation, a guided body scan exercise, and a Pseudo-Sensorium using pre-recorded data that did not reflect the subject’s own physiology. During all phases EEG, ECG, skin conductance, and respiration were recorded. A feedback questionnaire assessed the participants’ subjective reports of changes in well-being, perception, and life-spirit. The results indicate that the Sensorium sessions were not statistically inferior compared to their corresponding active control conditions with respect to improvements in subjective reports concerning well-being and perception. Additionally, the Sensorium was rated as being a more extraordinary experience, as compared to meditation. During the Sensorium conditions the EEG showed lower levels of theta2 (7–8.5 Hz), alpha (9–12 Hz) and beta (12.5–25 Hz) activity. Since participants reported benefit from the Sensorium experience regardless of any prior experience with meditation, we propose this novel method of meditative and extraordinary self-experience to be utilized as a modern alternative to more traditional forms of meditation.","2016-09","2023-07-05 07:15:22","2023-07-19 11:31:45","2023-07-05 07:15:22","315-329","","3","41","","Appl Psychophysiol Biofeedback","The Sensorium","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7GKD9JHL","bookSection","2021","Craveirinha, Rui; Pereira, Luís Lucas; Seiça, Mariana; Roque, Licínio","Aesthetic Perspectives on Computational Media Design","Entertainment Computing – ICEC 2021","978-3-030-89393-4 978-3-030-89394-1","","","https://link.springer.com/10.1007/978-3-030-89394-1_42","The main goal of this workshop is to open a collaborative space for reflection and debate on how Aesthetics can inform the design of computational media, in an attempt to grasp the multiplicity of aesthetic dimensions that can influence the reception, experience, and interpretation of such sociotechnical objects and experiences. We will approach the goal from the entertainment computing field where such concerns are of vital influence .","2021","2023-07-05 07:15:22","2023-07-20 00:00:06","2023-07-05 07:15:22","482-488","","","13056","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-89394-1_42","","","","","","Baalsrud Hauge, Jannicke; C. S. Cardoso, Jorge; Roque, Licínio; Gonzalez-Calero, Pedro A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2C36NNHZ","bookSection","2014","Gerino, Andrea; Alabastro, Nicolò; Bernareggi, Cristian; Ahmetovic, Dragan; Mascetti, Sergio","MathMelodies: Inclusive Design of a Didactic Game to Practice Mathematics","Computers Helping People with Special Needs","978-3-319-08595-1 978-3-319-08596-8","","","http://link.springer.com/10.1007/978-3-319-08596-8_88","Tablet computers are becoming a common tool to support learning since primary school. Indeed, many didactic applications are already available on online stores. Most of these applications engage the child by immersing the educational purpose of the software within an entertaining environment, often in the form of a game with sophisticated graphic and interaction. Unfortunately, this makes most of these applications inaccessible to visually impaired children. In this contribution we present MathMelodies, an iPad application that supports math learning in primary school and that is designed to be accessible also to visually impaired children. We describe the main challenges we faced during the development of this didactic application that is both engaging and accessible. The application, currently publicly available, is collecting enthusiastic reviews from teachers, who often contribute with precious insight for improving the solution.","2014","2023-07-05 07:15:22","2023-07-19 23:51:58","2023-07-05 07:15:22","564-571","","","8547","","","MathMelodies","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-08596-8_88","","","","","","Miesenberger, Klaus; Fels, Deborah; Archambault, Dominique; Peňáz, Petr; Zagler, Wolfgang","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BT3PTYK2","journalArticle","2014","Han, Yoon Chung; Han, Byeong-jun","Virtual pottery: a virtual 3D audiovisual interface using natural hand motions","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-013-1382-3","http://link.springer.com/10.1007/s11042-013-1382-3","In this paper, we present our approach towards designing and implementing a virtual 3D sound sculpting interface that creates audiovisual results using hand motions in real time. In the interface “Virtual Pottery,” we use the metaphor of pottery creation in order to adopt the natural hand motions to 3D spatial sculpting. Users can create their own pottery pieces by changing the position of their hands in real time, and also generate 3D sound sculptures based on pre-existing rules of music composition. The interface of Virtual Pottery can be categorized by shape design and camera sensing type. This paper describes how we developed the two versions of Virtual Pottery and implemented the technical aspects of the interfaces. Additionally, we investigate the ways of translating hand motions into musical sound. The accuracy of the detection of hand motions is crucial for translating natural hand motions into virtual reality. According to the results of preliminary evaluations, the accuracy of both motion-capture tracking system and portable depth sensing camera is as high as the actual data. We carried out user studies, which took into account information about the two exhibitions along with the various ages of users. Overall, Virtual Pottery serves as a bridge between the virtual environment and traditional art practices, with the consequence that it can lead to the cultivation of the deep potential of virtual musical instruments and future art education programs.","2014-11","2023-07-05 07:15:22","2023-07-21 04:32:23","2023-07-05 07:15:22","917-933","","2","73","","Multimed Tools Appl","Virtual pottery","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"489PA4FK","bookSection","2019","Torres-Cardona, Hector Fabio; Aguirre-Grisales, Catalina; Castro-Londoño, Victor Hugo; Rodriguez-Sotelo, Jose Luis","Interpolation, a Model for Sound Representation Based on BCI","Augmented Cognition","978-3-030-22418-9 978-3-030-22419-6","","","http://link.springer.com/10.1007/978-3-030-22419-6_34","Brain state control has been well established in the area of Brain-computer interfaces over the last decades in which the active applications allow controlling external devices consciously. The purpose of this study was to develop a real-time graphical sound representation system based on an interaction design that allows navigating through the motor imagery cognitive task in a bidimensional plane. This representation was developed using the OpenBCI EEG acquisition system in order to record the necessary information which was sent and processed in Max/MSP software. The system operates under a metaphorical Graphical User Interface (GUI) programmed in Processing. The system was tested through an experiment under controlled conditions in which six professional musicians participated. From the experimental results, it was found that all participants achieved different control levels associated to their static and dynamic response with an average of 26.73% and 73.27% respectively.","2019","2023-07-05 07:15:22","2023-07-19 11:40:02","2023-07-05 07:15:22","471-483","","","11580","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-22419-6_34","","","","","","Schmorrow, Dylan D.; Fidopiastis, Cali M.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RGNZMV52","bookSection","2014","Boyer, Eric O.; Pyanet, Quentin; Hanneton, Sylvain; Bevilacqua, Frédéric","Learning Movement Kinematics with a Targeted Sound","Sound, Music, and Motion","978-3-319-12975-4 978-3-319-12976-1","","","https://link.springer.com/10.1007/978-3-319-12976-1_14","This study introduces an experiment designed to analyze the sensorimotor adaptation to a motion-based sound synthesis system. We investigated a sound-oriented learning task, namely to reproduce a targeted sound. The motion of a small handheld object was used to control a sound synthesizer. The object angular velocity was measured by a gyroscope and transmitted in real time wirelessly to the sound system. The targeted sound was reached when the motion matched a given reference angular velocity profile with a given accuracy. An incorrect velocity profile produced either a noisier sound or a sound with a louder high harmonic, depending on the sign of the velocity error. The results showed that the participants were generally able to learn to reproduce sounds very close to the targeted sound. A corresponding motor adaptation was also found to occur, at various degrees, in most of the participants when the profile is altered.","2014","2023-07-05 07:15:22","2023-07-21 05:01:09","2023-07-05 07:15:22","218-233","","","8905","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-12976-1_14","","/Users/minsik/Zotero/storage/QCTS6D7A/Boyer et al. - 2014 - Learning Movement Kinematics with a Targeted Sound.pdf","","","","Aramaki, Mitsuko; Derrien, Olivier; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HLKWXKUS","bookSection","2020","Polydorou, Doros; Ben-Tal, Oded; Damsma, Atser; Schlichting, Nadine","VR: Time Machine","Human-Computer Interaction. Human Values and Quality of Life","978-3-030-49064-5 978-3-030-49065-2","","","http://link.springer.com/10.1007/978-3-030-49065-2_21","Time Machine is an immersive Virtual Reality installation that explains – in simple terms – the Striatal Beat Frequency (SBF) model of time perception. The installation was created as a collaboration between neuroscientists within the field of time perception along with a team of digital designers and audio composers/engineers. This paper outlines the process, as well as the lessons learned, while designing the virtual reality experience that aims to simplify a complex idea to a novice audience. The authors describe in detail the process of creating the world, the user experience mechanics and the methods of placing information in the virtual place in order to enhance the learning experience. The work was showcased at the 4th International Conference on Time Perspective, where the authors collected feedback from the audience. The paper concludes with a reflection on the work and some suggestions for the next iteration of the project.","2020","2023-07-05 07:15:22","2023-07-20 06:32:05","2023-07-05 07:15:22","294-306","","","12183","","","VR","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-49065-2_21","","/Users/minsik/Zotero/storage/WLSJEXQI/Polydorou et al. - 2020 - VR Time Machine.pdf","","","","Kurosu, Masaaki","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z6T8V3BK","journalArticle","2022","Lau, Chng Wei; Qu, Zhonglin; Draper, Daniel; Quan, Rosa; Braytee, Ali; Bluff, Andrew; Zhang, Dongmo; Johnston, Andrew; Kennedy, Paul J.; Simoff, Simeon; Nguyen, Quang Vinh; Catchpoole, Daniel","Virtual reality for the observation of oncology models (VROOM): immersive analytics for oncology patient cohorts","Scientific Reports","","2045-2322","10.1038/s41598-022-15548-1","https://www.nature.com/articles/s41598-022-15548-1","Abstract             The significant advancement of inexpensive and portable virtual reality (VR) and augmented reality devices has re-energised the research in the immersive analytics field. The immersive environment is different from a traditional 2D display used to analyse 3D data as it provides a unified environment that supports immersion in a 3D scene, gestural interaction, haptic feedback and spatial audio. Genomic data analysis has been used in oncology to understand better the relationship between genetic profile, cancer type, and treatment option. This paper proposes a novel immersive analytics tool for cancer patient cohorts in a virtual reality environment, virtual reality to observe oncology data models. We utilise immersive technologies to analyse the gene expression and clinical data of a cohort of cancer patients. Various machine learning algorithms and visualisation methods have also been deployed in VR to enhance the data interrogation process. This is supported with established 2D visual analytics and graphical methods in bioinformatics, such as scatter plots, descriptive statistical information, linear regression, box plot and heatmap into our visualisation. Our approach allows the clinician to interrogate the information that is familiar and meaningful to them while providing them immersive analytics capabilities to make new discoveries toward personalised medicine.","2022-07-05","2023-07-05 07:15:22","2023-07-05 07:15:22","2023-07-05 07:15:22","11337","","1","12","","Sci Rep","Virtual reality for the observation of oncology models (VROOM)","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/H49VLSZV/Lau et al. - 2022 - Virtual reality for the observation of oncology mo.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6A59AYXC","journalArticle","2016","Tordini, Francesco; Bregman, Albert S.; Cooperstock, Jeremy R.","Prioritizing foreground selection of natural chirp sounds by tempo and spectral centroid","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-016-0223-x","http://link.springer.com/10.1007/s12193-016-0223-x","Salience shapes the involuntary perception of a sound scene into foreground and background. Auditory interfaces, such as those used in continuous process monitoring, rely on the prominence of those sounds that are perceived as foreground. We propose to distinguish between the salience of sound events and that of streams, and introduce a paradigm to study the latter using repetitive patterns of natural chirps. Since streams are the sound objects populating the auditory scene, we suggest the use of global descriptors of perceptual dimensions to predict their salience, and hence, the organization of the objects into foreground and background. However, there are many possible independent features that can be used to describe sounds. Based on the results of two experiments, we suggest a parsimonious interpretation of the rules guiding foreground formation: after loudness, tempo and brightness are the dimensions that have higher priority. Our data show that, under equal-loudness conditions, patterns with fast tempo and lower brightness tend to emerge and that the interaction between tempo and brightness in foreground selection seems to increase with task difficulty. We propose to use the relations we uncovered as the underpinnings for a computational model of foreground selection, and also, as design guidelines for stream-based sonification applications.","2016-09","2023-07-05 07:15:22","2023-07-20 07:05:05","2023-07-05 07:15:22","221-234","","3","10","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y4YEMT6L","journalArticle","2019","Plazak, Joseph; DiGiovanni, Daniel A.; Collins, D. Louis; Kersten-Oertel, Marta","Cognitive load associations when utilizing auditory display within image-guided neurosurgery","International Journal of Computer Assisted Radiology and Surgery","","1861-6410, 1861-6429","10.1007/s11548-019-01970-w","http://link.springer.com/10.1007/s11548-019-01970-w","","2019-08","2023-07-05 07:18:09","2023-07-05 07:18:09","2023-07-05 07:18:09","1431-1438","","8","14","","Int J CARS","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PUE9A7LD","bookSection","2016","Rodrigues, Ana; Costa, Ernesto; Cardoso, Amílcar; Machado, Penousal; Cruz, Tiago","Evolving L-Systems with Musical Notes","Evolutionary and Biologically Inspired Music, Sound, Art and Design","978-3-319-31007-7 978-3-319-31008-4","","","http://link.springer.com/10.1007/978-3-319-31008-4_13","Over the years researchers have been interested in devising computational approaches for music and image generation. Some of the approaches rely on generative rewriting systems like L-systems. More recently, some authors questioned the interplay of music and images, that is, how we can use one type to drive the other. In this paper we present a new method for the algorithmic generations of images that are the result of a visual interpretation of an L-system. The main novelty of our approach is based on the fact that the L-system itself is the result of an evolutionary process guided by musical elements. Musical notes are decomposed into elements – pitch, duration and volume in the current implementation – and each of them is mapped into corresponding parameters of the L-system – currently line length, width, color and turning angle. We describe the architecture of our system, based on a multi-agent simulation environment, and show the results of some experiments that provide support to our approach.","2016","2023-07-05 07:18:09","2023-07-20 00:04:28","2023-07-05 07:18:09","186-201","","","9596","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-31008-4_13","","","","","","Johnson, Colin; Ciesielski, Vic; Correia, João; Machado, Penousal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JKN5X9ZE","journalArticle","2008","Fornari, José; Maia, Adolfo; Manzolli, Jônatas","Soundscape Design Through Evolutionary Engines","Journal of the Brazilian Computer Society","","0104-6500, 1678-4804","10.1007/BF03192564","https://journal-bcs.springeropen.com/articles/10.1007/BF03192564","Abstract                            Two implementations of an Evolutionary Sound Synthesis method using the Interaural Time Difference (ITD) and psychoacoustic descriptors are presented here as a way to develop criteria for fitness evaluation. We also explore a relationship between adaptive sound evolution and three soundscape characteristics: keysounds, key-signals and sound-marks. Sonic Localization Field is defined using a sound attenuation factor and ITD azimuth angle, respectively (I               i               , L               i               ). These pairs are used to build Spatial Sound Genotypes (SSG) and they are extracted from a waveform population set. An explanation on how our model was initially written in MATLAB is followed by a recent Pure Data (Pd) implementation. It also elucidates the development and use of: parametric scores, a triplet of psychoacoustic descriptors and the correspondent graphical user interface.","2008-09","2023-07-05 07:18:09","2023-07-05 07:18:09","2023-07-05 07:18:09","51-64","","3","14","","J Braz Comp Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/862GIVWD/Fornari et al. - 2008 - Soundscape Design Through Evolutionary Engines.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A4MK7PGM","bookSection","2017","Yairi, Ikuko Eguchi","Designing Interfaces to Make Information More Tangible for Visually Impaired People","Universal Access in Human–Computer Interaction. Designing Novel Interactions","978-3-319-58702-8 978-3-319-58703-5","","","https://link.springer.com/10.1007/978-3-319-58703-5_27","This paper introduces our two research projects. One is to propose the graphic representation method with touch and sound as the universal designed touch screen interface for visually impaired people. Another is to investigate the good design of the collaborative work environment of the visually impaired. The proposed graphic representation method and interfaces are basic techniques for developing plug-ins which help blind people to use ordinary mass-produced computer devices with touch screens, such as smartphones and iPads. Our idea is so simple that musical scales enable users to trace graphics by their fingers and to memorize their position on the touch screen. Our recent progress including digital textbook application for visually impaired children is also reported. To investigate the design of the collaborative work environment, we have developed the collaborative music composition application with a tangible interface using daily goods that would attract the attention of both visually impaired and sighted people, and to induce collaborative communication among them. After evaluating this application, we focused our interest on the moment in which the visually impaired people are having fun, and on the factor of the excitement and concentration. This paper introduces our experimental system, which is a shooting game application without visual information, to investigate the factor of the excitement and concentration of the collaboration between visually impaired people. Recent analysis results of the collaboration are reported.","2017","2023-07-05 07:18:09","2023-07-21 05:10:42","2023-07-05 07:18:09","366-378","","","10278","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-58703-5_27","","","","","","Antona, Margherita; Stephanidis, Constantine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y2BXJBYR","journalArticle","2022","Sorensen, Vibeke; Lansing, John Stephen; Thummanapalli, Nagaraju; Cambria, Erik","Mood of the Planet: Challenging Visions of Big Data in the Arts","Cognitive Computation","","1866-9956, 1866-9964","10.1007/s12559-020-09766-w","https://link.springer.com/10.1007/s12559-020-09766-w","Mood of the Planet is an interactive physical-digital sculpture that has as its center-piece a large “arch” or “doorway” that emits colored light and sound as a form of visualization and sonification of the changing, live emotions expressed by people all around the Earth. It is the product of several disciplines, including the arts, computer science, linguistics and psychology. In particular, we use artificial intelligence to collect and analyze social media data and extract emotions from these using a brain-inspired and psychologically motivated emotion categorization model. Such emotions are then translated into colors and sounds that the audience can experience while passing through the arch. Feedback from the audience proved the Mood of the Planet to provide a more accurate, personal and tangible experience about the data-emotions dichotomy.","2022-01","2023-07-05 07:18:09","2023-07-19 23:45:24","2023-07-05 07:18:09","310-321","","1","14","","Cogn Comput","Mood of the Planet","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/5ZA2PWYI/Sorensen et al. - 2022 - Mood of the Planet Challenging Visions of Big Dat.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"95TML2SX","bookSection","2014","Holtzman, Benjamin; Candler, Jason; Turk, Matthew; Peter, Daniel","Seismic Sound Lab: Sights, Sounds and Perception of the Earth as an Acoustic Space","Sound, Music, and Motion","978-3-319-12975-4 978-3-319-12976-1","","","https://link.springer.com/10.1007/978-3-319-12976-1_10","We construct a representation of earthquakes and global seismic waves through sound and animated images. The seismic wave field is the ensemble of elastic waves that propagate through the planet after an earthquake, emanating from the rupture on the fault. The sounds are made by time compression (i.e. speeding up) of seismic data with minimal additional processing. The animated images are renderings of numerical simulations of seismic wave propagation in the globe. Synchronized sounds and images reveal complex patterns and illustrate numerous aspects of the seismic wave field. These movies represent phenomena occurring far from the time and length scales normally accessible to us, creating a profound experience for the observer. The multi-sensory perception of these complex phenomena may also bring new insights to researchers.","2014","2023-07-05 07:18:09","2023-07-21 05:01:30","2023-07-05 07:18:09","161-174","","","8905","","","Seismic Sound Lab","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-12976-1_10","","","","","","Aramaki, Mitsuko; Derrien, Olivier; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VB74FRL5","journalArticle","2005","Zotkin, Dmitry N.; Chi, Taishih; Shamma, Shihab A.; Duraiswami, Ramani","Neuromimetic Sound Representation for Percept Detection and Manipulation","EURASIP Journal on Advances in Signal Processing","","1687-6180","10.1155/ASP.2005.1350","https://asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.1350","The acoustic wave received at the ears is processed by the human auditory system to separate different sounds along the intensity, pitch, and timbre dimensions. Conventional Fourier-based signal processing, while endowed with fast algorithms, is unable to easily represent a signal along these attributes. In this paper, we discuss the creation of maximally separable sounds in auditory user interfaces and use a recently proposed cortical sound representation, which performs a biomimetic decomposition of an acoustic signal, to represent and manipulate sound for this purpose. We briefly overview algorithms for obtaining, manipulating, and inverting a cortical representation of a sound and describe algorithms for manipulating signal pitch and timbre separately. The algorithms are also used to create sound of an instrument between a “guitar” and a “trumpet.” Excellent sound quality can be achieved if processing time is not a concern, and intelligible signals can be reconstructed in reasonable processing time (about ten seconds of computational time for a one-second signal sampled at 8 kHz). Work on bringing the algorithms into the real-time processing domain is ongoing.","2005-12","2023-07-05 07:18:09","2023-07-20 00:01:28","2023-07-05 07:18:09","486137","","9","2005","","EURASIP J. Adv. Signal Process.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/6PHT4UWJ/Zotkin et al. - 2005 - Neuromimetic Sound Representation for Percept Dete.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JTEBYB2B","bookSection","2006","Frauenberger, C; Stockman, T; Putz, V; Höldrich, R","Design Patterns for Auditory Displays","People and Computers XIX — The Bigger Picture","978-1-84628-192-1 978-1-84628-249-2","","","http://link.springer.com/10.1007/1-84628-249-7_30","This paper proposes the use of patterns in the design process for auditory displays and /or interfaces realized in other modalities. We introduce a meta-domain in which user interfaces can be designed using these patterns without determining their means of realization. The mode-independent description of such interfaces can then be used to create the real interface maintaining the strengths of the different interaction channels. While this work is focused on how this approach can be applied on auditory displays, we keep in mind that the approach shall be applicable on other interaction modalities equally. The development of a set of mode independent interaction patterns is shown along with descriptions of their representations in the auditory domain. A real world application was chosen to evaluate the approach; Microsoft Explorer was analysed, described through the mode independent interaction patterns and transformed into the auditory domain making extensive use of 3D audio rendering techniques. The result, a file manager created in a virtual audio environment, was evaluated with sighted persons as well as visually impaired and blind participants showing the feasibility and usability of the approach.","2006","2023-07-05 07:18:09","2023-07-21 04:42:20","2023-07-05 07:18:09","473-488","","","","","","","","","","","Springer London","London","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/1-84628-249-7_30","","","","","","McEwan, Tom; Gulliksen, Jan; Benyon, David","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T9FV3STF","journalArticle","2021","Woods, Carl T.; Araújo, Duarte; Davids, Keith; Rudd, James","From a Technology That Replaces Human Perception–Action to One That Expands It: Some Critiques of Current Technology Use in Sport","Sports Medicine - Open","","2199-1170, 2198-9761","10.1186/s40798-021-00366-y","https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-021-00366-y","Abstract                            Information technology has been integrated into most areas of sport, providing new insights, improving the efficiency of operational processes, and offering unique opportunities for exploration and inquiry. While acknowledging this positive impact, this paper explores whether sufficient consideration has been directed towards what technology risks detracting from the learning and developmental experiences of its users. Specifically, viewed through the philosophical lens of the device paradigm, and considering a more ecological account of technological implementation, we discuss how technology               use               in sport could subtly disengage educators and applied sports scientists from performance environments. Insights gleaned from such an ecological account of technology implementation could lead sports science and educational teams to ask and reflect on tough questions of current practice: i.e.               has too much control been given to technological devices to ‘solve’ problems and communicate knowledge (about) in sport? Has technology improved the skills of players and performance staff? Or are performance staff at risk of becoming over-reliant on technology, and as a result, reducing the value of experiential knowledge (of) and intuition?               Questions like these should be asked if technological devices, purported to support aspects of practice, are continually integrated into the sporting landscape.","2021-12","2023-07-05 07:18:09","2023-07-05 07:18:09","2023-07-05 07:18:09","76","","1","7","","Sports Med - Open","From a Technology That Replaces Human Perception–Action to One That Expands It","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/JR2K3IPF/Woods et al. - 2021 - From a Technology That Replaces Human Perception–A.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CYGUN2QM","journalArticle","2022","Ochsner, Beate; Spöhrer, Markus; Stock, Robert","Rethinking Assistive Technologies: Users, Environments, Digital Media, and App-Practices of Hearing","NanoEthics","","1871-4757, 1871-4765","10.1007/s11569-020-00381-5","https://link.springer.com/10.1007/s11569-020-00381-5","Abstract                            Against the backdrop of an aging world population increasingly affected by a diverse range of abilities and disabilities as well as the rise of ubiquitous computing and digital app cultures, this paper questions how mobile technologies mediate between heterogeneous environments and sensing beings. To approach the current technological manufacturing of the senses, two lines of thought are of importance: First, there is a need to critically reflect upon the concept of assistive technologies (AT) as artifacts providing tangible solutions for a specific disability. Second, the conventional distinction between user and environment requires a differentiated consideration. This contribution will first review James Gibson’s concept of “affordances” and modify this approach by introducing theories and methods of Science and Technology Studies (STS) and Actor-Network Theory (ANT). Then, we present two case studies where we explore the relations between recent “assistive” app technologies and human sensory perception. As hearing and seeing are key in this regard, we concentrate on two specific media technologies: ReSound LINX               2               , a hearing aid which allows for direct connect (via Bluetooth) with iPhone, iPad, or iPod Touch, and Camassia, an IOS app for sonic wayfinding for blind people. We emphasize the significance of dis-/abling practices for manufacturing novel forms of hearing and seeing and drawing on sources like promotional materials by manufacturers, ads, or user testimonials and reviews. Our analysis is interested in the reciprocal relationships between users and their socio-technical and media environments. By and large, this contribution will provide crucial insights into the contemporary entanglement of algorithm-driven technologies, daily practices, and sensing subjects: the production of techno-sensory arrangements.","2022-04","2023-07-05 07:18:09","2023-07-05 07:18:09","2023-07-05 07:18:09","65-79","","1","16","","Nanoethics","Rethinking Assistive Technologies","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/P2XK89IR/Ochsner et al. - 2022 - Rethinking Assistive Technologies Users, Environm.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FXJL5UBP","journalArticle","2008","Evreinova, Tatiana V.; Evreinov, Grigori; Raisamo, Roope","A camera-joystick for sound-augmented non-visual navigation and target acquisition: a case study","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-007-0109-5","http://link.springer.com/10.1007/s10209-007-0109-5","This paper presents the results of a comparative study of user input with a camera-joystick and a manual joystick used in a target acquisition task when neither targets nor pointer could be perceived visually. The camera-joystick is an input technique in which each on-screen item is accessible from the center with a predefined vector of head motion. Absolute pointing was implemented with an acceleration factor of 1.7 and a moving average on 5 detected head positions. The underlying assumption was that, in order to provide a robust input for blind users, the interaction technique has to be based on perceptually well-discriminated human movements, which compose a basic framework of an accessible virtual workspace demanding minimum external auxiliary cues. The target spots, having a diameter of 35 mm and a distance between the centers of adjacent spots of 60 mm, were arranged in a rectangular grid of 5 rows by 5 columns. The targets were captured from a distance of 600 mm. The results have shown that the camera input is a promising technique for non-visual human–computer interaction. The subjects demonstrated, more than twice, better performance in the target acquisition task with the camera-joystick versus the manual joystick. All the participants reported that the camera-joystick was a robust and preferable input technique when visual information was not available. Blind interaction techniques could be significantly further improved allowing a user-dependent activation of the navigational cues to better coordinate feedbacks with exploratory behavior.","2008-09","2023-07-05 07:18:09","2023-07-21 05:11:23","2023-07-05 07:18:09","129-144","","3","7","","Univ Access Inf Soc","A camera-joystick for sound-augmented non-visual navigation and target acquisition","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KZXJ3CVL","journalArticle","2016","Lech, Michal; Kostek, Bozena; Czyzewski, Andrzej","Multimedia polysensory integration training system dedicated to children with educational difficulties","Journal of Intelligent Information Systems","","0925-9902, 1573-7675","10.1007/s10844-015-0390-3","http://link.springer.com/10.1007/s10844-015-0390-3","This paper aims at presenting a multimedia system providing polysensory training for pupils with educational difficulties. The particularly interesting aspect of the system lies in the sonic interaction with image projection in which sounds generated lead to stimulation of a particular part of the human brain. The system architecture, video processing methods, therapeutic exercises and guidelines for children’s interaction with the system are presented. Results of pupils’ improvements after several weeks of exercising with the system are provided. The outcome of this study suggests that learning and developing through the interactive method helped to improve children’s spatial orientation skills.","2016-12","2023-07-05 07:18:09","2023-07-20 06:47:34","2023-07-05 07:18:09","531-552","","3","47","","J Intell Inf Syst","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/SEMICDAM/Lech et al. - 2016 - Multimedia polysensory integration training system.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H8AU69I9","journalArticle","2017","Jakus, Grega; Stojmenova, Kristina; Tomažič, Sašo; Sodnik, Jaka","A system for efficient motor learning using multimodal augmented feedback","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-016-3774-7","http://link.springer.com/10.1007/s11042-016-3774-7","Numerous studies have established that using various forms of augmented feedback improves human motor learning. In this paper, we present a system that enables real-time analysis of motion patterns and provides users with objective information on their performance of an executed set of motions. This information can be used to identify individual segments of improper motion early in the learning process, thus preventing improperly learned motion patterns that can be difficult to correct once fully learned. The primary purpose of the proposed system is to serve as a general tool in the research on impact of different feedback modalities on the process of motor learning, for example, in sports or rehabilitation. The key advantages of the system are high-speed and high-accuracy tracking, as well as its flexibility, as it supports various types of feedback (auditory and visual, concurrent or terminal). The practical application of the proposed system is demonstrated through the example of learning a golf swing.","2017-10","2023-07-05 07:18:09","2023-07-21 04:32:55","2023-07-05 07:18:09","20409-20421","","20","76","","Multimed Tools Appl","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z53T4A3K","bookSection","2019","Mannone, Maria; Favali, Federico","Categories, Musical Instruments, and Drawings: A Unification Dream","Mathematics and Computation in Music","978-3-030-21391-6 978-3-030-21392-3","","","http://link.springer.com/10.1007/978-3-030-21392-3_5","The mathematical formalism of category theory allows to investigate musical structures at both low and high levels, performance practice (with musical gestures) and music analysis. Mathematical formalism can also be used to connect music with other disciplines such as visual arts. In our analysis, we extend former studies on category theory applied to musical gestures, including musical instruments and playing techniques. Some basic concepts of categories may help navigate within the complexity of several branches of contemporary music research, giving it a unitarian character. Such a ‘unification dream,’ that we can call ‘cARTegory theory,’ also includes metaphorical references to topos theory.","2019","2023-07-05 07:18:09","2023-07-21 04:28:36","2023-07-05 07:18:09","59-72","","","11502","","","Categories, Musical Instruments, and Drawings","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-21392-3_5","","","","","","Montiel, Mariana; Gomez-Martin, Francisco; Agustín-Aquino, Octavio A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5IKV8DBT","bookSection","2022","Rhodes, Chris; Allmendinger, Richard; Climent, Ricardo","Classifying Biometric Data for Musical Interaction Within Virtual Reality","Artificial Intelligence in Music, Sound, Art and Design","978-3-031-03788-7 978-3-031-03789-4","","","https://link.springer.com/10.1007/978-3-031-03789-4_25","Since 2015, commercial gestural interfaces have widened accessibility for researchers and artists to use novel Electromyographic (EMG) biometric data. EMG data measures musclar amplitude and allows us to enhance Human-Computer Interaction (HCI) through providing natural gestural interaction with digital media. Virtual Reality (VR) is an immersive technology capable of simulating the real world and abstractions of it. However, current commercial VR technology is not equipped to process and use biometric information. Using biometrics within VR allows for better gestural detailing and use of complex custom gestures, such as those found within instrumental music performance, compared to using optical sensors for gesture recognition in current commercial VR equipment. However, EMG data is complex and machine learning must be used to employ it. This study uses a Myo armband to classify four custom gestures in Wekinator and observe their prediction accuracies and representations (including or omitting signal onset) to compose music within VR. Results show that specific regression and classification models, according to gesture representation type, are the most accurate when classifying four music gestures for advanced music HCI in VR. We apply and record our results, showing that EMG biometrics are promising for future interactive music composition systems in VR.","2022","2023-07-05 07:18:09","2023-07-19 11:34:01","2023-07-05 07:18:09","385-400","","","13221","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-031-03789-4_25","","/Users/minsik/Zotero/storage/AIF88GPN/Rhodes et al. - 2022 - Classifying Biometric Data for Musical Interaction.pdf","","","","Martins, Tiago; Rodríguez-Fernández, Nereida; Rebelo, Sérgio M.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DIQA6TNM","bookSection","2022","Kania, Damian; Szurmik, Tomasz; Bibrowicz, Karol; Romaniszyn-Kania, Patrycja; Czak, Mirosław; Mańka, Anna; Rosiak, Maria; Turner, Bruce; Pollak, Anita; Mitas, Andrzej W.","The Effect of Therapeutic Commands on the Teaching of Maintaining Correct Static Posture","Information Technology in Biomedicine","978-3-031-09134-6 978-3-031-09135-3","","","https://link.springer.com/10.1007/978-3-031-09135-3_33","The article presents the results of a preliminary study analy-sing the physiological parameters obtained during exercises that teach the patient’s correct body posture while sitting. Electrodermal activity (EDA), blood volume pulse (BVP), and electromyographic (EMG) signals were recorded and analysed during the training process for position shaping. A music preference and musicality questionnaire was carried out before the study. The JAWS questionnaire was completed twice by the respondent, before and after exercises. The physiotherapists provided instructions with respect to the stimulation of the autonomic nervous system, observed in EDA, heart rate and the subsequent motor units. While performing the exercises, the subjects felt positive emotions, which can be perceived as a positive experience for the probands and suggests their willingness to learn and maintain correct body posture while sitting. The sonification of the therapist’s commands and their sonic emotional content is further researched.","2022","2023-07-05 07:18:09","2023-07-20 06:34:24","2023-07-05 07:18:09","393-405","","","1429","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Advances in Intelligent Systems and Computing DOI: 10.1007/978-3-031-09135-3_33","","","","","","Pietka, Ewa; Badura, Pawel; Kawa, Jacek; Wieclawek, Wojciech","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6JBPVVA4","journalArticle","2020","Rozé, Jocelyn; Aramaki, Mitsuko; Kronland-Martinet, Richard; Ystad, Sølvi","Cellists’ sound quality is shaped by their primary postural behavior","Scientific Reports","","2045-2322","10.1038/s41598-020-70705-8","https://www.nature.com/articles/s41598-020-70705-8","During the last 20 years, the role of musicians’ body movements has emerged as a central question in instrument practice: Why do musicians make so many postural movements, for instance, with their torsos and heads, while playing musical instruments? The musical significance of such ancillary gestures is still an enigma and therefore remains a major pedagogical challenge, since one does not know if these movements should be considered essential embodied skills that improve musical expressivity. Although previous studies established clear connections between musicians’ body movements and musical structures (particularly for clarinet, piano or violin performances), no evidence of direct relationships between body movements and the quality of the produced timbre has ever been found. In this study, focusing on the area of bowed-string instruments, we address the problem by showing that cellists use a set of primary postural directions to develop fluid kinematic bow features (velocity, acceleration) that prevent the production of poor quality (i.e., harsh, shrill, whistling) sounds. By comparing the body-related angles between normal and posturally constrained playing situations, our results reveal that the chest rotation and vertical inclination made by cellists act as coordinative support for the kinematics of the bowing gesture. These findings support the experimental works of Alexander, especially those that showed the role of head movements with respect to the upper torso (the so-called primary control) in ensuring the smooth transmission of fine motor control in musicians all the way to the produced sound. More generally, our research highlights the importance of focusing on this fundamental postural sense to improve the quality of human activities across different domains (music, dance, sports, rehabilitation, working positions, etc.).","2020-08-17","2023-07-05 07:19:05","2023-07-05 07:19:05","2023-07-05 07:19:05","13882","","1","10","","Sci Rep","","","","","","","","en","2020 The Author(s)","","","","www.nature.com","","Number: 1 Publisher: Nature Publishing Group","","/Users/minsik/Zotero/storage/VUV6HSIR/Rozé et al. - 2020 - Cellists’ sound quality is shaped by their primary.pdf","","","Acoustics; Skeleton","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C57BXEDK","journalArticle","2006","Barrass, Stephen; Barrass, Tim","Musical creativity in collaborative virtual environments","Virtual Reality","","1359-4338, 1434-9957","10.1007/s10055-006-0043-5","http://link.springer.com/10.1007/s10055-006-0043-5","A review of musical creativity in collaborative virtual environments (CVE) shows recurring interaction metaphors that tend from precise control of individual parameters to higher level gestural influence over whole systems. Musical performances in CVE also show a consistent re-emergence of a unique form of collaboration called “melding” in which individual virtuosity is subsumed to that of the group. Based on these observations, we hypothesized that CVE could be a medium for creating new forms of music, and developed the audiovisual augmented reality system (AVIARy) to explore higher level metaphors for composing spatial music in CVE. This paper describes the AVIARy system, the initial experiments with interaction metaphors, and the application of the system to develop and stage a collaborative musical performance at a sound art concert. The results from these experiments indicate that CVE can be a medium for new forms of musical creativity and distinctive forms of music.","2006-09-27","2023-07-05 07:21:00","2023-07-21 05:14:03","2023-07-05 07:21:00","149-157","","2","10","","Virtual Reality","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V2CEYTXK","journalArticle","2022","Wu, Yongmeng; Bryan-Kinns, Nick; Zhi, Jinyi","Exploring visual stimuli as a support for novices’ creative engagement with digital musical interfaces","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-022-00393-3","https://link.springer.com/10.1007/s12193-022-00393-3","Visual materials are a widely used tool for stimulating creativity. This paper explores the potential for visual stimuli to support novices’ creative engagement with multimodal digital musical interfaces. An empirical study of 24 participants was conducted to compare the effect of abstract and literal forms of graphical scores on novices’ creative engagement, and whether being informed or uninformed about meanings of symbols in the score had any impact on creative engagement. The results suggest that abstract visual stimuli can provide an effective scaffold for creative engagement when participants are not informed about their design. It was found that providing information about visual stimuli has both advantages and disadvantages, depending largely on the stimuli’s visual style. Being informed about the meaning of a literal visual stimuli helped participants in making interpretations and gaining inspiration, whereas having information about abstract stimuli led to frustration. Qualitative data indicates that both forms of visual stimuli support creative engagement but at different stages of a creative process, and a descriptive model is presented to explain this. The findings highlight the benefits of visual stimuli in supporting creative engagement in the process of music making – a multimodal interaction domain typically involving few or no visual activities.","2022-09","2023-07-05 07:21:00","2023-07-20 07:05:59","2023-07-05 07:21:00","343-356","","3","16","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2Q3DHWJU","bookSection","2011","Merer, Adrien; Ystad, Sølvi; Kronland-Martinet, Richard; Aramaki, Mitsuko","Abstract Sounds and Their Applications in Audio and Perception Research","Exploring Music Contents","978-3-642-23125-4 978-3-642-23126-1","","","http://link.springer.com/10.1007/978-3-642-23126-1_12","Recognition of sound sources and events is an important process in sound perception and has been studied in many research domains. Conversely sounds that cannot be recognized are not often studied except by electroacoustic music composers. Besides, considerations on recognition of sources might help to address the problem of stimulus selection and categorization of sounds in the context of perception research. This paper introduces what we call abstract sounds with the existing musical background and shows their relevance for different applications.","2011","2023-07-05 07:21:00","2023-07-20 00:05:42","2023-07-05 07:21:00","176-187","","","6684","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-23126-1_12","","/Users/minsik/Zotero/storage/VJMSD29S/Merer et al. - 2011 - Abstract Sounds and Their Applications in Audio an.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W9XWQJVC","bookSection","2011","Makeig, Scott; Leslie, Grace; Mullen, Tim; Sarma, Devpratim; Bigdely-Shamlo, Nima; Kothe, Christian","First Demonstration of a Musical Emotion BCI","Affective Computing and Intelligent Interaction","978-3-642-24570-1 978-3-642-24571-8","","","http://link.springer.com/10.1007/978-3-642-24571-8_61","Development of EEG-based brain computer interface (BCI) methods has largely focused on creating a communication channel for subjects with intact cognition but profound loss of motor control from stroke or neurodegenerative disease that allows such subjects to communicate by spelling out words on a personal computer. However, other important human communication channels may also be limited or unavailable for handicapped subjects -- direct non-linguistic emotional communication by gesture, vocal prosody, facial expression, etc.. We report and examine a first demonstration of a musical ‘emotion BCI’ in which, as one element of a live musical performance, an able-bodied subject successfully engaged the electronic delivery of an ordered sequence of five music two-tone bass frequency drone sounds by imaginatively re-experiencing the human feeling he had spontaneously associated with the sound of each drone sound during training sessions. The EEG data included activities of both brain and non-brain sources (scalp muscles, eye movements). Common Spatial Pattern classification gave 84% correct pseudo-online performance and 5-of-5 correct classification in live performance. Re-analysis of the training session data including only the brain EEG sources found by multiple-mixture Amica ICA decomposition achieved five-class classification accuracy of 59-70%, confirming that different voluntary emotion imagination experiences may be associated with distinguishable brain source EEG dynamics.","2011","2023-07-05 07:21:00","2023-07-19 11:13:24","2023-07-05 07:21:00","487-496","","","6975","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-24571-8_61","","/Users/minsik/Zotero/storage/IMIB4MXK/Makeig et al. - 2011 - First Demonstration of a Musical Emotion BCI.pdf","","","","D’Mello, Sidney; Graesser, Arthur; Schuller, Björn; Martin, Jean-Claude","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RX97YFC5","bookSection","2017","Arango, Julián Jaramillo","Sound Interaction Design and Creation in the Context of Urban Space","Bridging People and Sound","978-3-319-67737-8 978-3-319-67738-5","","","http://link.springer.com/10.1007/978-3-319-67738-5_8","This paper reports current theoretical and creative results of a postdoctoral research study entitled Sound Design for Urban Spaces. It focuses on the design process of novel audio devices and on encouraging people in transit through the city to explore technology-empowered listening strategies. In this paper, the practice of urban sound design will be raised and some conceptual contributions concerning listening, acoustic analysis and sonic interaction will be discussed. We will extract some design insights that have been inspiring in the creation process of the Smartphone Ensemble, The AirQ Jacket and Lumina Nocte. These projects have been got under way along with MA students from the Design and Creation program at the Caldas University in Manizales, Colombia.","2017","2023-07-05 07:21:00","2023-07-19 23:36:53","2023-07-05 07:21:00","137-149","","","10525","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-67738-5_8","","","","","","Aramaki, Mitsuko; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JWNSGHSY","bookSection","2017","Sinclair, Peter; Cahen, Roland; Tanant, Jonathan; Gena, Peter","New Atlantis: Audio Experimentation in a Shared Online World","Bridging People and Sound","978-3-319-67737-8 978-3-319-67738-5","","","http://link.springer.com/10.1007/978-3-319-67738-5_14","Computer games and virtual worlds are ""traditionally"" visually orientated, and their audio dimension often secondary. In this paper we will describe New Atlantis a virtual world that aims to put sound first. We will describe the motivation, the history and the development of this FrancoAmerican project and the serendipitous use made of the distance between partner structures. We explain the overall architecture of the world and discuss the reasons for certain key structural choices. New Atlantis' first aim is to provide a platform for audio-graphic design and practice, for students as well as artists and researchers, engaged in higher education art or media curricula. We describe the integration of student’s productions through workshops and exchanges and discuss and the first public presentations of NA that took place from January 2016. Finally we will unfold perspectives for future research and the further uses of New Atlantis.","2017","2023-07-05 07:21:00","2023-07-19 23:37:15","2023-07-05 07:21:00","229-246","","","10525","","","New Atlantis","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-67738-5_14","","/Users/minsik/Zotero/storage/94G8V58N/Sinclair et al. - 2017 - New Atlantis Audio Experimentation in a Shared On.pdf","","","","Aramaki, Mitsuko; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UUN6243X","bookSection","2011","Riedenklau, Eckard; Hermann, Thomas; Ritter, Helge","Saving and Restoring Mechanisms for Tangible User Interfaces through Tangible Active Objects","Human-Computer Interaction. Interaction Techniques and Environments","978-3-642-21604-6 978-3-642-21605-3","","","http://link.springer.com/10.1007/978-3-642-21605-3_12","In this paper we present a proof of concept for saving and restoring mechanisms for Tangible User Interfaces (TUIs). We describe our actuated Tangible Active Objects (TAOs) and explain the design which allows equal user access to a dial-based fully tangible actuated menu metaphor. We present a new application extending an existing TUI for interactive sonification of process data with saving and restoring mechanisms and we outline another application proposal for family therapists.","2011","2023-07-05 07:21:00","2023-07-20 06:32:42","2023-07-05 07:21:00","110-118","","","6762","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-21605-3_12","","/Users/minsik/Zotero/storage/7443DGDV/Riedenklau et al. - 2011 - Saving and Restoring Mechanisms for Tangible User .pdf","","","","Jacko, Julie A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KLU2NE9F","bookSection","2007","Taylor, Robyn; Kazakevich, Maryia; Boulanger, Pierre; Garcia, Manuel; Bischof, Walter F.","Multi-modal Interface for Fluid Dynamics Simulations Using 3–D Localized Sound","Smart Graphics","978-3-540-73213-6 978-3-540-73214-3","","","http://link.springer.com/10.1007/978-3-540-73214-3_17","Multi-modal capabilities can be added to a simulation system in order to enhance data comprehension. We describe a system for adding sonification capabilities to a real-time computational fluid dynamics (CFD) simulator. Our system uses Max/MSP modules to add sonic properties to CFD solutions. The enhancements described in this paper allow users to locate sound sources in a 3–D environment using stereo auditory cues to identify data features.","2007","2023-07-05 07:21:00","2023-07-21 04:59:55","2023-07-05 07:21:00","182-187","","","4569","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73214-3_17","","/Users/minsik/Zotero/storage/H5BA33JS/Taylor et al. - 2007 - Multi-modal Interface for Fluid Dynamics Simulatio.pdf","","","","Butz, Andreas; Fisher, Brian; Krüger, Antonio; Olivier, Patrick; Owada, Shigeru","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XAXJBXM7","bookSection","2016","Mascetti, Sergio; Rossetti, Chiara; Gerino, Andrea; Bernareggi, Cristian; Picinali, Lorenzo; Rizzi, Alessandro","Towards a Natural User Interface to Support People with Visual Impairments in Detecting Colors","Computers Helping People with Special Needs","978-3-319-41263-4 978-3-319-41264-1","","","http://link.springer.com/10.1007/978-3-319-41264-1_23","A mobile application that detects an item’s color is potentially very useful for visually impaired people. However, users could run into difficulties when centering the target item in the mobile device camera field of view. To address this problem, in this contribution we propose a mobile application that detects the color of the item pointed by the user with one finger. In its current version, the application requires the user to wear a marker on the finger used for pointing. A preliminary evaluation conducted with blind users confirms the usefulness of the application, and encourages further development.","2016","2023-07-05 07:21:00","2023-07-19 23:52:14","2023-07-05 07:21:00","171-178","","","9758","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-41264-1_23","","/Users/minsik/Zotero/storage/7T9TQ6SU/Mascetti et al. - 2016 - Towards a Natural User Interface to Support People.pdf","","","","Miesenberger, Klaus; Bühler, Christian; Penaz, Petr","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YPCMAJ59","bookSection","2019","Sakhardande, Prabodh; Joshi, Anirudha; Jadhav, Charudatta; Joshi, Manjiri","Comparing User Performance on Parallel-Tone, Parallel-Speech, Serial-Tone and Serial-Speech Auditory Graphs","Human-Computer Interaction – INTERACT 2019","978-3-030-29380-2 978-3-030-29381-9","","","http://link.springer.com/10.1007/978-3-030-29381-9_16","Visualization techniques such as bar graphs and pie charts let sighted users quickly understand and explore numerical data. These techniques remain by and large inaccessible for visually impaired users. Even when these are made accessible, they remain slow and cumbersome, and not as useful as they might be to sighted users. Previous research has studied two methods of improving perception and speed of navigating auditory graphs - using non-speech audio (such as tones) instead of speech to communicate data and using two audio streams in parallel instead of in series. However, these studies were done in the early 2000s and speech synthesis techniques have improved considerably in recent times, as has the familiarity of visually impaired users with smartphones and speech systems. We systematically compare user performance on four modes that can be used for the generation of auditory graphs: parallel-tone, parallel-speech, serialtone, and serial-speech. We conducted two within-subjects studies - one with 20 sighted users and the other with 20 visually impaired users. Each user group performed point estimation and point comparison tasks with each technique on two sizes of bar graphs. We assessed task time, errors and user preference. We found that while tone was faster than speech, speech was more accurate than tone. The parallel modality was faster than serial modality and visually impaired users were faster than their sighted counterparts. Further, users showed a strong personal preference towards the serial-speech technique. To the best of our knowledge, this is the first empirical study that systematically compares these four techniques.","2019","2023-07-05 07:21:00","2023-07-20 06:29:25","2023-07-05 07:21:00","247-266","","","11746","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-29381-9_16","","/Users/minsik/Zotero/storage/FI8GS2KP/Sakhardande et al. - 2019 - Comparing User Performance on Parallel-Tone, Paral.pdf","","","","Lamas, David; Loizides, Fernando; Nacke, Lennart; Petrie, Helen; Winckler, Marco; Zaphiris, Panayiotis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NF9GB98Q","bookSection","2012","Paalasmaa, Joonas; Murphy, David J.; Holmqvist, Ove","Analysis of Noisy Biosignals for Musical Performance","Advances in Intelligent Data Analysis XI","978-3-642-34155-7 978-3-642-34156-4","","","http://link.springer.com/10.1007/978-3-642-34156-4_23","Biosignal sensors are now small, affordable, and wireless. We desire to include such sensors (e.g. heart rate, respiration, acceleration) in a live musical performance, which sets requirements on the reliability and variability of the data. Unfortunately the raw signals from such devices are unable to meet these requirements. We contribute our solutions for overcoming the shortcomings of these sensors in two parts. The first is an online data processing and analysis system, including on-line generative models that describe the signals but add consistency. The second is the end-to-end system for capturing wireless signal data for the analysis system and integrating the resulting output into a popular digital audio workstation in a very flexible manner conducive to live performance. We also explore the role of “analysis supervisor”—a member of the performing act who ensures that the results of biosignal analysis fall within the desired ranges to contribute to the music effectively.","2012","2023-07-05 07:21:00","2023-07-19 11:12:11","2023-07-05 07:21:00","241-252","","","7619","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-34156-4_23","","","","","","Hollmén, Jaakko; Klawonn, Frank; Tucker, Allan","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6PF6KEW8","bookSection","2009","Suzuki, Kenji","Embodied Sound Media Technology for the Enhancement of the Sound Presence","Human-Computer Interaction. Novel Interaction Methods and Techniques","978-3-642-02576-1 978-3-642-02577-8","","","http://link.springer.com/10.1007/978-3-642-02577-8_82","In this paper, the paradigms of Embodied Sound Media (ESM) technology are described with several case studies. The ESM is designed to formalize a musical sound-space based on the conversion of free human movement into sounds. This technology includes the measurement of human motion, processing, acoustic conversion and output. The first idea was to introduce direct and intuitive sound feedbacks within the context of not only embodied interaction between humans and devices but also social interaction among humans. The developed system is a sort of active aid for an embodied performance that allows the users to get feedback for emotional stimuli in terms of sound surrounding the users. The overviews of several devices developed in this scenario and the potential applications to physical fitness, exercise, entertainment, assistive technology and rehabilitation are also addressed.","2009","2023-07-05 07:21:00","2023-07-20 06:32:55","2023-07-05 07:21:00","745-751","","","5611","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-02577-8_82","","/Users/minsik/Zotero/storage/NGQEEDSV/Suzuki - 2009 - Embodied Sound Media Technology for the Enhancemen.pdf","","","","Jacko, Julie A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2Z8NDBVP","bookSection","2008","Vy, Quoc V.; Mori, Jorge A.; Fourney, David W.; Fels, Deborah I.","EnACT: A Software Tool for Creating Animated Text Captions","Computers Helping People with Special Needs","978-3-540-70539-0 978-3-540-70540-6","","","http://link.springer.com/10.1007/978-3-540-70540-6_87","Music in captioning is often represented by only its title and/or a music note. This representation provides little to no information of the intended effect or emotion of the music. In this paper, we present a software tool that was created to enable users to mark emotions in a script or lyrics and then render those marks into animated text for display as captions. A pilot study was conducted to collect initial responses to, preferences and understanding of the animated lyrics of one song by a deaf and hard of hearing audience. Participants were able to identify the animated lyrics as belonging to a song and found that the animations helped them understand the portrayed emotions. They also identified the shaking style of animation portraying fear as least preferable.","2008","2023-07-05 07:21:00","2023-07-19 23:53:40","2023-07-05 07:21:00","609-616","","","5105","","","EnACT","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-70540-6_87","","","","","","Miesenberger, Klaus; Klaus, Joachim; Zagler, Wolfgang; Karshmer, Arthur","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QDZ38DTX","bookSection","2010","Valle, Andrea; Lombardo, Vincenzo; Schirosa, Mattia","Simulating the Soundscape through an Analysis/Resynthesis Methodology","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_17","This paper presents a graph-based system for the dynamic generation of soundscapes and its implementation in an application that allows for an interactive, real-time exploration of the resulting soundscapes. The application can be used alone, as a pure sonic exploration device, but can also be integrated into a virtual reality engine. In this way, the soundcape can be acoustically integrated in the exploration of an architectonic/urbanistic landscape. The paper is organized as follows: after taking into account the literature on soundscape, we provide a formal definition of the concept; then, a model is introduced, and finally, we describe a software application together with a case-study.","2010","2023-07-05 07:21:00","2023-07-19 11:38:50","2023-07-05 07:21:00","330-357","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_17","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E2XEYUIM","bookSection","1997","Fernström, M.; Bannon, L.","Explorations in Sonic Browsing","People and Computers XII","978-3-540-76172-3 978-1-4471-3601-9","","","http://link.springer.com/10.1007/978-1-4471-3601-9_8","","1997","2023-07-05 07:21:00","2023-07-05 07:21:00","2023-07-05 07:21:00","117-131","","","","","","","","","","","Springer London","London","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-1-4471-3601-9_8","","","","","","Thimbleby, Harold; O’Conaill, Brid; Thomas, Peter J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5QMF9SNP","bookSection","2010","Gygi, Brian; Shafiro, Valeriy","From Signal to Substance and Back: Insights from Environmental Sound Research to Auditory Display Design","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_16","","2010","2023-07-05 07:21:00","2023-07-05 07:21:00","2023-07-05 07:21:00","306-329","","","5954","","","From Signal to Substance and Back","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_16","","/Users/minsik/Zotero/storage/RWBXT8ZF/Gygi and Shafiro - 2010 - From Signal to Substance and Back Insights from E.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M3CJCR8B","bookSection","2014","Neidlinger, Kristin; Ju, Wendy","Sound Bending – Talking Bodies Quantum Sound Suits","Design, User Experience, and Usability. User Experience Design for Diverse Interaction Platforms and Environments","978-3-319-07625-6 978-3-319-07626-3","","","http://link.springer.com/10.1007/978-3-319-07626-3_56","The QuantumSound Suits are an innovative technological solution for creating sounds from movement. Made in collaboration with contortionists, a multidisciplinary team designed custom body-painted silicone suits embedded with flexible sensors. A healing sound artist mapped the tones of the eleven sensors to movement, animating the physical motion and providing sonic feedback of body’s position. This is an exploration of real-time movement notation and human activity recognition of body location in space.","2014","2023-07-05 07:21:00","2023-07-19 23:55:39","2023-07-05 07:21:00","598-605","","","8518","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-07626-3_56","","/Users/minsik/Zotero/storage/TRVLALV4/Neidlinger and Ju - 2014 - Sound Bending – Talking Bodies Quantum Sound Suits.pdf","","","","Marcus, Aaron","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Kobsa, Alfred; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Doug; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VUHVMM9U","journalArticle","2021","Kolel-Veetil, Manoj; Sen, Ayusman; Buehler, Markus J.","Surface adhesion of viruses and bacteria: Defend only and/or vibrationally extinguish also?! A perspective","MRS Advances","","2059-8521","10.1557/s43580-021-00079-0","https://link.springer.com/10.1557/s43580-021-00079-0","Coronaviruses COVID-19, SARS-CoV and NL63 use spikes in their corona to bind to angiotensin converting enzyme 2 (ACE2) sites on cytoskeletal membranes of host cells to deliver their viral payload. While groups such as disulfides in ACE2’s zinc metallopeptidase, and also in COVID-19’s spikes, facilitate such binding, it is worth exploring how similar complementary sites on materials such as polymers, metals, ceramics, fabrics, and biomaterials promote binding of viruses and bacteria and how they could be further engineered to prevent bioactivity, or to act as agents to collect viral payloads in filters or similar devices. In that vein, this article offers a perspective on novel tools and approaches for chemically and topologically modifying most utilitarian surfaces via defensive topological vibrational engineering to either prevent such adhesion or to enhance adhesion and elicit vibrational characteristics/’musical signatures’ from the surfaces so that the structure of the binding sites of viruses and bacteria is permanently altered and/or their cellular machinery is permanently disabled by targeted chemical transformations.","2021-06","2023-07-05 07:21:00","2023-07-21 04:30:22","2023-07-05 07:21:00","355-361","","13","6","","MRS Advances","Surface adhesion of viruses and bacteria","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/4UKRKN7T/Kolel-Veetil et al. - 2021 - Surface adhesion of viruses and bacteria Defend o.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9ZI6JH3A","bookSection","2010","Wersényi, György","Auditory Representations of a Graphical User Interface for a Better Human-Computer Interaction","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_5","As part of a project to improve human computer interaction mostly for blind users, a survey with 50 blind and 100 sighted users included a questionnaire about their user habits during everyday use of personal computers. Based on their answers, the most important functions and applications were selected and results of the two groups were compared. Special user habits and needs of blind users are described. The second part of the investigation included collecting of auditory representations (auditory icons, spearcons etc.), mapping with visual information and evaluation with the target groups. Furthermore, a new design method for auditory events and class was introduced, called “auditory emoticons”. These use non-verbal human voice samples to represent additional emotional content. Blind and sighted users evaluated different auditory representations for the selected events, including spearcons for different languages. Auditory icons using environmental, familiar sounds as well emoticons are received very well, whilst spearcons seem to be redundant except menu navigation for blind users.","2010","2023-07-05 07:21:00","2023-07-19 11:39:45","2023-07-05 07:21:00","80-102","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_5","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T56HM7KS","bookSection","2003","Bennett, David J.","Effects of Navigation and Position on Task When Presenting Diagrams to Blind People uUsing Sound","Diagrammatic Representation and Inference","978-3-540-43561-7 978-3-540-46037-4","","","http://link.springer.com/10.1007/3-540-46037-3_19","This paper questions how we could and should present diagrams to blind people using computer-generated sound. Using systems that present information about one part of the diagram at a time, rather than the whole, leads to two problems. The first problem is how to present information so that users can integrate the information into a coherent overall picture. The second is how to select the area to be presented. This is looked at by using a system that presents graphs representing central heating system schematics. The system presents information by user choice through either a hierarchical split of information and navigation system, or a connection oriented split of information and navigation system. Further, we have a split as to whether a simple system of presenting location of nodes is used, or not. Tasks, classified as being based on hierarchical information or connection-based information, were set using the system and the effect of the different models was recorded. It was found that there was a match of task to navigation system, but that presentation of position had no discernable effect.","2003","2023-07-05 07:23:08","2023-07-19 23:56:09","2023-07-05 07:23:08","161-175","","","2317","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-46037-3_19","","","","","","Hegarty, Mary; Meyer, Bernd; Narayanan, N. Hari","Goos, G.; Hartmanis, J.; Van Leeuwen, J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9USIX3JA","journalArticle","2012","Grond, Florian","Safety Certificate: an audification performance of high-speed trains","AI & SOCIETY","","0951-5666, 1435-5655","10.1007/s00146-011-0351-5","http://link.springer.com/10.1007/s00146-011-0351-5","Safety Certificate is a musical performance based on sensor data from high-speed trains. The original purpose of this data is to provide a basis for the assessments of the mechanical aspects of train safety. In this performance, the data, which represents dynamical processes below the audible range, are converted into sound through audification. The sound that is generated live during the performance is manipulated through the Manta control interface, which allows for the convenient layering of 48 different timbres. Safety Certificate was premiered at Seconde Nature in Aix-en-Provence in March 2010 during the Sonification symposium–What, Where, How, Why, organized by Locus Sonus. The following short article gives details about the data, the audification technique, use of the control interface, and the musical structure of the performance.","2012-05","2023-07-05 07:23:08","2023-07-19 11:20:41","2023-07-05 07:23:08","293-295","","2","27","","AI & Soc","Safety Certificate","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7RXV8FBY","bookSection","2019","Colombo, Roberto; Mazzone, Alessandra; Delconte, Carmen; Raglio, Alfredo","Patient Motivation and Rewarding to Maximize Outcome: A Sensory Perspective","Converging Clinical and Engineering Research on Neurorehabilitation III","978-3-030-01844-3 978-3-030-01845-0","","","http://link.springer.com/10.1007/978-3-030-01845-0_42","Motivation is an important topic in rehabilitation and frequently used as a determinant of rehabilitation outcome. Several factors can influence patient motivation and so improve exercise adherence. This paper presents some techniques to improve patient motivation and maximize outcome during technology-assisted rehabilitation. In particular, we present some examples of multimodal augmented feedback. This approach provide feedback on the performance during training through the stimulation of different sensory channels such as vision, audition, proprioception etc. It is believed to have several advantages over unimodal feedback and it can promote the patient’s motivation and maximize outcome.","2019","2023-07-05 07:23:08","2023-07-19 23:53:49","2023-07-05 07:23:08","213-217","","","21","","","Patient Motivation and Rewarding to Maximize Outcome","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Biosystems & Biorobotics DOI: 10.1007/978-3-030-01845-0_42","","","","","","Masia, Lorenzo; Micera, Silvestro; Akay, Metin; Pons, José L.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GLNAQEXS","journalArticle","2017","Braund, Edward; Miranda, Eduardo Reck","On Building Practical Biocomputers for Real-world Applications: Receptacles for Culturing Slime Mould Memristors and Component Standardisation","Journal of Bionic Engineering","","1672-6529, 2543-2141","10.1016/S1672-6529(16)60386-4","http://link.springer.com/10.1016/S1672-6529(16)60386-4","Our application of bionic engineering is novel: we are interested in developing hybrid hardware-wetware systems for music. This paper introduces receptacles for culturing Physarum polycephalum-based memristors that are highly accessible to the creative practitioner. The myxomycete Physarum polycephalum is an amorphous unicellular organism that has been found to exhibit memristive properties. Such a discovery has potential to allow us to move towards engineering electrical systems that encompass Physarum polycephalum components. To realise this potential, it is necessary to address some of the constraints associated with harnessing living biological entities in systems for real-time application. Within the paper, we present 3D printed receptacles designed to standardise both the production of components and memristive observations. Subsequent testing showed a significant decrease in growth time, increased lifespan, and superior similarity in component-to-component responses. The results indicate that our receptacle design may provide means of implementing hybrid electrical systems for music technology.","2017-03","2023-07-05 07:23:08","2023-07-20 06:46:29","2023-07-05 07:23:08","151-162","","1","14","","J Bionic Eng","On Building Practical Biocomputers for Real-world Applications","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/AI2GKMV2/Braund and Miranda - 2017 - On Building Practical Biocomputers for Real-world .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U2YUAAAP","bookSection","2021","Spagnol, Gabriela Salim; Ling, Li Hui; Li, Li Min; Manzolli, Jônatas","A Proposal of Emotion Evocative Sound Compositions for Therapeutic Purposes","Perception, Representations, Image, Sound, Music","978-3-030-70209-0 978-3-030-70210-6","","","http://link.springer.com/10.1007/978-3-030-70210-6_26","Recognition and understanding of emotions is a path for self healing. We have worked with Mandalas of Emotions, derived from the Traditional Chinese Medicine (TCM), as a complementary therapy. In this paper, we present the conceptual framework related to the creation of sound collages for the five elements of TCM and assessment of these compositions by experienced holistic therapists. Results present quantitative data, according to scales for relaxation, arousal and valence, and qualitative data from transcription and analysis of the recorded responses of volunteers. In our study, the most common perceptions were warmth, irritation, peace and fear. The innovation of this proposal may stimulate further research on emotion-evoking sounds, and in sound composition.","2021","2023-07-05 07:23:08","2023-07-21 04:45:31","2023-07-05 07:23:08","396-408","","","12631","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-70210-6_26","","","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Aramaki, Mitsuko","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3NGN6G6Q","bookSection","2013","Conan, Simon; Aramaki, Mitsuko; Kronland-Martinet, Richard; Ystad, Sølvi","Intuitive Control of Rolling Sound Synthesis","From Sounds to Music and Emotions","978-3-642-41247-9 978-3-642-41248-6","","","http://link.springer.com/10.1007/978-3-642-41248-6_6","This paper presents a rolling sound synthesis model which can be intuitively controlled. To propose this model, different aspects of the rolling phenomenon are explored : physical modeling, perceptual attributes and signal morphology. A source-filter model for rolling sounds synthesis is presented with associated intuitive controls.","2013","2023-07-05 07:23:08","2023-07-20 00:05:52","2023-07-05 07:23:08","99-109","","","7900","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-41248-6_6","","/Users/minsik/Zotero/storage/J8ZM9YPV/Conan et al. - 2013 - Intuitive Control of Rolling Sound Synthesis.pdf","","","","Aramaki, Mitsuko; Barthet, Mathieu; Kronland-Martinet, Richard; Ystad, Sølvi","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JGP4VDNK","bookSection","2016","Rickman, Jordan; Tanenbaum, Theresa Jean","GeoPoetry: Designing Location-Based Combinatorial Electronic Literature Soundtracks for Roadtrips","Interactive Storytelling","978-3-319-48278-1 978-3-319-48279-8","","","https://link.springer.com/10.1007/978-3-319-48279-8_8","In this paper we present GeoPoetry, a location-based work of electronic literature that generates poetic language and dynamic soundtracks for road-trips that reflect the mood of people in the surrounding area. GeoPoetry takes recent nearby geotagged Twitter data and generates strings of combinatorial poetry from them using simple Markov-chain text generation. It also performs a sentiment analysis on the local Twitter traffic, which it uses to seed a playlist on Spotify, using a simple model of affect. The result is a sonic reflection of the social geography traversed by the user that responds to its situatedness in both space and time. GeoPoetry participates in a long tradition of public and locative artwork which has the potential to inspire exciting new works of interactive narrative.","2016","2023-07-05 07:23:08","2023-07-20 06:37:09","2023-07-05 07:23:08","85-96","","","10045","","","GeoPoetry","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-48279-8_8","","","","","","Nack, Frank; Gordon, Andrew S.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UKNB87AE","journalArticle","2017","Mathew, Justin Dan; Huot, Stéphane; Katz, Brian F. G.","Survey and implications for the design of new 3D audio production and authoring tools","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-017-0245-z","http://link.springer.com/10.1007/s12193-017-0245-z","3D audio production tools vary from lowlevel programming libraries to higher-level user interfaces that are used across a wide range of applications today. However, many of these user interfaces are underdeveloped with limited functionality, forcing users to resort to ad hoc solutions with other tools or programming languages. Identifying these limitations and custom methods are needed to inform the development of new user interfaces. Towards this end, an on-line survey was conducted with current practitioners to gather ethnographic information on their tools, methods, and opinions. Results of the survey identified specific methods and limitations regarding Audio Rendering, Visual Feedback, Functionality, and Workflow Integration. These results also shed light on three basic tasks that have to be performed interactively with 3D audio production tools: Defining the Rendering Space, Creation and Manipulation of Audio Objects, and Use of Feedback. This classification helps organize the creative needs for 3D audio tools that address issues within the workflow and low-level functionality of systems.","2017-09","2023-07-05 07:23:08","2023-07-20 07:02:21","2023-07-05 07:23:08","277-287","","3","11","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/J369T77E/Mathew et al. - 2017 - Survey and implications for the design of new 3D a.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QI476AR6","journalArticle","2018","Ghai, Shashank; Ghai, Ishan; Schmitz, Gerd; Effenberg, Alfred O.","Effect of rhythmic auditory cueing on parkinsonian gait: A systematic review and meta-analysis","Scientific Reports","","2045-2322","10.1038/s41598-017-16232-5","https://www.nature.com/articles/s41598-017-16232-5","Abstract             The use of rhythmic auditory cueing to enhance gait performance in parkinsonian patients’ is an emerging area of interest. Different theories and underlying neurophysiological mechanisms have been suggested for ascertaining the enhancement in motor performance. However, a consensus as to its effects based on characteristics of effective stimuli, and training dosage is still not reached. A systematic review and meta-analysis was carried out to analyze the effects of different auditory feedbacks on gait and postural performance in patients affected by Parkinson’s disease. Systematic identification of published literature was performed adhering to PRISMA guidelines, from inception until May 2017, on online databases; Web of science, PEDro, EBSCO, MEDLINE, Cochrane, EMBASE and PROQUEST. Of 4204 records, 50 studies, involving 1892 participants met our inclusion criteria. The analysis revealed an overall positive effect on gait velocity, stride length, and a negative effect on cadence with application of auditory cueing. Neurophysiological mechanisms, training dosage, effects of higher information processing constraints, and use of cueing as an adjunct with medications are thoroughly discussed. This present review bridges the gaps in literature by suggesting application of rhythmic auditory cueing in conventional rehabilitation approaches to enhance motor performance and quality of life in the parkinsonian community.","2018-01-11","2023-07-05 07:23:08","2023-07-05 07:23:08","2023-07-05 07:23:08","506","","1","8","","Sci Rep","Effect of rhythmic auditory cueing on parkinsonian gait","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/JWMVXCDW/Ghai et al. - 2018 - Effect of rhythmic auditory cueing on parkinsonian.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BHKHTFNQ","bookSection","2010","Brazil, Eoin","A Review of Methods and Frameworks for Sonic Interaction Design: Exploring Existing Approaches","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_3","This article presents a review of methods and frameworks focused on the early conceptual design of sonic interactions. The aim of the article is to provide novice and expert designers in human computer interaction an introduction to sonic interaction design, to auditory displays, and to the methods used to design the sounds and interactions. A range of the current best practices are analysed. These are discussed with regard to the key methods and concepts, by providing examples from existing work in the field. A complementary framework is presented to highlight how these methods can be used together by an auditory display designer at the early conceptual design stage. These methods are reflected upon and provides a closing discussion on the future directions of research that can be explored using these approaches.","2010","2023-07-05 07:23:08","2023-07-19 11:36:54","2023-07-05 07:23:08","41-67","","","5954","","","A Review of Methods and Frameworks for Sonic Interaction Design","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_3","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IMULEYKI","journalArticle","2020","Rocchesso, Davide; Mannone, Maria","A quantum vocal theory of sound","Quantum Information Processing","","1570-0755, 1573-1332","10.1007/s11128-020-02772-9","https://link.springer.com/10.1007/s11128-020-02772-9","Abstract                            Concepts and formalism from acoustics are often used to exemplify quantum mechanics. Conversely, quantum mechanics could be used to achieve a new perspective on acoustics, as shown by Gabor studies. Here, we focus in particular on the study of human voice, considered as a probe to investigate the world of sounds. We present a theoretical framework that is based on               observables               of vocal production, and on some               measurement apparati               that can be used both for analysis and synthesis. In analogy to the description of spin states of a particle, the quantum-mechanical formalism is used to describe the relations between the fundamental states associated with phonetic labels such as phonation, turbulence, and supraglottal myoelastic vibrations. The intermingling of these states, and their temporal evolution, can still be interpreted in the Fourier/Gabor plane, and effective extractors can be implemented. The bases for a quantum vocal theory of sound, with implications in sound analysis and design, are presented.","2020-09","2023-07-05 07:23:08","2023-07-05 07:23:08","2023-07-05 07:23:08","292","","9","19","","Quantum Inf Process","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/KURYRRVZ/Rocchesso and Mannone - 2020 - A quantum vocal theory of sound.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7ZV97AQW","journalArticle","2019","De Leo-Winkler, M. A.; Wilson, G.; Green, W.; Chute, L.; Henderson, E.; Mitchell, T.","The Vibrating Universe: Astronomy for the Deaf","Journal of Science Education and Technology","","1059-0145, 1573-1839","10.1007/s10956-018-9761-1","http://link.springer.com/10.1007/s10956-018-9761-1","The Deaf have often been overlooked when designing informal STEM education and public outreach activities. Astronomers at UC Riverside and teachers at the California School for the Deaf, Riverside (CSDR), have designed an astronomy workshop aimed specifically for the Deaf using the school’s on-site sound lab. We have used astronomy for this workshop because the field has a significant edge over other sciences to act as portal for K-12 engagement in science given the imagery it presents, the answers it offers to grand questions, and its interdisciplinary nature. The workshop is an unconventional activity that excites the students and provides a positive experience in astronomy, based on knowledge that they already acquired beforehand in the classroom. Our workshop uses electromagnetic emissions, enhanced sounds and sonification processes of cosmic phenomena that have low frequencies and sufficiently distinguishable patterns which are delivered to students through a specialized designed sound lab for the Deaf. Storytelling paired with videos and images are used to give understandable meaning to the sounds of the Universe. Positive feedback was collected from over 80 students who participated in our workshop. Our activity can be reproduced elsewhere to further engage the Deaf community in science.","2019-06","2023-07-05 07:23:08","2023-07-20 06:49:40","2023-07-05 07:23:08","222-230","","3","28","","J Sci Educ Technol","The Vibrating Universe","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/2ZH73UBN/De Leo-Winkler et al. - 2019 - The Vibrating Universe Astronomy for the Deaf.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SVGTKTKY","journalArticle","2020","Magnusson, Charlotte; Rassmus-Gröhn, Kirsten; Rydeman, Bitte","Developing a mobile activity game for stroke survivors—lessons learned","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-020-00342-y","https://link.springer.com/10.1007/s12193-020-00342-y","Abstract             Persons who have survived a stroke might lower the risk of having recurrent strokes by adopting a healthier lifestyle with more exercise. One way to promote exercising is by fitness or exergame apps for mobile phones. Health and fitness apps are used by a significant portion of the consumers, but these apps are not targeted to stroke survivors, who may experience cognitive limitations (like fatigue and neglect), have problems with mobility due to hemiplegia, and balance problems. We outline the design process, implementation and user involvement in the design of an exergame app that is specifically targeted to stroke survivors, and present the lessons learned during the design process.","2020-09","2023-07-05 07:23:08","2023-07-05 07:23:08","2023-07-05 07:23:08","303-312","","3","14","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/WMA9WA22/Magnusson et al. - 2020 - Developing a mobile activity game for stroke survi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YLH36GPD","journalArticle","2022","Canessa, Enrique","Wave-like behaviour in (0,1) binary sequences","Scientific Reports","","2045-2322","10.1038/s41598-022-18360-z","https://www.nature.com/articles/s41598-022-18360-z","Abstract                            A comprehensive study of the properties of finite (0,1) binary systems from the mathematical viewpoint of quantum theory is presented. This is a quantum-inspired extension of the GenomeBits model to characterize observed genome sequences, where a complex wavefunction                                                   $$\psi _{n}$$                                                               ψ                       n                                                                                       is considered as an analogous probability measure and it is related to an alternating (0,1) binary series having independent distributed terms. The real and imaginary spectrum of                                                   $$\psi _{n}$$                                                               ψ                       n                                                                                       vs.               the nucleotide base positions display characteristic features of sound waves. This approach represents a novel perspective for identifying and “observing” emergent properties of genome sequences in the form of wavefunctions via superposition states. The motivation is to develop a simple algorithm to perform wave calculations from binary sequences and to apply these wave functions to sonification.","2022-08-17","2023-07-05 07:23:08","2023-07-05 07:23:08","2023-07-05 07:23:08","13971","","1","12","","Sci Rep","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/H2AXEUN2/Canessa - 2022 - Wave-like behaviour in (0,1) binary sequences.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DYE7PWM6","journalArticle","2012","Bakker, Saskia; Van Den Hoven, Elise; Eggen, Berry","Knowing by ear: leveraging human attention abilities in interaction design","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0062-8","http://link.springer.com/10.1007/s12193-011-0062-8","In a world in which intelligent technologies are integrated in everyday objects and environments, users are at risk of being overburdened with information and interaction possibilities. Calm technology therefore aims at designing interactions that may reside in the periphery of the user’s attention and only shift to the center of the attention when required. However, for such designs to be effective, a detailed understanding of human attention abilities is needed. In this paper, we therefore present a qualitative study on the everyday periphery of the attention. As we expected, we found that sound plays a major role in this, which supports our approach to use interactive sonification as an interaction style for peripheral interaction. We present a range of rich examples of everyday situations that lay out the design space for peripheral interaction and support these findings by describing three initial designs that use interactive sonification for peripheral interaction.","2012-05","2023-07-05 07:23:08","2023-07-20 06:50:41","2023-07-05 07:23:08","197-209","","3-4","5","","J Multimodal User Interfaces","Knowing by ear","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/X399P5R7/Bakker et al. - 2012 - Knowing by ear leveraging human attention abiliti.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9IEG444D","journalArticle","2012","Katz, Brian F. G.; Kammoun, Slim; Parseihian, Gaëtan; Gutierrez, Olivier; Brilhault, Adrien; Auvray, Malika; Truillet, Philippe; Denis, Michel; Thorpe, Simon; Jouffrais, Christophe","NAVIG: augmented reality guidance system for the visually impaired: Combining object localization, GNSS, and spatial audio","Virtual Reality","","1359-4338, 1434-9957","10.1007/s10055-012-0213-6","http://link.springer.com/10.1007/s10055-012-0213-6","Navigating complex routes and finding objects of interest are challenging tasks for the visually impaired. The project NAVIG (Navigation Assisted by artificial VIsion and GNSS) is directed toward increasing personal autonomy via a virtual augmented reality system. The system integrates an adapted geographic information system with different classes of objects useful for improving route selection and guidance. The database also includes models of important geolocated objects that may be detected by real-time embedded vision algorithms. Object localization (relative to the user) may serve both global positioning and sensorimotor actions such as heading, grasping, or piloting. The user is guided to his desired destination through spatialized semantic audio rendering, always maintained in the head-centered reference frame. This paper presents the overall project design and architecture of the NAVIG system. In addition, details of a new type of detection and localization device are presented. This approach combines a bio-inspired vision system that can recognize and locate objects very quickly and a 3D sound rendering system that is able to perceptually position a sound at the location of the recognized object. This system was developed in relation to guidance directives developed through participative design with potential users and educators for the visually impaired.","2012-11","2023-07-05 07:23:08","2023-07-21 05:14:20","2023-07-05 07:23:08","253-269","","4","16","","Virtual Reality","NAVIG","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DBMQDUIX","journalArticle","2021","Takabatake, Kazuhiko; Kunii, Naoto; Nakatomi, Hirofumi; Shimada, Seijiro; Yanai, Kei; Takasago, Megumi; Saito, Nobuhito","Musical Auditory Alpha Wave Neurofeedback: Validation and Cognitive Perspectives","Applied Psychophysiology and Biofeedback","","1090-0586, 1573-3270","10.1007/s10484-021-09507-1","https://link.springer.com/10.1007/s10484-021-09507-1","Abstract             Neurofeedback through visual, auditory, or tactile sensations improves cognitive functions and alters the activities of daily living. However, some people, such as children and the elderly, have difficulty concentrating on neurofeedback for a long time. Constant stressless neurofeedback for a long time may be achieved with auditory neurofeedback using music. The primary purpose of this study was to clarify whether music-based auditory neurofeedback increases the power of the alpha wave in healthy subjects. During neurofeedback, white noise was superimposed on classical music, with the noise level inversely correlating with normalized alpha wave power. This was a single-blind, randomized control crossover trial in which 10 healthy subjects underwent, in an assigned order, normal and random feedback (NF and RF), either of which was at least 4 weeks long. Cognitive functions were evaluated before, between, and after each neurofeedback period. The secondary purpose was to assess neurofeedback-induced changes in cognitive functions. A crossover analysis showed that normalized alpha-power was significantly higher in NF than in RF; therefore, music-based auditory neurofeedback facilitated alpha wave induction. A composite category-based analysis of cognitive functions revealed greater improvements in short-term memory in subjects whose alpha-power increased in response to NF. The present study employed a long period of auditory alpha neurofeedback and achieved successful alpha wave induction and subsequent improvements in cognitive functions. Although this was a pilot study that validated a music-based alpha neurofeedback system for healthy subjects, the results obtained are encouraging for those with difficulty in concentrating on conventional alpha neurofeedback.             Trial registration: 2018077NI, date of registration: 2018/11/27","2021-12","2023-07-05 07:23:08","2023-07-05 07:23:08","2023-07-05 07:23:08","323-334","","4","46","","Appl Psychophysiol Biofeedback","Musical Auditory Alpha Wave Neurofeedback","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/QD2LCQ6E/Takabatake et al. - 2021 - Musical Auditory Alpha Wave Neurofeedback Validat.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K54MBR6J","bookSection","2017","Mauceri, Frank; Majercik, Stephen M.","A Swarm Environment for Experimental Performance and Improvisation","Computational Intelligence in Music, Sound, Art and Design","978-3-319-55749-6 978-3-319-55750-2","","","http://link.springer.com/10.1007/978-3-319-55750-2_13","This paper describes Swarm Performance and Improvisation (Swarm-PI), a real-time computer environment for music improvisation that uses swarm algorithms to control sound synthesis and to mediate interactions with a human performer. Swarm models are artificial, multi-agent systems where the organized movements of large groups are the result of simple, local rules between individuals. Swarms typically exhibit self-organization and emergent behavior. In Swarm-PI, multiple acoustic descriptors from a live audio feed generate parameters for an independent swarm among multiple swarms in the same space, and each swarm is used to synthesize a stream of sound using granular sampling. This environment demonstrates the effectiveness of using swarms to model human interactions typical to group improvisation and to generate organized patterns of synthesized sound.","2017","2023-07-05 07:23:08","2023-07-19 23:47:33","2023-07-05 07:23:08","190-200","","","10198","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-55750-2_13","","","","","","Correia, João; Ciesielski, Vic; Liapis, Antonios","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J4ZVZ9GW","bookSection","2014","Rodrigues, Mailis G.; Wanderley, Marcelo M.; Lopes, Paulo F.","Intonaspacio: A Digital Musical Instrument for Exploring Site-Specificities in Sound","Sound, Music, and Motion","978-3-319-12975-4 978-3-319-12976-1","","","https://link.springer.com/10.1007/978-3-319-12976-1_24","The integration of space as a parameter in the composition of an art work as been relegated to a secondary role. Site-specific art is a branch of the visual arts whose main goal is to fuse space in the art work, i.e., the piece belongs to the space where it is placed and its meaning is lost once it is removed. There’s an idea of bi-directionality beneath the conception of the work, where space defines the perception of the piece and the piece interferes in the perception of space. In music and especially in sound art, there are some examples of site-specific works, but they are sparse and mainly centered on the idea of installation. Site-specificity in sound is an open and not yet fully explored field. Our research purposes a new digital musical instrument (DMI) Intonaspacio, which allows the access to the sound of the room, and the integration of it in the music in real time. Intonaspacio provides the performer with tools to create site-specific sound, i.e., to integrate space as part of the creative work. In this paper we present Intonaspacio, focusing attention on the design of the physical interface. Up until now we have designed two versions of Intonaspacio. From the first version to the second one, we have modified the material of the frame, which led us to reconsider some of our previous decisions on the choice of sensors and their placement. We also designed different mappings that present several approaches to the site-specific question.","2014","2023-07-05 07:23:08","2023-07-21 05:02:01","2023-07-05 07:23:08","393-415","","","8905","","","Intonaspacio","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-12976-1_24","","","","","","Aramaki, Mitsuko; Derrien, Olivier; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PY44H8GV","bookSection","2001","McCormack, Jon","Eden: An Evolutionary Sonic Ecosystem","Advances in Artificial Life","978-3-540-42567-0 978-3-540-44811-2","","","http://link.springer.com/10.1007/3-540-44811-X_13","This paper describes an Artificial Life system for music composition. An evolving ecology of sonic entities populate a virtual world and compete for limited resources. Part of their genetic representation permits the creatures to make and listen to sounds. Complex musical and sonic relationships can develop as the creatures use sound to aid in their survival and mating prospects.","2001","2023-07-05 07:23:08","2023-07-19 11:10:01","2023-07-05 07:23:08","133-142","","","2159","","","Eden","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-44811-X_13","","","","","","Kelemen, Jozef; Sosík, Petr","Goos, G.; Hartmanis, J.; Van Leeuwen, J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"23LMEIPG","bookSection","2002","Lorho, Gaëtan; Hiipakka, Jarmo; Marila, Juha","Structured Menu Presentation Using Spatial Sound Separation","Human Computer Interaction with Mobile Devices","978-3-540-44189-2 978-3-540-45756-5","","","http://link.springer.com/10.1007/3-540-45756-9_51","","2002","2023-07-05 07:27:40","2023-07-05 07:27:40","2023-07-05 07:27:40","419-424","","","2411","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-45756-9_51","","","","","","Paternò, Fabio","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VEB28LYZ","bookSection","2021","Delle Monache, Stefano; Rocchesso, Davide","Exploring Design Cognition in Voice-Driven Sound Sketching and Synthesis","Perception, Representations, Image, Sound, Music","978-3-030-70209-0 978-3-030-70210-6","","","http://link.springer.com/10.1007/978-3-030-70210-6_30","Conceptual design and communication of sonic ideas are critical, and still unresolved aspects of current sound design practices, especially when teamwork is involved. Design cognition studies in the visual domain represent a valuable resource to look at, to better comprehend the reasoning of designers when they approach a sound-based project. A design exercise involving a team of professional sound designers is analyzed, and discussed in the framework of the Function-BehaviorStructure ontology of design. The use of embodied sound representations of concepts fosters team-building and a more effective communication, in terms of shared mental models.","2021","2023-07-05 07:27:40","2023-07-21 04:45:04","2023-07-05 07:27:40","465-480","","","12631","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-70210-6_30","","/Users/minsik/Zotero/storage/NHMUMHQV/Delle Monache and Rocchesso - 2021 - Exploring Design Cognition in Voice-Driven Sound S.pdf","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Aramaki, Mitsuko","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F8E5HD4W","bookSection","2023","Navarro-Cáceres, Juan José; Mendes, André Sales; Blas, Hector Sánchez San; González, Gabriel Villarrubia; Navarro-Cáceres, María","MusicFactory: Application of a Convolutional Neural Network for the Generation of Soundscapes from Images","New Trends in Disruptive Technologies, Tech Ethics and Artificial Intelligence","978-3-031-14858-3 978-3-031-14859-0","","","https://link.springer.com/10.1007/978-3-031-14859-0_14","A soundscape is a sound description of a concrete environment. Therefore, the soundscapes are always connected to a visual component, as it might capture sounds from an urban city, a countryside, or a domestic place. In this work, we present a system that generate soundscapes from images. Firstly, we recognize some objects in the image. In a second step the system searches the most adequate sounds according to the entities identified in the picture. Finally, a soundscape is synthesized by combining the short sound files found. The results obtained according to the subjective evaluation are promising and encouraging to deepen our research in the soundscape generation.","2023","2023-07-05 07:27:40","2023-07-21 04:39:06","2023-07-05 07:27:40","156-164","","","1430","","","MusicFactory","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Advances in Intelligent Systems and Computing DOI: 10.1007/978-3-031-14859-0_14","","","","","","De La Iglesia, Daniel H.; De Paz Santana, Juan F.; López Rivero, Alfonso J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EWQYEYH6","journalArticle","2011","Miranda, Eduardo R.; Adamatzky, Andrew; Jones, Jeff","Sounds synthesis with slime mould of Physarum Polycephalum","Journal of Bionic Engineering","","1672-6529, 2543-2141","10.1016/S1672-6529(11)60016-4","http://link.springer.com/10.1016/S1672-6529(11)60016-4","Physarum polycephalum is a huge single cell with thousands of nuclei, which behaves like a giant amoeba. During its foraging behaviour this plasmodium produces electrical activity corresponding to different physiological states. We developed a method to render sounds from such electrical activity and thus represent spatio-temporal behaviour of slime mould in a form apprehended by humans. We show to control behaviour of slime mould to shape it towards reproduction of required range of sounds.","2011-06","2023-07-05 07:27:40","2023-07-20 06:46:38","2023-07-05 07:27:40","107-113","","2","8","","J Bionic Eng","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/UFBD3KCV/Miranda et al. - 2011 - Sounds synthesis with slime mould of Physarum Poly.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3856TWWF","bookSection","2013","Rinaldi, Claudia; Santic, Marco; Pomante, Luigi; Graziosi, Fabio","Exploiting Latest Technologies for RF Sounding’s Evolution","Arts and Technology","978-3-642-37981-9 978-3-642-37982-6","","","http://link.springer.com/10.1007/978-3-642-37982-6_5","In this paper we present the most recent technological innovations introduced into the artistic installation we called RF Sounding, keep on maintaining our fundamental goals: creating an artistic installation that can be used for educational purposes as well. Indeed we have been inspired by the impossible human dream of flying that we reasoned on the acoustic dimension. We decided to make the inaudible, audible by a translation in the audio bandwidth of signals coming from cellular networks. We thus want to provide the user, entering the specifically defined area, with awareness of radio frequency signals characterizing the cellular networks band. With respect to the prototype presented in previous papers we finally exploit the information coming from a spectrum analyser, thus taking into account also the whole uplink, and position data acquired from a Microsoft Kinect in order to realize localization inside the equipped area, without the need for the users to wear an active device.","2013","2023-07-05 07:27:40","2023-07-19 11:35:24","2023-07-05 07:27:40","33-40","","","116","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering DOI: 10.1007/978-3-642-37982-6_5","","","","","","De Michelis, Giorgio; Tisato, Francesco; Bene, Andrea; Bernini, Diego","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PABNE37B","bookSection","2014","Laffineur, Ludovic; Degeest, Alexandra; Frisson, Christian; Giot, Rudi","Interactive Network Installation","Intelligent Technologies for Interactive Entertainment","978-3-319-08188-5 978-3-319-08189-2","","","http://link.springer.com/10.1007/978-3-319-08189-2_20","The work discussed is this paper deals with a interactive installation to monitor the network flow in a artistic way. The system is developed in C++ grabs packets using LibPCap, analyses them at low level (e.g. packet length) and also provides high-level information (e.g. port number). This new approach is based more on the network flow analysis than on network services analysis. The software communicates with ©Resolume Avenue and ©Reaktor through OSC protocol. ©Resolume Avenue is a software for Video Jockey (VJ) purposes and ©Reaktor is a modular software music studio developed by Native Instrument. Users can actively take part to an interactive audiovisual exhibition system using their mobile device to send e-mails, listen to a web radio, surf on a website, read RSS feeds, in short, the experience begins once visitors exchange data with the network.","2014","2023-07-05 07:27:40","2023-07-20 06:36:04","2023-07-05 07:27:40","140-143","","","136","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering DOI: 10.1007/978-3-319-08189-2_20","","","","","","Reidsma, Dennis; Choi, Insook; Bargar, Robin","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PQ822AV4","journalArticle","2010","Roma, Gerard; Janer, Jordi; Kersten, Stefan; Schirosa, Mattia; Herrera, Perfecto; Serra, Xavier","Ecological Acoustics Perspective for Content-Based Retrieval of Environmental Sounds","EURASIP Journal on Audio, Speech, and Music Processing","","1687-4714, 1687-4722","10.1155/2010/960863","http://asmp.eurasipjournals.com/content/2010/1/960863","In this paper we present a method to search for environmental sounds in large unstructured databases of user-submitted audio, using a general sound events taxonomy from ecological acoustics. We discuss the use of Support Vector Machines to classify sound recordings according to the taxonomy and describe two use cases for the obtained classification models: a content-based web search interface for a large audio database and a method for segmenting field recordings to assist sound design.","2010","2023-07-05 07:27:40","2023-07-21 07:39:39","2023-07-05 07:27:40","1-11","","","2010","","EURASIP Journal on Audio, Speech, and Music Processing","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/IYKDSVIJ/Roma et al. - 2010 - Ecological Acoustics Perspective for Content-Based.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7WT5TM5X","journalArticle","2009","Lugmayr, Artur; Risse, Thomas; Stockleben, Bjoern; Laurila, Kari; Kaario, Juha","Semantic ambient media—an introduction","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-009-0282-z","http://link.springer.com/10.1007/s11042-009-0282-z","The medium is the message! And the message was literacy, media democracy and music charts. Mostly one single distinguishable medium such as TV, the Web, the radio, or books transmitted the message. Now in the age of ubiquitous and pervasive computing, where information flows through a plethora of distributed interlinked media—what is the message ambient media will tell us? What does semantic mean in this context? Which experiences will it open to us? What is content in the age of ambient media? Ambient media are embedded throughout the natural environment of the consumer—in his home, in his car, in restaurants, and on his mobile device. Predominant sample services are smart wallpapers in homes, location based services, RFID based entertainment services for children, or intelligent homes. The goal of this article is to define semantic ambient media and discuss the contributions to the Semantic Ambient Media Experience (SAME) workshop, which was held in conjunction with the ACM Multimedia conference in Vancouver in 2008. The results of the workshop can be found on: www.ambientmediaassociation.org.","2009-09","2023-07-05 07:27:40","2023-07-21 04:33:10","2023-07-05 07:27:40","337-359","","3","44","","Multimed Tools Appl","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TXTC9EMU","journalArticle","1992","Rasmussen, Susan J.","Reflections on tamazai, a Tuareg idiom of suffering","Culture, Medicine and Psychiatry","","0165-005X, 1573-076X","10.1007/BF00052154","http://link.springer.com/10.1007/BF00052154","In this essay, I explore ways in which the theatrical and the medical are inextricably mixed and affect each other, in the Nigerien Tuareg idiom of tamazai. Tamazai is locally-described as “an illness of the heart and soul, not curable by Koranic verses,” but by exorcism of spirits. A case study and analysis of healing rituals demonstrate how this idiom communicates women's relationships and empowers dramatic framing.","1992-09","2023-07-05 07:27:40","2023-07-19 23:54:24","2023-07-05 07:27:40","337-365","","3","16","","Cult Med Psych","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XDTT797K","bookSection","2011","Eigenfeldt, Arne; Pasquier, Philippe","A Sonic Eco-System of Self-Organising Musical Agents","Applications of Evolutionary Computation","978-3-642-20519-4 978-3-642-20520-0","","","http://link.springer.com/10.1007/978-3-642-20520-0_29","We present a population of autonomous agents that exist within a sonic eco-system derived from real-time analysis of live audio. In this system, entitled Coming Together: Shoals, agents search for food consisting of CataRT unit analyses, which, when found, are consumed through granulation. Individual agents are initialised with random synthesis parameters, but communicate these parameters to agents in local neighborhoods. Agents form social networks, and converge their parameters within these networks, thereby creating unified grain streams. Separate gestures thus emerge through the self-organisation of the population.","2011","2023-07-05 07:27:40","2023-07-19 11:30:38","2023-07-05 07:27:40","283-292","","","6625","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-20520-0_29","","","","","","Di Chio, Cecilia; Brabazon, Anthony; Di Caro, Gianni A.; Drechsler, Rolf; Farooq, Muddassar; Grahl, Jörn; Greenfield, Gary; Prins, Christian; Romero, Juan; Squillero, Giovanni; Tarantino, Ernesto; Tettamanzi, Andrea G. B.; Urquhart, Neil; Uyar, A. Şima","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MPGV8ETA","journalArticle","2004","Coward, Sean W.; Stevens, Catherine J.","Extracting Meaning from Sound: Nomic Mappings, Everyday Listening, and Perceiving Object Size from Frequency","The Psychological Record","","0033-2933, 2163-3452","10.1007/BF03395478","http://link.springer.com/10.1007/BF03395478","In developing a theoretical framework for the field of ecological acoustics, Gaver (1993b) distinguished between the experience of musical listening (perceiving sounds) and everyday listening (perceiving sources of sounds). Within the everyday listening experience, Gaver (1993a) proposed that the frequency of an object results from, and therefore specifies, the size of that object. The relation in which frequency and object size stand to one another is an example of a nomic mapping. A symbolic mapping involves the pairing of unrelated dimensions and, relative to a nomic mapping, requires an additional step in recognition and learning. Using a perceptual identification task, an experiment investigated the hypothesis that nomic mappings are identified more easily than symbolic mappings. It was predicted that the advantage manifests only during the everyday listening experience, and that the initially superior recognition of nomic mappings is equaled by symbolic mappings after extended exposure. The results provided support for the hypotheses. Theoretical implications of the differential recognition of nomic and symbolic mappings are discussed, together with practical applications of nomic relations.","2004-07","2023-07-05 07:27:40","2023-07-21 05:07:20","2023-07-05 07:27:40","349-364","","3","54","","Psychol Rec","Extracting Meaning from Sound","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VTELXM9N","bookSection","2020","Moesgaard, Frederik; Hulgaard, Lasse; Bødker, Mads","Involving Users in Sound Design","Design, User Experience, and Usability. Interaction Design","978-3-030-49712-5 978-3-030-49713-2","","","http://link.springer.com/10.1007/978-3-030-49713-2_28","Sound plays an important role in our well-being, our experience of the world around us and our understanding of products, services and interactions. Sound affects our sense of place, and it can modulate our feelings, agency and attention. In a world of increasingly ubiquitous digital technologies, sound may prove a valuable resource for sense making as well as experience- and UX design. Yet the possibilities and challenges of user participation in sound design processes are not well understood. This paper reports on a pilot study examining how participants can be involved in different phases of a sound design process. The results and reflections aim to help researchers and designers in an effort to better understand some of the dynamics of moving from a largely expert driven approach to sound design towards a more user-oriented and participatory approaches.","2020","2023-07-05 07:27:40","2023-07-19 23:55:29","2023-07-05 07:27:40","405-425","","","12200","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-49713-2_28","","","","","","Marcus, Aaron; Rosenzweig, Elizabeth","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J3AQ6VNY","journalArticle","2021","Feng, Feng; Li, Puhong; Stockman, Tony","Exploring crossmodal perceptual enhancement and integration in a sequence-reproducing task with cognitive priming","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-020-00326-y","https://link.springer.com/10.1007/s12193-020-00326-y","Abstract             Crossmodal correspondence, a perceptual phenomenon which has been extensively studied in cognitive science, has been shown to play a critical role in people’s information processing performance. However, the evidence has been collected mostly based on strictly-controlled stimuli and displayed in a noise-free environment. In real-world interaction scenarios, background noise may blur crossmodal effects that designers intend to leverage. More seriously, it may induce additional crossmodal effects, which can be mutually exclusive to the intended one, leading to unexpected distractions from the task at hand. In this paper, we report two experiments designed to tackle these problems with cognitive priming techniques. The first experiment examined how to enhance the perception of specific crossmodal stimuli, namely pitch–brightness and pitch–elevation stimuli. The second experiment investigated how people perceive and respond to crossmodal stimuli that were mutually exclusive. Results showed that first, people’s crossmodal perception was affected by cognitive priming, though the effect varies according to the combination of crossmodal stimuli and the types of priming material. Second, when two crossmodal stimuli are mutually exclusive, priming on only the dominant one (Pitch–elevation) lead to improved performance. These results can help inform future design of multisensory systems by presenting details of how to enhance crossmodal information with cognitive priming.","2021-03","2023-07-05 07:27:40","2023-07-05 07:27:40","2023-07-05 07:27:40","45-59","","1","15","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/GN7QDXU5/Feng et al. - 2021 - Exploring crossmodal perceptual enhancement and in.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S4EE49DQ","bookSection","2004","Nicholson, Mark; Vickers, Paul","Pen-Based Gestures: An Approach to Reducing Screen Clutter in Mobile Computing","Mobile Human-Computer Interaction - MobileHCI 2004","978-3-540-23086-1 978-3-540-28637-0","","","http://link.springer.com/10.1007/978-3-540-28637-0_30","Mobile computing is an area of high growth despite having some serious design issues. It is difficult to increase the size of the screen because of the device’s physical constraints. Consequently, as mobile applications have incorporated more functionality, screen clutter has increased. One method of reducing clutter is to remove visual controls and use pen-based gestures instead. We describe a cinema listing application for a Palm OS device that implements pen-based gestures as the main input method. Two methods are used to communicate the options available on each screen: audio cues and small visual prompts. Preliminary results suggest that buttons can be removed from the screen without detriment to task accuracy or user performance.","2004","2023-07-05 07:27:40","2023-07-21 04:30:13","2023-07-05 07:27:40","320-324","","","3160","","","Pen-Based Gestures","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-28637-0_30","","","","","","Brewster, Stephen; Dunlop, Mark","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3MLNBFVM","bookSection","2020","Rajko, Jessica J.","Designing Palpable Data Representations","HCI International 2020 - Late Breaking Papers: User Experience Design and Case Studies","978-3-030-60113-3 978-3-030-60114-0","","","http://link.springer.com/10.1007/978-3-030-60114-0_32","This paper discusses a multisensory approach to data representation with a specific focus on haptic media. In this, I provide a philosophical and methodological overview of my design process informed by the following themes and topics: 1) the haptic subject; 2) touch as political; 3) co-formed knowledge; and 4) arts-based research methods. The overview is further contextualized by a thorough analysis of collaborative work Vibrant Lives, a 4-year project that includes a suite of unique, custom-designed, vibrotactile interfaces that give audiences a real-time experience of their own personal data output. I continue my analysis by sharing observations from a series of workshops I conducted with haptified archive data. In conclusion, I reflect on issues of user ethics, agency, and control when designing touch-based experiences of data in a multisensory installation setting.","2020","2023-07-05 07:27:40","2023-07-20 05:49:55","2023-07-05 07:27:40","464-480","","","12423","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-60114-0_32","","","","","","Stephanidis, Constantine; Marcus, Aaron; Rosenzweig, Elizabeth; Rau, Pei-Luen Patrick; Moallem, Abbas; Rauterberg, Matthias","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7S8NHLUN","bookSection","2003","Goldstein, Mikael; Öquist, Gustav; Björk, Staffan","Evaluating Sonified Rapid Serial Visual Presentation: An Immersive Reading Experience on a Mobile Device","Universal Access Theoretical Perspectives, Practice, and Experience","978-3-540-00855-2 978-3-540-36572-3","","","http://link.springer.com/10.1007/3-540-36572-9_39","Can the addition of sound enhance the reading experience on small screens when using Rapid Serial Visual Presentation (RSVP) for dynamic text presentation? In this paper we introduce Sonified RSVP and report findings from a usability evaluation where the experience of reading texts enhanced with nomic auditory icons was evaluated. At a comfortable pace 12 subjects read long Swedish texts of equal difficulty with and without the addition of sound on a handheld device. Reading speed (M≈217 wpm) and comprehension (M≈58% correct) did not differ significantly between the two conditions. The evaluation revealed a rather high task load for both conditions but no significant differences. However, the subjective rating of Immersion was rated significantly higher for the Sonified condition. Causes, implications and directions for further work are discussed based on these findings.","2003","2023-07-05 07:27:40","2023-07-21 05:13:24","2023-07-05 07:27:40","508-523","","","2615","","","Evaluating Sonified Rapid Serial Visual Presentation","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-36572-9_39","","","","","","Carbonell, Noëlle; Stephanidis, Constantine","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XY3CG4GL","bookSection","2012","Rowland, Duncan; Connor, Katy","PURE FLOW: Gallery Installation / Mobile Application","Advances in Computer Entertainment","978-3-642-34291-2 978-3-642-34292-9","","","http://link.springer.com/10.1007/978-3-642-34292-9_33","This paper describes the two phase development of the digital art piece PURE FLOW. The first deployment of this work was as a gallery based exhibit in which digital noise sampled from the Global Positioning System was exposed as dynamic sound and projected visual displays. The second piece extended these initial themes onto a handheld platform (iPhone) whereby the user could continually sample digital noise from positioning systems at their surrounding environment and generate an audio and visual experience specifically created for their immediate location. Aesthetic considerations are described along with implementation details leading to general reflections relating to collaborations between artists and technical specialists.","2012","2023-07-05 07:30:22","2023-07-19 11:11:13","2023-07-05 07:30:22","445-452","","","7624","","","PURE FLOW","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-34292-9_33","","","","","","Nijholt, Anton; Romão, Teresa; Reidsma, Dennis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AB2FU5R6","bookSection","2000","Harding, Chris; Kakadiaris, Ioannis; Loftin, R. Bowen","A Multimodal User Interface for Geoscientific Data Investigation","Advances in Multimodal Interfaces — ICMI 2000","978-3-540-41180-2 978-3-540-40063-9","","","http://link.springer.com/10.1007/3-540-40063-X_80","In this paper, we report on our ongoing research into multimodal investigation of geoscientific data. Our system integrates three-dimensional, interactive computer graphics, touch (haptics) and real-time sound synthesis into a multimodal interface. We present applications of multimodal investigations of geoscientific data that pertain to surface meshes on which several typical properties were mapped and to geophysical volume data. Finally, we report on the preliminary results of a psychological study, which is being conducted to increase our understanding of the recognition of audio value in an absolute sense.","2000","2023-07-05 07:30:22","2023-07-19 11:12:31","2023-07-05 07:30:22","615-623","","","1948","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-40063-X_80","","","","","","Tan, Tieniu; Shi, Yuanchun; Gao, Wen","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VW2CH83P","bookSection","2009","Ferati, Mexhid; Bolchini, Davide; Mannheimer, Steve","Towards a Modeling Language for Designing Auditory Interfaces","Universal Access in Human-Computer Interaction. Applications and Services","978-3-642-02712-3 978-3-642-02713-0","","","http://link.springer.com/10.1007/978-3-642-02713-0_53","","2009","2023-07-05 07:30:22","2023-07-05 07:30:22","2023-07-05 07:30:22","502-511","","","5616","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-02713-0_53","","/Users/minsik/Zotero/storage/ERRAD3TU/Ferati et al. - 2009 - Towards a Modeling Language for Designing Auditory.pdf","","","","Stephanidis, Constantine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VKDSJYMU","bookSection","1994","Reed, Daniel A.","Experimental analysis of parallel systems: Techniques and open problems","Computer Performance Evaluation Modelling Techniques and Tools","978-3-540-58021-8 978-3-540-48416-5","","","http://link.springer.com/10.1007/3-540-58021-2_2","Massively parallel systems pose daunting performance instrumentation and data analysis problems. Balancing instrumentation detail, application perturbation, data reduction costs, and presentation complexity requires a mix of science, engineering, and art. This paper surveys current techniques for performance instrumentation and data presentation, illustrates one approach to tool extensibility, and discusses the implications of massive parallelism for performance analysis environments.","1994","2023-07-05 07:30:22","2023-07-19 23:50:15","2023-07-05 07:30:22","25-51","","","794","","","Experimental analysis of parallel systems","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-58021-2_2","","","","","","Haring, Günter; Kotsis, Gabriele","Goos, G.; Hartmanis, J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BR4A6LR2","journalArticle","2015","Csapó, Ádám; Wersényi, György; Nagy, Hunor; Stockman, Tony","A survey of assistive technologies and applications for blind users on mobile platforms: a review and foundation for research","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-015-0182-7","http://link.springer.com/10.1007/s12193-015-0182-7","","2015-12","2023-07-05 07:30:22","2023-07-05 07:30:22","2023-07-05 07:30:22","275-286","","4","9","","J Multimodal User Interfaces","A survey of assistive technologies and applications for blind users on mobile platforms","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/R7JFSBQT/Csapó et al. - 2015 - A survey of assistive technologies and application.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DFJSETKP","bookSection","2014","Bertacchini, Francesca; Bilotta, Eleonora; Carini, Manuela; Gabriele, Lorella; Pantano, Pietro; Tavernise, Assunta","Learning in the Smart City: A Virtual and Augmented Museum Devoted to Chaos Theory","New Horizons in Web Based Learning","978-3-662-43453-6 978-3-662-43454-3","","","http://link.springer.com/10.1007/978-3-662-43454-3_27","This paper presents a virtual museum introducing the interactive VR and MEMS applications related to the learning of chaos and complexity theory. In this museum, the user can learn the history of the dynamical systems and how to build Chua’s circuit, as well as realize artistic artifacts transforming attractors into sounds and music. This environment can be used in the city in order to create new ways of experiencing science, turning physical activities into virtual ones, an important step towards being able to have the museum in the smart city. Moreover, some applications have been developed to work on iPad and iPhone and can be used as a guide in the real exhibitions. A user-centred design strategy with 40 students has been carried out in order to implement the Virtual Museum of Chua’s Attractors, aiming at widening the experience in the smart city and allowing a considerable public participation.","2014","2023-07-05 07:30:22","2023-07-21 04:38:50","2023-07-05 07:30:22","261-270","","","7697","","","Learning in the Smart City","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-662-43454-3_27","","","","","","Chiu, Dickson K. W.; Wang, Minhong; Popescu, Elvira; Li, Qing; Lau, Rynson","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Kobsa, Alfred; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Doug; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J8N58A5W","bookSection","2021","Voisin, Frédéric; Bidotti, Arnaud; Mourey, France","Designing Soundscapes for Alzheimer’s Disease Care, with Preliminary Clinical Observations","Perception, Representations, Image, Sound, Music","978-3-030-70209-0 978-3-030-70210-6","","","https://link.springer.com/10.1007/978-3-030-70210-6_34","Acoustic environment is a prime source of conscious and unconscious information which allows listeners to place themselves, to communicate, to feel, to remember. Recently, there has been a growing interest to the acoustic environment and its perceptual counterparts of care facilities. In this contribution, the authors describe the process of designing a new audio interactive apparatus for Alzheimer’s Disease care in the context of an active multidisciplinary research project led by a sound designer since 2018, in collaboration with a residential longterm care (EHPAD) in France, a geriatrician, a gerontologist, psychologists and caregivers. The apparatus, named «Madeleines Sonores» in reference to Proust’s madeleine, has been providing virtual soundscapes for two years 24/7 to elderly people suffering from Alzheimer disease. The configuration and sound processes of the apparatus are presented in relation to Alzheimer Disease care. Preliminary psychological and clinical observations are discussed in relation to dementia and to the activity of caring to evaluate the benefits of such a disposal in Alzheimer’s disease therapy and in caring dementia.","2021","2023-07-05 07:30:22","2023-07-21 04:45:50","2023-07-05 07:30:22","533-553","","","12631","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-70210-6_34","","","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Aramaki, Mitsuko","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZC4ACNH4","bookSection","2012","Aramaki, Mitsuko; Kronland-Martinet, Richard; Ystad, Sølvi","Perceptual Control of Environmental Sound Synthesis","Speech, Sound and Music Processing: Embracing Research in India","978-3-642-31979-2 978-3-642-31980-8","","","http://link.springer.com/10.1007/978-3-642-31980-8_13","In this article we explain how perceptual control of synthesis processes can be achieved through a multidisciplinary approach relating physical and signal properties of sound sources to evocations induced by sounds. This approach is applied to environmental and abstract sounds in 3 different experiments. In the first experiment a perceptual control of synthesized impact sounds evoking sound sources of different materials and shapes is presented. The second experiment describes an immersive environmental synthesizer simulating different kinds of environmental sounds evoking natural events such as rain, waves, wind and fire. In the last example motion evoked by abstract sounds is investigated. A tool for describing perceived motion through drawings is proposed in this case.","2012","2023-07-05 07:30:22","2023-07-21 05:02:37","2023-07-05 07:30:22","172-186","","","7172","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-31980-8_13","","/Users/minsik/Zotero/storage/F6P77WX4/Aramaki et al. - 2012 - Perceptual Control of Environmental Sound Synthesi.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer; Mohanty, Sanghamitra","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U5RUPYTT","journalArticle","2008","Zimmermann, Andreas; Lorenz, Andreas","LISTEN: a user-adaptive audio-augmented museum guide","User Modeling and User-Adapted Interaction","","0924-1868, 1573-1391","10.1007/s11257-008-9049-x","http://link.springer.com/10.1007/s11257-008-9049-x","Modern personalized information systems have been proven to support the user with information at the appropriate level and in the appropriate form. In specific environments like museums and exhibitions, focusing on the control of such a system is contradictory to establishing a relationship with the artifacts and exhibits. Preferably, the technology becomes invisible to the user and the physical reality becomes the interface to an additional virtual layer: by naturally moving in the space and/or manipulating physical objects in our surroundings the user will access information and operate the virtual layer. The LISTEN project is an attempt to make use of the inherent “everyday” integration of aural and visual perception, developing a tailored, immersive audio-augmented environment for the visitors of art exhibitions. The challenge of the LISTEN project is to provide a personalized immersive augmented environment, an aim which goes beyond the guiding purpose. The visitors of the museum implicitly interact with the system because the audio presentation is adapted to the users’ contexts (e.g. interests, preferences, motion, etc.), providing an intelligent audio-based environment. This article describes the realization and user evaluation of the LISTEN system focusing on the personalization component. As this system has been installed at the Kunstmuseum Bonn in the context of an exhibition comprising artworks of the painter August Macke, a detailed evaluation could be conducted.","2008-11","2023-07-05 07:30:22","2023-07-21 05:13:39","2023-07-05 07:30:22","389-416","","5","18","","User Model User-Adap Inter","LISTEN","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VFQ2SPJW","bookSection","2007","Oakley, Ian; Park, Jun-Seok","Designing Eyes-Free Interaction","Haptic and Audio Interaction Design","978-3-540-76701-5","","","http://link.springer.com/10.1007/978-3-540-76702-2_13","As the form factors of computational devices diversify, the concept of eyes-free interaction is becoming increasingly relevant: it is no longer hard to imagine use scenarios in which screens are inappropriate. However, there is currently little consensus about this term. It is regularly employed in different contexts and with different intents. One key consequence of this multiplicity of meanings is a lack of easily accessible insights into how to best build an eyes-free system. This paper seeks to address this issue by thoroughly reviewing the literature, proposing a concise definition and presenting a set of design principles. The application of these principles is then elaborated through a case study of the design of an eyes-free motion input system for a wearable device.","2007","2023-07-05 07:30:22","2023-07-20 00:16:03","2023-07-05 07:30:22","121-132","","","4813","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-76702-2_13","","","","","","Oakley, Ian; Brewster, Stephen","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4YPS9TCW","bookSection","2009","Zhang, Jingjing; Lotto, Beau; Bergstom, Ilias; Andreou, Lefkothea; Miyadera, Youzou; Yokoyama, Setsuo","Development of a Visualised Sound Simulation Environment: An e-Approach to a Constructivist Way of Learning","Human-Computer Interaction. Interacting in Various Application Domains","978-3-642-02582-2 978-3-642-02583-9","","","http://link.springer.com/10.1007/978-3-642-02583-9_30","","2009","2023-07-05 07:30:22","2023-07-05 07:30:22","2023-07-05 07:30:22","266-275","","","5613","","","Development of a Visualised Sound Simulation Environment","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-02583-9_30","","/Users/minsik/Zotero/storage/P7RRUGCA/Zhang et al. - 2009 - Development of a Visualised Sound Simulation Envir.pdf","","","","Jacko, Julie A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XENVQ5N2","bookSection","2009","Grant, Jane; Matthias, John; Hodgson, Tim; Miranda, Eduardo","Hearing Thinking","Applications of Evolutionary Computing","978-3-642-01128-3 978-3-642-01129-0","","","http://link.springer.com/10.1007/978-3-642-01129-0_70","This paper describes early experiments, which attempt to reconfigure the sound of a breath using a network of artificial spiking cortical neurons. The connectivity of the network evolves according to a spike timing dependent plasticity algorithm and the instrument triggers grains of sound from recordings of the breath when any of the neurons fire.","2009","2023-07-05 07:30:22","2023-07-19 11:31:20","2023-07-05 07:30:22","609-614","","","5484","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-01129-0_70","","","","","","Giacobini, Mario; Brabazon, Anthony; Cagnoni, Stefano; Di Caro, Gianni A.; Ekárt, Anikó; Esparcia-Alcázar, Anna Isabel; Farooq, Muddassar; Fink, Andreas; Machado, Penousal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L6JF6TCM","journalArticle","2021","Rosso, Mattia; Maes, Pieter J.; Leman, Marc","Modality-specific attractor dynamics in dyadic entrainment","Scientific Reports","","2045-2322","10.1038/s41598-021-96054-8","https://www.nature.com/articles/s41598-021-96054-8","Abstract             Rhythmic joint coordination is ubiquitous in daily-life human activities. In order to coordinate their actions towards shared goals, individuals need to co-regulate their timing and move together at the collective level of behavior. Remarkably, basic forms of coordinated behavior tend to emerge spontaneously as long as two individuals are exposed to each other’s rhythmic movements. The present study investigated the dynamics of spontaneous dyadic entrainment, and more specifically how they depend on the sensory modalities mediating informational coupling. By means of a novel interactive paradigm, we showed that dyadic entrainment systematically takes place during a minimalistic rhythmic task despite explicit instructions to ignore the partner. Crucially, the interaction was organized by clear dynamics in a modality-dependent fashion. Our results showed highly consistent coordination patterns in visually-mediated entrainment, whereas we observed more chaotic and more variable profiles in the auditorily-mediated counterpart. The proposed experimental paradigm yields empirical evidence for the overwhelming tendency of dyads to behave as coupled rhythmic units. In the context of our experimental design, it showed that coordination dynamics differ according to availability and nature of perceptual information. Interventions aimed at rehabilitating, teaching or training sensorimotor functions can be ultimately informed and optimized by such fundamental knowledge.","2021-09-15","2023-07-05 07:30:22","2023-07-05 07:30:22","2023-07-05 07:30:22","18355","","1","11","","Sci Rep","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/5SSNDS97/Rosso et al. - 2021 - Modality-specific attractor dynamics in dyadic ent.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UFTWXXD5","journalArticle","2011","Hoffman, Guy; Weinberg, Gil","Interactive improvisation with a robotic marimba player","Autonomous Robots","","0929-5593, 1573-7527","10.1007/s10514-011-9237-0","http://link.springer.com/10.1007/s10514-011-9237-0","Shimon is a interactive robotic marimba player, developed as part of our ongoing research in Robotic Musicianship. The robot listens to a human musician and continuously adapts its improvisation and choreography, while playing simultaneously with the human. We discuss the robot’s mechanism and motion-control, which uses physics simulation and animation principles to achieve both expressivity and safety. We then present an interactive improvisation system based on the notion of physical gestures for both musical and visual expression. The system also uses anticipatory action to enable real-time improvised synchronization with the human player. We describe a study evaluating the effect of embodiment on one of our improvisation modules: antiphony, a call-and-response musical synchronization task. We conducted a 3×2 within-subject study manipulating the level of embodiment, and the accuracy of the robot’s response. Our findings indicate that synchronization is aided by visual contact when uncertainty is high, but that pianists can resort to internal rhythmic coordination in more predictable settings. We find that visual coordination is more effective for synchronization in slow sequences; and that occluded physical presence may be less effective than audio-only note generation. Finally, we test the effects of visual contact and embodiment on audience appreciation. We find that visual contact in joint Jazz improvisation makes for a performance in which audiences rate the robot as playing better, more like a human, as more responsive, and as more inspired by the human. They also rate the duo as better synchronized, more coherent, communicating, and coordinated; and the human as more inspired and more responsive.","2011-10","2023-07-05 07:30:22","2023-07-19 11:40:11","2023-07-05 07:30:22","133-153","","2-3","31","","Auton Robot","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WX84XM6G","bookSection","2008","Wilkie, Sonia; Stevens, Catherine; Dean, Roger","Psychoacoustic Manipulation of the Sound-Induced Illusory Flash","Computer Music Modeling and Retrieval. Sense of Sounds","978-3-540-85034-2 978-3-540-85035-9","","","http://link.springer.com/10.1007/978-3-540-85035-9_15","Psychological research on cross-modal perception has focused on the manipulation of sensory information predominantly by visual information. There is a lacuna in using auditory stimuli to manipulate other sensory information. The Sound Induced Illusory Flash is one illusory paradigm that uses the auditory system to bias other sensory information. However, more research is needed into the different conditions under which the Sound Induced Illusory Flash manifests and is enhanced or reduced. The experiment reported here investigates the effect of new auditory variables on the Sound Induced Illusory Flash. The variables to be discussed include the use of pitch intervals and harmonic relationships. The ultimate aim is to develop the illusory effect as a basis for new multi-media techniques and creative applications for the temporal manipulation and spatialisation of visual objects.","2008","2023-07-05 07:30:22","2023-07-19 23:49:37","2023-07-05 07:30:22","223-234","","","4969","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-85035-9_15","","/Users/minsik/Zotero/storage/HEQA4QTY/Wilkie et al. - 2008 - Psychoacoustic Manipulation of the Sound-Induced I.pdf","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IX6A26B5","bookSection","2016","Balvis, Luca; Boratto, Ludovico; Mulas, Fabrizio; Spano, Lucio Davide; Carta, Salvatore; Fenu, Gianni","Keep the Beat: Audio Guidance for Runner Training","Human-Centered and Error-Resilient Systems Development","978-3-319-44901-2 978-3-319-44902-9","","","http://link.springer.com/10.1007/978-3-319-44902-9_16","Understanding how to map the feedback by fitness apps into concrete actions during the exercise performance is crucial for their effectiveness, for both inexperienced and advanced users. In this paper we focus on audio feedback for running, describing a beat-rhythm representation of the target cadence for helping the user in keeping it. We designed the feedback system in order to balance two conflicting objectives: its effectiveness in helping the user in reaching the training goal and its intrusiveness with respect to concurrent activities (e.g., listening to the music). We detail how we track the user’s cadence through standard smartphone sensors, how and when we generate the audio messages. Finally, we discuss the results of a user-study, showing effectiveness with respect to the adherence to the exercise goal and the overall usability.","2016","2023-07-05 07:30:22","2023-07-20 05:54:14","2023-07-05 07:30:22","246-257","","","9856","","","Keep the Beat","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-44902-9_16","","/Users/minsik/Zotero/storage/6MEWSQFU/Balvis et al. - 2016 - Keep the Beat Audio Guidance for Runner Training.pdf","","","","Bogdan, Cristian; Gulliksen, Jan; Sauer, Stefan; Forbrig, Peter; Winckler, Marco; Johnson, Chris; Palanque, Philippe; Bernhaupt, Regina; Kis, Filip","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MQMJD8R3","journalArticle","2018","Ishizawa, Fumiko; Sakamoto, Mizuki; Nakajima, Tatsuo","Extracting intermediate-level design knowledge for speculating digital–physical hybrid alternate reality experiences","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-017-5595-8","http://link.springer.com/10.1007/s11042-017-5595-8","This paper reports a process to derive intermediate-level knowledge as a service design and analysis framework for designing digital services to offer alternate reality experiences, and analyzes the possible opportunities and pitfalls of the framework. The user experience felt by refining the meaning of real space through virtuality is defined as alternate reality experiences. Alternate reality experiences are typically achieved by modifying our eyesight or replacing our five senses to others, and they make our world interactive by implicitly influencing human attitudes and behaviors. First, the paper extracts observations for deriving the intermediate-level knowledge through the discussions raised in exploration workshops. In the workshops, the three digital services that utilize diverse strategies to offer alternate reality experiences are chosen. The workshops’ main focus is to examine how a person could have a sense of values in alternate reality experiences via the three digital services. Second, the paper shows how to derive the proposed service design and analysis framework from the extracted observations through expert analysis, then an overview of the framework is explained. Finally, the paper presents feasibility analysis of the proposed framework through a new digital service named Mindful Reminder as a case study for refining the service through focus group discussions. The approach described in the paper is to report a concrete process through which extracted observations can be converted into intermediate-level knowledge that can be used to design alternate reality experiences. Traditionally, the process for generating intermediate-level knowledge has not been well-documented; however, documenting the process is very important in theorizing the design of alternate reality experiences and helps effectively develop a variety of emerging advanced digital services that will offer alternate reality experiences in the future.","2018-08","2023-07-05 07:30:22","2023-07-21 04:32:47","2023-07-05 07:30:22","21329-21370","","16","77","","Multimed Tools Appl","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IKB7XBCX","bookSection","2006","Sánchez, Jaime; Baloian, Nelson","Issues in Implementing Awareness in Collaborative Software for Blind People","Computers Helping People with Special Needs","978-3-540-36020-9 978-3-540-36021-6","","","http://link.springer.com/10.1007/11788713_190","There is no doubt among the members of the CSCW community that awareness is a key issue in the design of successful collaborative software. In many systems awareness mechanisms have been implemented through displaying graphic information over the system’s interface. However, this strategy does not apply when the end users of the system are blind people. In this work we report the problems we encountered when implementing a collaborative game for supporting the learning of music and sound by blind people when trying to develop effective awareness mechanisms. The preliminary results have helped us to be ""aware"" about some characteristics awareness mechanisms should have for blind people which are not as prominent and problematic for sighted people.","2006","2023-07-05 07:30:22","2023-07-19 23:52:58","2023-07-05 07:30:22","1318-1325","","","4061","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11788713_190","","","","","","Miesenberger, Klaus; Klaus, Joachim; Zagler, Wolfgang L.; Karshmer, Arthur I.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4JFNPTYX","journalArticle","2014","Toharia, Pablo; Morales, Juan; De Juan, Octavio; Fernaud, Isabel; Rodríguez, Angel; DeFelipe, Javier","Musical Representation of Dendritic Spine Distribution: A New Exploratory Tool","Neuroinformatics","","1539-2791, 1559-0089","10.1007/s12021-013-9195-0","http://link.springer.com/10.1007/s12021-013-9195-0","Dendritic spines are small protrusions along the dendrites of many types of neurons in the central nervous system and represent the major target of excitatory synapses. For this reason, numerous anatomical, physiological and computational studies have focused on these structures. In the cerebral cortex the most abundant and characteristic neuronal type are pyramidal cells (about 85 % of all neurons) and their dendritic spines are the main postsynaptic target of excitatory glutamatergic synapses. Thus, our understanding of the synaptic organization of the cerebral cortex largely depends on the knowledge regarding synaptic inputs to dendritic spines of pyramidal cells. Much of the structural data on dendritic spines produced by modern neuroscience involves the quantitative analysis of image stacks from light and electron microscopy, using standard statistical and mathematical tools and software developed to this end. Here, we present a new method with musical feedback for exploring dendritic spine morphology and distribution patterns in pyramidal neurons. We demonstrate that audio analysis of spiny dendrites with apparently similar morphology may “sound” quite different, revealing anatomical substrates that are not apparent from simple visual inspection. These morphological/music translations may serve as a guide for further mathematical analysis of the design of the pyramidal neurons and of spiny dendrites in general.","2014-01-07","2023-07-05 07:32:03","2023-07-21 04:37:23","2023-07-05 07:32:03","","","","","","Neuroinform","Musical Representation of Dendritic Spine Distribution","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/QTSJ8C47/Toharia et al. - 2014 - Musical Representation of Dendritic Spine Distribu.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9D6BU49R","journalArticle","2017","Eckstein, Justin","Radiolab’s Sound Strategic Maneuvers","Argumentation","","0920-427X, 1572-8374","10.1007/s10503-016-9416-4","http://link.springer.com/10.1007/s10503-016-9416-4","How might argumentation scholars approach sound? Using the analytics afforded by strategic maneuvering, this essay identifies three unique features of sonic presentational devices: they are immersive, immediate and embodied. Although these features offer arguers presentational resource, they also pose new problems to the reasonable resolution of disagreement: immersion hazards overlap (mask), immediacy risks rate of delivery beyond reflection (velocity), and materiality can coerce listeners (force). To theorize strategic use of sound, I reconstruct and analyze a popular Radiolab segment “The Unconscious Toscanini of the Brain.” I find Radiolab uses three different sonic figures: (1) synchronicity, or the translation of data into sound to foreground temporal relations; (2) musical stings, an auditory invocation of embodied memory and (3) the wave, a sonic strategy to arouse and narrow attention. I conclude that Radiolab’s use of sound is reasonable because it extends the critical discussion.","2017-12","2023-07-05 07:32:03","2023-07-19 11:32:30","2023-07-05 07:32:03","663-680","","4","31","","Argumentation","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FPKG5566","bookSection","2010","Hug, Daniel","Investigating Narrative and Performative Sound Design Strategies for Interactive Commodities","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_2","Computing technologies turn everyday artefacts into narrative, procedural objects. This observation suggests that the narrative sound design strategies used in films and many video games could also be applied for the design of interactive commodities. However, it is unknown whether these strategies from immersive media can be applied in physical artefacts of everyday use. In this paper we describe methodological considerations and outline a structure of a revisable, design oriented, participatory research process, which allows to explore narrative sound designs and their possible application in interactive commodities in a systematic yet explorative way. The process, which focused on interpretational aspects, has been applied in two workshops and their results are reported and discussed. The experience of the prototyping and evaluation method, which made use of theatrical strategies, raised important questions about the role of performativity in the emergence of meaning and the possible limitations of a strictly hermeneutic aesthetics, when dealing with sonically enhanced interactive commodities.","2010","2023-07-05 07:32:03","2023-07-19 11:37:48","2023-07-05 07:32:03","12-40","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_2","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q2AZ3CAK","bookSection","2023","Rodrigues, Telma; Maçãs, Catarina; Rodrigues, Ana","Visual Representation of the Internet Consumption in the European Union","Artificial Intelligence in Music, Sound, Art and Design","978-3-031-29955-1 978-3-031-29956-8","","","https://link.springer.com/10.1007/978-3-031-29956-8_16","The impact of internet usage on the environment is a contradictory topic. While it can help reduce carbon emissions, with smart grids or the automation of services and resources, it can also increase e-waste that end up affecting the environment. To draw attention to the impact of energy consumption on the environment, we proposed and developed a computational artifact that unites the areas of Data Aesthetics and Interaction Design. The artifact, displayed in an interactive installation, was divided into three panels: (i) the left panel, which represents the countries—from the European Union (EU)—with the lowest energy consumption impact on the environment; (ii) the central panel, which use swarming boids to represent the internet usage at the installation site and its impact; and (iii) the right panel, which represents the EU countries with the highest energy impact on the environment. The arrangement of the three panels in a single interactive installation aims to establish a visual connection between the energy consumption in the EU and the energy consumption in the installation’s site and to promote awareness of its impact on the environment.","2023","2023-07-05 07:32:03","2023-07-19 11:34:20","2023-07-05 07:32:03","244-259","","","13988","","","","","","","","Springer Nature Switzerland","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-031-29956-8_16","","","","","","Johnson, Colin; Rodríguez-Fernández, Nereida; Rebelo, Sérgio M.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CFHB45EM","bookSection","2006","Murphy, Emma; Pirhonen, Antti; McAllister, Graham; Yu, Wai","A Semiotic Approach to the Design of Non-speech Sounds","Haptic and Audio Interaction Design","978-3-540-37595-1 978-3-540-37596-8","","","http://link.springer.com/10.1007/11821731_12","In the field of auditory display there is currently a lack of theoretical support for the design of non-speech sounds as elements of a user interface. Sound design methods are often based on ad hoc choices or the personal preferences of the designer. A method is proposed in this paper based on a semiotic approach to the design of non-speech sounds. In this approach, the design process is conceptualised by referring to structural semiotics, acknowledging the unique qualities of non-speech sounds, as a mode of conveying information. This method is based on a rich use scenario presented to a design panel. A case study where the design method has been applied is presented and evaluated. Finally recommendations for a practical design method are presented supported by this empirical investigation.","2006","2023-07-05 07:32:03","2023-07-20 00:15:41","2023-07-05 07:32:03","121-132","","","4129","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11821731_12","","","","","","McGookin, David; Brewster, Stephen","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7GQ7PISZ","bookSection","1997","Duce, David A.","Theory and practice in interactionally rich distributed systems","SOFSEM'97: Theory and Practice of Informatics","978-3-540-63774-5 978-3-540-69645-2","","","http://link.springer.com/10.1007/3-540-63774-5_105","","1997","2023-07-05 07:32:03","2023-07-05 07:32:03","2023-07-05 07:32:03","163-182","","","1338","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-63774-5_105","","","","","","Plášil, František; Jeffery, Keith G.","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZKMFKNVG","journalArticle","2012","Drioli, Carlo; Rocchesso, Davide","Acoustic rendering of particle-based simulation of liquids in motion","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0063-7","http://link.springer.com/10.1007/s12193-011-0063-7","In interaction and interface design, the representation of continuous processes often uses liquid metaphors, such as dripping or streaming. When an auditory display of such processes is required, an approach to sound-synthesis based on the physics of liquids in motion would be the most convincing, especially when real-time interaction is into play. In order to bridge the complexity of fluid-dynamic simulations with the needs of interactive sonification, we propose a multi-rate sound synthesis of liquid phenomena. Low-rate smoothed-particle hydrodynamics is used to model liquids in motion and to trigger sound-emitting events. Such events, such as solid-liquid collision, or bubble formation, are synthesized at audio rate. The proposed method is applied to the two important cases of liquid falling into a vessel, and of solid object falling into a liquid. Some example applications in interaction design are presented.","2012-05","2023-07-05 07:32:03","2023-07-20 06:53:33","2023-07-05 07:32:03","187-195","","3-4","5","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q3EHH9B4","journalArticle","2012","Cristofol, Jean","Elephant fish and GPS","AI & SOCIETY","","0951-5666, 1435-5655","10.1007/s00146-011-0336-4","http://link.springer.com/10.1007/s00146-011-0336-4","Elephant fish and GPS is an attempt to reflect on data flux, and artistic practice considered as a way to implement an experience specific to a flux. Sonification is particularly well suited to this type of implementation. As such, it leads us to question the nature of this type of experience, the position of the person who is faced with the artistic object, and the position and function of the artist. It allows us to query the status of devices produced by such an artistic practice and the status of what is, traditionally, called a “work of art”. The approach is derived from neither the study of specific artistic projects that could be considered as examples from which a general model could be extrapolated, nor, conversely, that of the enunciation of models illustrated by concrete references. The intention is not to categorize practice or behavior. It is rather an approach that seeks to shift concepts and ways of thinking through progressive analogies aimed at critically questioning the notion of experience in a transition from “object” based logic to “flux” or “field”, based logic, coming from an aesthetic perspective.","2012-05","2023-07-05 07:32:03","2023-07-19 11:18:58","2023-07-05 07:32:03","183-187","","2","27","","AI & Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3QGHTI46","bookSection","2011","Milne, Andrew J.; Carlé, Martin; Sethares, William A.; Noll, Thomas; Holland, Simon","Scratching the Scale Labyrinth","Mathematics and Computation in Music","978-3-642-21589-6 978-3-642-21590-2","","","http://link.springer.com/10.1007/978-3-642-21590-2_14","In this paper, we introduce a new approach to computeraided microtonal improvisation by combining methods for (1) interactive scale navigation, (2) real-time manipulation of musical patterns and (3) dynamical timbre adaption in solidarity with the respective scales. On the basis of the theory of well-formed scales we offer a visualization of the underlying combinatorial ramifications in terms of a scale labyrinth. This involves the selection of generic well-formed scales on a binary tree (based on the Stern-Brocot tree) as well as the choice of specific tunings through the specification of the sizes of a period (pseudo-octave) and a generator (pseudo-fifth), whose limits are constrained by the actual position on the tree. We also introduce a method to enable transformations among the modes of a chosen scale (generalized and refined “diatonic” and “chromatic” transpositions). To actually explore the scales and modes through the shaping and transformation of rhythmically and melodically interesting tone patterns, we propose a playing technique called Fourier Scratching. It is based on the manipulation of the “spectra” (DFT) of playing gestures on a sphere. The coordinates of these gestures affect score and performance parameters such as scale degree, loudness, and timbre. Finally, we discuss a technique to dynamically match the timbre to the selected scale tuning.","2011","2023-07-05 07:32:03","2023-07-21 04:28:47","2023-07-05 07:32:03","180-195","","","6726","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-21590-2_14","","/Users/minsik/Zotero/storage/N2EGAIU2/Milne et al. - 2011 - Scratching the Scale Labyrinth.pdf","","","","Agon, Carlos; Andreatta, Moreno; Assayag, Gérard; Amiot, Emmanuel; Bresson, Jean; Mandereau, John","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6QUWWTIS","bookSection","2008","Merer, Adrien; Ystad, Sølvi; Kronland-Martinet, Richard; Aramaki, Mitsuko","Semiotics of Sounds Evoking Motions: Categorization and Acoustic Features","Computer Music Modeling and Retrieval. Sense of Sounds","978-3-540-85034-2 978-3-540-85035-9","","","http://link.springer.com/10.1007/978-3-540-85035-9_9","The current study is part of a larger project aiming at offering intuitive mappings of control parameters piloting synthesis models by semantic descriptions of sounds, i.e. simple verbal labels related to various feelings, emotions, gestures or motions. Hence, this work is directly related to the general problem of semiotics of sounds. We here put a special interest in sounds evoking different perceived motions. In this paper, the experimental design of the listening tests is described and the results obtained from behavioural data are discussed. Then a set of signal descriptors is compared to categories using feature selection methods. A special interest is given to applications for sound synthesis.","2008","2023-07-05 07:32:03","2023-07-19 23:49:16","2023-07-05 07:32:03","139-158","","","4969","","","Semiotics of Sounds Evoking Motions","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-85035-9_9","","","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UIBP4XRT","journalArticle","2022","Cobos, Maximo; Ahrens, Jens; Kowalczyk, Konrad; Politis, Archontis","An overview of machine learning and other data-based methods for spatial audio capture, processing, and reproduction","EURASIP Journal on Audio, Speech, and Music Processing","","1687-4722","10.1186/s13636-022-00242-x","https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-022-00242-x","Abstract             The domain of spatial audio comprises methods for capturing, processing, and reproducing audio content that contains spatial information. Data-based methods are those that operate directly on the spatial information carried by audio signals. This is in contrast to model-based methods, which impose spatial information from, for example, metadata like the intended position of a source onto signals that are otherwise free of spatial information. Signal processing has traditionally been at the core of spatial audio systems, and it continues to play a very important role. The irruption of deep learning in many closely related fields has put the focus on the potential of learning-based approaches for the development of data-based spatial audio applications. This article reviews the most important application domains of data-based spatial audio including well-established methods that employ conventional signal processing while paying special attention to the most recent achievements that make use of machine learning. Our review is organized based on the topology of the spatial audio pipeline that consist in capture, processing/manipulation, and reproduction. The literature on the three stages of the pipeline is discussed, as well as on the spatial audio representations that are used to transmit the content between them, highlighting the key references and elaborating on the underlying concepts. We reflect on the literature based on a juxtaposition of the prerequisites that made machine learning successful in domains other than spatial audio with those that are found in the domain of spatial audio as of today. Based on this, we identify routes that may facilitate future advancement.","2022-05-16","2023-07-05 07:32:03","2023-07-05 07:32:03","2023-07-05 07:32:03","10","","1","2022","","J AUDIO SPEECH MUSIC PROC.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/RN7V7XKU/Cobos et al. - 2022 - An overview of machine learning and other data-bas.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9AICZU54","bookSection","2012","Chan, Shih-Han; Natkin, Stéphane; Tiger, Guillaume; Topol, Alexandre","Extensible Sound Description in COLLADA: A Unique File for a Rich Sound Design","Advances in Computer Entertainment","978-3-642-34291-2 978-3-642-34292-9","","","http://link.springer.com/10.1007/978-3-642-34292-9_11","Most standard scene description languages include a sound description and factorize common elements needed by the description of visual and auditory information. Both aspects are described with the same coordinate system for example. However, as soon as a dynamic description or external data are required, this benefit is lost and all the glue must be done by a programming solution that does not fit designers or authors usual skills. In this paper we address this problem and propose a solution to give back to designers the bigger role even when the scene is dynamic or based on procedural synthesizers. This solution is based on the COLLADA file format in which we have added sound support, scripting capabilities and external extensions. The use of this augmented COLLADA language is illustrated through the creation of a dynamic urban soundscape.","2012","2023-07-05 07:32:03","2023-07-19 11:10:43","2023-07-05 07:32:03","151-166","","","7624","","","Extensible Sound Description in COLLADA","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-34292-9_11","","","","","","Nijholt, Anton; Romão, Teresa; Reidsma, Dennis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TIYFDF3G","bookSection","2005","Nicol, Craig; Brewster, Stephen; Gray, Philip","A System for Manipulating Audio Interfaces Using Timbre Spaces","Computer-Aided Design of User Interfaces IV","978-1-4020-3145-8","","","http://link.springer.com/10.1007/1-4020-3304-4_29","The creation of audio interfaces is currently hampered by the difficulty of designing sounds for them. This paper presents a novel system for generating and manipulating non-speech sounds. The system is designed to generate Auditory Icons and Earcons through a common interface. It has been developed to make the design of audio interfaces easier. Using a Timbre Space representation of the sound, it generates output via an FM synthesiser. The Timbre Space has been compiled in both Fourier and Constant Q Transform versions using Principal Components Analysis (PCA). The design of the system and initial evaluations of these two versions are discussed, showing that the Fourier analysis appears to be better, contrary to initial expectations.","2005","2023-07-05 07:32:03","2023-07-19 23:50:47","2023-07-05 07:32:03","361-374","","","","","","","","","","","Springer-Verlag","Berlin/Heidelberg","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/1-4020-3304-4_29","","","","","","Jacob, Robert J.K.; Limbourg, Quentin; Vanderdonckt, Jean","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"75J4Q6BR","bookSection","2017","Ucar, Ezgi","Eclipse: A Wearable Instrument for Performance Based Storytelling","Bridging People and Sound","978-3-319-67737-8 978-3-319-67738-5","","","http://link.springer.com/10.1007/978-3-319-67738-5_7","","2017","2023-07-05 07:32:03","2023-07-05 07:32:03","2023-07-05 07:32:03","125-133","","","10525","","","Eclipse","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-67738-5_7","","","","","","Aramaki, Mitsuko; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"39DQR6VX","bookSection","2016","Solèr, Matthias; Bazin, Jean-Charles; Wang, Oliver; Krause, Andreas; Sorkine-Hornung, Alexander","Suggesting Sounds for Images from Video Collections","Computer Vision – ECCV 2016 Workshops","978-3-319-48880-6 978-3-319-48881-3","","","http://link.springer.com/10.1007/978-3-319-48881-3_59","Given a still image, humans can easily think of a sound associated with this image. For instance, people might associate the picture of a car with the sound of a car engine. In this paper we aim to retrieve sounds corresponding to a query image. To solve this challenging task, our approach exploits the correlation between the audio and visual modalities in video collections. A major difficulty is the high amount of uncorrelated audio in the videos, i.e., audio that does not correspond to the main image content, such as voice-over, background music, added sound effects, or sounds originating off-screen. We present an unsupervised, clustering-based solution that is able to automatically separate correlated sounds from uncorrelated ones. The core algorithm is based on a joint audio-visual feature space, in which we perform iterated mutual kNN clustering in order to effectively filter out uncorrelated sounds. To this end we also introduce a new dataset of correlated audio-visual data, on which we evaluate our approach and compare it to alternative solutions. Experiments show that our approach can successfully deal with a high amount of uncorrelated audio.","2016","2023-07-05 07:32:03","2023-07-19 23:50:23","2023-07-05 07:32:03","900-917","","","9914","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-48881-3_59","","","","","","Hua, Gang; Jégou, Hervé","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PK5JEEIG","bookSection","2019","Kalonaris, Stefano","Evolutionary Games for Audiovisual Works: Exploring the Demographic Prisoner’s Dilemma","Computational Intelligence in Music, Sound, Art and Design","978-3-030-16666-3 978-3-030-16667-0","","","https://link.springer.com/10.1007/978-3-030-16667-0_7","This paper presents a minimalist audiovisual display of an evolutionary game known as the Demographic Prisoner’s Dilemma, in which cooperation emerges as an evolutionary stable behaviour. Abiding by a dialogical approach foregrounding the dynamical negotiation of the author’s aesthetic aspirational levels, the cross-space mapping between the formal model and the audiovisual work is explored, and the system undergoes several variations and modifications. Questions regarding computational measures of beauty are raised and discussed.","2019","2023-07-05 07:32:03","2023-07-19 23:47:25","2023-07-05 07:32:03","98-109","","","11453","","","Evolutionary Games for Audiovisual Works","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-16667-0_7","","","","","","Ekárt, Anikó; Liapis, Antonios; Castro Pena, María Luz","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MVL9VS6J","bookSection","2011","Löchtefeld, Markus; Gehring, Sven; Jung, Ralf; Krüger, Antonio","Using Mobile Projection to Support Guitar Learning","Smart Graphics","978-3-642-22570-3 978-3-642-22571-0","","","http://link.springer.com/10.1007/978-3-642-22571-0_9","The guitar is one of the most widespread instruments amongst autodidacts, but even though a huge amount of learning material exists, it is still hard to learn especially without a guitar teacher. In this paper we propose an Augmented Reality concept that assists guitar students mastering their instrument using a mobile projector. With the projector mounted onto the headstock of the guitar, it is possible to project instructions directly onto the strings of the guitar. With that the user is easily able to realize where the fingers have to be placed on the fretboard (fingering) to play a certain chord or a tone sequence correctly.","2011","2023-07-05 07:32:03","2023-07-21 04:59:39","2023-07-05 07:32:03","103-114","","","6815","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-22571-0_9","","","","","","Dickmann, Lutz; Volkmann, Gerald; Malaka, Rainer; Boll, Susanne; Krüger, Antonio; Olivier, Patrick","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"62GRN6KI","bookSection","2012","Endo, Hideyuki; Yoshioka, Hideki","Enhancing Tactile Imagination through Sound and Light","Advances in Computer Entertainment","978-3-642-34291-2 978-3-642-34292-9","","","http://link.springer.com/10.1007/978-3-642-34292-9_40","Drive Mind is a unique electro-acoustic system, which offers an audience a new sonic experience produced by the refraction of light. The main feature of this system is to visualize abstract figures of sound using a ray of LED light and to manipulate the system using acrylic objects. By this manipulation, the system creates a refraction of light and attendant positional data. This positional data is used to produce sound. The complexity of refraction of the light and the frame rate of the camera cause subtle fluctuations and produce distinctive sounds. The object is to enhance an audience’s imagination by enabling them to identify with the performer’s action visually, and help understanding of complex digital expression, using not only physical material but also physical phenomena when operating the system. This system helps the audience to become familiar with complex digital expression and experience the new possibilities of sound art.","2012","2023-07-05 07:32:03","2023-07-19 11:10:54","2023-07-05 07:32:03","481-484","","","7624","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-34292-9_40","","","","","","Nijholt, Anton; Romão, Teresa; Reidsma, Dennis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DTLMU87D","bookSection","2009","Aramaki, Mitsuko; Brancheriau, Loïc; Kronland-Martinet, Richard; Ystad, Sølvi","Perception of Impacted Materials: Sound Retrieval and Synthesis Control Perspectives","Computer Music Modeling and Retrieval. Genesis of Meaning in Sound and Music","978-3-642-02517-4 978-3-642-02518-1","","","http://link.springer.com/10.1007/978-3-642-02518-1_9","In this study, we aimed at determining statistical models that allowed for the classification of impact sounds according to the perceived material (Wood, Metal and Glass). For that purpose, everyday life sounds were recorded, analyzed and resynthesized to insure the generation of realistic sounds. Listening tests were conducted to define sets of typical sounds of each material category by using a statistical approach. For the construction of statistical models, acoustic descriptors known to be relevant for timbre perception and for material identification were investigated. These models were calibrated and validated using a binary logistic regression method. A discussion about the applications of these results in the context of sound synthesis concludes the article.","2009","2023-07-05 07:32:03","2023-07-19 23:48:40","2023-07-05 07:32:03","134-146","","","5493","","","Perception of Impacted Materials","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-02518-1_9","","","","","","Ystad, Sølvi; Kronland-Martinet, Richard; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VD97JLIQ","journalArticle","2005","Srinivasan, Uma; Pfeiffer, Silvia; Nepal, Surya; Lee, Michael; Gu, Lifang; Barrass, Stephen","A Survey of MPEG-1 Audio, Video and Semantic Analysis Techniques","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-005-2716-6","http://link.springer.com/10.1007/s11042-005-2716-6","Digital audio & video data have become an integral part of multimedia information systems. To reduce storage and bandwidth requirements, they are commonly stored in a compressed format, such as MPEG-1. Increasing amounts of MPEG encoded audio and video documents are available online and in proprietary collections. In order to effectively utilise them, we need tools and techniques to automatically analyse, segment, and classify MPEG video content. Several techniques have been developed both in the audio and visual domain to analyse videos. This paper presents a survey of audio and visual analysis techniques on MPEG-1 encoded media that are useful in supporting a variety of video applications. Although audio and visual feature analyses have been carried out extensively, they become useful to applications only when they convey a semantic meaning of the video content. Therefore, we also present a survey of works that provide semantic analysis on MPEG-1 encoded videos.","2005-09","2023-07-05 07:34:57","2023-07-21 04:33:25","2023-07-05 07:34:57","105-141","","1","27","","Multimed Tools Appl","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KKFQDJ5C","bookSection","2010","Bevilacqua, Frédéric; Zamborlin, Bruno; Sypniewski, Anthony; Schnell, Norbert; Guédy, Fabrice; Rasamimanana, Nicolas","Continuous Realtime Gesture Following and Recognition","Gesture in Embodied Communication and Human-Computer Interaction","978-3-642-12552-2 978-3-642-12553-9","","","http://link.springer.com/10.1007/978-3-642-12553-9_7","We present a HMM based system for real-time gesture analysis. The system outputs continuously parameters relative to the gesture time progression and its likelihood. These parameters are computed by comparing the performed gesture with stored reference gestures. The method relies on a detailed modeling of multidimensional temporal curves. Compared to standard HMM systems, the learning procedure is simplified using prior knowledge allowing the system to use a single example for each class. Several applications have been developed using this system in the context of music education, music and dance performances and interactive installation. Typically, the estimation of the time progression allows for the synchronization of physical gestures to sound files by time stretching/compressing audio buffers or videos.","2010","2023-07-05 07:34:57","2023-07-20 00:08:19","2023-07-05 07:34:57","73-84","","","5934","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12553-9_7","","","","","","Kopp, Stefan; Wachsmuth, Ipke","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KG2I7XZ8","bookSection","2012","Necciari, Thibaud; Balazs, Peter; Kronland-Martinet, Richard; Ystad, Sølvi; Laback, Bernhard; Savel, Sophie; Meunier, Sabine","Auditory Time-Frequency Masking: Psychoacoustical Data and Application to Audio Representations","Speech, Sound and Music Processing: Embracing Research in India","978-3-642-31979-2 978-3-642-31980-8","","","http://link.springer.com/10.1007/978-3-642-31980-8_12","In this paper, the results of psychoacoustical experiments on auditory time-frequency (TF) masking using stimuli (masker and target) with maximal concentration in the TF plane are presented. The target was shifted either along the time axis, the frequency axis, or both relative to the masker. The results show that a simple superposition of spectral and temporal masking functions does not provide an accurate representation of the measured TF masking function. This confirms the inaccuracy of simple models of TF masking currently implemented in some perceptual audio codecs. In the context of audio signal processing, the present results constitute a crucial basis for the prediction of auditory masking in the TF representations of sounds. An algorithm that removes the inaudible components in the wavelet transform of a sound while causing no audible difference to the original sound after re-synthesis is proposed. Preliminary results are promising, although further development is required.","2012","2023-07-05 07:34:57","2023-07-21 05:02:48","2023-07-05 07:34:57","146-171","","","7172","","","Auditory Time-Frequency Masking","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-31980-8_12","","/Users/minsik/Zotero/storage/VEZRZMSL/Necciari et al. - 2012 - Auditory Time-Frequency Masking Psychoacoustical .pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer; Mohanty, Sanghamitra","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6EPSVILY","journalArticle","2023","El Raheb, Katerina; Buccoli, Michele; Zanoni, Massimiliano; Katifori, Akrivi; Kasomoulis, Aristotelis; Sarti, Augusto; Ioannidis, Yannis","Towards a general framework for the annotation of dance motion sequences: A framework and toolkit for collecting movement descriptions as ground-truth datasets","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-022-12602-y","https://link.springer.com/10.1007/s11042-022-12602-y","In this paper, we present a conceptual framework and toolkit for movement annotation. We explain how the design of the annotation systems, based on the framework, if combined with specific strategies for the process of annotation, can enhance the collection of ground-truth datasets for training algorithms. Computational algorithms, such as machine learning, show promising results for massive and scalable automatic movement annotation. Nevertheless, the need for reliable ground-truth datasets annotated by human experts, to train the machine learning algorithms and for bridging the gap between machine measurable and human perceived expressive aspects remains an open issue. This need constitutes a challenging task, due to the complexity of human movement and diversity of possible descriptors, as well as the high subjectivity that accompanies movement characterisation by both experts and non-expert users. We contribute to addressing this problem, by proposing a conceptual framework for dance movement manual annotation which we evaluate through the development and deployment of the toolkit. Finally, we discuss how the different design choices affect the process and the reliability of collecting data sets regarding qualitative aspects of movement.","2023-01","2023-07-05 07:34:57","2023-07-21 04:32:02","2023-07-05 07:34:57","3363-3395","","3","82","","Multimed Tools Appl","Towards a general framework for the annotation of dance motion sequences","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4GI89L8J","journalArticle","2006","Mion, Luca; D’Incà, Gianluca","Analysis of expression in simple musical gestures to enhance audio in interfaces","Virtual Reality","","1359-4338, 1434-9957","10.1007/s10055-006-0029-3","http://link.springer.com/10.1007/s10055-006-0029-3","","2006-05","2023-07-05 07:34:57","2023-07-05 07:34:57","2023-07-05 07:34:57","62-70","","1","10","","Virtual Reality","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4HLT36JP","bookSection","2009","Grosshauser, Tobias; Hermann, Thomas","Augmented Haptics – An Interactive Feedback System for Musicians","Haptic and Audio Interaction Design","978-3-642-04075-7 978-3-642-04076-4","","","http://link.springer.com/10.1007/978-3-642-04076-4_11","This paper presents integrated vibrotactiles, a novel interface for movement and posture tuition that provides real-time feedback in a tactile form by means of interactive haptic feedback, thereby conveying information neither acoustically nor visually and it is a promising feedback means for movements in 3D-space. In this paper we demonstrate haptic augmentation for applications for musicians, since it (a) doesn’t affect the visual sense, occupied by reading music and communication, (b) doesn’t disturb in bang sensitive situations such as concerts, (c) allows to relate feedback information in the same tactile medium as the output of the musical instrument, so that an important feedback channel for musical instrument playing is extended and trained supportive. Even more, instructions from the teacher and the computer can be transmitted directly and unobtrusively in this channel. This paper presents a prototype system together with demonstrations of applications that support violinists during musical instrument learning.","2009","2023-07-05 07:34:57","2023-07-20 00:14:52","2023-07-05 07:34:57","100-108","","","5763","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-04076-4_11","","","","","","Altinsoy, M. Ercan; Jekosch, Ute; Brewster, Stephen","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DYJMQXNY","journalArticle","2021","Moumdjian, Lousin; Vervust, Thomas; Six, Joren; Schepers, Ivan; Lesaffre, Micheline; Feys, Peter; Leman, Marc","The Augmented Movement Platform For Embodied Learning (AMPEL): development and reliability","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-020-00354-8","http://link.springer.com/10.1007/s12193-020-00354-8","Background. Balance and gait impairments are highly prevalent in the neurological population. Although current rehabilitation strategies focus on motor learning principles, it is of interest to expand into embodied sensori-motor learning; that is learning through a continuous interaction between the cognition and the motor system, within an enriched sensory environment. Current developments in engineering allow for the development of enriched sensory environments through interactive feedback. Methodology. The Augmented Movement Platform for Embodied Learning (AMPEL) was developed, both in terms of hardware and software by an inter-disciplinary circular participatory design strategy. The developed device was then tested for in-between session reliability for the outcome measures inter-step interval and total onset time was investigated. Ten healthy participants walked in four experimental paths on the device in two different sessions, and between session correlations were calculated. Results. AMPEL was developed both in terms of software and hardware, with three Plug-In systems (auditory, visual, auditory + visual). The auditory Plug-In allows for flexible application of augmented feedback. The in-between session reliability of the outcomes measured by the system were between high and very high on all 4 walked paths, tested on ten healthy participants [mean age 41.8 ± 18.5; BMI 24.8 ± 6.1]. Conclusion. AMPEL shows full functionality, and has shown between session reliability for the measures of inter-step-intervals and total-onset-time in healthy controls during walking on different paths.","2021-03","2023-07-05 07:34:57","2023-07-20 07:03:10","2023-07-05 07:34:57","77-83","","1","15","","J Multimodal User Interfaces","The Augmented Movement Platform For Embodied Learning (AMPEL)","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/ZPHS49HK/Moumdjian et al. - 2021 - The Augmented Movement Platform For Embodied Learn.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GK7DW9RS","journalArticle","2018","Rachman, Laura; Liuni, Marco; Arias, Pablo; Lind, Andreas; Johansson, Petter; Hall, Lars; Richardson, Daniel; Watanabe, Katsumi; Dubal, Stéphanie; Aucouturier, Jean-Julien","DAVID: An open-source platform for real-time transformation of infra-segmental emotional cues in running speech","Behavior Research Methods","","1554-3528","10.3758/s13428-017-0873-y","http://link.springer.com/10.3758/s13428-017-0873-y","We present an open-source software platform that transforms emotional cues expressed by speech signals using audio effects like pitch shifting, inflection, vibrato, and filtering. The emotional transformations can be applied to any audio file, but can also run in real time, using live input from a microphone, with less than 20-ms latency. We anticipate that this tool will be useful for the study of emotions in psychology and neuroscience, because it enables a high level of control over the acoustical and emotional content of experimental stimuli in a variety of laboratory situations, including real-time social situations. We present here results of a series of validation experiments aiming to position the tool against several methodological requirements: that transformed emotions be recognized at above-chance levels, valid in several languages (French, English, Swedish, and Japanese) and with a naturalness comparable to natural speech.","2018-02","2023-07-05 07:34:57","2023-07-19 23:34:25","2023-07-05 07:34:57","323-343","","1","50","","Behav Res","DAVID","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/U742KSQ3/Rachman et al. - 2018 - DAVID An open-source platform for real-time trans.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2TDKMK55","journalArticle","2023","De Melo, Hyrandir Cabral","Plants detect and respond to sounds","Planta","","0032-0935, 1432-2048","10.1007/s00425-023-04088-1","https://link.springer.com/10.1007/s00425-023-04088-1","Plants are responsive to environmental stimuli such as sound. However, little is known about their sensory apparatus, mechanisms, and signaling pathways triggered by these stimuli. Thus, it is important to understand the effect of sounds on plants and their technological potential. This review addresses the effects of sounds on plants, the sensory elements inherent to sound detection by the cell, as well as the triggering of signaling pathways that culminate in plant responses. The importance of sound standardization for the study of phytoacoustics is demonstrated. Studies on the sounds emitted or reflected by plants, acoustic stress in plants, and recognition of some sound patterns by plants are also explored.","2023-03","2023-07-05 07:34:57","2023-07-21 04:54:56","2023-07-05 07:34:57","55","","3","257","","Planta","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RZPC4G4E","bookSection","2020","Smrčina, Michal","Approaching Urban Experience Through Rhythmanalysis","Design, User Experience, and Usability. Case Studies in Public and Personal Interactive Systems","978-3-030-49756-9 978-3-030-49757-6","","","http://link.springer.com/10.1007/978-3-030-49757-6_10","The paper revolves around the urban environment and its particular way of analysis. It aims to provide for a proper understanding of space and its meaningful design. It represents a theoretical basis of a more extensive research design that focuses on the dynamics of transit places, namely railway stations. There are three key parts. In the first one, I explain and contextualize rhythmanalysis. In the second one, I position it into the urban environment and discuss issues of spaces and places. The third part envisions the future case studies as it concerns the particular case of a railway station and shows possible approaches of the method from this distinct perspective. These research phases are illustrated by several hands-on examples.","2020","2023-07-05 07:34:57","2023-07-19 23:55:19","2023-07-05 07:34:57","142-161","","","12202","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-49757-6_10","","","","","","Marcus, Aaron; Rosenzweig, Elizabeth","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PEA7FLJ8","journalArticle","2012","McMullen, Shannon C.; Winkler, Fabian","Images of nature: collaboration at the intersection of nature, art and technology","The Environmentalist","","0251-1088, 1573-2991","10.1007/s10669-011-9355-4","http://link.springer.com/10.1007/s10669-011-9355-4","This paper argues that interdisciplinary collaboration between the sciences, the arts/humanities and engineering will provide innovative responses to important changes in our natural environment. Specifically, it will introduce “Images of Nature”, a case study on creative collaboration and a multi-level research project at Purdue University, headed by Prof. Shannon McMullen and Prof. Fabian Winkler. By bringing together scientists, engineers and artists, “Images of Nature” aims to convey the significance of new understandings of nature in images and tangible artifacts (e.g., data visualization, functional devices, generative and kinetic installations) for the public. It is our hope that this project will be the starting point for a flexible network connecting science, engineering and the arts on Purdue’s West Lafayette campus to enrich STEM education and provide a local model for STE(A)M (STEM disciplines plus Art), which emphasizes creativity and innovation; critical thinking and problem solving; flexibility and adaptability; cross-disciplinary communication and collaboration in the context of our changing natural environment.","2012-09","2023-07-05 07:34:57","2023-07-21 05:07:07","2023-07-05 07:34:57","311-317","","3","32","","Environmentalist","Images of nature","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KWMTBM2X","bookSection","2022","Kasibhatla, Raghav; Mahmud, Saifuddin; Sourave, Redwanul Haque; Arnett, Marcus; Kim, Jong-Hoon","Design of a Smart Puppet Theatre System for Computational Thinking Education","Intelligent Human Computer Interaction","978-3-030-98403-8 978-3-030-98404-5","","","https://link.springer.com/10.1007/978-3-030-98404-5_29","Many efforts have failed to achieve tangible models of a robotic theatre, as opposed to virtual or simulated theatres, despite many attempts to merge the progress of robotics with the growth of theatre and the performing arts. Many of the initiatives that have achieved significant progress in these domains are on a considerably larger scale, with the primary goal of entertaining rather than demonstrating the interdisciplinary nature of Robotics and Engineering. The purpose of this paper is to correctly unite the principles of Science, Technology, Engineering, Arts, and Mathematics in a small size robotic theatre that will allow for a more portable and changeable exhibition. The Tortoise and Hare play will be performed in the theatre, which is made up of both stage and puppet elements. A pan and tilt lighting system, audio integration via an external device, automated curtains with stepper motors, props, and a grid stage are among the stage’s components. A camera tracking module in the light system detects the location of a robot and communicates with the light fixtures to angle the spotlight. A transportable module that interacts wirelessly with its environment, as well as simple-moving, decorative puppet cutouts protruding from the module, make up the smart puppets. The mBlock IDE is used to edit the story in the theatre software, providing for a simple technique of programming the scene. The Smart Mini Theatre’s production of the Tortoise and Hare play intends to encourage performing arts students to experiment with robots and programming to create their own shows, in the hopes of inspiring them to pursue Robotics and Engineering as a potential career choice.","2022","2023-07-05 07:34:57","2023-07-20 06:34:59","2023-07-05 07:34:57","301-312","","","13184","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-98404-5_29","","","","","","Kim, Jong-Hoon; Singh, Madhusudan; Khan, Javed; Tiwary, Uma Shanker; Sur, Marigankar; Singh, Dhananjay","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SK65TNMB","bookSection","2010","Bakker, Saskia; Van Den Hoven, Elise; Eggen, Berry","Exploring Interactive Systems Using Peripheral Sounds","Haptic and Audio Interaction Design","978-3-642-15840-7 978-3-642-15841-4","","","http://link.springer.com/10.1007/978-3-642-15841-4_7","","2010","2023-07-05 07:34:57","2023-07-05 07:34:57","2023-07-05 07:34:57","55-64","","","6306","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-15841-4_7","","/Users/minsik/Zotero/storage/JBZLV4YA/Bakker et al. - 2010 - Exploring Interactive Systems Using Peripheral Sou.pdf","","","","Nordahl, Rolf; Serafin, Stefania; Fontana, Federico; Brewster, Stephen","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X2QUFNUU","bookSection","2007","Foursa, Maxim; Wesche, Gerold","Movement-Based Interaction and Event Management in Virtual Environments with Optical Tracking Systems","Human-Computer Interaction. HCI Intelligent Multimodal Interaction Environments","978-3-540-73108-5 978-3-540-73110-8","","","http://link.springer.com/10.1007/978-3-540-73110-8_67","In this paper we present our experience in using optical tracking systems in Virtual Environment applications. First we briefly describe the tracking systems we used, and then we describe the application scenarios and present how we adapted the scenarios for the tracking systems. One of the tracking systems is markerless, that means that a user doesn’t have to wear any specific devices to be tracked and can interact with an application with free hand movements. With our application we compare the performance of different tracking systems and demonstrate that it is possible to perform complex actions in an intuitive way with just small special knowledge of the system and without any specific devices. This is a step forward to a more natural human-computer interface.","2007","2023-07-05 07:34:57","2023-07-20 06:31:25","2023-07-05 07:34:57","615-624","","","4552","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73110-8_67","","","","","","Jacko, Julie A.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P562EPXX","bookSection","2007","Mahmud, Murni; Sporka, Adam J.; Kurniawan, Sri H.; Slavík, Pavel","A Comparative Longitudinal Study of Non-verbal Mouse Pointer","Human-Computer Interaction – INTERACT 2007","978-3-540-74799-4 978-3-540-74800-7","","","http://link.springer.com/10.1007/978-3-540-74800-7_44","A longitudinal study of two non-speech continuous cursor control systems is presented in this paper: Whistling User Interface (U3I) and Vocal Joystick (VJ). This study combines the quantitative and qualitative methods to get a better understanding of novice users’ experience over time. Three hypotheses were tested in this study. The quantitative data show that U3I performed better in error rate and in simulating a mouse click; VJ was better on other measures. The qualitative data indicate that the participants’ opinions regarding both tools improved day-by-day. U3I was perceived as less fatiguing than VJ. U3I approached the performance of VJ at the end of the study period, indicating that these two systems can achieve similar performances as users get more experienced in using them. This study supports two hypotheses but does not provide enough evidence to support one hypothesis.","2007","2023-07-05 07:34:57","2023-07-20 06:28:22","2023-07-05 07:34:57","489-502","","","4663","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-74800-7_44","","/Users/minsik/Zotero/storage/E8QMIJRY/Mahmud et al. - 2007 - A Comparative Longitudinal Study of Non-verbal Mou.pdf","","","","Baranauskas, Cécilia; Palanque, Philippe; Abascal, Julio; Barbosa, Simone Diniz Junqueira","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TRX9VIVK","bookSection","1998","Brewster, Stephen","Using Earcons to Improve the Usability of a Graphics Package","People and Computers XIII","978-3-540-76261-4 978-1-4471-3605-7","","","http://link.springer.com/10.1007/978-1-4471-3605-7_18","","1998","2023-07-05 07:37:11","2023-07-05 07:37:11","2023-07-05 07:37:11","287-302","","","","","","","","","","","Springer London","London","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-1-4471-3605-7_18","","/Users/minsik/Zotero/storage/K5S3KZTY/Brewster - 1998 - Using Earcons to Improve the Usability of a Graphi.pdf","","","","Johnson, Hilary; Nigay, Lawrence; Roast, Christopher","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y96RR4ZH","bookSection","2001","Walker, Ashley; Brewster, Stephen; McGookin, David; Ng, Adrian","Diary in the Sky: A Spatial Audio Display for a Mobile Calendar","People and Computers XV—Interaction without Frontiers","978-1-85233-515-1 978-1-4471-0353-0","","","http://link.springer.com/10.1007/978-1-4471-0353-0_33","We present a spatial audio display technique that overcomes the presentation rate bottleneck of traditional monauralaudio displays. Our compact speech display works by encoding message semantics into the acoustic spatialisation. In user testing, this display facilitated better recall of events than acon ventionalsmall screen visual display.Mor eover, results showed that this mapping aided in the recall of the absolute position of events — as opposed to merely their relativeorders — in atemporally ordered data set.","2001","2023-07-05 07:37:11","2023-07-21 04:42:43","2023-07-05 07:37:11","531-539","","","","","","Diary in the Sky","","","","","Springer London","London","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-1-4471-0353-0_33","","/Users/minsik/Zotero/storage/B8BSUPL3/Walker et al. - 2001 - Diary in the Sky A Spatial Audio Display for a Mo.pdf","","","","Blandford, Ann; Vanderdonckt, Jean; Gray, Phil","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BB9T8EE2","journalArticle","2012","Vesna, Victoria","Vibration matters: collective blue morph effect","AI & SOCIETY","","0951-5666, 1435-5655","10.1007/s00146-011-0359-x","http://link.springer.com/10.1007/s00146-011-0359-x","Once an artist takes on the challenge of making the invisible visible, or the inaudible audible, he/she is almost immediately thrown into the realm of energy at the edge of art and science. The established art world based on visual culture finds it difficult to place this kind of work. The scientific community, used to working in this realm in a reductionist way, finds it hard to comprehend. Yet, the public seems to be drawn to artwork residing “in between,” and there seems to be a universal need for a connection to the spiritual realm beyond what established religions offer. As many speculative ideas in the West circulate around ideas of energetic approach to matter in general, particularly the body and mind, alternative medicine and other Eastern philosophies are thriving. This essay will show how, in collaboration with nanoscientist James Gimzewski, we have investigated these ideas from the sounds of cells to the concept and realization of the Blue Morph installation at the Integratron [the Integratron is the creation of George Van Tassel and is based on the design of Moses’ Tabernacle, the writings of Nikola Tesla and telepathic directions from extraterrestrials. This one-of-a-kind building is a 38-foot-high, 55-foot-diameter, nonmetallic structure originally designed by Van Tassel as a rejuvenation and time machine (The Integratron 2009)].","2012-05","2023-07-05 07:37:11","2023-07-19 11:28:18","2023-07-05 07:37:11","319-323","","2","27","","AI & Soc","Vibration matters","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9U65N9AA","bookSection","2006","Hermann, Thomas; Paschalidou, Stella; Beckmann, Dirk; Ritter, Helge","Gestural Interactions for Multi-parameter Audio Control and Audification","Gesture in Human-Computer Interaction and Simulation","978-3-540-32624-3 978-3-540-32625-0","","","http://link.springer.com/10.1007/11678816_37","This paper presents an interactive multi-modal system for real-time multi-parametric gestural control of audio processing applications. We claim that this can ease the use / control of different tasks and for this we present the following as a demonstration: (1) A musical application, i.e. the multi-parametric control of digital audio effects, and (2) a scientific application, i.e. the interactive navigation of audifications. In the first application we discuss the use of PCA-based control axes and clustering to obtain dimensionality reduced control variables. In the second application we show how the tightly closed human-computer loop actively supports the detection and discovery of features in data under analysis.","2006","2023-07-05 07:37:11","2023-07-20 00:09:11","2023-07-05 07:37:11","335-338","","","3881","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11678816_37","","","","","","Gibet, Sylvie; Courty, Nicolas; Kamp, Jean-François","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KBYQZF8H","bookSection","2021","Pereira, Frederico; Marques, Rui; Vieria, Joana","Auditory Alarms Design Tool: Spectral Masking Estimation Based on a Psychoacoustic Model","Advances in Design, Music and Arts","978-3-030-55699-0 978-3-030-55700-3","","","http://link.springer.com/10.1007/978-3-030-55700-3_43","Human ability to detect auditory alarms in the presence of noise has been identified as an issue in various working environments, with potentially serious consequences. Spectral masking of alarms is recognized as a contributing factor to response failures. In this paper, a GNU OCTAVE code implementation for the estimation of spectral masking is detailed. The method is based on a psychoacoustic model of the peripheral human auditory system and is suggested as a tool to support the design of efficient auditory alarms. Three scenarios are investigated using standardized clinical auditory alarms as test stimuli: (1) pair of same priority alarms; (2) pair of different priority alarms and (3) alarm in the presence of environmental noise. The implemented method offers a visualization of estimated masked signal spectral components, enabling the sound designer to evaluate manipulations for masking avoidance.","2021","2023-07-05 07:37:11","2023-07-19 11:11:38","2023-07-05 07:37:11","621-639","","","9","","","Auditory Alarms Design Tool","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Springer Series in Design and Innovation DOI: 10.1007/978-3-030-55700-3_43","","","","","","Raposo, Daniel; Neves, João; Silva, José; Correia Castilho, Luísa; Dias, Rui","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NFNVPUEM","bookSection","2023","Mocchi, Martino; Sillano, Carlotta; Rocca, Lorena","Sound Beyond the Hedge. Towards an Acoustic Construction of Images","Proceedings of the 3rd International and Interdisciplinary Conference on Image and Imagination","978-3-031-25905-0 978-3-031-25906-7","","","https://link.springer.com/10.1007/978-3-031-25906-7_33","The use of images for representing the world should be considered as the product of a generative act of the subject more than the result of an objective process. It could be said that the construction of images is produced by a consonance, or even a resonance – a common vibration between the real and the subject, amplified in his mental processing: a physical as well as emotional, symbolic, cultural connection. Replacing the metaphorical sense of “resonance” with its proper meaning, there are many artistic, architectural, literary experiences that have attempted a translation of sound into visual images and vice versa – sometimes aiming at a synesthetic stimulation of our perception, other times configuring “silent media” able to inspire our imagination. More peculiarly, the role of sound in the process of unveiling and building images assumes a great importance in the experience of blind people. Sound strongly contributes to develop a “gaze” capable of perceiving and judging the outer world. This extends the scope of the image beyond the mere visual, placing it in a multisensory dimension. Through the method of interviewing and analysis, the paper focuses on the acoustic dynamics that affect the symbolic horizon at the basis of the construction of the image, favoring an inclusive perspective, with possible repercussions in the field of communication, art, society and environment.","2023","2023-07-05 07:37:11","2023-07-21 04:55:16","2023-07-05 07:37:11","307-314","","","631","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Networks and Systems DOI: 10.1007/978-3-031-25906-7_33","","","","","","Villa, Daniele; Zuccoli, Franca","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LUAY5U99","journalArticle","1994","McLellan, Hilary","Virtual reality and multiple intelligences: Potentials for higher education","Journal of Computing in Higher Education","","1042-1726","10.1007/BF02948570","http://link.springer.com/10.1007/BF02948570","IN THIS PAPER WE EXAMINE how virtual reality, an emerging computer-based technology, can promote learning that engages all seven of the multiple intelligences proposed by Harvard educational psychologist Howard Gardner. We provides an overview of virtual reality technologies and an overview of Gardner’s multiple intelligences. There is an extensive discussion of how virtual reality supports learning within and across seven intelligence domains. Finally, there is a review of technical and conceptual issues concerning the implementation of virtual reality in education. Educational experiences that promote the various multiple intelligences and interlinkages are needed in the emerging electronic age more than at any previous time.","1994-03","2023-07-05 07:37:11","2023-07-20 06:47:06","2023-07-05 07:37:11","33-66","","2","5","","J. Comput. High. Educ.","Virtual reality and multiple intelligences","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZHZ752V9","journalArticle","2017","Black, David; Hansen, Christian; Nabavi, Arya; Kikinis, Ron; Hahn, Horst","A Survey of auditory display in image-guided interventions","International Journal of Computer Assisted Radiology and Surgery","","1861-6410, 1861-6429","10.1007/s11548-017-1547-z","http://link.springer.com/10.1007/s11548-017-1547-z","Purpose—This article investigates the current state of the art of the use of auditory display in image-guided medical interventions. Auditory display is a means of conveying information using sound, and we review the use of this approach to support navigated interventions. We discuss the benefits and drawbacks of published systems and outline directions for future investigation. Methods—We undertook a review of scientific articles on the topic of auditory rendering in image-guided intervention. This includes methods for avoidance of risk structures and instrument placement and manipulation. The review did not include auditory display for status monitoring, for instance in anesthesia. Results—We identified 13 publications in the course of the search. Most of the literature (62%) investigates the use of auditory display to convey distance of a tracked instrument to an object using proximity or safety margins. The remainder discuss continuous guidance for navigated instrument placement. Four of the articles present clinical evaluations, 9 present laboratory evaluations, and 3 present informal evaluation (3 present both laboratory and clinical evaluations). Conclusion—Auditory display is a growing field that has been largely neglected in research in image-guided intervention. Despite benefits of auditory displays reported in both the reviewed literature and non-medical fields, adoption in medicine has been slow. Future challenges include increasing interdisciplinary cooperation with auditory display investigators to develop more meaningful auditory display designs and comprehensive evaluations which target the benefits and drawbacks of auditory display in image guidance.","2017-10","2023-07-05 07:37:11","2023-07-20 06:41:13","2023-07-05 07:37:11","1665-1676","","10","12","","Int J CARS","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/KE2FI8PQ/Black et al. - 2017 - A Survey of auditory display in image-guided inter.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RC9799LZ","journalArticle","2019","Indans, Reinis; Hauthal, Eva; Burghardt, Dirk","Towards an Audio-Locative Mobile Application for Immersive Storytelling","KN - Journal of Cartography and Geographic Information","","2524-4957, 2524-4965","10.1007/s42489-019-00007-1","http://link.springer.com/10.1007/s42489-019-00007-1","We live in an age in which digital media is omnipresent and augmented reality is beginning to find its way into our everyday lives, GPS allows us to determine our position with meter precision and the sensor capabilities of smartphones are increasing. All these technologies in combination enable us to explore one of the oldest human art forms in a new way: storytelling. The presented work aims at developing approaches that combine different elements of audio media playback, geolocation and other sensor capabilities of smartphones to allow the creation of immersive geolocated narratives within a mobile application. Thus, the narrative can be precisely adjusted to the specific spatial and temporal context of the user. One of the main goals of such a geolocated narrative would be to influence the cognitive processing of the users. As the visual perception of the location stays the same while using the app, it is only the auditory cognition that causes a subtle enhancement and thus alteration of the reality which could result in a very strong sense of immersion. There are many location-based audio applications on the market. Most of them are intended for tourism in the form of an audio tour guide. Some of those applications have been analyzed before developing our own ideas on how to intertwine sound, space and time into an immersive mobile application for storytelling with semi-linear narrative structures including a map. Possible features could be influenceability of the narrative by, e.g., speed or approach direction of the user, sound movemen","2019-05","2023-07-05 07:37:11","2023-07-21 04:28:03","2023-07-05 07:37:11","41-50","","1","69","","KN J. Cartogr. Geogr. Inf.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/V8JTA4QE/Indans et al. - 2019 - Towards an Audio-Locative Mobile Application for I.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QPGYV4I3","bookSection","2004","Velleman, Eric; Van Tol, Richard; Huiberts, Sander; Verwey, Hugo","3D Shooting Games, Multimodal Games, Sound Games and More Working Examples of the Future of Games for the Blind","Computers Helping People with Special Needs","978-3-540-22334-4 978-3-540-27817-7","","","http://link.springer.com/10.1007/978-3-540-27817-7_39","Blind people shooting virtual monsters on a real football field. Is that possible? The European Media Master of Arts-program of the Utrecht School of the Arts (Arts, Media & Technology) and the Bartiméus Accessibility Foundation in Zeist have developed a curriculum for accessible game and program development together. Within this curriculum already many spectacular games have been developed like Drive, The Curb Game, Hall of Sound, Powerchords, Wow, Demor and others. The games include the use of user panels and extensive user testing and present the future possibilities of gaming.","2004","2023-07-05 07:37:11","2023-07-19 23:53:30","2023-07-05 07:37:11","257-263","","","3118","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-27817-7_39","","","","","","Miesenberger, Klaus; Klaus, Joachim; Zagler, Wolfgang L.; Burger, Dominique","Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SZAB7UPX","bookSection","2014","Cordeiro, João; Barbosa, Álvaro","Privacy in Sound-Based Social Networks","Multidisciplinary Social Networks Research","978-3-662-45070-3 978-3-662-45071-0","","","http://link.springer.com/10.1007/978-3-662-45071-0_29","In this paper we address the issue of privacy in Online Social Networks (OSN), focusing on those that use environmental sound as a contextual cue for users activity. Through the use of a costume-made research tool consisting of an Online Sound-Based Social Network (OSBSN), we undertook scientific experiments aiming to assess how users deal with the use of sound. Results show that contextual sound is regarded as important and useful for OSN but raises important privacy concerns. In order to deal with this constraint, we propose a system based on the automatic classification of sound environment rather than capturing and sharing actual audio.","2014","2023-07-05 07:37:11","2023-07-21 04:30:41","2023-07-05 07:37:11","355-367","","","473","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Communications in Computer and Information Science DOI: 10.1007/978-3-662-45071-0_29","","","","","","Wang, Leon Shyue-Liang; June, Jason J.; Lee, Chung-Hong; Okuhara, Koji; Yang, Hsin-Chang","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HT9HDM25","journalArticle","2013","Yamabe, Tetsuo; Nakajima, Tatsuo","Playful training with augmented reality games: case studies towards reality-oriented system design","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-011-0979-7","http://link.springer.com/10.1007/s11042-011-0979-7","In this paper, we propose a reality-oriented augmentation approach to support training activities. The approach aims at adding new value and playful features to traditional training environments with keeping their original look-and-feel. For example, a game monitoring service enables to automatically record game events so that players can review a gaming process and strategy for soul-searching, or replay most impressive scenes to share the experience with others after the game finishes. Even several services are running on background, digital devices and services are seamlessly integrated to the game environment in unobtrusive way so that players can concentrate on training as usual. The concept can be applied to both traditional games (e.g., poker and the game of Go) and non-gaming activities (e.g., calligraphy and drumming). We developed four case studies on the concept: Augmented Reality Go, EmoPoker, Augmented Calligraphy and AR Drum Kit. We discuss design issues in the reality-oriented augmentation process based on user study results.","2013-01","2023-07-05 07:37:11","2023-07-21 04:33:34","2023-07-05 07:37:11","259-286","","1","62","","Multimed Tools Appl","Playful training with augmented reality games","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PIEHX9RY","bookSection","2021","Hoy, Rory; Van Nort, Doug","Augmentation of Sonic Meditation Practices: Resonance, Feedback and Interaction Through an Ecosystemic Approach","Perception, Representations, Image, Sound, Music","978-3-030-70209-0 978-3-030-70210-6","","","http://link.springer.com/10.1007/978-3-030-70210-6_38","This paper describes the design and creation of an interactive sound environment project, titled dispersion.eLabOrate. The system is defined by a ceiling array of microphones, audio input analysis, and synthesis directly driven by this analysis. Created to augment a Deep Listening performative environment, this project explores the role that interactive installations can fulfill within a structured listening context. Echoing, modulating, and extending what it hears, the system generates an environment in which its output is a product of ambient sound, feedback, and participant input. Relating to and building upon the ecosystemic model, we discuss the benefit of designing for participant incorporation within such a responsive listening environment.","2021","2023-07-05 07:37:11","2023-07-21 04:45:23","2023-07-05 07:37:11","591-599","","","12631","","","Augmentation of Sonic Meditation Practices","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-70210-6_38","","","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Aramaki, Mitsuko","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NR7SD4RN","bookSection","2010","Neff, Flaithri; Mehigan, Tracey J.; Pitt, Ian","Accelerometer & Spatial Audio Technology: Making Touch-Screen Mobile Devices Accessible","Computers Helping People with Special Needs","978-3-642-14096-9 978-3-642-14097-6","","","http://link.springer.com/10.1007/978-3-642-14097-6_28","","2010","2023-07-05 07:37:11","2023-07-05 07:37:11","2023-07-05 07:37:11","170-177","","","6179","","","Accelerometer & Spatial Audio Technology","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-14097-6_28","","","","","","Miesenberger, Klaus; Klaus, Joachim; Zagler, Wolfgang; Karshmer, Arthur","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"37T3Z946","bookSection","2013","Jeon, Myounghoon; Lee, Ju-Hwan","The Ecological AUI (Auditory User Interface) Design and Evaluation of User Acceptance for Various Tasks on Smartphones","Human-Computer Interaction. Interaction Modalities and Techniques","978-3-642-39329-7 978-3-642-39330-3","","","http://link.springer.com/10.1007/978-3-642-39330-3_6","","2013","2023-07-05 07:37:11","2023-07-05 07:37:11","2023-07-05 07:37:11","49-58","","","8007","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-39330-3_6","","","","","","Kurosu, Masaaki","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VBW3LF8G","bookSection","2001","Murphy, David; Pitt, Ian","Spatial Sound Enhancing Virtual Story Telling","Virtual Storytelling Using Virtual Reality Technologies for Storytelling","978-3-540-42611-0 978-3-540-45420-5","","","http://link.springer.com/10.1007/3-540-45420-9_3","","2001","2023-07-05 07:37:11","2023-07-05 07:37:11","2023-07-05 07:37:11","20-29","","","2197","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-45420-9_3","","","","","","Balet, Olivier; Subsol, Gérard; Torguet, Patrice","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8ERHQKXF","bookSection","2022","Schöning, Julius; Diers, Julia; Lindner, Dennis; Paßfeld, Thorsten","Olfactory UIs: New Possibilities for Displaying System and Application States","Advances in Information and Communication","978-3-030-98011-5 978-3-030-98012-2","","","https://link.springer.com/10.1007/978-3-030-98012-2_50","Monitoring of critical system and application states is mainly done via graphical user interfaces (UI). Consequently, the user’s visual sense focuses on both their primary task and the monitoring task. Audiovisual in nature, the main task typically receives more attention from the user so that the monitoring task is neglected. As a solution that maps the monitoring of system and application states to the user’s sense of smell, this work systematically designs olfactory UIs by human-centered design (HCD). These olfactory UIs allow users to focus on an audiovisual task without neglecting the monitoring. Based on an online survey, a user study, and usability tests, this paper provides evidence that olfactory UIs are suitable to reliably establish an association between a specific scent and the severity of Syslog messages. The presented results were also transferred into a design solution for an olfactory UI indicating system and application states by scents.","2022","2023-07-05 07:39:56","2023-07-19 11:12:02","2023-07-05 07:39:56","704-721","","","438","","","Olfactory UIs","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Networks and Systems DOI: 10.1007/978-3-030-98012-2_50","","","","","","Arai, Kohei","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VDWGMMSC","journalArticle","2017","Geelhoed, Erik; Singh-Barmi, Kuldip; Biscoe, Ian; Cesar, Pablo; Jansen, Jack; Wang, Chen; Kaiser, Rene","Co-present and remote audience experiences: intensity and cohesion","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-016-3879-z","http://link.springer.com/10.1007/s11042-016-3879-z","This article presents the results of modelling audience response to new types of networked theatre plays. As the main contribution of the work we introduce two types of metrics: intensity, relating to how intensively co-present and remote aspects of a performance are rated, and cohesion, relating to how a performance as a whole, the combination of copresent and remote aspects, affects an audience. In particular, we model audience response based on two in the wild evaluations, staged by a low budget theatre company, a streamed and a distributed performance. The streamed performance is similar to NT Live, where a theatre play is delivered to other theatres with an audience. The distributed performance, on the other hand, connects actors in two different theatres (with audiences) creating one single play. The streamed performance was experienced as less intense as well as less cohesive by the remote audience, whilst the distributed performance integrated co-present and remote aspects tightly. Remote aspects of the distributed performance were still experienced as less intense, but the performance as a whole was highly cohesive. Apart from the identification of these two new metrics (intensity and cohesion), based on our experiences we argue that an innovative way of bundling relevant emerging technologies is needed to give a voice to the, as yet silent, remote audience.","2017-02","2023-07-05 07:39:56","2023-07-21 04:32:17","2023-07-05 07:39:56","5573-5606","","4","76","","Multimed Tools Appl","Co-present and remote audience experiences","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/74I5WBA4/Geelhoed et al. - 2017 - Co-present and remote audience experiences intens.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FYPHFMG8","bookSection","2009","Dorin, Alan","Habitat: Engineering in a Simulated Audible Ecosystem","Applications of Evolutionary Computing","978-3-642-01128-3 978-3-642-01129-0","","","http://link.springer.com/10.1007/978-3-642-01129-0_55","This paper introduces a novel appr oach to generating audio or visual heterogeneity by simulating multi -level habitat formation by ecosystem engineer organisms. Ecosystem engineers generate habitat by modulation of environmental factors, such as erosion or radiation exposure, and provision of substrate. We describe Habitat, a simulation that runs on a two -dimensional grid occupied by an evolving population of stationary agents. The bodies of these agents provide local, differentiated habitat for new agents. Agents evolve using a conventional evolutionary algorithm that acts on their habitat preferences, habitat provision and lifespan, to populate the space and one another. This generates heterogeneous, dynamic structures that have been used in a prototype sonic artwork and simple visualisatio n.","2009","2023-07-05 07:39:56","2023-07-19 11:31:04","2023-07-05 07:39:56","488-497","","","5484","","","Habitat","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-01129-0_55","","/Users/minsik/Zotero/storage/XY7X9SMB/Dorin - 2009 - Habitat Engineering in a Simulated Audible Ecosys.pdf","","","","Giacobini, Mario; Brabazon, Anthony; Cagnoni, Stefano; Di Caro, Gianni A.; Ekárt, Anikó; Esparcia-Alcázar, Anna Isabel; Farooq, Muddassar; Fink, Andreas; Machado, Penousal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6K4Y9ACW","bookSection","2010","Graf, Christian","Verbally Annotated Tactile Maps – Challenges and Approaches","Spatial Cognition VII","978-3-642-14748-7 978-3-642-14749-4","","","http://link.springer.com/10.1007/978-3-642-14749-4_26","","2010","2023-07-05 07:39:56","2023-07-05 07:39:56","2023-07-05 07:39:56","303-318","","","6222","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-14749-4_26","","","","","","Hölscher, Christoph; Shipley, Thomas F.; Olivetti Belardinelli, Marta; Bateman, John A.; Newcombe, Nora S.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U676VH53","journalArticle","2017","Wang, Qi; Markopoulos, Panos; Yu, Bin; Chen, Wei; Timmermans, Annick","Interactive wearable systems for upper body rehabilitation: a systematic review","Journal of NeuroEngineering and Rehabilitation","","1743-0003","10.1186/s12984-017-0229-y","http://jneuroengrehab.biomedcentral.com/articles/10.1186/s12984-017-0229-y","Background: The development of interactive rehabilitation technologies which rely on wearable-sensing for upper body rehabilitation is attracting increasing research interest. This paper reviews related research with the aim: 1) To inventory and classify interactive wearable systems for movement and posture monitoring during upper body rehabilitation, regarding the sensing technology, system measurements and feedback conditions; 2) To gauge the wearability of the wearable systems; 3) To inventory the availability of clinical evidence supporting the effectiveness of related technologies. Method: A systematic literature search was conducted in the following search engines: PubMed, ACM, Scopus and IEEE (January 2010–April 2016). Results: Forty-five papers were included and discussed in a new cuboid taxonomy which consists of 3 dimensions: sensing technology, feedback modalities and system measurements. Wearable sensor systems were developed for persons in: 1) Neuro-rehabilitation: stroke (n = 21), spinal cord injury (n = 1), cerebral palsy (n = 2), Alzheimer (n = 1); 2) Musculoskeletal impairment: ligament rehabilitation (n = 1), arthritis (n = 1), frozen shoulder (n = 1), bones trauma (n = 1); 3) Others: chronic pulmonary obstructive disease (n = 1), chronic pain rehabilitation (n = 1) and other general rehabilitation (n = 14). Accelerometers and inertial measurement units (IMU) are the most frequently used technologies (84% of the papers). They are mostly used in multiple sensor configurations to measure upper limb kinematics and/or trunk posture. Sensors are placed mostly on the trunk, upper arm, the forearm, the wrist, and the finger. Typically sensors are attachable rather than embedded in wearable devices and garments; although studies that embed and integrate sensors are increasing in the last 4 years. 16 studies applied knowledge of result (KR) feedback, 14 studies applied knowledge of performance (KP) feedback and 15 studies applied both in various modalities. 16 studies have conducted their evaluation with patients and reported usability tests, while only three of them conducted clinical trials including one randomized clinical trial. Conclusions: This review has shown that wearable systems are used mostly for the monitoring and provision of feedback on posture and upper extremity movements in stroke rehabilitation. The results indicated that accelerometers and IMUs are the most frequently used sensors, in most cases attached to the body through ad hoc contraptions for the purpose of improving range of motion and movement performance during upper body rehabilitation. Systems featuring sensors embedded in wearable appliances or garments are only beginning to emerge. Similarly, clinical evaluations are scarce and are further needed to provide evidence on effectiveness and pave the path towards implementation in clinical settings.","2017-12","2023-07-05 07:39:56","2023-07-21 07:42:26","2023-07-05 07:39:56","20","","1","14","","J NeuroEngineering Rehabil","Interactive wearable systems for upper body rehabilitation","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/8T2JKEX6/Wang et al. - 2017 - Interactive wearable systems for upper body rehabi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LIV92Q9D","bookSection","2008","Pirhonen, Antti; Tuuri, Kai","In Search for an Integrated Design Basis for Audio and Haptics","Haptic and Audio Interaction Design","978-3-540-87882-7 978-3-540-87883-4","","","http://link.springer.com/10.1007/978-3-540-87883-4_9","Audio and haptics as interaction modalities share properties, which make them highly appropriate to be handled within a single conceptual framework. This paper outlines such framework, gaining ingredients from the literature concerning cross-modal integration and embodied cognition. The resulting framework is bound up with a concept of physical embodiment, which has been introduced within several scientific disciplines to reveal the role of bodily experience and the corresponding mental imagery as the core of meaning-creation. In addition to theoretical discussion, the contribution of the proposed approach in design is outlined.","2008","2023-07-05 07:39:56","2023-07-20 00:16:23","2023-07-05 07:39:56","81-90","","","5270","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-87883-4_9","","","","","","Pirhonen, Antti; Brewster, Stephen","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MTWHW5CZ","bookSection","2009","McCormack, Jon; Bown, Oliver","Life’s What You Make: Niche Construction and Evolutionary Art","Applications of Evolutionary Computing","978-3-642-01128-3 978-3-642-01129-0","","","http://link.springer.com/10.1007/978-3-642-01129-0_59","This paper advances new methods for ecosystemic approaches to evolutionary music and art. We explore the biological concept of the niche and its role in evolutionary dynamics, applying it to creative computational systems. Using the process of niche construction organisms are able to change and adapt their environment, and potentially that of other species. Constructed niches may become heritable environments for offspring, paralleling the way genes are passed from parent to child. In a creative ecosystem, niche construction can be used by agents to increase the diversity and heterogeneity of their output. We illustrate the usefulness of this technique by applying niche construction to line drawing and music composition.","2009","2023-07-05 07:39:56","2023-07-19 11:31:35","2023-07-05 07:39:56","528-537","","","5484","","","Life’s What You Make","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-01129-0_59","","","","","","Giacobini, Mario; Brabazon, Anthony; Cagnoni, Stefano; Di Caro, Gianni A.; Ekárt, Anikó; Esparcia-Alcázar, Anna Isabel; Farooq, Muddassar; Fink, Andreas; Machado, Penousal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YITJGPMX","bookSection","2008","Jokiniemi, Maria; Raisamo, Roope; Lylykangas, Jani; Surakka, Veikko","Crossmodal Rhythm Perception","Haptic and Audio Interaction Design","978-3-540-87882-7 978-3-540-87883-4","","","http://link.springer.com/10.1007/978-3-540-87883-4_12","Research on rhythm perception has mostly been focused on the auditory and visual modalities. Previous studies have shown that the auditory modality dominates rhythm perception. Rhythms can also be perceived through the tactile senses, for example, as vibrations, but only few studies exist. We investigated unimodal and crossmodal rhythm perception with auditory, tactile, and visual modalities. Pairs of rhythm patterns were presented to the subject who made a same-different judgment. We used all possible combinations of the three modalities. The results showed that the unimodal auditory condition had the highest rate (79.2%) of correct responses. The unimodal tactile condition (75.0%) and the auditory-tactile condition (74.2%) were close. The average rate remained under 61.7% when the visual modality was involved. The results confirmed that auditory and tactile modalities are suitable for presenting rhythmic information, and they are also preferred by the users.","2008","2023-07-05 07:39:56","2023-07-20 00:15:01","2023-07-05 07:39:56","111-119","","","5270","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-87883-4_12","","","","","","Pirhonen, Antti; Brewster, Stephen","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WDT422YX","bookSection","2015","Sun, Yuanjing; Jeon, Myounghoon","Lyricon (Lyrics + Earcons) Improves Identification of Auditory Cues","Design, User Experience, and Usability: Users and Interactions","978-3-319-20897-8 978-3-319-20898-5","","","http://link.springer.com/10.1007/978-3-319-20898-5_37","Auditory researchers have developed various non-speech cues in designing auditory user interfaces. A preliminary study of “lyricons” (lyrics + earcons [1]) has provided a novel approach to devising auditory cues in electronic products, by combining the concurrent two layers of musical speech and earcons (short musical motives). An experiment on sound-function meaning mapping was conducted between earcons and lyricons. It demonstrated that lyricons significantly more enhanced the relevance between the sound and the meaning compared to earcons. Further analyses on error type and confusion matrix show that lyricons showed a higher identification rate and a shorter mapping time than earcons. Factors affecting auditory cue identification and application directions of lyricons are discussed.","2015","2023-07-05 07:39:56","2023-07-19 23:55:07","2023-07-05 07:39:56","382-389","","","9187","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-20898-5_37","","","","","","Marcus, Aaron","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LVFJR25F","journalArticle","2020","Schnass, Karin; Teixeira, Flavio","Compressed Dictionary Learning","Journal of Fourier Analysis and Applications","","1069-5869, 1531-5851","10.1007/s00041-020-09738-6","http://link.springer.com/10.1007/s00041-020-09738-6","In this paper we show that the computational complexity of the Iterative Thresholding and K-residual-Means (ITKrM) algorithm for dictionary learning can be significantly reduced by using dimensionality-reduction techniques based on the Johnson-Lindenstrauss lemma. The dimensionality reduction is efficiently carried out with the fast Fourier transform. We introduce the Iterative compressed-Thresholding and K-Means (IcTKM) algorithm for fast dictionary learning and study its convergence properties. We show that IcTKM can locally recover an incoherent, overcomplete generating dictionary of K atoms from training signals of sparsity level S with high probability. Fast dictionary learning is achieved by embedding the training data and the dictionary into m < d dimensions, and recovery is shown to be locally stable with an embedding dimension which scales as low as m = O(S log4 S log3 K). The compression effectively shatters the data dimension bottleneck in the computational cost of ITKrM, reducing it by a factor O(m/d). Our theoretical results are complemented with numerical simulations which demonstrate that IcTKM is a powerful, low-cost algorithm for learning dictionaries from high-dimensional data sets.","2020-04","2023-07-05 07:39:56","2023-07-20 06:47:14","2023-07-05 07:39:56","33","","2","26","","J Fourier Anal Appl","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/PJJTZ3WY/Schnass and Teixeira - 2020 - Compressed Dictionary Learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SQ7WHQTB","journalArticle","2008","Peticolas, L. M.; Craig, N.; Kucera, T.; Michels, D. J.; Gerulskis, J.; MacDowall, R. J.; Beisser, K.; Chrissotimos, C.; Luhmann, J. G.; Galvin, A. B.; Ratta, L.; Drobnes, E.; Méndez, B. J.; Hill, S.; Marren, K.; Howard, R.","The Solar Terrestrial Relations Observatory (STEREO) Education and Outreach (E/PO) Program","Space Science Reviews","","0038-6308, 1572-9672","10.1007/s11214-007-9287-y","http://link.springer.com/10.1007/s11214-007-9287-y","The STEREO mission’s Education and Outreach (E/PO) program began early enough its team benefited from many lessons learned as NASA’s E/PO profession matured. Originally made up of discrete programs, by launch the STEREO E/PO program had developed into a quality suite containing all the program elements now considered standard: education workshops, teacher/student guides, national and international collaboration, etc. The benefit of bringing so many unique programs together is the resulting diverse portfolio, with scientists, E/PO professionals, and their education partners all of whom can focus on excellent smaller programs. The drawback is a less cohesive program nearly impossible to evaluate in its entirety with the given funding. When individual components were evaluated, we found our programs mostly made positive impact. In this paper, we elaborate on the programs, hoping that others will effectively use or improve upon them. When possible, we indicate the programs’ effects on their target audiences.","2008-04","2023-07-05 07:39:56","2023-07-21 05:02:18","2023-07-05 07:39:56","627-646","","1-4","136","","Space Sci Rev","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MGUAN2HG","journalArticle","1997","Edwards, Lynne K.; Link, Stephen W.; Null, Cynthia H.","High-performance computer applications in the behavioral sciences","Behavior Research Methods, Instruments, &amp Computers","","0743-3808, 1532-5970","10.3758/BF03200580","http://link.springer.com/10.3758/BF03200580","This symposium revisited the 1985conference on Advanced Computing for Psychology. That meeting examined the application of new supercomputers in the behavioral sciences. The present symposium reviewed high-performance computing as applied to psychological models, human vision, neuralphysiological processes, and statistical analysis. The recent past and the projected future of high-performance computing in the behavioral sciences were evaluated.","1997-03","2023-07-05 07:39:56","2023-07-19 23:34:35","2023-07-05 07:39:56","122-125","","1","29","","Behavior Research Methods, Instruments, & Computers","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/WTKRVH9K/Edwards et al. - 1997 - High-performance computer applications in the beha.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CVQASUYR","bookSection","2008","Jones, Daniel","AtomSwarm: A Framework for Swarm Improvisation","Applications of Evolutionary Computing","978-3-540-78760-0 978-3-540-78761-7","","","http://link.springer.com/10.1007/978-3-540-78761-7_45","This paper introduces AtomSwarm, a framework for sound-based performance using swarm dynamics. The classical ruleset for flocking simulations is augmented with genetically-encoded behaviours, hormonal flows, and viral ‘memes’, creating a complex sonic ecosystem that is capable of temporal adaptation and self-regulation. The architecture and sound design methodologies are summarised here, with critical reference to its biomimetic design process, sonic spatialisation and self-organising capabilities. It is finally suggested that the system’s lifelikeness is a product of its relational complexity, creating empathic engagement purely through abstract formal structures.","2008","2023-07-05 07:39:56","2023-07-19 11:31:27","2023-07-05 07:39:56","423-432","","","4974","","","AtomSwarm","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-78761-7_45","","","","","","Giacobini, Mario; Brabazon, Anthony; Cagnoni, Stefano; Di Caro, Gianni A.; Drechsler, Rolf; Ekárt, Anikó; Esparcia-Alcázar, Anna Isabel; Farooq, Muddassar; Fink, Andreas; McCormack, Jon; O’Neill, Michael; Romero, Juan; Rothlauf, Franz; Squillero, Giovanni; Uyar, A. Şima; Yang, Shengxiang","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8K23ADKV","bookSection","2008","Murray-Smith, Roderick; Strachan, Steven","Rotational Dynamics for Design of Bidirectional Feedback during Manual Interaction","Fun and Games","978-3-540-88321-0 978-3-540-88322-7","","","http://link.springer.com/10.1007/978-3-540-88322-7_1","Rotational dynamic system models can be used to enrich tightlycoupled embodied control of movement-sensitive mobile devices, and support a more bidirectional, negotiated style of interaction. This can provide a constructive, as well as informative, approach to the design of engaging, playful elements in interaction mechanisms. A simulated rotational spring system is used for natural eyes-free feedback in both the audio and haptic channels, and in a Mobile Spatial Interaction application, using twisting and tilting motions to drag and drop content, where users perceived the effect of varying the parameters of the simulated dynamic system.","2008","2023-07-05 07:39:56","2023-07-20 00:07:17","2023-07-05 07:39:56","1-10","","","5294","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-88322-7_1","","/Users/minsik/Zotero/storage/2LBBLA34/Murray-Smith and Strachan - 2008 - Rotational Dynamics for Design of Bidirectional Fe.pdf","","","","Markopoulos, Panos; De Ruyter, Boris; IJsselsteijn, Wijnand; Rowland, Duncan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I66YN99X","journalArticle","2012","El-Shimy, Dalia; Grond, Florian; Olmos, Adriana; Cooperstock, Jeremy R.","Eyes-free environmental awareness for navigation","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0065-5","http://link.springer.com/10.1007/s12193-011-0065-5","We consider the challenge of delivering location-based information through rich audio representations of the environment, and the associated opportunities that such an approach offers to support navigation tasks. This challenge is addressed by In-Situ Audio Services, or ISAS, a system intended primarily for use by the blind and visually impaired communities. It employs spatialized audio rendering to convey the relevant content, which may include information about the immediate surroundings, such as restaurants, cultural sites, public transportation locations, and other points of interest. Information is aggregated mostly from online data resources, converted using text-to-speech technology, and “displayed”, either as speech or more abstract audio icons, through a location-aware mobile device or smartphone. This is suitable not only for the specific constraints of the target population, but is equally useful for general mobile users whose visual attention is otherwise occupied with navigation. We designed and conducted an experiment to evaluate two techniques for delivering spatialized audio content to users via interactive auditory maps: the shockwave mode and the radar mode. While neither mode proved to be significantly better than the other, subjects proved competent at navigating the maps using these rendering strategies, and reacted positively to the system, demonstrating that spatial audio can be an effective technique for conveying location-based information. The results of this experiment and its implications to our project are described here.","2012-05","2023-07-05 07:39:56","2023-07-20 06:54:13","2023-07-05 07:39:56","131-141","","3-4","5","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WCFCN4XY","journalArticle","2021","Gilat, Moran; Ginis, Pieter; Zoetewei, Demi; De Vleeschhauwer, Joni; Hulzinga, Femke; D’Cruz, Nicholas; Nieuwboer, Alice","A systematic review on exercise and training-based interventions for freezing of gait in Parkinson’s disease","npj Parkinson's Disease","","2373-8057","10.1038/s41531-021-00224-4","https://www.nature.com/articles/s41531-021-00224-4","Freezing of gait (FOG) in Parkinson’s disease (PD) causes severe patient burden despite pharmacological management. Exercise and training are therefore advocated as important adjunct therapies. In this meta-analysis, we assess the existing evidence for such interventions to reduce FOG, and further examine which type of training helps the restoration of gait function in particular. The primary meta-analysis across 41 studies and 1838 patients revealed a favorable moderate effect size (ES = −0.37) of various training modalities for reducing subjective FOG-severity (p < 0.00001), though several interventions were not directly aimed at FOG and some included non-freezers. However, exercise and training also proved beneficial in a secondary analysis on freezers only (ES = −0.32, p = 0.007). We further revealed that dedicated training aimed at reducing FOG episodes (ES = −0.24) or ameliorating the underlying correlates of FOG (ES = −0.40) was moderately effective (p < 0.01), while generic exercises were not (ES = −0.14, p = 0.12). Relevantly, no retention effects were seen after cessation of training (ES = −0.08, p = 0.36). This review thereby supports the implementation of targeted training as a treatment for FOG with the need for long-term engagement.","2021-09-10","2023-07-05 07:40:37","2023-07-05 07:40:37","2023-07-05 07:40:37","1-18","","1","7","","npj Parkinsons Dis.","","","","","","","","en","2021 The Author(s)","","","","www.nature.com","","Number: 1 Publisher: Nature Publishing Group","","/Users/minsik/Zotero/storage/L92NU4EQ/Gilat et al. - 2021 - A systematic review on exercise and training-based.pdf","","","Rehabilitation; Parkinson's disease","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CJTKMJ63","bookSection","2004","Lyon, Kirstin; Nürnberg, Peter J.","Interface Design – Use of Audio as an Output","Metainformatics","978-3-540-22010-7 978-3-540-24647-3","","","http://link.springer.com/10.1007/978-3-540-24647-3_7","","2004","2023-07-05 07:43:00","2023-07-05 07:43:00","2023-07-05 07:43:00","79-88","","","3002","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-24647-3_7","","","","","","Hicks, David L.","Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CBEX9KQB","journalArticle","2023","Ahmedien, Diaa Ahmed Mohamed","Analysing bio-art’s epistemic landscape: from metaphoric to post-metaphoric structure","BioSocieties","","1745-8552, 1745-8560","10.1057/s41292-022-00270-y","https://link.springer.com/10.1057/s41292-022-00270-y","Abstract             Since its emergence, bio-art has developed numerous metaphors central to the transfer of concepts of modern biology, genetics, and genomics to the public domain that reveal several cultural, ethical, and social variations in their related themes. This article assumes that a general typology of metaphors developed by practices related to bio-art can be categorised into two categories: pictorial and operational metaphors. Through these, information regarding several biological issues is transferred to the public arena. Based on the analysis, this article attempts to answer the following questions: How does bio-art develop metaphors to advance epistemic and discursive agendas that constitute public understanding of a set of deeply problematic assumptions regarding how today’s biology operates? Under the influence of today’s synthetic biology, could bio-media operationally reframe these epistemic agendas by reframing complex and multi-layered metaphors towards post-metaphoric structures? Finally, what are the scientific, cultural, and social implications of reframing?","2023-06","2023-07-05 07:43:00","2023-07-05 07:43:00","2023-07-05 07:43:00","308-334","","2","18","","BioSocieties","Analysing bio-art’s epistemic landscape","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/D2H2PGU9/Ahmedien - 2023 - Analysing bio-art’s epistemic landscape from meta.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZKWFMAKR","journalArticle","1998","Loomis, Jack M.; Klatzky, Roberta L.; Philbeck, John W.; Golledge, Reginald G.","Assessing auditory distance perception using perceptually directed action","Perception & Psychophysics","","0031-5117, 1532-5962","10.3758/BF03211932","http://link.springer.com/10.3758/BF03211932","Three experiments investigated auditory distance perception under natural listening conditions in a large open field. Targets varied in egocentric distance from 3 to 16 m. By presenting visual targets at these same locations on other trials, we were able to compare visual and auditory distance perception under similar circumstances. In some experimental conditions, observers made verbal reports of target distance. In others, observers viewed or listened to the target and then, without further perceptual information about the target, attempted to face the target, walk directly to it, or walk along a two-segment indirect path to it. The primary results were these. First, the verbal and walking responses were largely concordant, with the walking responses exhibiting less between-observer variability. Second, different motoric responses provided consistent estimates of the perceived target locations and, therefore, of the initially perceived distances. Third, under circumstances for which visual targets were perceived more or less correctly in distance using the more precise walking response, auditory targets were generally perceived with considerable systematic error. In particular, the perceived locations of the auditory targets varied only about half as much in distance as did the physical targets; in addition, there was a tendency to underestimate target distance, except for the closest targets.","1998-09","2023-07-05 07:43:00","2023-07-21 04:43:49","2023-07-05 07:43:00","966-980","","6","60","","Perception & Psychophysics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/4SLXJCKL/Loomis et al. - 1998 - Assessing auditory distance perception using perce.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MZSDFSKT","journalArticle","2015","Larsson, Matz","Tool-use-associated sound in the evolution of language","Animal Cognition","","1435-9448, 1435-9456","10.1007/s10071-015-0885-x","http://link.springer.com/10.1007/s10071-015-0885-x","Proponents of the motor theory of language evolution have primarily focused on the visual domain and communication through observation of movements. In the present paper, it is hypothesized that the production and perception of sound, particularly of incidental sound of locomotion (ISOL) and tool-use sound (TUS), also contributed. Human bipedalism resulted in rhythmic and more predictable ISOL. It has been proposed that this stimulated the evolution of musical abilities, auditory working memory, and abilities to produce complex vocalizations and to mimic natural sounds. Since the human brain proficiently extracts information about objects and events from the sounds they produce, TUS, and mimicry of TUS, might have achieved an iconic function. The prevalence of sound symbolism in many extant languages supports this idea. Self-produced TUS activates multimodal brain processing (motor neurons, hearing, proprioception, touch, vision), and TUS stimulates primate audiovisual mirror neurons, which is likely to stimulate the development of association chains. Tool use and auditory gestures involve motor processing of the forelimbs, which is associated with the evolution of vertebrate vocal communication. The production, perception, and mimicry of TUS may have resulted in a limited number of vocalizations or protowords that were associated with tool use. A new way to communicate about tools, especially when out of sight, would have had selective advantage. A gradual change in acoustic properties and/or meaning could have resulted in arbitrariness and an expanded repertoire of words. Humans have been increasingly exposed to TUS over millions of years, coinciding with the period during which spoken language evolved. ISOL and tool-use-related sound are worth further exploration.","2015-09","2023-07-05 07:43:00","2023-07-19 11:28:49","2023-07-05 07:43:00","993-1005","","5","18","","Anim Cogn","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GRZGSGGB","journalArticle","2023","Rubab, Sadia; Yu, Lingyun; Tang, Junxiu; Wu, Yingcai","Exploring Effective Relationships Between Visual-Audio Channels in Data Visualization","Journal of Visualization","","1343-8875, 1875-8975","10.1007/s12650-023-00909-3","https://link.springer.com/10.1007/s12650-023-00909-3","In recent years, there has been a growing trend towards taking advantage of audio--visual representations. Previous research has aimed at improving users’ performance and engagement with these representations. The attainment of these benefits primarily depends on the effectiveness of audio--visual relationships used to represent the data. However, the visualization field yet lacks an empirical study that guides the effective relationships. Given the compatibility effect between visual and auditory channels, this research presents the effectiveness of four audio channels (timbre, pitch, loudness, and tempo) with six visual channels (spatial position, color, position, length, angle, and area). In six experiments, one per visual channel, we observed how each audio channel, when used with a visual channel, impacted users’ ability to perform the differentiation or similarity task accurately. Each experiment provided the ranking of audio channels along a visual channel. Central to our experiments was the evaluation at two stages, and accordingly, we identified the effectiveness. Our results showed that timbre, with spatial position and color, aided in more accurate target identification than the three other audio channels. With position and length, pitch allowed a more accurate judgment of the magnitude of data than loudness and tempo but was less accurate than the other two channels along angle and area. Overall, our experiments showed that the choice of representation methods and tasks had impacted the effectiveness of audio channels.","2023-08","2023-07-05 07:43:00","2023-07-20 06:50:02","2023-07-05 07:43:00","937-956","","4","26","","J Vis","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EHXIRA8S","journalArticle","2017","Buzzi, Maria Claudia; Buzzi, Marina; Leporini, Barbara; Trujillo, Amaury","Analyzing visually impaired people’s touch gestures on smartphones","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-016-3594-9","http://link.springer.com/10.1007/s11042-016-3594-9","We present an analysis of how visually impaired people perform gestures on touch-screen smartphones and report their preferences, explaining the procedure and technical implementation that we followed to collect gesture samples. To that end, we recruited 36 visually impaired participants and divided them into two main groups of low-vision and blind people respectively. We then examined their touch-based gesture preferences in terms of number of strokes, multi-touch, and shape angle, as well as their execution in geometric, kinematic and relative terms. For this purpose, we developed a wireless system to simultaneously record sample gestures from several participants, with the possibility of monitoring the capture process. Our results are consistent with previous research regarding the preference of visually impaired users for simple gestures: with one finger, a single stroke, and in one or two cardinal directions. Of the two groups of participants, blind people are less consistent with multi-stroke gestures. In addition, they are more likely than low-vision people to go outside the bounds of the display in the absence of its physical delimitation of, especially with multi-touch gestures. In the case of more complex gestures, rounded shapes are greatly preferred to angular ones, especially by blind people, who have difficulty performing straight gestures with steep or right angles. Based on these results and on previous related research, we offer suggestions to improve gesture accessibility of handheld touchscreen devices.","2017-02","2023-07-05 07:43:00","2023-07-21 04:31:47","2023-07-05 07:43:00","5141-5169","","4","76","","Multimed Tools Appl","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2DZVHFDT","bookSection","2014","Hülsmann, Adrian; Maicher, Julian","HOUDINI: Introducing Object Tracking and Pen Recognition for LLP Tabletops","Human-Computer Interaction. Advanced Interaction Modalities and Techniques","978-3-319-07229-6 978-3-319-07230-2","","","http://link.springer.com/10.1007/978-3-319-07230-2_23","Tangible objects on a \tabletop offer a lot of different opportunities to interact with an application. Most of the current tabletops are built using optical tracking principles and especially LLP tabletops provide very good tracking results for touch input. In this paper we introduce HOUDINI as a method for LLP object tracking and pen recognition, which is based on three different sizes of touch points that help us to identify touch points belonging to fingers, objects and pens. As a result, the whole recognition process is performed at the level of touch information rather than frame by frame image analysis. This leads to a very efficient and reliable tracking, thus allowing the objects to be moved very fast without being lost.","2014","2023-07-05 07:43:00","2023-07-20 06:30:31","2023-07-05 07:43:00","234-244","","","8511","","","HOUDINI","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-07230-2_23","","/Users/minsik/Zotero/storage/VKFKH68Z/Hülsmann and Maicher - 2014 - HOUDINI Introducing Object Tracking and Pen Recog.pdf","","","","Kurosu, Masaaki","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Kobsa, Alfred; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Doug; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4I84YDUF","journalArticle","2018","Marchetti, Emanuela; Valente, Andrea","Interactivity and multimodality in language learning: the untapped potential of audiobooks","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-017-0549-5","http://link.springer.com/10.1007/s10209-017-0549-5","In this work we present three case studies, involving classes in primary and secondary schools, in Denmark. The studies, conducted in the past 2 years, show how audio contents can be generated and shared among teachers and learners, how audio materials can be made more interactive to offer fruition similar to that of digital games, and how language learning can benefit from adding a social dimension to audiobooks. All case studies were conducted in a user centered fashion and build on social semiotics, in which interactive audiobooks are seen as providing new ways to receive, interpret and share literary texts. Local primary and secondary schools were involved in ethnographic user studies and qualitative evaluations with semi-functioning prototypes. In the main case study presented, social interaction was chosen as key feature to allow high-school students and teachers to annotate audiobooks, then share and comment on the annotations; the social context in this case is a digitally-augmented English teaching class. To better investigate the potential of sharable audiobook annotations we also created a mockup supporting the workflow of the main case study, using standard YouTube annotations and freely available audiobooks. The findings and technical solutions explored in the three studies are the basis for design guidelines aiming at making audiobooks interactive and better integrated in learning contexts.","2018-06","2023-07-05 07:43:00","2023-07-21 05:11:52","2023-07-05 07:43:00","257-274","","2","17","","Univ Access Inf Soc","Interactivity and multimodality in language learning","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/TF46TL2E/Marchetti and Valente - 2018 - Interactivity and multimodality in language learni.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EB6NUHS7","journalArticle","2006","Chen, Xiaoyu; Tremaine, Marilyn; Lutz, Robert; Chung, Jae-woo; Lacsina, Patrick","AudioBrowser: a mobile browsable information access for the visually impaired","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-006-0019-y","http://link.springer.com/10.1007/s10209-006-0019-y","","2006-06","2023-07-05 07:43:00","2023-07-05 07:43:00","2023-07-05 07:43:00","4-22","","1","5","","Univ Access Inf Soc","AudioBrowser","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5KZYIYKZ","journalArticle","2022","Buehler, Markus J.","DeepFlames: Neural network-driven self-assembly of flame particles into hierarchical structures","MRS Communications","","2159-6867","10.1557/s43579-022-00171-y","https://link.springer.com/10.1557/s43579-022-00171-y","The spontaneous assembly of materials from elementary building blocks is one of the most intriguing natural phenomena. Conventional modeling relies physical approaches to examine such processes. In this paper, a framework is proposed to offer an alternative paradigm, via the use of deep learning, and specifically the use of generative adversarial models as well as a combination of natural language processing and transformer neural nets to create hierarchical assemblies of building blocks. We study the assembly of elementary flame particles into hierarchical materials with features across scales, illustrating the Universality–Diversity Principle (UDP), and create novel material using additive manufacturing.","2022-04","2023-07-05 07:43:00","2023-07-21 04:30:32","2023-07-05 07:43:00","257-265","","2","12","","MRS Communications","DeepFlames","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CXV2UXKV","bookSection","2010","Tuuri, Kai","Gestural Attributions as Semantics in User Interface Sound Design","Gesture in Embodied Communication and Human-Computer Interaction","978-3-642-12552-2 978-3-642-12553-9","","","http://link.springer.com/10.1007/978-3-642-12553-9_23","This paper proposes a gesture-based approach to user interface sound design, which utilises projections of body movements in sounds as meaningful attributions. The approach is founded on embodied conceptualisation of human cognition and it is justified through a literature review on the subject of interpersonal action understanding. According to the resulting hypothesis, stereotypical gestural cues, which correlate with, e.g., a certain communicative intention, represent specific non-linguistic meanings. Based on this theoretical framework, a model of a process is also outlined where stereotypical gestural cues are implemented in sound design","2010","2023-07-05 07:43:00","2023-07-20 00:08:43","2023-07-05 07:43:00","257-268","","","5934","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12553-9_23","","","","","","Kopp, Stefan; Wachsmuth, Ipke","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VXYC4EBS","bookSection","2010","Okada, Noriko; Miki, Mitsunori; Hiroyasu, Tomoyuki; Yoshimi, Masato","Classified-Chime Sound Generation Support System Using an Interactive Genetic Algorithm","Artifical Intelligence and Soft Computing","978-3-642-13231-5 978-3-642-13232-2","","","http://link.springer.com/10.1007/978-3-642-13232-2_21","","2010","2023-07-05 07:43:00","2023-07-05 07:43:00","2023-07-05 07:43:00","173-180","","","6114","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-13232-2_21","","","","","","Rutkowski, Leszek; Scherer, Rafał; Tadeusiewicz, Ryszard; Zadeh, Lotfi A.; Zurada, Jacek M.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5VL7T6GD","bookSection","2008","Devallez, Delphine; Rocchesso, Davide; Fontana, Federico","An Audio-Haptic Interface Concept Based on Depth Information","Haptic and Audio Interaction Design","978-3-540-87882-7 978-3-540-87883-4","","","http://link.springer.com/10.1007/978-3-540-87883-4_11","We present an interaction tool based on rendering distance cues for ordering sound sources in depth. The user interface consists of a linear position tactile sensor made by conductive material. The touch position is mapped onto the listening position on a rectangular virtual membrane, modeled by a bidimensional Digital Waveguide Mesh and providing distance cues. Spatialization of sound sources in depth allows a hierarchical display of multiple audio streams, as in auditory menus. Besides, the similar geometries of the haptic interface and the virtual auditory environment allow a direct mapping between the touch position and the listening position, providing an intuitive and continuous interaction tool for auditory navigation.","2008","2023-07-05 07:43:00","2023-07-20 00:14:39","2023-07-05 07:43:00","102-110","","","5270","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-87883-4_11","","","","","","Pirhonen, Antti; Brewster, Stephen","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RNMYFI44","bookSection","2005","Wieczorkowska, Alicja A.; Raś, Zbigniew W.","Do We Need Automatic Indexing of Musical Instruments?","Intelligent Media Technology for Communicative Intelligence","978-3-540-29035-3 978-3-540-31738-8","","","http://link.springer.com/10.1007/11558637_24","Increasing growth and popularity of multimedia resources available on the Web brought the need to provide new, more advanced tools needed for their search. However, searching through multimedia data is highly non-trivial task that requires content-based indexing of the data. Our research is focused on automatic extraction of information about the sound timbre, and indexing sound data with information about musical instrument(s) playing in a given segment. Our goal is to perform automatic classification of musical instrument sound from real recordings for broad range of sounds, independently on the fundamental frequency of the sound.","2005","2023-07-05 07:43:00","2023-07-20 06:35:40","2023-07-05 07:43:00","239-245","","","3490","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11558637_24","","","","","","Bolc, Leonard; Michalewicz, Zbigniew; Nishida, Toyoaki","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A3E4YU69","bookSection","2006","Röber, Niklas; Huber, Cornelius; Hartmann, Knut; Feustel, Matthias; Masuch, Maic","Interactive Audiobooks: Combining Narratives with Game Elements","Technologies for Interactive Digital Storytelling and Entertainment","978-3-540-49934-3 978-3-540-49935-0","","","http://link.springer.com/10.1007/11944577_36","The authoring and the design of immersive, non-linear plots remains one of the main challenges in interactive digital storytelling. This paper introduces the concept of interactive audiobooks, which combines the potential of complex (non-)linear narratives (e.g. books and radio plays) with interactive elements from computer games. The design concentrates on a flexible degree of interaction, in a way that the listener’s experience ranges between a passive listening to an interactive audio-only computer game. In this paper we discuss the story-engine used in interactive audiobooks, as well as present an authoring framework along several design guidelines to create them. Finally, we demonstrate the capabilities of our system with an adaptation of a short story from Edgar Allen Poe.","2006","2023-07-05 07:43:00","2023-07-21 05:04:00","2023-07-05 07:43:00","358-369","","","4326","","","Interactive Audiobooks","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11944577_36","","","","","","Göbel, Stefan; Malkewitz, Rainer; Iurgel, Ido","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EJ3HKRJV","journalArticle","2010","Shimomura, Yayoi; Hvannberg, Ebba Thora; Hafsteinsson, Hjalmtyr","Accessibility of audio and tactile interfaces for young blind people performing everyday tasks","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-009-0183-y","http://link.springer.com/10.1007/s10209-009-0183-y","Increasingly, computers are becoming tools of communication, information exploring and studying for young people, regardless of their abilities. Scientists have been building knowledge on how blind people can substitute hearing or touch for sight or how the combination of senses, i.e., multimodalities, can provide the user with an effective way of exploiting the power of computers. Evaluation of such multimodal user interfaces in the right context, i.e., appropriate users, tasks, tools and environment, is essential to give designers accurate feedback on blind users’ needs. This paper presents a study on how young blind people use computers for everyday tasks with the aids of assistive technologies, aiming to understand what hindrances they encounter when interacting with a computer using individual senses, and what supports them. A common assistive technology is a screen reader, producing output to a speech synthesizer or a Braille display. Those two modes are often used together, but the research studied how visually impaired students interact with computers using either form, i.e., a speech synthesizer or a Braille display. A usability test has been performed to assess blind grade-school students’ ability to carry out common tasks with the help of a computer, including solving mathematical problems, navigating the web, communicating with e-mail and using word processing. During the usability tests, students were allowed to use either auditory mode or tactile mode. Although blind users most commonly use a speech synthesizer (audio), the results indicate that this was not always the most suitable modality. While the effectiveness of the Braille display (tactile user interface) to accomplish certain tasks was similar to that of the audio user interface, the users’ satisfaction rate was higher. The contribution of this work lies in answering two research questions by analysing two modes of interaction (tactile and speech), while carrying out tasks of varying genre, i.e., web searching, collaboration through e-mail, word processing and mathematics. A second contribution of this work is the classification of observations into four categories: usability and accessibility, software fault, cognitive mechanism and learning method. Observations, practical recommendations and open research problems are then presented and discussed. This provides a framework for similar studies in the future. A third contribution of this work is the elaboration of practical recommendations for user interface designers and a research agenda for scientists.","2010-11","2023-07-05 07:43:00","2023-07-21 05:12:50","2023-07-05 07:43:00","297-310","","4","9","","Univ Access Inf Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9MJF229T","journalArticle","1980","Luyster, Robert","King ego and the double-sex dancer","Journal of Religion and Health","","0022-4197, 1573-6571","10.1007/BF01006424","http://link.springer.com/10.1007/BF01006424","The god representative of the energy of life in ancient Greece was Dionysos. As such, he was bisexual, for life in its wholeness contains all the individual forms it generates. The emphasis upon androgyny and sexual fusion in his cult results from man's effort to realize the wholeness of that life of which he is merely a fragment. The achievement of that fullness, life's experience of itself, is ecstasy; suffering, on the other hand, is fundamentally life's alienation from itself. In the myths and rituals of Dionysos we witness the dialectic of affirmation and denial of the life force.","1980","2023-07-05 07:44:26","2023-07-20 06:49:27","2023-07-05 07:44:26","121-129","","2","19","","J Relig Health","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"69K2REIA","journalArticle","2021","Ranganathan, Rajiv; Tomlinson, Aimee D.; Lokesh, Rakshith; Lin, Tzu-Hsiang; Patel, Priya","A tale of too many tasks: task fragmentation in motor learning and a call for model task paradigms","Experimental Brain Research","","0014-4819, 1432-1106","10.1007/s00221-020-05908-6","http://link.springer.com/10.1007/s00221-020-05908-6","Motor learning encompasses a broad set of phenomena that requires a diverse set of experimental paradigms. However, excessive variation in tasks across studies creates fragmentation that can adversely affect the collective advancement of knowledge. Here, we show that motor learning studies tend toward extreme fragmentation in the choice of tasks, with almost no overlap between task paradigms across studies. We argue that this extreme level of task fragmentation poses serious theoretical and methodological barriers to advancing the field. To address these barriers, we propose the need for developing common ‘model’ task paradigms which could be widely used across labs. Combined with the open sharing of methods and data, we suggest that these model task paradigms could be an important step in increasing the robustness of the motor learning literature and facilitate the cumulative process of science.","2021-01","2023-07-05 07:44:26","2023-07-20 00:05:20","2023-07-05 07:44:26","1-19","","1","239","","Exp Brain Res","A tale of too many tasks","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L4KPGDSS","journalArticle","2019","Issachar, Gil; Bar-Shalita, Tami; Baruch, Yair; Horing, Bar; Portnoy, Sigal","Design and Implementation of a Novel Subject-Specific Neurofeedback Evaluation and Treatment System","Annals of Biomedical Engineering","","0090-6964, 1573-9686","10.1007/s10439-019-02228-x","http://link.springer.com/10.1007/s10439-019-02228-x","Electroencephalography (EEG)-based neurofeedback (NF) is a safe, non-invasive, non-painful method for treating various conditions. Current NF systems enable the selection of only one NF parameter, so that two parameters cannot be feedback simultaneously. Consequently, the ability to individually-tailor the treatment to a patient is limited, and treatment efficiency may therefore be compromised. We aimed to design, implement and test an all-in-one, novel, computerized platform for closed-loop NF treatment, based on principles from learning theories. Our prototype performs numeric evaluation based on quantifying resting EEG and event-related EEG responses to various sensory stimuli. The NF treatment was designed according to principles of efficient learning, and implemented as a gradual, patient-adaptive 1D or 2D computer game, that utilizes automatic EEG feature extraction. Verification was performed as we compared the mean area under curve (AUC) of the theta band of a dozen subjects staring at a wall or performing the NF. Most of the subjects (75%) increased their theta band AUC during the NF session compared with the trial staring at the wall (p = 0.041). Our system enables multiple feature selection and its machine learning capabilities allow an accurate discovery of patient-specific biomarkers and treatment targets. Its novel characteristics may allow for improved evaluation of patients and treatment outcomes.","2019-05","2023-07-05 07:44:26","2023-07-19 11:30:22","2023-07-05 07:44:26","1203-1211","","5","47","","Ann Biomed Eng","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZGJ5WIYP","bookSection","2006","Marcus, Aaron","Cross-Cultural User-Experience Design","Diagrammatic Representation and Inference","978-3-540-35623-3 978-3-540-35624-0","","","http://link.springer.com/10.1007/11783183_4","Designers of information visualization and user interfaces must take account of culture in the design of metaphors, mental models, navigation, interaction, and appearance. Culture models define dimensions of difference and similarity among groups of people regarding signs, rituals, heroes/heroines, and values. Examples on the Web reveal these dimensions. Developers will increasingly need to take into account culture and other factors in development to better ensure usability, usefulness, and appeal.","2006","2023-07-05 07:44:26","2023-07-19 23:56:19","2023-07-05 07:44:26","16-24","","","4045","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11783183_4","","","","","","Barker-Plummer, Dave; Cox, Richard; Swoboda, Nik","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RGBR5QC8","bookSection","2010","Ortega-González, Vladimir; Garbaya, Samir; Merienne, Frédéric","Reducing Reversal Errors in Localizing the Source of Sound in Virtual Environment without Head Tracking","Haptic and Audio Interaction Design","978-3-642-15840-7 978-3-642-15841-4","","","http://link.springer.com/10.1007/978-3-642-15841-4_10","This paper presents a study about the effect of using additional audio cueing and Head-Related Transfer Function (HRTF) on human performance in sound source localization task without using head movement. The existing techniques of sound spatialization generate reversal errors. We intend to reduce these errors by introducing sensory cues based on sound effects. We conducted and experimental study to evaluate the impact of additional cues in sound source localization task. The results showed the benefit of combining the additional cues and HRTF in terms of the localization accuracy and the reduction of reversal errors. This technique allows significant reduction of reversal errors compared to the use of the HRTF separately. For instance, this technique could be used to improve audio spatial alerting, spatial tracking and target detection in simulation applications when head movement is not included.","2010","2023-07-05 07:44:26","2023-07-20 00:16:14","2023-07-05 07:44:26","85-96","","","6306","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-15841-4_10","","/Users/minsik/Zotero/storage/VW574XAS/Ortega-González et al. - 2010 - Reducing Reversal Errors in Localizing the Source .pdf","","","","Nordahl, Rolf; Serafin, Stefania; Fontana, Federico; Brewster, Stephen","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A22ECMQF","journalArticle","2018","Black, David; Hahn, Horst K.; Kikinis, Ron; Wårdell, Karin; Haj-Hosseini, Neda","Auditory display for fluorescence-guided open brain tumor surgery","International Journal of Computer Assisted Radiology and Surgery","","1861-6410, 1861-6429","10.1007/s11548-017-1667-5","http://link.springer.com/10.1007/s11548-017-1667-5","Purpose—Protoporphyrin (PpIX) fluorescence allows discrimination of tumor and normal brain tissue during neurosurgery. A handheld fluorescence (HHF) probe can be used for spectroscopic measurement of 5-ALA-induced PpIX to enable objective detection compared to visual evaluation of fluorescence. However, current technology requires that the surgeon either views the measured values on a screen or employs an assistant to verbally relay the values. An auditory feedback system was developed and evaluated for communicating measured fluorescence intensity values directly to the surgeon. Methods—The auditory display was programmed to map the values measured by the HHF probe to the playback of tones that represented three fluorescence intensity ranges and one error signal. Ten persons with no previous knowledge of the application took part in a laboratory evaluation. After a brief training period, participants performed measurements on a tray of 96 wells of liquid fluorescence phantom and verbally stated the perceived measurement values for each well. The latency and accuracy of the participants’ verbal responses were recorded. The long-term memorization of sound function was evaluated in a second set of 10 participants 2–3 and 712 days after training. Results—The participants identified the played tone accurately for 98% of measurements after training. The median response time to verbally identify the played tones was 2 pulses. No correlation was found between the latency and accuracy of the responses, and no significant correlation with the musical proficiency of the participants was observed on the function responses. Responses for the memory test were 100% accurate. Conclusion—The employed auditory display was shown to be intuitive, easy to learn and remember, fast to recognize, and accurate in providing users with measurements of fluorescence intensity or error signal. The results of this work establish a basis for implementing and further evaluating auditory displays in clinical scenarios involving fluorescence guidance and other areas for which categorized auditory display could be useful.","2018-01","2023-07-05 07:44:26","2023-07-20 06:40:43","2023-07-05 07:44:26","25-35","","1","13","","Int J CARS","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/UBG7BRH2/Black et al. - 2018 - Auditory display for fluorescence-guided open brai.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QYH7CJS8","journalArticle","2017","Dawid, Herbert; Decker, Reinhold; Hermann, Thomas; Jahnke, Hermann; Klat, Wilhelm; König, Rolf; Stummer, Christian","Management science in the era of smart consumer products: challenges and research perspectives","Central European Journal of Operations Research","","1435-246X, 1613-9178","10.1007/s10100-016-0436-9","http://link.springer.com/10.1007/s10100-016-0436-9","Smart products not only provide novel functionalities, but also may establish new business models, markets, or distribution channels, strengthen relationships with consumers, and/or add smart remote services. While many technical obstacles of such products have already been overcome, the broad market dissemination of smart products still poses some vital managerial challenges for decision makers. In this paper, we outline the technical potential and future trends of smart consumer products, discuss economic challenges in four scopes, namely, preference-based new product development, market analysis, supply chain design, and industry development, and, in particular, we highlight research perspectives for management science in this promising field.","2017-03","2023-07-05 07:44:26","2023-07-19 23:37:46","2023-07-05 07:44:26","203-230","","1","25","","Cent Eur J Oper Res","Management science in the era of smart consumer products","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/6X69VUPD/Dawid et al. - 2017 - Management science in the era of smart consumer pr.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"45ZDUZHP","journalArticle","2020","Simonis, Sarah; Canfyn, Michaël; Van Dijck, Anton; Van Havere, Tina; Deconinck, Eric; Blanckaert, Peter; Gremeaux, Lies","Awareness of users and motivational factors for using new psychoactive substances in Belgium","Harm Reduction Journal","","1477-7517","10.1186/s12954-020-00393-0","https://harmreductionjournal.biomedcentral.com/articles/10.1186/s12954-020-00393-0","Abstract                            Background               Few data on motivations for using new psychoactive substances (NPS) are available. However, the cost, the legal status, and their accessibility through channels like internet contributed to the popularity of NPS. The objective of this article are first to gain a deeper understanding of the culture surrounding NPS in Belgium and second to define the awareness of the users concerning the content of the NPS they are consuming.                                         Methods               Snowball sampling and partners in the drug demand reduction field were used as a gateway in order to reach a heterogeneous study population. In total, 45 users were recruited and in-depth interviews were conducted. The personal experiences of NPS users and their needs for support along the continuum of care were explored through an interview guideline, while subjects were given the opportunity to deposit a NPS sample for forensic analysis in a recognized laboratory.                                         Results               A diversity of profiles was found among NPS users but also a wide diversity in the motives to consume NPS: personal reasons such as pleasure, mind exploration, being connected to others, or out of curiosity, but also external reasons such as price, accessibility or the specific effects procured by certain NPS. The results showed as well that a majority of NPS users seem to be aware of the substances they are using.                                         Conclusion               Understanding the motivations of use is of importance to determine which type of NPS targeted interventions are adapted to different profiles of users.","2020-12","2023-07-05 07:44:26","2023-07-05 07:44:26","2023-07-05 07:44:26","52","","1","17","","Harm Reduct J","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/2MUBBDXG/Simonis et al. - 2020 - Awareness of users and motivational factors for us.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A2HQ52Y9","bookSection","2011","Mertens, Alexander; Przybysz, Philipp; Groß, Alexander; Koch-Koerfges, David; Nick, Claudia; Kaethner, Martin; Schlick, Christopher M.","Age-Adapted Psychoacoustics: Target Group Oriented Sound Schemes for the Interaction with Telemedical Systems","Universal Access in Human-Computer Interaction. Applications and Services","978-3-642-21656-5 978-3-642-21657-2","","","https://link.springer.com/10.1007/978-3-642-21657-2_44","For the interaction of elderly people with IT systems, an ergonomic and intuitive design as well as self-explanatory handling processes are particularly relevant. Herein adequate acoustic feedback, which accounts for the specific needs and experience of the target group, provides high efficacy and acceptance of technology with regard to Human-Computer Interaction. In this study, five different types of sound schemes are evaluated on their intuitive understanding and memorization by older users. The participants assign audible feedback to typical applications of telemedical monitoring and have to reminisce given classifications. This approach makes it possible to elicit the homogeneity of psychoacoustic models of elderly people and give recommendations for the design of acoustic feedback mechanisms for this audience. As a result, the use of familiar sounds from everyday situations has been found significantly better in terms of the consistency of the intuitive mapping and memorization for use cases in a telemedical context, in comparison to synthetic sounds that obtain their semantic denotation just by convention.","2011","2023-07-05 07:44:26","2023-07-21 05:09:19","2023-07-05 07:44:26","406-415","","","6768","","","Age-Adapted Psychoacoustics","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-21657-2_44","","","","","","Stephanidis, Constantine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WRFZS7MH","bookSection","2003","Djennane, Safia","3D-Audio News Presentation Modeling","Universal Access Theoretical Perspectives, Practice, and Experience","978-3-540-00855-2 978-3-540-36572-3","","","http://link.springer.com/10.1007/3-540-36572-9_22","With the emergence of the mobile Internet combined with wearable personal computing, we are now entering a new information era where PCs are self-adapting their resources to human bodies, minds and preferences, prevailing a more effective work environment. In this new world, working effectively is inextricably related to accessing reliable information sources when needed. Therefore, people eager to stay connected, consume daily information in a myriad of forms: news, weather, business, road/traffic reports, voicemails, emails, as well as information associated to their daily activities or interests. In this paper, we propose innovative UI information presentations based on three-dimensional (3D) audio modeling. In this framework, we illustrate how news, weather and business reports are extracted, spatialized and presented to end-users using 3D audio modality.","2003","2023-07-05 07:44:26","2023-07-21 05:13:15","2023-07-05 07:44:26","280-286","","","2615","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-36572-9_22","","","","","","Carbonell, Noëlle; Stephanidis, Constantine","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DRXXRNSK","journalArticle","2014","Mi, Na; Cavuoto, Lora A.; Benson, Kenneth; Smith-Jackson, Tonya; Nussbaum, Maury A.","A heuristic checklist for an accessible smartphone interface design","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-013-0321-4","http://link.springer.com/10.1007/s10209-013-0321-4","Smartphone technology has evolved into a multi-functional device with advanced capabilities, but this mobile technology remains inaccessible to many individuals with visual impairments or upper extremity disabilities. This paper provides a heuristic checklist for accessible smartphone interface design, developed through reviewing existing design standards and guidelines and validating these guidelines with user involvement. Specifically, a set of preliminary user requirements (59 items) was extracted from existing standards, guidelines, and user requirements regarding mobile handheld device accessibility. Subsequently, the requirement set was filtered using a participatory method and then integrated to create an operational version of design guidelines. These guidelines were then used in a heuristic evaluation and usability testing on high-fidelity prototypes produced by a commercial manufacturer. A heuristic checklist for designing accessible smartphones was formed, which may also be applicable to other touchscreen handheld devices (e.g., printer screen) in terms of accessibility features. The initial set of 59 user requirements was re-organized into 44 statements in six general categories: mechanical controls, display, speech and general operation controls, audio feedback controls, touch-operated controls, and others. Using results from both qualitative and quantitative methods provides support, though with some limitations, for this accessibility checklist. This checklist is intended as a practical design support tool for use in early design phases of handheld products. A number of challenges and limitations are discussed as well.","2014-11","2023-07-05 07:44:26","2023-07-21 05:12:00","2023-07-05 07:44:26","351-365","","4","13","","Univ Access Inf Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GAI6RGSF","bookSection","2000","Semwal, Sudhanshu Kumar; Evans-Kamp, Debra Lee","Virtual Environments for Visually Impaired","Virtual Worlds","978-3-540-67707-9 978-3-540-45016-0","","","http://link.springer.com/10.1007/3-540-45016-5_25","We provide a systematic study for generating interactive, virtual environments for the blind. We present our system as a tool for shape recognition and mobility training for the blind. In our system, head movement can be detected to indicate horizontal and vertical movements. Audio feedback is used for reinforcement. Our experiment for shape learning can guide the user in tracing the surface of a sphere by using the audio feedback. We also present a compelling case for using force feedback devices for visually impaired, and our experience with the PHANToM(TM) force feedback device is summarized. A detailed survey of present research is also presented.","2000","2023-07-05 07:44:26","2023-07-21 05:15:15","2023-07-05 07:44:26","270-285","","","1834","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-45016-5_25","","","","","","Heudin, Jean-Claude","Goos, G.; Hartmanis, J.; Van Leeuwen, J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GX4NTQLC","bookSection","2009","Bologna, Guido; Deville, Benoît; Pun, Thierry","Blind Navigation along a Sinuous Path by Means of the See ColOr Interface","Bioinspired Applications in Artificial and Natural Computation","978-3-642-02266-1 978-3-642-02267-8","","","http://link.springer.com/10.1007/978-3-642-02267-8_26","The See ColOr interface transforms a small portion of a coloured video image into sound sources represented by spatialized musical instruments. This interface aims at providing visually impaired people with a capability of perception of the environment. In this work, the purpose is to verify the hypothesis that it is possible to use sounds from musical instruments to replace colour. Compared to state of the art devices, a quality of the See ColOr interface is that it allows the user to receive a feed-back auditory signal from the environment and its colours, promptly. An experiment based on a head mounted camera has been performed. Specifically, this experiment is related to outdoor navigation for which the purpose is to follow a sinuous path. Our participants successfully went along a red serpentine path for more than 80 meters.","2009","2023-07-05 07:44:26","2023-07-19 23:34:53","2023-07-05 07:44:26","235-243","","","5602","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-02267-8_26","","","","","","Mira, José; Ferrández, José Manuel; Álvarez, José R.; De La Paz, Félix; Toledo, F. Javier","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IXKCGSLW","bookSection","2015","Erkut, Cumhur; Rajala-Erkut, Anu; Dahl, Sofia","Exploring Felt Qualities of Embodied Interaction with Movement and Sound","Arts and Technology","978-3-319-18835-5 978-3-319-18836-2","","","http://link.springer.com/10.1007/978-3-319-18836-2_10","","2015","2023-07-05 07:44:26","2023-07-05 07:44:26","2023-07-05 07:44:26","77-85","","","145","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering DOI: 10.1007/978-3-319-18836-2_10","","","","","","Brooks, Anthony Lewis; Ayiter, Elif; Yazicigil, Onur","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IKPH6YSH","bookSection","2002","Baloian, Nelson; Luther, Wolfram","Visualization for the Mind’s Eye","Software Visualization","978-3-540-43323-1 978-3-540-45875-3","","","http://link.springer.com/10.1007/3-540-45875-1_28","Software visualization has been almost exclusively tackled from the visual point of view; this means visualization occurs exclusively through the visual channel. This approach has its limitations. Considering previous work for blind people we propose that complementing usual approaches with those techniques used to develop interfaces for non-sighted people can enhance user awareness of logical structures or data types using different perception channels. To achieve better comprehension, we deal with new or augmented interfaces built on top of standard systems for data visualization and algorithm animation. The notion of specific concept keyboards is introduced. As a consequence, modern information and learning systems can be designed in such a way that not only sighted but also blind users can navigate within these systems.","2002","2023-07-05 07:44:26","2023-07-21 05:00:59","2023-07-05 07:44:26","354-367","","","2269","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-45875-1_28","","/Users/minsik/Zotero/storage/S6U8PI9T/Baloian and Luther - 2002 - Visualization for the Mind’s Eye.pdf","","","","Diehl, Stephan","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C6H7D4EC","bookSection","2014","Ford, Rebecca; Penn, Joe; Liu, Yu-Chieh; Nixon, Ken; Cronje, Willie; McCulloch, Malcolm","User-Centred Design of an Audio Feedback System for Power Demand Management","Design, User Experience, and Usability. User Experience Design for Everyday Life Applications and Services","978-3-319-07634-8 978-3-319-07635-5","","","http://link.springer.com/10.1007/978-3-319-07635-5_51","Low-income houses in South Africa are supplied with a pre-payment meter and a circuit breaker that trips at a low power level (about 20A, 4.5kW), resulting in many nuisance trips. Four categories of audio cues, each being able to represent five levels of power consumption, are assessed. A survey of 62 people was conducted. The numerical analysis of the results and the perceptions of the respondents both indicate that the use of changing tempo and texture is the most effective at conveying feedback information on the power consumption in the home.","2014","2023-07-05 07:44:26","2023-07-19 23:55:50","2023-07-05 07:44:26","530-541","","","8519","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-07635-5_51","","/Users/minsik/Zotero/storage/EECRG6X7/Ford et al. - 2014 - User-Centred Design of an Audio Feedback System fo.pdf","","","","Marcus, Aaron","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z79KI63Q","bookSection","2000","Nesbitt, Keith","Designing Multi-sensory Models for Finding Patterns in Stock Market Data","Advances in Multimodal Interfaces — ICMI 2000","978-3-540-41180-2 978-3-540-40063-9","","","http://link.springer.com/10.1007/3-540-40063-X_4","The rapid increase in available information has lead to many attempts to automatically locate patterns in large, abstract, multi-attributed information spaces. These techniques are often called ‘Data Mining’ and have met with varying degrees of success. An alternative approach to automatic pattern detection is to keep the user in the ‘exploration loop’. A domain expert is often better able to search data for relationships. Furthermore, it is now possible to construct user interfaces that provide multi-sensory interactions. For example, interfaces can be designed which utilise 3D visual spaces and also provide auditory and haptic feedback. These multi-sensory interfaces may assist in the interpretation of abstract information spaces by providing models that map different attributes of data to different senses. While this approach has the potential to assist in exploring these large information spaces what is unclear is how to choose the best models to define mappings between the abstract information and the human sensory channels. This paper describes some simple guidelines based on real world analogies for designing these models. These principles are applied to the problem of finding new patterns in stock market data.","2000","2023-07-05 07:44:26","2023-07-19 11:12:39","2023-07-05 07:44:26","24-31","","","1948","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-40063-X_4","","","","","","Tan, Tieniu; Shi, Yuanchun; Gao, Wen","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZJGIPK68","journalArticle","2016","Halim, Zahid; Kalsoom, Rizwana; Bashir, Shariq; Abbas, Ghulam","Artificial intelligence techniques for driving safety and vehicle crash prediction","Artificial Intelligence Review","","0269-2821, 1573-7462","10.1007/s10462-016-9467-9","http://link.springer.com/10.1007/s10462-016-9467-9","Accident prediction is one of the most critical aspects of road safety, whereby an accident can be predicted before it actually occurs and precautionary measures taken to avoid it. For this purpose, accident prediction models are popular in road safety analysis. Artificial intelligence (AI) is used in many real world applications, especially where outcomes and data are not same all the time and are influenced by occurrence of random changes. This paper presents a study on the existing approaches for the detection of unsafe driving patterns of a vehicle used to predict accidents. The literature covered in this paper is from the past 10 years, from 2004 to 2014. AI techniques are surveyed for the detection of unsafe driving style and crash prediction. A number of statistical methods which are used to predict the accidents by using different vehicle and driving features are also covered in this paper. The approaches studied in this paper are compared in terms of datasets and prediction performance. We also provide a list of datasets and simulators available for the scientific community to conduct research in the subject domain. The paper also identifies some of the critical open questions that need to be addressed for road safety using AI techniques.","2016-10","2023-07-05 07:44:26","2023-07-19 11:34:27","2023-07-05 07:44:26","351-387","","3","46","","Artif Intell Rev","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8RLKIUWI","journalArticle","1997","Hollier, M. P.; Rimmell, A. N.; Burraston, D.","Spatial audio technology for telepresence","BT Technology Journal","","1573-1995","10.1023/A:1018675327815","https://doi.org/10.1023/A:1018675327815","As people start to exploit new telepresence technologies to meet and work, they will be able to exploit all of their senses as they transmit, receive, and monitor information. An essential part of such three-dimensional spaces is the audio landscape. People are able to detect a wide variety of sounds and separate them in space. Spatial separation improves the detection and intelligibility of speech from multiple talkers, and enables simultaneous monitoring of multiple information streams through the use of multiple alert sounds. On-going research at BT Laboratories into spatial audio has resulted in a number of leading edge demonstrations and patent applications. This paper introduces the technologies employed to create spatial audio for real-time synthetic worlds including single and multiple users, and non-ideal acoustic environments.","1997-10-01","2023-07-05 07:44:35","2023-07-05 07:44:35","2023-07-05 07:44:35","33-41","","4","15","","BT Technology Journal","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/PVS5AEPY/Hollier et al. - 1997 - Spatial audio technology for telepresence.pdf","","","Human Computer Interaction; Multimedia Information; Patent Application; Spatial Separation; User Interface","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IC6EG6ND","bookSection","2014","Kim, Jee-Eun; Bessho, Masahiro; Koshizuka, Noboru; Sakamura, Ken","SaSYS: A Swipe Gesture-Based System for Exploring Urban Environments for the Visually Impaired","Mobile Computing, Applications, and Services","978-3-319-05451-3 978-3-319-05452-0","","","http://link.springer.com/10.1007/978-3-319-05452-0_5","Exploring and learning an environment is a particularly challenging issue faced by visually impaired people. Existing interaction techniques for allowing users to learn an environment may not be useful while traveling because they often use dedicated hardware or require users to focus on tactile or auditory feedback. In this paper, we introduce an intuitive interaction technique for selecting areas of interests in urban environments by performing simple swipe gestures on touchscreen. Based on the swipe-based interaction, we developed SaSYS, a location-aware system that enables users to discover points of interest (POI) around them using off-the-shelf smartphones. Our approach can be easily implemented on handheld devices without requiring any dedicated hardware and having users to constantly focus on tactile or auditory feedback. SaSYS also provides a fine-grained control over Text-to-Speech (TTS). Our user study shows that 9 of 11 users preferred swipe-based interaction to existing pointing-based interaction.","2014","2023-07-05 07:46:04","2023-07-21 04:29:50","2023-07-05 07:46:04","54-71","","","130","","","SaSYS","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering DOI: 10.1007/978-3-319-05452-0_5","","","","","","Memmi, Gérard; Blanke, Ulf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MFLPVVYL","bookSection","2020","Reynal, Maxime; Aricò, Pietro; Imbert, Jean-Paul; Hurter, Christophe; Borghini, Gianluca; Di Flumeri, Gianluca; Sciaraffa, Nicolina; Di Florio, Antonio; Terenzi, Michela; Ferreira, Ana; Pozzi, Simone; Betti, Viviana; Marucci, Matteo; Babiloni, Fabio","Involving Hearing, Haptics and Kinesthetics into Non-visual Interaction Concepts for an Augmented Remote Tower Environment","Computer Vision, Imaging and Computer Graphics Theory and Applications","978-3-030-41589-1 978-3-030-41590-7","","","http://link.springer.com/10.1007/978-3-030-41590-7_4","We investigated the contribution of specific HCI concepts to provide multimodal information to Air Traffic Controlers in the context of Remote Control Towers (i.e. when an airport is controlled from a distant location). We considered interactive spatial sound, tactile stimulation and body movements to design four different interaction and feedback modalities. Each of these modalities have been designed to provide specific solutions to typical Air Traffic Control identified use cases. Sixteen professional Air Traffic Controllers (ATCos) participated in the experiment, which was structured in four distinct scenarios. ATCos were immersed in an ecological setup, in which they were asked to control (i) one airport without augmentations modalities, (ii) two airports without augmentations, (iii) one airport with augmentations and (iv) two airports with augmentations. These experimental conditions constituted the four distinct experimental scenarios. Behavioral results shown a significant increase in overall participants’ performance when augmentation modalities were activated in remote control tower operations for one airport.","2020","2023-07-05 07:46:04","2023-07-19 23:50:34","2023-07-05 07:46:04","73-100","","","1182","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Communications in Computer and Information Science DOI: 10.1007/978-3-030-41590-7_4","","","","","","Cláudio, Ana Paula; Bouatouch, Kadi; Chessa, Manuela; Paljic, Alexis; Kerren, Andreas; Hurter, Christophe; Tremeau, Alain; Farinella, Giovanni Maria","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZAEVHB2W","bookSection","2001","Hoch, Michael; Jää-Aro, Kai-Mikael; Bowers, John","Round Table: A Physical Interface for Virtual Camera Deployment in Electronic Arenas","Immersive Projection Technology and Virtual Environments 2001","978-3-211-83671-2 978-3-7091-6221-7","","","http://link.springer.com/10.1007/978-3-7091-6221-7_27","In this paper, we describe a physical input device for the control of virtual cameras. The so called RoundTable has a round projection area where physical icons are used to stipulate the position of virtual cameras. With this scenario we propose a hybrid mixed reality environment for use by production personnel for real-time camera control during a live-broadcast. We present first results of using the RoundTable to support the managing of events in electronic arenas and compare them with traditional interfaces for camera control. We also comment on findings from a scenario in the field of sound mixing and sound composition.","2001","2023-07-05 07:46:04","2023-07-20 06:33:30","2023-07-05 07:46:04","267-276","","","","","","Round Table","","","","","Springer Vienna","Vienna","","","","","","DOI.org (Crossref)","","Series Title: Eurographics DOI: 10.1007/978-3-7091-6221-7_27","","/Users/minsik/Zotero/storage/MURQVVCY/Hoch et al. - 2001 - Round Table A Physical Interface for Virtual Came.pdf","","","","Fröhlich, Bernd; Deisinger, Joachim; Bullinger, Hans-Jörg","Hansmann, W.; Purgathofer, W.; Sillion, F.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FQMKRX3M","journalArticle","2022","Bach, Patric; Frank, Cornelia; Kunde, Wilfried","Why motor imagery is not really motoric: towards a re-conceptualization in terms of effect-based action control","Psychological Research","","0340-0727, 1430-2772","10.1007/s00426-022-01773-w","https://link.springer.com/10.1007/s00426-022-01773-w","Abstract             Overt and imagined action seem inextricably linked. Both have similar timing, activate shared brain circuits, and motor imagery influences overt action and vice versa. Motor imagery is, therefore, often assumed to recruit the same motor processes that govern action execution, and which allow one to play through or simulate actions offline. Here, we advance a very different conceptualization. Accordingly, the links between imagery and overt action do not arise because action imagery is intrinsically motoric, but because action planning is intrinsically imaginistic and occurs in terms of the perceptual effects one want to achieve. Seen like this, the term ‘motor imagery’ is a misnomer of what is more appropriately portrayed as ‘effect imagery’. In this article, we review the long-standing arguments for effect-based accounts of action, which are often ignored in motor imagery research. We show that such views provide a straightforward account of motor imagery. We review the evidence for imagery-execution overlaps through this new lens and argue that they indeed emerge because every action we execute is planned, initiated and controlled through an imagery-like process. We highlight findings that this new view can now explain and point out open questions.","2022-12-14","2023-07-05 07:46:04","2023-07-05 07:46:04","2023-07-05 07:46:04","","","","","","Psychological Research","Why motor imagery is not really motoric","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/T3DZHSD4/Bach et al. - 2022 - Why motor imagery is not really motoric towards a.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4Z8E9DT7","bookSection","2023","Chopda, Rishabh; Khan, Aayan; Goenka, Anuj; Dhere, Dakshal; Gupta, Shiwani","An Intelligent Voice Assistant Engineered to Assist the Visually Impaired","Intelligent Computing and Networking","978-981-9900-70-1 978-981-9900-71-8","","","https://link.springer.com/10.1007/978-981-99-0071-8_12","Visually handicapped people’s lives are subject to a multitude of unrelenting challenges because they’ve been made bereft of the gift of sight. The proposed solution is a wearable Smart Voice Assistant that is developed to accommodate the needs of the visually impaired to aid them in every aspect of their everyday lives. It takes advantage of recent breakthroughs in the fields of language processing and computer vision to provide a broad spectrum of applications, including emergency response functionality, object recognition, and optical character recognition. It comprises hardware components that provide feedback in the form of sound, haptics, and speech to help with obstacle avoidance. The voice assistant also interacts with a smartphone application to enhance the user’s experience by enabling them to read the messages from their phone, send an SOS message to their closest connections in an emergency, customize the device settings through the mobile application, and find the device with the press of a button if it is misplaced. The proposed solution will enable the user to live a life in relative safety and comfort, which is essential for people suffering from varying levels of visual impairment.","2023","2023-07-05 07:46:04","2023-07-20 06:34:51","2023-07-05 07:46:04","143-155","","","632","","","","","","","","Springer Nature Singapore","Singapore","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Networks and Systems DOI: 10.1007/978-981-99-0071-8_12","","","","","","Balas, Valentina Emilia; Semwal, Vijay Bhaskar; Khandare, Anand","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MNAEFA48","journalArticle","2017","The CSRA Team; Wrede, Sebastian; Leichsenring, Christian; Holthaus, Patrick; Hermann, Thomas; Wachsmuth, Sven","The Cognitive Service Robotics Apartment: A Versatile Environment for Human–Machine Interaction Research","KI - Künstliche Intelligenz","","0933-1875, 1610-1987","10.1007/s13218-017-0492-x","http://link.springer.com/10.1007/s13218-017-0492-x","The emergence of cognitive interaction technology offering intuitive and personalized support for humans in daily routines is essential for the success of future smart environments. Social robotics and ambient assisted living are well-established, active research fields but in the real world the number of smart environments that support humans efficiently on a daily basis is still rather low. We argue that research on ambient intelligence and human–robot interaction needs to be conducted in a strongly interdisciplinary process to facilitate seamless integration of assistance technologies into the users daily lives. With the cognitive service robotics apartment (CSRA), we are developing a novel kind of laboratory following this interdisciplinary approach. It combines a smart home with ambient intelligence functionalities with a cognitive social robot with advanced manipulation capabilities to explore the all day use of cognitive interaction technology for human assistance. This lab in conjunction with our development approach opens up new lines of inquiry and allows us to address new research questions in human–machine, human–agent and human–robot interaction","2017-08","2023-07-05 07:46:04","2023-07-21 04:27:40","2023-07-05 07:46:04","299-304","","3","31","","Künstl Intell","The Cognitive Service Robotics Apartment","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4FSNAF6Q","bookSection","2017","Rouse, Rebecca; Barba, Evan","Design for Emerging Media: How MR Designers Think About Storytelling, Process, and Defining the Field","Interactive Storytelling","978-3-319-71026-6 978-3-319-71027-3","","","http://link.springer.com/10.1007/978-3-319-71027-3_20","Given mixed reality’s (MR) unique status as an emerging medium that incorporates both the physical and the virtual in hybrid space, it is a particularly interesting field in which to study the design process as a whole, and interactive narrative design in particular. How prominently does story figure in MR design? What kinds of stories are being told? As MR tools become more accessible, the field is opening up to a wider variety of practitioners. However, the full breadth of methods and techniques being brought to bear in design for MR has not yet been studied. This paper presents findings from an interview study with fifteen leading MR designers, and describes the multiplicity of approaches they use. These approaches are presented as a matrix, composed of a opportunistic—deterministic spectrum (based on designs planned in advance vs. improvisation), and a storytelling—sensationalizing spectrum (based on designs aimed at narrative creation vs. development of a sensory experience).","2017","2023-07-05 07:46:04","2023-07-20 06:36:52","2023-07-05 07:46:04","245-258","","","10690","","","Design for Emerging Media","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-71027-3_20","","","","","","Nunes, Nuno; Oakley, Ian; Nisi, Valentina","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5IB2AW9P","bookSection","2005","Slavík, Pavel; Němec, Vladislav; Sporka, Adam J.","Speech Based User Interface for Users with Special Needs","Text, Speech and Dialogue","978-3-540-28789-6 978-3-540-31817-0","","","http://link.springer.com/10.1007/11551874_6","The number of people using computers is permanently increasing in last years. Not all potential users have all capabilities that allow them to use computers without obstacles. This is especially true for handicapped and elderly users. For this class of users a special approach for design and implementation of user interfaces is needed. The missing capabilities of these users must be substituted by capabilities that these users have. In most of cases the use of sounds and speech offers a natural solution to this problem. In the paper the outline of problems related to special user interfaces will be discussed. In further the examples of application of user interfaces using special forms of speech and related acoustic communication will be given.","2005","2023-07-05 07:46:04","2023-07-21 05:04:17","2023-07-05 07:46:04","45-55","","","3658","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11551874_6","","","","","","Matoušek, Václav; Mautner, Pavel; Pavelka, Tomáš","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4METMI9W","bookSection","2007","McGookin, David K.; Brewster, Stephen A.","Graph Builder: Constructing Non-visual Visualizations","People and Computers XX — Engage","978-1-84628-588-2","","","http://link.springer.com/10.1007/978-1-84628-664-3_20","This paper introduces a novel application called Graph Builder, which allows visually impaired people to interactively construct bar graphs using a force feedback device. We discuss the limitations of current technology to allow such interactive construction and explain why, in educational environments, such interactive construction is important. Evaluations of Graph Builder showed that users could construct graphs accurately. However results showed that a large number of ‘off-by-one’ errors occurred, where the bar was set either one unit too high or too low. Revisions to the mechanism to manipulate bars were made, and further non-speech audio feedback was added. A further evaluation showing that the proportion of ‘off-by-one’ errors had been reduced.","2007","2023-07-05 07:46:04","2023-07-21 04:42:53","2023-07-05 07:46:04","263-278","","","","","","Graph Builder","","","","","Springer London","London","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-1-84628-664-3_20","","","","","","Bryan-Kinns, Nick; Blanford, Ann; Curzon, Paul; Nigay, Laurence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SL9LDAHR","bookSection","2011","Rutkowski, Tomasz M.","Auditory Brain-Computer/Machine-Interface Paradigms Design","Haptic and Audio Interaction Design","978-3-642-22949-7 978-3-642-22950-3","","","http://link.springer.com/10.1007/978-3-642-22950-3_12","","2011","2023-07-05 07:46:04","2023-07-05 07:46:04","2023-07-05 07:46:04","110-119","","","6851","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-22950-3_12","","","","","","Cooper, Eric W.; Kryssanov, Victor V.; Ogawa, Hitoshi; Brewster, Stephen","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"42IQRIVZ","journalArticle","2003","Pittarello, Fabio","Accessing information through multimodal 3D environments: towards universal access","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-003-0044-z","http://link.springer.com/10.1007/s10209-003-0044-z","","2003-06-01","2023-07-05 07:46:04","2023-07-05 07:46:04","2023-07-05 07:46:04","189-204","","2","2","","Universal Access in the Information Society","Accessing information through multimodal 3D environments","","","","","","","","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SMTNQPVT","bookSection","2007","Bologna, Guido; Deville, Benoît; Pun, Thierry; Vinckenbosch, Michel","Identifying Major Components of Pictures by Audio Encoding of Colours","Nature Inspired Problem-Solving Methods in Knowledge Engineering","978-3-540-73054-5 978-3-540-73055-2","","","http://link.springer.com/10.1007/978-3-540-73055-2_10","The goal of the See ColOr project is to achieve a non-invasive mobility aid for blind users that will use the auditory pathway to represent in real-time frontal image scenes. More particularly, we have developed a prototype which transforms HSL coloured pixels into spatialized classical instrument sounds lasting for 300 ms. Hue is sonified by the timbre of a musical instrument, saturation is one of four possible notes, and luminosity is represented by bass when luminosity is rather dark and singing voice when it is relatively bright. Our first experiments are devoted to static images on the computer screen. Six participants with their eyes covered by a dark tissue were trained to associate colours with musical instruments and then asked to determine on several pictures, objects with specific shapes and colours. In order to simplify the protocol of experiments, we used a tactile tablet, which took the place of the camera. Overall, experiment participants found that colour was helpful for the interpretation of image scenes.","2007","2023-07-05 07:46:04","2023-07-21 04:35:21","2023-07-05 07:46:04","81-89","","","4528","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73055-2_10","","","","","","Mira, José; Álvarez, José R.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9T82YPVE","bookSection","2013","Singh, Surya P. N.; Pounds, Paul E. I.; Kurniawati, Hanna","I-Ball: A Programmable Sporting Aid for Children with a Visual Impairment to Play Soccer","Universal Access in Human-Computer Interaction. Design Methods, Tools, and Interaction Techniques for eInclusion","978-3-642-39187-3 978-3-642-39188-0","","","http://link.springer.com/10.1007/978-3-642-39188-0_63","The Interactive Ball (“I-Ball”) is a programmable tonal soccer ball that varies its output based on measurements from an inertial sensor. As a sporting aid for children with blindness and low-vision it makes participation in team sports more accessible without a conspicuous constant tone and in a manner the provides information when stationary. The paper presents the design rationale of the system. Exploitative evaluation with visually impaired users indicates that the encoded information provides utility, but also that noise and wind are complicating external factors that can limit perceptual range.","2013","2023-07-05 07:46:04","2023-07-21 05:10:16","2023-07-05 07:46:04","584-591","","","8009","","","I-Ball","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-39188-0_63","","","","","","Stephanidis, Constantine; Antona, Margherita","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A2EG9QKE","journalArticle","2012","Metatla, Oussama; Bryan-Kinns, Nick; Stockman, Tony","Interactive hierarchy-based auditory displays for accessing and manipulating relational diagrams","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0067-3","http://link.springer.com/10.1007/s12193-011-0067-3","An approach to designing hierarchy-based auditory displays that supports non-visual interaction with relational diagrams is presented. The approach is motivated by an analysis of the functional and structural properties of relational diagrams in terms of their role as external representations. This analysis informs the design of a multiple perspective hierarchy-based model that captures modality independent features of a diagram when translating it into an audio accessible form. The paper outlines design lessons learnt from two user studies that were conducted to evaluate the proposed approach.","2012-05","2023-07-05 07:46:04","2023-07-20 07:02:31","2023-07-05 07:46:04","111-122","","3-4","5","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BUSF9V9C","journalArticle","2014","Omodei, Elisa; Bazzani, Armando; Rambaldi, Sandro; Michieletto, Paolo; Giorgini, Bruno","The physics of the city: pedestrians dynamics and crowding panic equation in Venezia","Quality & Quantity","","0033-5177, 1573-7845","10.1007/s11135-012-9773-5","http://link.springer.com/10.1007/s11135-012-9773-5","In this paper we present the physics of the city, a new approach in order to investigate the urban dynamics. In particular we focus on the citizens’ mobility observation and modeling. Being in principle the social dynamics not directly observable, our main idea is that observing the human mobility processes we can deduce some features and characteristics of social dynamics. We define the automata gas paradigm and we write a crowding equation able to predict, in a statistical sense, the threshold between a selforganized crowd and a chaotic one, which we interpret as the emergence of a possible panic scenario. We show also some specific results obtained on the Venezia pedestrian network. Firstly, analyzing the network we estimate the Venice complexity, secondly measuring the pedestrian flow on some bridges we find significant statistical correlations, and by the experimental data we design two different bridges flow profiles depending from the pedestrian populations. Furthermore considering a reduced portion of the city, i.e. Punta della Dogana, we build up a theoretical model via a Markov approach, with a stationary state solution. Finally implementing some individual characteristics of pedestrians, we simulate the flows finding a good agreement with the empirical distributions. We underline that these results can be the basis to construct an E-governance mobility system.","2014-01","2023-07-05 07:46:04","2023-07-21 04:56:27","2023-07-05 07:46:04","347-373","","1","48","","Qual Quant","The physics of the city","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"78L3QU96","bookSection","2009","Rath, Matthias; Bienert, Sascha","A Tangible Game as Distributable Platform for Psychophysical Examination","Haptic and Audio Interaction Design","978-3-642-04075-7 978-3-642-04076-4","","","http://link.springer.com/10.1007/978-3-642-04076-4_17","Through the use of built-in accelerometers a game-software for recent generation MacBooks allows control of a scenario of virtual moving objects by tilting the computer. Together with integrated visual and continuous auditory feedback from models based on physical principles the software forms a possible platform for online collection of psychophysical data.","2009","2023-07-05 07:46:04","2023-07-20 00:16:30","2023-07-05 07:46:04","155-164","","","5763","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-04076-4_17","","","","","","Altinsoy, M. Ercan; Jekosch, Ute; Brewster, Stephen","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z8ETQF7U","bookSection","2014","Bornschein, Jens; Prescher, Denise; Weber, Gerhard","SVGPlott – Generating Adaptive and Accessible Audio-Tactile Function Graphs","Computers Helping People with Special Needs","978-3-319-08595-1 978-3-319-08596-8","","","http://link.springer.com/10.1007/978-3-319-08596-8_91","Curve sketching is a hard task for blind and visually impaired pupils and students, but it is an essential part in education. To help those students as well as their colleges, teachers and other people to prepare good tactile function plots the platform independent console program SVGPlott was developed. It enables users without any special knowledge about creating graphics for blind or visually impaired people to prepare highly adaptable mathematical function plots in the SVG format, which can also be used for audio-tactile exploration. SVGPlott was developed in a user-centered design process, including teachers and users. We show that blind and sighted users can prepare function plots including key as well as an automatically generated textual description not only for tactile, audio-tactile and print output, but also for usage on a dynamic tactile pin device and as a high contrast visualization for low vision people.","2014","2023-07-05 07:46:04","2023-07-19 23:51:07","2023-07-05 07:46:04","588-595","","","8547","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-08596-8_91","","","","","","Miesenberger, Klaus; Fels, Deborah; Archambault, Dominique; Peňáz, Petr; Zagler, Wolfgang","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VUM8YGDS","bookSection","2010","Brock, Derek; McClimens, Brian; Wasylyshyn, Christina; Trafton, J. Gregory; McCurry, Malcolm","Evaluating the Utility of Auditory Perspective-Taking in Robot Speech Presentations","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_14","In speech interactions, people routinely reason about each other’s auditory perspective and adjust their manner of speaking accordingly by raising their voice to overcome noise or distance, and sometimes by pausing and resuming when conditions are more favorable for their listener. In this paper we report the findings of a listening study motivated by both this observation and a prototype auditory interface for a mobile robot that monitors the aural parameters of its environment to infer its user’s listening requirements. The results provide significant empirical evidence of the utility of simulated auditory perspective taking and the inferred use of loudness and/or pauses to overcome the potential of ambient noise to mask synthetic speech.","2010","2023-07-05 07:46:04","2023-07-19 11:37:05","2023-07-05 07:46:04","266-286","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_14","","/Users/minsik/Zotero/storage/RVJQB8MR/Brock et al. - 2010 - Evaluating the Utility of Auditory Perspective-Tak.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6NX2Q7VV","journalArticle","2018","Ahn, Jonggil; Kim, Gerard Jounghyun","SPRinT: A Mixed Approach to a Hand-Held Robot Interface for Telepresence","International Journal of Social Robotics","","1875-4791, 1875-4805","10.1007/s12369-017-0463-2","http://link.springer.com/10.1007/s12369-017-0463-2","In this paper, we present SPRinT, a control interface design for a telepresence robot that uses only a smart phone without any external sensors. In addition to basic controllability, we focus on providing a reasonable level of spatial understanding as well as a feeling of telepresence through the interaction. The central idea of SPRinT is to have the operator control and interact with the system by “posing” as the robot in the remote area in order to elicit a sense of telepresence, promote spatial task performance, and improve the overall interactive experience. We have applied the proposed interaction design principle to remote robot control, and compared it to a nominal touch-button based interface in terms of the controllability, the level of user-felt telepresence, and spatial understanding. Our experiments showed that the proprioceptive and spatial nature of the motion-based rotational control was critical in eliciting the sense of telepresence and spatial understanding, and this in turn was also important to ensure effective exploration and awareness of remote spaces. On the other hand, the traditional touch-button interface was more appropriate for translation for which a proper proprioceptive metaphoric command could not be designed. Overall, the mixed approach (body/motion based for rotation and touch based for translation) proved to offer a good middle ground since the interaction method was familiar and easy to use with a reasonable level of telepresence.","2018-09","2023-07-05 07:46:04","2023-07-20 06:43:43","2023-07-05 07:46:04","537-552","","4","10","","Int J of Soc Robotics","SPRinT","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PN7GAY2N","bookSection","2014","Másilko, Lukáš; Pecl, Jiří","Making Graph Theory Algorithms Accessible to Blind Students","Computers Helping People with Special Needs","978-3-319-08595-1 978-3-319-08596-8","","","http://link.springer.com/10.1007/978-3-319-08596-8_86","The authors of the proposal are teachers of mathematics for students with visual impairment at Masaryk University (Brno, Czech Republic). When giving instruction, they face the following problem: how can blind people use a given mathematical algorithm in view of the fact that they follow all the information in linear way. Often the instructors have to decide whether to adapt such an algorithm or let blind students work with it in the same manner as their sighted peers do. Their goal is to find an optimal set of methods which would respect blind people’s linear manner of working with information and at the same time be sufficiently effective. In their paper, the authors will present several adaptations of two algorithms of Graph Theory. They will assess the pros and cons of all the proposed modifications.","2014","2023-07-05 07:46:04","2023-07-19 23:52:24","2023-07-05 07:46:04","549-556","","","8547","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-08596-8_86","","","","","","Miesenberger, Klaus; Fels, Deborah; Archambault, Dominique; Peňáz, Petr; Zagler, Wolfgang","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8W22QGW5","bookSection","2013","Antunes, Rui Filipe; Leymarie, Frederic Fol","Real-Time Behavioral Animation of Humanoid Non-Player Characters with a Computational Ecosystem","Intelligent Virtual Agents","978-3-642-40414-6 978-3-642-40415-3","","","http://link.springer.com/10.1007/978-3-642-40415-3_34","A novel approach to a decentralized autonomous model of agency for general purpose Non-Player Characters (NPCs) is presented: Computational Ecosystems as a model of AI. We describe the technology used to animate a population of gregarious humanoid characters in the virtual world Where is Lourenco Marques? an ethnographic artistic work characterized as a virtual world inhabited by a population of NPCs interacting autonomously among themselves as well as with an audience of outsiders (human observers). First, we present the background and motivations for the project. Then, we describe the technical details about the algorithm that was developed to generate the movements and behaviors of a population of NPC ‘storytellers’. Finally, we layout some of the critical aspects of this particular implementation and contextualize the work with regards to a wider usage in virtual worlds.","2013","2023-07-05 07:46:46","2023-07-20 06:36:42","2023-07-05 07:46:46","382-395","","","8108","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-40415-3_34","","","","","","Aylett, Ruth; Krenn, Brigitte; Pelachaud, Catherine; Shimodaira, Hiroshi","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GDWHZLV8","bookSection","2005","Bologna, Guido; Vinckenbosch, Michel","Eye Tracking in Coloured Image Scenes Represented by Ambisonic Fields of Musical Instrument Sounds","Mechanisms, Symbols, and Models Underlying Cognition","978-3-540-26298-5 978-3-540-31672-5","","","http://link.springer.com/10.1007/11499220_34","","2005","2023-07-05 07:46:46","2023-07-05 07:46:46","2023-07-05 07:46:46","327-337","","","3561","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11499220_34","","","","","","Mira, José; Álvarez, José R.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3LBWPYQZ","journalArticle","2007","Shajahan, Peer; Irani, Pourang","One family, many voices: Can multiple synthetic voices be used as navigational cues in hierarchical interfaces?","International Journal of Speech Technology","","1381-2416, 1572-8110","10.1007/s10772-006-9000-7","http://link.springer.com/10.1007/s10772-006-9000-7","Many commercial applications use synthetic speech for conveying information. In many cases the structure of the information is hierarchical (e.g. menus). In this article, we describe the results of two experiments that examine the possibility of conveying hierarchies (family of trees) using multiple synthetic voices. We postulate that if hierarchical structures can be conveyed using synthetic speech, then navigation in these hierarchies can be improved. In the first experiment, hierarchies containing 10 nodes, with a depth of 3 levels, were created. We used synthetic voices to represent nodes in these hierarchies. A within-subjects study (N = 12) was conducted to compare multiple synthetic voices against single synthetic voices for locating the positions of nodes in a hierarchy. Multiple synthetic voices were created by manipulating synthetic voice parameters according to a set of design principles. Results of the first experiment showed that the subjects performed the tasks significantly better with multiple synthetic voices than with single synthetic voices. To investigate the effect of multiple synthetic voices on complex hierarchies a second experiment was conducted. A hierarchy of 27 nodes was created and a between-subjects study (N = 16) was carried out. The results of this experiment showed that the participants recalled 84.38% of the nodes accurately. Results from these studies imply that multiple synthetic voices can be effectively used to represent and provide navigation cues in interfaces structured as hierarchies.","2007-03-14","2023-07-05 07:46:46","2023-07-20 06:46:05","2023-07-05 07:46:46","1-15","","1-2","9","","Int J Speech Technol","One family, many voices","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HY979T55","bookSection","2013","Antunes, Rui Filipe; Leymarie, Frederic Fol","An Ecosystem Based Model for Real-Time Generative Animation of Humanoid Non-Player Characters","Progress in Artificial Intelligence","978-3-642-40668-3 978-3-642-40669-0","","","http://link.springer.com/10.1007/978-3-642-40669-0_7","In this paper a novel approach to a decentralized autonomous model of agency for general purpose Non-Player Characters (NPCs) is presented: the AI model of Computational Ecosystems. We describe the technology used to animate a population of gregarious humanoid avatars in a virtual world. This artistic work is an ethnographic project where a population of NPCs inhabit the virtual world and interact autonomously among themselves as well as with an audience of outsiders (human observers). First, we present the background, motivation and summary for the project. Then, we describe the algorithm that was developed to generate the movements and behaviors of the population of NPC “story-tellers”. Finally, we discuss some of the critical aspects of this implementation and contextualize the work with regards to a wider usage in computer games and virtual worlds.","2013","2023-07-05 07:46:46","2023-07-21 04:55:33","2023-07-05 07:46:46","66-77","","","8154","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-40669-0_7","","","","","","Correia, Luís; Reis, Luís Paulo; Cascalho, José","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G94PZB2L","journalArticle","2023","Bowman, Catherine D. D.; Elkins-Tanton, Linda T.; Talamante, Adriana; Bell, James F.; Cisneros, Ernest; Cook, Alexandra; Frieman, Jason D.; Gainor, Danya; Hunziker, Jamie; Khan, Shaheer; Lawler, Christopher R.; Maschino, Jessica; McCoy, Timothy J.; Nessi, Kaxandra; Oran, Rona; Seal, David; Simon, Amber; Singh, Rohit; Tolbert, Carol M.; Valentine, Karin; Weiss, Benjamin; Wenkert, Daniel D.; Williams, David A.","Mission to Psyche: Including Undergraduates and the Public on the Journey to a Metal World","Space Science Reviews","","0038-6308, 1572-9672","10.1007/s11214-023-00967-x","https://link.springer.com/10.1007/s11214-023-00967-x","Abstract             The NASA Psyche mission’s program to engage university undergraduates and the public in the mission is inspired by and built upon the extensive foundation of public engagement, educational outreach activities, and expertise of NASA and mission partner institutions. The program leverages the enthusiasm and contributions of undergraduates nationwide to the benefit of the mission, the students and their institutions and communities, and the broader public. Psyche Student Collaborations consists of four main programs, two (Psyche Capstone and Psyche Inspired) are available solely to undergraduates enrolled at universities or community colleges in the United States and its territories and two (Innovation Toolkit free online courses and Science Outreach Interns and Docents) invite broader participation by engaging the talents and creativity of undergraduate interns to help create content and events to reach the public and lifelong learners. Together, these offerings provide multiple entry points and a spectrum of intensity of experiences, numbers of participants, disciplinary diversity, and mode of delivery. Involving undergraduates in all phases of the program supports the development of the next generation of explorers, contributes to the nation’s workforce preparation, and complements NASA’s existing undergraduate offerings by providing long-term opportunities for students to participate with the mission through established postsecondary education structures like capstone courses.","2023-04","2023-07-05 07:46:46","2023-07-05 07:46:46","2023-07-05 07:46:46","25","","3","219","","Space Sci Rev","Mission to Psyche","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/ZDACBFAU/Bowman et al. - 2023 - Mission to Psyche Including Undergraduates and th.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CKUXV93A","journalArticle","2022","Gold, Nicolas E.; Krinke, Jens","Ethics in the mining of software repositories","Empirical Software Engineering","","1382-3256, 1573-7616","10.1007/s10664-021-10057-7","https://link.springer.com/10.1007/s10664-021-10057-7","Abstract             Research in Mining Software Repositories (MSR) is research involving human subjects, as the repositories usually contain data about developers’ and users’ interactions with the repositories and with each other. The ethics issues raised by such research therefore need to be considered before beginning. This paper presents a discussion of ethics issues that can arise in MSR research, using the mining challenges from the years 2006 to 2021 as a case study to identify the kinds of data used. On the basis of contemporary research ethics frameworks we discuss ethics challenges that may be encountered in creating and using repositories and associated datasets. We also report some results from a small community survey of approaches to ethics in MSR research. In addition, we present four case studies illustrating typical ethics issues one encounters in projects and how ethics considerations can shape projects before they commence. Based on our experience, we present some guidelines and practices that can help in considering potential ethics issues and reducing risks.","2022-01","2023-07-05 07:46:46","2023-07-05 07:46:46","2023-07-05 07:46:46","17","","1","27","","Empir Software Eng","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/J4T3VBPC/Gold and Krinke - 2022 - Ethics in the mining of software repositories.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""