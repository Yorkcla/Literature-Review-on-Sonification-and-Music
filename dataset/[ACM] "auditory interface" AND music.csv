"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"6ALIC5HM","journalArticle","2008","Andersen, Tue Haste; Zhai, Shumin","“Writing with music”: Exploring the use of auditory feedback in gesture interfaces","ACM Transactions on Applied Perception","","1544-3558","10.1145/1773965.1773968","https://dl.acm.org/doi/10.1145/1773965.1773968","We investigate the use of auditory feedback in pen-gesture interfaces in a series of informal and formal experiments. Initial iterative exploration showed that gaining performance advantage with auditory feedback was possible using absolute cues and state feedback after the gesture was produced and recognized. However, gaining learning or performance advantage from auditory feedback tightly coupled with the pen-gesture articulation and recognition process was more difficult. To establish a systematic baseline, Experiment 1 formally evaluated gesture production accuracy as a function of auditory and visual feedback. Size of gestures and the aperture of the closed gestures were influenced by the visual or auditory feedback, while other measures such as shape distance and directional difference were not, supporting the theory that feedback is too slow to strongly influence the production of pen stroke gestures. Experiment 2 focused on the subjective aspects of auditory feedback in pen-gesture interfaces. Participants' rating on the dimensions of being wonderful and stimulating was significantly higher with musical auditory feedback. Several lessons regarding pen gestures and auditory feedback are drawn from our exploration: a few simple functions such as indicating the pen-gesture recognition results can be achieved, gaining performance and learning advantage through tightly coupled process-based auditory feedback is difficult, pen-gesture sets and their recognizers can be designed to minimize visual dependence, and people's subjective experience of gesture interaction can be influenced using musical auditory feedback. These lessons may serve as references and stepping stones toward future research and development in pen-gesture interfaces with auditory feedback.","2008-06-18","2023-07-06 05:41:48","2023-07-06 05:41:48","2023-07-06 05:41:30","17:1–17:24","","3","7","","ACM Trans. Appl. Percept.","“Writing with music”","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/NBCDYUT9/Andersen and Zhai - 2008 - “Writing with music” Exploring the use of auditor.pdf","","","music; sound; gesture; feedback; Audio; auditory interface; pen; text input","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9IJI6KK4","journalArticle","2018","Reichinger, Andreas; Carrizosa, Helena Garcia; Wood, Joanna; Schröder, Svenja; Löw, Christian; Luidolt, Laura Rosalia; Schimkowitsch, Maria; Fuhrmann, Anton; Maierhofer, Stefan; Purgathofer, Werner","Pictures in Your Mind: Using Interactive Gesture-Controlled Reliefs to Explore Art","ACM Transactions on Accessible Computing","","1936-7228","10.1145/3155286","https://dl.acm.org/doi/10.1145/3155286","Tactile reliefs offer many benefits over the more classic raised line drawings or tactile diagrams, as depth, 3D shape, and surface textures are directly perceivable. Although often created for blind and visually impaired (BVI) people, a wider range of people may benefit from such multimodal material. However, some reliefs are still difficult to understand without proper guidance or accompanying verbal descriptions, hindering autonomous exploration. In this work, we present a gesture-controlled interactive audio guide (IAG) based on recent low-cost depth cameras that can be operated directly with the hands on relief surfaces during tactile exploration. The interactively explorable, location-dependent verbal and captioned descriptions promise rapid tactile accessibility to 2.5D spatial information in a home or education setting, to online resources, or as a kiosk installation at public places. We present a working prototype, discuss design decisions, and present the results of two evaluation studies: the first with 13 BVI test users and the second follow-up study with 14 test users across a wide range of people with differences and difficulties associated with perception, memory, cognition, and communication. The participant-led research method of this latter study prompted new, significant and innovative developments.","2018-03-22","2023-07-06 05:41:48","2023-07-06 05:41:48","2023-07-06 05:41:32","2:1–2:39","","1","11","","ACM Trans. Access. Comput.","Pictures in Your Mind","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/AMSH3MDU/Reichinger et al. - 2018 - Pictures in Your Mind Using Interactive Gesture-C.pdf","","","Blind; multimodal interaction; low vision; gestures; auditory interface; cognitive disability; design for all; learning disability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RLPGVYW8","journalArticle","2013","Csapó, Ádám; Wersényi, György","Overview of auditory representations in human-machine interfaces","ACM Computing Surveys","","0360-0300","10.1145/2543581.2543586","https://dl.acm.org/doi/10.1145/2543581.2543586","In recent years, a large number of research projects have focused on the use of auditory representations in a broadened scope of application scenarios. Results in such projects have shown that auditory elements can effectively complement other modalities not only in the traditional desktop computer environment but also in virtual and augmented reality, mobile platforms, and other kinds of novel computing environments. The successful use of auditory representations in this growing number of application scenarios has in turn prompted researchers to rediscover the more basic auditory representations and extend them in various directions. The goal of this article is to survey both classical auditory representations (e.g., auditory icons and earcons) and those auditory representations that have been created as extensions to earlier approaches, including speech-based sounds (e.g., spearcons and spindex representations), emotionally grounded sounds (e.g., auditory emoticons and spemoticons), and various other sound types used to provide sonifications in practical scenarios. The article concludes by outlining the latest trends in auditory interface design and providing examples of these trends.","2013-12-27","2023-07-06 05:41:48","2023-07-06 05:41:48","2023-07-06 05:41:34","19:1–19:23","","2","46","","ACM Comput. Surv.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/FPCXFXN5/Csapó and Wersényi - 2013 - Overview of auditory representations in human-mach.pdf","","","sonification; Auditory icon; earcon","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SIVQ4E6E","journalArticle","2008","Fontana, Federico; Rocchesso, Davide","Auditory distance perception in an acoustic pipe","ACM Transactions on Applied Perception","","1544-3558","10.1145/1402236.1402240","https://dl.acm.org/doi/10.1145/1402236.1402240","In a study of auditory distance perception, we investigated the effects of exaggeration the acoustic cue of reverberation where the intensity of sound did not vary noticeably. The set of stimuli was obtained by moving a sound source inside a 10.2-m long pipe having a 0.3-m diameter. Twelve subjects were asked to listen to a speech sound while keeping their head inside the pipe and then to estimate the egocentric distance from the sound source using a magnitude production procedure. The procedure was repeated eighteen times using six different positions of the sound source. Results show that the point at which perceived distance equals physical distance is located approximately 3.5 m away from the listening point, with an average range of distance estimates of approximately 3.3 m, i.e., 1.65 to 4.9 m. The absence of intensity cues makes the acoustic pipe a potentially interesting modeling paradigm for the design of auditory interfaces in which distance is rendered independently of loudness. The proposed acoustic environment also confirms the known unreliability of certain distance cues.","2008-09-12","2023-07-06 05:41:48","2023-07-06 05:41:48","2023-07-06 05:41:36","16:1–16:15","","3","5","","ACM Trans. Appl. Percept.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/BJIBGLVN/Fontana and Rocchesso - 2008 - Auditory distance perception in an acoustic pipe.pdf","","","auditory display; Acoustic pipe; distance perception","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BNCSVW5Q","journalArticle","2016","Vazquez-Alvarez, Yolanda; Aylett, Matthew P.; Brewster, Stephen A.; Jungenfeld, Rocio Von; Virolainen, Antti","Designing Interactions with Multilevel Auditory Displays in Mobile Audio-Augmented Reality","ACM Transactions on Computer-Human Interaction","","1073-0516","10.1145/2829944","https://dl.acm.org/doi/10.1145/2829944","Auditory interfaces offer a solution to the problem of effective eyes-free mobile interactions. In this article, we investigate the use of multilevel auditory displays to enable eyes-free mobile interaction with indoor location-based information in non-guided audio-augmented environments. A top-level exocentric sonification layer advertises information in a gallery-like space. A secondary interactive layer is used to evaluate three different conditions that varied in the presentation (sequential versus simultaneous) and spatialisation (non-spatialised versus egocentric/exocentric spatialisation) of multiple auditory sources. Our findings show that (1) participants spent significantly more time interacting with spatialised displays; (2) using the same design for primary and interactive secondary display (simultaneous exocentric) showed a negative impact on the user experience, an increase in workload and substantially increased participant movement; and (3) the other spatial interactive secondary display designs (simultaneous egocentric, sequential egocentric, and sequential exocentric) showed an increase in time spent stationary but no negative impact on the user experience, suggesting a more exploratory experience. A follow-up qualitative and quantitative analysis of user behaviour support these conclusions. These results provide practical guidelines for designing effective eyes-free interactions for far richer auditory soundscapes.","2016-12-31","2023-07-06 05:41:48","2023-07-06 05:41:48","2023-07-06 05:41:38","3:1–3:30","","1","23","","ACM Trans. Comput.-Hum. Interact.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/M7I8CQP3/Vazquez-Alvarez et al. - 2016 - Designing Interactions with Multilevel Auditory Di.pdf","","","auditory displays; spatial audio; exploratory behaviour; Eyes-free interaction; mobile audio-augmented reality","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JXTAXIZM","journalArticle","2012","Jeon, Myounghoon; Walker, Bruce N.; Srivastava, Abhishek","“Spindex” (Speech Index) Enhances Menus on Touch Screen Devices with Tapping, Wheeling, and Flicking","ACM Transactions on Computer-Human Interaction","","1073-0516","10.1145/2240156.2240162","https://dl.acm.org/doi/10.1145/2240156.2240162","Users interact with many electronic devices via menus such as auditory or visual menus. Auditory menus can either complement or replace visual menus. We investigated how advanced auditory cues enhance auditory menus on a smartphone, with tapping, wheeling, and flicking input gestures. The study evaluated a spindex (speech index), in which audio cues inform users where they are in a menu; 122 undergraduates navigated through a menu of 150 songs. Study variables included auditory cue type (text-to-speech alone or TTS plus spindex), visual display mode (on or off), and input gesture (tapping, wheeling, or flicking). Target search time and subjective workload were lower with spindex than without for all input gestures regardless of visual display mode. The spindex condition was rated subjectively higher than plain speech. The effects of input method and display mode on navigation behaviors were analyzed with the two-stage navigation strategy model. Results are discussed in relation to attention theories and in terms of practical applications.","2012-07-01","2023-07-06 05:41:48","2023-07-06 05:41:48","2023-07-06 05:41:41","14:1–14:27","","2","19","","ACM Trans. Comput.-Hum. Interact.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/IYSRJDHJ/Jeon et al. - 2012 - “Spindex” (Speech Index) Enhances Menus on Touch S.pdf","","","touch screen; spindex; Auditory menus; flicking; input gestures; tapping; wheeling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4ZFEWKLX","journalArticle","2011","Jeon, Myounghoon; Walker, Bruce N.","Spindex (Speech Index) Improves Auditory Menu Acceptance and Navigation Performance","ACM Transactions on Accessible Computing","","1936-7228","10.1145/1952383.1952385","https://dl.acm.org/doi/10.1145/1952383.1952385","Users interact with mobile devices through menus, which can include many items. Auditory menus have the potential to make those devices more accessible to a wide range of users. However, auditory menus are a relatively new concept, and there are few guidelines that describe how to design them. In this paper, we detail how visual menu concepts may be applied to auditory menus in order to help develop design guidelines. Specifically, we examine how to optimize the designs of a new contextual cue, called “spindex” (i.e., speech index). We developed and evaluated various design alternatives for spindex and iteratively refined the design with sighted users and visually impaired users. As a result, the “attenuated” spindex was the best in terms of preference as well as performance, across user groups. Nevertheless, sighted and visually impaired participants showed slightly different responses and feedback. Results are discussed in terms of acoustical theory, practical display design, and assistive technology design.","2011-04-01","2023-07-06 05:41:48","2023-07-06 05:41:48","2023-07-06 05:41:42","10:1–10:26","","3","3","","ACM Trans. Access. Comput.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/TGNBQR95/Jeon and Walker - 2011 - Spindex (Speech Index) Improves Auditory Menu Acce.pdf","","","assistive technology; spindex; Auditory menus","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7SWQIU47","journalArticle","2013","Moll, Jonas; Pysander, Eva-Lotta Sallnäs","A Haptic Tool for Group Work on Geometrical Concepts Engaging Blind and Sighted Pupils","ACM Transactions on Accessible Computing","","1936-7228","10.1145/2493171.2493172","https://dl.acm.org/doi/10.1145/2493171.2493172","In the study presented here, two haptic and visual applications for learning geometrical concepts in group work in primary school have been designed and evaluated. The aim was to support collaborative learning among sighted and visually impaired pupils. The first application is a static flattened 3D environment that supports learning to distinguish between angles by means of a 3D haptic device providing touch feedback. The second application is a dynamic 3D environment that supports learning of spatial geometry. The scene is a room with a box containing geometrical objects, which pupils can pick up and move around. The applications were evaluated in four schools with groups of two sighted and one visually impaired pupil. The results showed the support for the visually impaired pupil and for the collaboration to be satisfying. A shared understanding of the workspace could be achieved, as long as the virtual environment did not contain movable objects. Verbal communication was crucial for the work process but haptic guiding to some extent substituted communication about direction. When it comes to joint action between visually impaired and sighted pupils a number of interesting problems were identified when the dynamic and static virtual environments were compared. These problems require further investigation. The study extends prior work in the areas of assistive technology and multimodal communication by evaluating functions for joint haptic manipulation in the unique setting of group work in primary school.","2013-07-01","2023-07-06 05:41:48","2023-07-06 05:41:48","2023-07-06 05:41:44","14:1–14:37","","4","4","","ACM Trans. Access. Comput.","","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/TGTJILCQ/Moll and Pysander - 2013 - A Haptic Tool for Group Work on Geometrical Concep.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VTISEJET","journalArticle","2021","Hirskyj-Douglas, Ilyena; Piitulainen, Roosa; Lucero, Andrés","Forming the Dog Internet: Prototyping a Dog-to-Human Video Call Device","Proceedings of the ACM on Human-Computer Interaction","","","10.1145/3488539","https://dl.acm.org/doi/10.1145/3488539","Over the past decade, many systems have been developed for humans to remotely connect to their pets at home. Yet little attention has been paid to how animals can control such systems and what the implications are of animals using internet systems. This paper explores the creation of a video call device to allow a dog to remotely call their human, giving the animal control and agency over technology in their home. After building and prototyping a novel interaction method over several weeks and iterations, we test our system with a dog and a human. Analysing our experience and data, we reflect on power relations, how to quantify an animal's user experience and what interactive internet systems look like with animal users. This paper builds upon Human-Computer Interaction methods for unconventional users, uncovering key questions that advance the creation of animal-to-human interfaces and animal internet devices.","2021-11-05","2023-07-06 05:41:48","2023-07-06 05:41:48","2023-07-06 05:41:46","494:1–494:20","","ISS","5","","Proc. ACM Hum.-Comput. Interact.","Forming the Dog Internet","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/LJRZDH5F/Hirskyj-Douglas et al. - 2021 - Forming the Dog Internet Prototyping a Dog-to-Hum.pdf","","","participatory design; animal-computer interaction; remote human-animal systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G2VGXZVR","journalArticle","2008","Zhao, Haixia; Plaisant, Catherine; Shneiderman, Ben; Lazar, Jonathan","Data Sonification for Users with Visual Impairment: A Case Study with Georeferenced Data","ACM Transactions on Computer-Human Interaction","","1073-0516","10.1145/1352782.1352786","https://dl.acm.org/doi/10.1145/1352782.1352786","We describe the development and evaluation of a tool, iSonic, to assist users with visual impairment in exploring georeferenced data using coordinated maps and tables, augmented with nontextual sounds and speech output. Our in-depth case studies with 7 blind users during 42 hours of data collection, showed that iSonic enabled them to find facts and discover trends in georeferenced data, even in unfamiliar geographical contexts, without special devices. Our design was guided by an Action-by-Design-Component (ADC) framework, which was also applied to scatterplots to demonstrate its generalizability. Video and download is available at www.cs.umd.edu/hcil/iSonic/.","2008-05-01","2023-07-06 05:41:48","2023-07-06 05:41:48","2023-07-06 05:41:48","4:1–4:28","","1","15","","ACM Trans. Comput.-Hum. Interact.","Data Sonification for Users with Visual Impairment","","","","","","","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/JKMVRK6L/Zhao et al. - 2008 - Data Sonification for Users with Visual Impairment.pdf","","","Interactive sonification; auditory user interfaces; information seeking; universal usability; users with visual impairment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VED3FJPK","conferencePaper","2013","Misawa, Daichi","Transparent sculpture: an embodied auditory interface for sound sculpture","Proceedings of the 7th International Conference on Tangible, Embedded and Embodied Interaction","978-1-4503-1898-3","","10.1145/2460625.2460707","https://dl.acm.org/doi/10.1145/2460625.2460707","Toward ecologically distributed interactions of sound in the real world, this paper presents an embodied auditory interface for a sound sculpture; it is composed of orientations' structure of sounds from directional speakers and a pedestal to capture a certain real space.","2013-02-10","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","389–390","","","","","","Transparent sculpture","TEI '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/RIULZBK4/Misawa - 2013 - Transparent sculpture an embodied auditory interf.pdf","","","interaction; sound sculpture; transparent interface","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5PLYWYC5","conferencePaper","2010","Costanza, Enrico; Panchard, Jacques; Zufferey, Guillaume; Nembrini, Julien; Freudiger, Julien; Huang, Jeffrey; Hubaux, Jean-Pierre","SensorTune: a mobile auditory interface for DIY wireless sensor networks","Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","978-1-60558-929-9","","10.1145/1753326.1753675","https://dl.acm.org/doi/10.1145/1753326.1753675","Wireless Sensor Networks (WSNs) allow the monitoring of activity or environmental conditions over a large area, from homes to industrial plants, from agriculture fields to forests and glaciers. They can support a variety of applications, from assisted living to natural disaster prevention. WSNs can, however, be challenging to setup and maintain, reducing the potential for real-world adoption. To address this limitation, this paper introduces SensorTune, a novel mobile interface to support non-expert users in iteratively setting up a WSN. SensorTune uses non-speech audio to present to its users information regarding the connectivity of the network they are setting up, allowing them to decide how to extend it. To simplify the interpretation of the data presented, the system adopts the metaphor of tuning a consumer analog radio, a very common and well known operation. A user study was conducted in which 20 subjects setup real multi-hop networks inside a large building using a limited number of wireless nodes. Subjects repeated the task with SensorTune and with a comparable mobile GUI interface. Experimental results show a statistically significant difference in the task completion time and a clear preference of users for the auditory interface.","2010-04-10","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","2317–2326","","","","","","SensorTune","CHI '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/PNH3HFJB/Costanza et al. - 2010 - SensorTune a mobile auditory interface for DIY wi.pdf","","","sonification; wireless sensor network; user study; mobile hci; network deployment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JL4GJLG8","conferencePaper","2016","Yu, Bin; Hu, Jun; Funk, Mathias; Feijs, Loe","A Study on User Acceptance of Different Auditory Content for Relaxation","Proceedings of the Audio Mostly 2016","978-1-4503-4822-5","","10.1145/2986416.2986418","https://dl.acm.org/doi/10.1145/2986416.2986418","The use of auditory interface at the relaxation-assisted interactive system is becoming increasingly popular. This study aims to investigate the effects of different types of auditory content on the subjective relaxation experience. The participants listened to fifteen sound samples from five categories: (a) nature white noise, (b) natural soundscape, (c) ambient music, (d) instrumental music, (e) instrumental music mixed with the natural soundscape. These auditory contents were selected or designed specifically for assisting relaxation. The study measured the subjective relaxation rating after listening to each sample and interviewed the listeners to understand what causes the differences in relaxation experience. The results indicate that the instrumental music and the combination of nature soundscape and music might be a better auditory content or audio form to induce relaxation compared to the ambient music, pure natural soundscape, and nature white noise. The findings of this study can be used in the design of musical and auditory display in many interactive systems for stress mitigation and relaxation exercises.","2016-10-04","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","69–76","","","","","","","AM '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/RTE3TZWK/Yu et al. - 2016 - A Study on User Acceptance of Different Auditory C.pdf","","","Music; Auditory interface; Nature sounds; Relaxation; User experience","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A89E3MJA","conferencePaper","2011","Sato, Daisuke; Zhu, Shaojian; Kobayashi, Masatomo; Takagi, Hironobu; Asakawa, Chieko","Sasayaki: augmented voice web browsing experience","Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","978-1-4503-0228-9","","10.1145/1978942.1979353","https://dl.acm.org/doi/10.1145/1978942.1979353","Auditory user interfaces have great Web-access potential for billions of people with visual impairments, with limited literacy, who are driving, or who are otherwise unable to use a visual interface. However a sequential speech-based representation can only convey a limited amount of information. In addition, typical auditory user interfaces lose the visual cues such as text styles and page structures, and lack effective feedback about the current focus. To address these limitations, we created Sasayaki (from whisper in Japanese), which augments the primary voice output with a secondary whisper of contextually relevant information, automatically or in response to user requests. It also offers new ways to jump to semantically meaningful locations. A prototype was implemented as a plug-in for an auditory Web browser. Our experimental results show that the Sasayaki can reduce the task completion times for finding elements in webpages and increase satisfaction and confidence.","2011-05-07","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","2769–2778","","","","","","Sasayaki","CHI '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/BK9LM3X8/Sato et al. - 2011 - Sasayaki augmented voice web browsing experience.pdf","","","auditory interface; web accessibility; multiple voices; Sasayaki; voice augmented browsing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WQEJGMUK","conferencePaper","2021","van Rheden, Vincent; Harbour, Eric; Finkenzeller, Thomas; Burr, Lisa Anneke; Meschtscherjakov, Alexander; Tscheligi, Manfred","Run, Beep, Breathe: Exploring the Effects on Adherence and User Experience of 5 Breathing Instruction Sounds while Running","Proceedings of the 16th International Audio Mostly Conference","978-1-4503-8569-5","","10.1145/3478384.3478412","https://dl.acm.org/doi/10.1145/3478384.3478412","Running is a popular sport, especially during the recent pandemic. Breathing techniques can positively impact running, for example to prevent side-stitches or increase lung capacity. Sound instruction is a promising method to administer breathing techniques during running, as it is an established, low-friction method utilized in other contexts such as cycling and meditation. This paper describes an initial study (N=11) exploring the effects of five distinct breathing instruction sounds while running. Sounds were designed with varying information richness and tonality. The study focused on user adherence to the sound and subjective experience of running with the sound. Results show that all sounds were effective in stabilizing the breathing rate. Two-tone sounds were subjectively easier to follow; however, metronome sounds might be preferred for longer studies due to their simplicity and lower invasiveness.","2021-10-15","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","16–23","","","","","","Run, Beep, Breathe","AM '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/FH9SEKCN/van Rheden et al. - 2021 - Run, Beep, Breathe Exploring the Effects on Adher.pdf","","","human-computer interaction; sports; user experience; Auditory interface; sound-richness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K88KWVVV","conferencePaper","2015","Drossos, Konstantinos; Zormpas, Nikolaos; Giannakopoulos, George; Floros, Andreas","Accessible games for blind children, empowered by binaural sound","Proceedings of the 8th ACM International Conference on PErvasive Technologies Related to Assistive Environments","978-1-4503-3452-5","","10.1145/2769493.2769546","https://dl.acm.org/doi/10.1145/2769493.2769546","Accessible games have been researched and developed for many years, however, blind people still have very limited access and knowledge of them. This can pose a serious limitation, especially for blind children, since in recent years electronic games have become one of the most common and wide spread means of entertainment and socialization. For our implementation we use binaural technology which allows the player to hear and navigate the game space by adding localization information to the game sounds. With our implementation and user studies we provide insight on what constitutes an accessible game for blind people as well as a functional game engine for such games. The game engine developed allows the quick development of games for the visually impaired. Our work provides a good starting point for future developments on the field and, as the user studies show, was very well perceived by the visually impaired children that tried it.","2015-07-01","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","1–8","","","","","","","PETRA '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/I7LZY5QE/Drossos et al. - 2015 - Accessible games for blind children, empowered by .pdf","","","audio only games; auditory interface; binaural processing; games for the visually impaired","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MEY7N26Q","conferencePaper","2010","Wolf, Katrin; Dicke, Christina; Grasset, Raphael","Touching the void: gestures for auditory interfaces","Proceedings of the fifth international conference on Tangible, embedded, and embodied interaction","978-1-4503-0478-8","","10.1145/1935701.1935772","https://dl.acm.org/doi/10.1145/1935701.1935772","Nowadays, mobile devices provide new possibilities for gesture interaction due to the large range of embedded sensors they have and their physical form factor. In addition, auditory interfaces can now be more easily supported through advanced mobile computing capabilities. Although different types of gesture techniques have been proposed for handheld devices, there is still little knowledge about the acceptability and use of some of these techniques, especially in the context of an auditory interface. In this paper, we propose a novel approach to the problem by studying the design space of gestures proposed by end-users for a mobile auditory interface. We discuss the results of this explorative study, in terms of the scope of the gestures proposed, the tangible aspects, and the users' preferences. This study delivers some initial gestures recommendations for eyes-free auditory interfaces.","2010-01-22","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","305–308","","","","","","Touching the void","TEI '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/A48UE85J/Wolf et al. - 2010 - Touching the void gestures for auditory interface.pdf","","","auditory display; gestures; embodied interaction; mobile; eyes-free; participatory design.; tangible interface","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B2IYWEF7","conferencePaper","2012","Oki, Maho; Tsukada, Koji; Kurihara, Kazutaka; Siio, Itiro","HomeOrgel: interactive music box for the aural representation of home activities","Proceedings of the 10th asia pacific conference on Computer human interaction","978-1-4503-1496-1","","10.1145/2350046.2350083","https://dl.acm.org/doi/10.1145/2350046.2350083","We propose a music-box-type interface, ""HomeOrgel"", that can express various activities in the home using sound. Users can also control the volume and content using common methods for controlling a music box: opening the cover and winding the spring. Users can hear the sounds of past home activities, such as cooking and the opening/closing of doors with the background music (BGM) mechanism of the music box. We developed the HomeOrgel device and installed it in an actual house. We also verify the effectiveness of our system through evaluation and discussion.","2012-08-28","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","177–186","","","","","","HomeOrgel","APCHI '12","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/3JJEYCYN/Oki et al. - 2012 - HomeOrgel interactive music box for the aural rep.pdf","","","auditory display; smart home; ubiquitous computing; music box","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EZKRLVMG","conferencePaper","2016","Goudarzi, Visda","Exploration of sonification design process through an interdisciplinary workshop","Proceedings of the Audio Mostly 2016","978-1-4503-4822-5","","10.1145/2986416.2986422","https://dl.acm.org/doi/10.1145/2986416.2986422","In sonification of scientific data, designers know very little about the domain science and domain scientists are not familiar with the sonification methodology. The knowledge about the domain science is not given, but evolved during the problem-solving process. We discuss design challenges in auditory display design regarding user-centered approaches and suggest a method to involve domain scientists throughout sonification designs. We explore this within a workshop in which sonification experts, domain experts, and programmers worked together to better understand and solve problems collaboratively. The sonification framework that is used during the workshops is briefly described and the workshop process and how each group worked together during the workshop sessions are examined. Participants worked on pre-defined and exploratory tasks to sonify climate data. Furthermore, they grasped each other's domains; climate scientists especially became more open to use auditory display and sonification as a tool in their data mining tasks. Resulting sonification prototypes and workshop sessions are documented on a wiki to be used by the sonification community. To get started, we used some of the sonification designs created during the workshop for an online study where participants from science, engineering, and humanities were asked questions about the data behavior by listening to sonifcations of bivariate time series. Results indicate that sonic representation of data from resulting sonification allows most users (even with little or no knowledge of sound and music) to successfully complete some common data exploration tasks.","2016-10-04","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","147–153","","","","","","","AM '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/4LLPJDB2/Goudarzi - 2016 - Exploration of sonification design process through.pdf","","","Sonification; Auditory Display; Participatory Design; User Experience","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3RAHPVEJ","conferencePaper","2021","Rönnberg, Niklas","Sonification for Conveying Data and Emotion","Proceedings of the 16th International Audio Mostly Conference","978-1-4503-8569-5","","10.1145/3478384.3478387","https://dl.acm.org/doi/10.1145/3478384.3478387","In the present study a sonification of running data was evaluated. The aim of the sonification was to both convey information about the data and convey a specific emotion. The sonification was evaluated in three parts, firstly as an auditory graph, secondly together with additional text information, and thirdly together with an animated visualization, with a total of 150 responses. The results suggest that the sonification could convey an emotion similar to that intended, but at the cost of less good representation of the data. The addition of visual information supported understanding of the sonification, and the auditory representation of data. The results thus suggest that it is possible to design sonification that is perceived as both interesting and fun, and convey an emotional impression, but that there may be a trade off between musical experience and clarity in sonification.","2021-10-15","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","56–63","","","","","","","AM '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/C7KYC6JT/Rönnberg - 2021 - Sonification for Conveying Data and Emotion.pdf","","","sonification; model of affect; user evaluation; visualization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BCXHLM9Z","conferencePaper","2021","Boger, Tal; Ananthabhotla, Ishwarya; Paradiso, Joseph","Manipulating Causal Uncertainty in Sound Objects","Proceedings of the 16th International Audio Mostly Conference","978-1-4503-8569-5","","10.1145/3478384.3478405","https://dl.acm.org/doi/10.1145/3478384.3478405","Causal uncertainty – how sure we are in what produced a sound that we are listening to – is a fundamental aspect of auditory cognition. It is known to be a driver of affect perception, attention, and memory, among other processes. Here, we present an optimization pipeline that systematically manipulates a sound object’s intrinsic causal uncertainty by applying a set of acoustic transforms, such as scaling a sound’s pitch, amplitude, playback speed, etc. The optimization estimator attempts to produce parameter values for these transforms that modify a sound’s causal uncertainty (Hcu), as measured by the prediction confidence of an audio classification neural network, while minimizing changes to the resulting prediction labels and transform magnitudes. We then conduct a listening test with N=20 participants to confirm that the causal uncertainty changes resulting from our proposed procedure align with human perception. Though a simple approach, this work demonstrates a first step towards generative audio systems that operate along cognitive dimensions, with powerful implications for user experience design.","2021-10-15","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","9–15","","","","","","","AM '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/7RPB6YQG/Boger et al. - 2021 - Manipulating Causal Uncertainty in Sound Objects.pdf","","","auditory perception; auditory cognition; causal uncertainty; optimization; sound manipulation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U4GLNYAJ","conferencePaper","2011","Yuksel, Kamer Ali; Buyukbas, Sinan; Adali, Serdar Hasan","Designing mobile phones using silent speech input and auditory feedback","Proceedings of the 13th International Conference on Human Computer Interaction with Mobile Devices and Services","978-1-4503-0541-9","","10.1145/2037373.2037492","https://dl.acm.org/doi/10.1145/2037373.2037492","In this work, we have propose a novel design for a basic mobile phone, which is focused on the essence of mobile communication and connectivity, based on a silent speech interface and auditory feedback. This assistive interface takes the advantages of voice control systems while discarding its disadvantages such as the background noise, privacy and social acceptance. The proposed device utilizes low-cost and commercially available hardware components. Thus, it would be affordable and accessible by majority of users including disabled, elderly and illiterate people.","2011-08-30","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","711–713","","","","","","","MobileHCI '11","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/95PS46ZY/Yuksel et al. - 2011 - Designing mobile phones using silent speech input .pdf","","","mobile phone; silent speech interface; vibration sensors","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5AF6QN2M","conferencePaper","2022","Zhang, Lotus; Shao, Jingyao; Liu, Augustina Ao; Jiang, Lucy; Stangl, Abigale; Fourney, Adam; Morris, Meredith Ringel; Findlater, Leah","Exploring Interactive Sound Design for Auditory Websites","Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems","978-1-4503-9157-3","","10.1145/3491102.3517695","https://dl.acm.org/doi/10.1145/3491102.3517695","Auditory interfaces increasingly support access to website content, through recent advances in voice interaction. Typically, however, these interfaces provide only limited audio styling, collapsing rich visual design into a static audio output style with a single synthesized voice. To explore the potential for more aesthetic and intuitive sound design for websites, we prompted 14 professional sound designers to create auditory website mockups and interviewed them about their designs and rationale. Our findings reveal their prioritized design considerations (aesthetics and emotion, user engagement, audio clarity, information dynamics, and interactivity), specific sound design ideas to support each consideration (e.g., replacing spoken labels with short, memorable audio expressions), and challenges with applying sound design practices to auditory websites. These findings provide promising direction for how to support designers in creating richer auditory website experiences.","2022-04-29","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","1–16","","","","","","","CHI '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/ZDNYAESS/Zhang et al. - 2022 - Exploring Interactive Sound Design for Auditory We.pdf","","","audio display; interaction design; voice interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ESGHEIMY","conferencePaper","2008","Song, Hong Jun; Beilharz, Kirsty","Aesthetic and auditory enhancements for multi-stream information sonification","Proceedings of the 3rd international conference on Digital Interactive Media in Entertainment and Arts","978-1-60558-248-1","","10.1145/1413634.1413678","https://dl.acm.org/doi/10.1145/1413634.1413678","Sonification is an emerging modality of information representation, the auditory equivalent of visualization employing non-speech sound to display attributes of form, pattern, recurrence and trends in abstract data. Like data-art or visual and auditory art-forms driven by data content directly mapped to their rendering, sonification shares the goal of aesthetic representation (auditory graphing) in a way to better and more accessibly convey the message to broader consumer audiences. Often, the simple re-contextualization of dense abstract data in an auditory graph (or sonification) is sufficient to highlight long-term trends, to hear regularities (patterns) and anomalies in periodicity of time-series data and to assimilate very subtle and fine transformations. Sonification is also optimal for certain working or ambient situations that are visually rich or visually saturated, when we seek to command topical and peripheral attention with relevant cues. Auditory display is also an alternative to visualization for people with visual impairments. Exploring the premise that sonification should be both aesthetic and informative, i.e. listenable, attractive and engaging, this paper summarises the findings of 3 experiments conducted to determine ways to better represent and access dense information mapped on to more than one concurrent stream of information. Specifically, we show evidence that spatialization of informative events coinciding in time can be more clearly distinguished and that timbre (or tone colour / tone quality) characteristics can serve to further reinforce spatial and stream separation. These findings combine to develop comprehensible methods for representing complex data-sets. We consider human cognition, auditory perception and audio reproduction technologies that each influence the ability to display information sonically.","2008-09-10","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","224–231","","","","","","","DIMEA '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/4FNJAPRV/Song and Beilharz - 2008 - Aesthetic and auditory enhancements for multi-stre.pdf","","","auditory stream segregation; concurrent auditory graphing; information sonification; spatialization; timbre","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VY94ZRU8","conferencePaper","2009","Kurdyukova, Ekaterina","Inspire, guide, and entertain: designing a mobile assistant for runners","Proceedings of the 11th International Conference on Human-Computer Interaction with Mobile Devices and Services","978-1-60558-281-8","","10.1145/1613858.1613947","https://dl.acm.org/doi/10.1145/1613858.1613947","The paper presents the design of a mobile assistant for runners. We propose visual and auditory user interface for a mobile assistant, called Mobota. The system supports navigation on a new track, provides competition against a virtual rival, monitors real time user performance, entertains and encourages runners. We introduce entertaining and inspiring community notes that convey emotional messages from other sportsmen who exercise on the same track. The design and evaluation of Mobota provides an insight into the specifics of visual and auditory design of running assistants.","2009-09-15","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","1–2","","","","","","Inspire, guide, and entertain","MobileHCI '09","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/SJ3QLCVZ/Kurdyukova - 2009 - Inspire, guide, and entertain designing a mobile .pdf","","","auditory design; mobile sports; mobile UI; training assistant","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7LFA2UEF","conferencePaper","2008","Dicke, Christina; Deo, Shaleen; Billinghurst, Mark; Adams, Nathan; Lehikoinen, Juha","Experiments in mobile spatial audio-conferencing: key-based and gesture-based interaction","Proceedings of the 10th international conference on Human computer interaction with mobile devices and services","978-1-59593-952-4","","10.1145/1409240.1409251","https://dl.acm.org/doi/10.1145/1409240.1409251","In this paper we describe an exploration into the usability of spatial sound and multimodal interaction techniques for a mobile phone conferencing application. We compared traditional keypad based-interaction to that of a newer approach using the phone itself as a device to navigate within a virtual spatial auditory environment. While the traditional keypad interaction proved to be more straightforward to use, there was no significant impact on task completion times or number of interaction movements made between the techniques. Overall, users felt that the spatial audio application supported group awareness while aiding peripheral task monitoring. They also felt it aided the feeling of social connectedness and offered enhanced support for communication.","2008-09-02","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","91–100","","","","","","Experiments in mobile spatial audio-conferencing","MobileHCI '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/CNRL4RNV/Dicke et al. - 2008 - Experiments in mobile spatial audio-conferencing .pdf","","","spatial audio; gesture interaction; mobile HCI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K5KYEIRC","conferencePaper","2013","Chittaro, Luca; Zuliani, Francesco","Exploring audio storytelling in mobile exergames to affect the perception of physical exercise","Proceedings of the 7th International Conference on Pervasive Computing Technologies for Healthcare","978-1-936968-80-0","","10.4108/icst.pervasivehealth.2013.252016","https://dl.acm.org/doi/10.4108/icst.pervasivehealth.2013.252016","Exergames (video games that combine exercise and play) can make the experience of physical activities more enjoyable. Mobile exergames are particularly interesting as they can be used for outdoors, open-air physical activities. Unfortunately, current mobile exergames tend to require the player to frequently or continuously look at the screen. This can be hard to do while exercising, and it also requires the player to considerably distract visual attention from the surrounding physical environment, introducing safety issues in activities such as outdoor running. In this paper, we focus on two main goals. First, we explore how to use audio storytelling techniques to make physical exercise more engaging and enjoyable by exploiting a soundscape that provides prompt feedback in response to players' activity and does not require the player to look at the screen during running. Second, we study if the exergame is fun for users and if it positively affects the perception of the running experience. We measure important variables such as level of physical activity in player's lifestyle and player's physical activity enjoyment through validated methods employed in the medical literature. The results of the study show that the use of audio storytelling techniques in mobile exergames is appreciated by users, and the exergame has positive effects on the perception of physical exercise.","2013-05-05","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","1–8","","","","","","","PervasiveHealth '13","","","","ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)","Brussels, BEL","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/U65W44CN/Chittaro and Zuliani - 2013 - Exploring audio storytelling in mobile exergames t.pdf","","","exergames; audio; mobile; health; enjoyment; pervasive technology; physical exercise; safety; storytelling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UGFA6EGL","conferencePaper","2010","Vazquez Alvarez, Yolanda; Brewster, Stephen A.","Designing spatial audio interfaces to support multiple audio streams","Proceedings of the 12th international conference on Human computer interaction with mobile devices and services","978-1-60558-835-3","","10.1145/1851600.1851642","https://dl.acm.org/doi/10.1145/1851600.1851642","Auditory interfaces offer a solution to the problem of effective eyes-free mobile interactions. However, a problem with audio, as opposed to visual displays, is dealing with multiple simultaneous outputs. Any audio interface needs to consider: 1) simultaneous versus sequential presentation of multiple audio streams, 2) 3D audio techniques to place sounds in different spatial locations versus a single point of presentation, 3) dynamic movement versus fixed locations of audio sources. We present an experiment using a divided-attention task where a continuous podcast and an audio menu compete for attention. A sequential presentation baseline assessed the impact of cognitive load, and as expected, dividing attention had a significant effect on overall performance. However, spatial audio still increased the users' ability to attend to two streams, while dynamic movement of streams led to higher perceived workload. These results will provide guidelines for designers when building eyes-free auditory interfaces for mobile applications.","2010-09-07","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","253–256","","","","","","","MobileHCI '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/XZ3RCAH6/Vazquez Alvarez and Brewster - 2010 - Designing spatial audio interfaces to support mult.pdf","","","spatial audio; auditory interfaces; divided-attention task; mobile systems; multiple audio streams","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U69BRXX9","conferencePaper","2007","Stahl, Christoph","The roaring navigator: a group guide for the zoo with shared auditory landmark display","Proceedings of the 9th international conference on Human computer interaction with mobile devices and services","978-1-59593-862-6","","10.1145/1377999.1378042","https://dl.acm.org/doi/10.1145/1377999.1378042","In this paper, we introduce a shared auditory landmark display which conveys spatial survey knowledge and navigational aid to multiple users. Our guide is situated in a zoo environment, so we use recordings of animal voices to indicate the location of the animal enclosures. Spatial audio manipulates the volume and stereo balance of the sound clips, so that the listener can identify their distance and direction. The system also proactively presents audio clips with detailed information about each animal. To avoid the typical effect of social isolation through audio guides, we use shared audio so that the same sounds will be presented to each user at the same time. We have conducted an initial user study of paired visitors in the zoo to evaluate the usability of the system with positive results. The participants reported that the system is easy to use and has a stimulating influence on the communication between the visitors. As a further result, the study indicates that 'lightweight' navigational aid can be sufficient for wayfinding tasks in certain environments, which provides only the linear distance and direction of the destination.","2007-09-09","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","383–386","","","","","","The roaring navigator","MobileHCI '07","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/4N8ZGSNS/Stahl - 2007 - The roaring navigator a group guide for the zoo w.pdf","","","spatial audio; audio guide; auditory landmark display; electronic guidebooks; pedestrian navigation; shared audio","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TE4Z9DPY","conferencePaper","2008","Wang, Lei; Roe, Paul; Pham, Binh; Tjondronegoro, Dian","An audio wiki supporting mobile collaboration","Proceedings of the 2008 ACM symposium on Applied computing","978-1-59593-753-7","","10.1145/1363686.1364145","https://dl.acm.org/doi/10.1145/1363686.1364145","Wikis have proved to be very effective collaboration and knowledge management tools in large variety of fields thanks to their simplicity and flexible nature. Another important development for the internet is the emergence of powerful mobile devices supported by fast and reliable wireless networks. The combination of these developments begs the question of how to extend wikis on mobile devices and how to leverage mobile devices' rich modalities to supplement current wikis. Realizing that composing and consuming through auditory channel is the most natural and efficient way for mobile device user, this paper explores the use of audio as the medium of wiki. Our work, as the first step towards this direction, creates a framework called Mobile Audio Wiki which facilitates asynchronous audio-mediated collaboration on the move. In this paper, we present the design of Mobile Audio Wiki. As a part of such design, we propose an innovative approach for a light-weight audio content annotation system for enabling group editing, versioning and cross-linking among audio clips. To elucidate the novel collaboration model introduced by Mobile Audio Wiki, its four usage modes are identified and presented in storyboard format. Finally, we describe the initial design for presentation and navigation of Mobile Audio Wiki.","2008-03-16","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","1889–1896","","","","","","","SAC '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/GWTR9Q2Z/Wang et al. - 2008 - An audio wiki supporting mobile collaboration.pdf","","","mobile; asynchronous audio-mediated collaboration; wiki","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5J8AH95H","conferencePaper","2010","Murphy, Emma; Bates, Enda; Fitzpatrick, Dónal","Designing auditory cues to enhance spoken mathematics for visually impaired users","Proceedings of the 12th international ACM SIGACCESS conference on Computers and accessibility","978-1-60558-881-0","","10.1145/1878803.1878819","https://dl.acm.org/doi/10.1145/1878803.1878819","Visual mathematic notation provides a succinct and unambiguous description of the structure of mathematical formulae in a manner that is difficult to replicate through the linear channels of synthesized speech and Braille. It is proposed that the use of auditory cues can enhance accessibility to mathematical material and reduce common ambiguities encountered through spoken mathematics. However, the use of additional complex hierarchies of non-speech sounds to represent the structure and scope of equations may be cognitively demanding to process. This can detract from the users' understanding of the mathematical content. In this paper, a new system is presented, which uses a mixture of non-speech auditory cues, modified speech (spearcons) and binaural spatialization to disambiguate the structure of mathematical formulae. A design study, involving an online survey with 56 users, was undertaken to evaluate an existing set of auditory cues and to brainstorm alternative ideas and solutions from users before implementing modified designs and conducting a separate controlled evaluation. It is proposed that by involving a wide number of users in the creative design process, intuitive auditory cues will be implemented with the potential to enhance spoken mathematics for visually impaired users.","2010-10-25","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","75–82","","","","","","","ASSETS '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/5L3CUKI3/Murphy et al. - 2010 - Designing auditory cues to enhance spoken mathemat.pdf","","","accessibility; spearcons; non-speech sound; visually impaired users; design methods for user interfaces; mathematics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A4XSQ2QY","conferencePaper","2019","Bennett, Cynthia L.; Stangl, Abigale; Siu, Alexa F.; Miele, Joshua A.","Making Nonvisually: Lessons from the Field","Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility","978-1-4503-6676-2","","10.1145/3308561.3355619","https://dl.acm.org/doi/10.1145/3308561.3355619","The Maker movement promises access to activities from crafting to digital fabrication for anyone to invent and customize technology. But people with disabilities, who could benefit from Making, still encounter significant barriers to do so. In response, we share our personal experiences Making nonvisually and supporting its instruction. Specifically, we draw on examples from a series of workshops where we introduced Arduino to blind hobbyists and guided assembly of an accessible voltmeter prototype [24]. In so doing, we offer future directions for accessible Making research and application.","2019-10-24","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","279–285","","","","","","Making Nonvisually","ASSETS '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/HMZNBGS6/Bennett et al. - 2019 - Making Nonvisually Lessons from the Field.pdf","","","accessibility; blind; arduino; making","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I55ZRYXF","conferencePaper","2008","Garzonis, Stavros; Bevan, Chris; O'Neill, Eamonn","Mobile service audio notifications: intuitive semantics and noises","Proceedings of the 20th Australasian Conference on Computer-Human Interaction: Designing for Habitus and Habitat","978-0-9803063-4-7","","10.1145/1517744.1517793","https://dl.acm.org/doi/10.1145/1517744.1517793","It is hoped that context-aware systems will present users with an increasing number of relevant services in an increasingly wide range of contexts. With this expansion, numerous service notifications could overwhelm users. Therefore, careful design of the notification mechanism is needed. In this paper, we investigate how semantic richness of different types of audio stimuli can be utilised to shape the intuitiveness of mobile service notifications. In order to do so, we first develop a categorisation of mobile services so that clustered services can share the same notifications. Not surprisingly, it was found that overall speech performed better than non-speech sounds, and auditory icons performed overall better than earcons. However, exceptions were observed when richer semantics were utilised in the seemingly poorer medium. We argue that success and subjective preference of auditory mobile service notifications heavily depends on the success and level of directness of the metaphors used.","2008-12-08","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","156–163","","","","","","Mobile service audio notifications","OZCHI '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/LECD8C6W/Garzonis et al. - 2008 - Mobile service audio notifications intuitive sema.pdf","","","auditory icons; earcons; context awareness; intuitiveness of audio notifications; mobile audio notifications; mobile services categorisation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VHIKH8RM","conferencePaper","2022","Han, Ying; Zhang, Yuqing","Application of Efficient Emotional Arousal in the ""Internet +"" Smart-classroom Teaching Environment","Proceedings of the 2022 3rd International Conference on Education Development and Studies","978-1-4503-9627-1","","10.1145/3528137.3528158","https://dl.acm.org/doi/10.1145/3528137.3528158","In the context of artificial intelligence, the application of ""Internet +"" smart-classroom teaching environment is of great value to modern teaching practice. In education environment, teachers can control the teaching environment and adopt emotional awakening as the main penetration principle, which can effectively promote students' learning efficiency. This study is based on the ecological valence theory (EVT), Combining Thomas and Chess PTQ as well as the odor preference questionnaire, 339 Chinese children aged 3 – 7 were randomly selected as subjects, The results show that 1, olfactory sensitivity can be used as an important environmental science application element in the smart-classroom of children aged 3-7; 2, it can effectively improve Chinese children's mood and persistence that the intelligent ecosystem in the environment through the linkage application; 3, it contributes to the accurate dissemination of odor categories in smart classrooms that establishment of child trait big data analysis; 4, uses different sensing devices and man-machine interaction modules to effectively increase the control of the teaching space, enhance students' sense of self-efficacy.","2022-05-31","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","42–51","","","","","","","ICEDS '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/M4995XDN/Han and Zhang - 2022 - Application of Efficient Emotional Arousal in the .pdf","","","emotional arousal, temperament, olfactory sensitivity; Key words: Internet +; smart classroom","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EVARKYXF","conferencePaper","2020","Quinton, Michael; McGregor, Iain; Benyon, David","Sonification of an exoplanetary atmosphere","Proceedings of the 15th International Audio Mostly Conference","978-1-4503-7563-4","","10.1145/3411109.3411117","https://dl.acm.org/doi/10.1145/3411109.3411117","This study investigates the effectiveness of user design methods to create a sonification for an astronomer who analyses exoplanet meteorological data situated in habitable zones. Requirements about the astronomer's work, the dataset and how to sonify it utilising Grounded Theory were identified. Parameter mapping sonification was used to represent effective transiting radii measurements through subtractive synthesis and spatialization. The design was considered to be effective, allowing the instantaneous identification of a water feature overlooked on a visual graph, even when noise within the dataset overlapped the source signal. The results suggest that multiple parameter mappings provide richer auditory stimuli and semantic qualities in order to allow an improved understanding of the dataset.","2020-09-16","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","191–198","","","","","","","AM '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/GLA6QMAT/Quinton et al. - 2020 - Sonification of an exoplanetary atmosphere.pdf","","","sonification; astronomy; exoplanet atmospheres; grounded theory; parameter mapping sonification; user centred design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EBIRI4QZ","conferencePaper","2015","Feng, Feng; Stockman, Tony; Bryan-Kinns, Nick; AI-Thani, Dena","An investigation into the comprehension of map information presented in audio","Proceedings of the XVI International Conference on Human Computer Interaction","978-1-4503-3463-1","","10.1145/2829875.2829896","https://dl.acm.org/doi/10.1145/2829875.2829896","The growth in mobile and multimodal Computing is leading to the consideration of alternative modes of information presentation, particularly in situations such as driving or walking in unfamiliar locations where the eyes are needed for primary navigation. We report the results of an experiment in which map information is presented to 10 normally sighted participants using an auditory display. Several measures of performance are reported, including the time to navigate a virtual route, keystroke errors and the ability to construct a visual representation of the route travelled based on audio instructions only. The results show significant variability in levels of performance between individuals, though most participants were able to make sense of the auditory display and produce a reasonable visual representation of the virtual route i.e. participants were able to comprehend the presented audio map.","2015-09-07","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","1–8","","","","","","","Interacci&#xf3;n '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/XXD8PJFG/Feng et al. - 2015 - An investigation into the comprehension of map inf.pdf","","","Auditory display; audio information understanding; audio map system; spatial information","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"39K3R3XL","conferencePaper","2008","Coleman, Graeme W.; Macaulay, Catriona; Newell, Alan F.","Sonic mapping: towards engaging the user in the design of sound for computerized artifacts","Proceedings of the 5th Nordic conference on Human-computer interaction: building bridges","978-1-59593-704-9","","10.1145/1463160.1463170","https://dl.acm.org/doi/10.1145/1463160.1463170","This paper argues for new approaches to the design of sound for contemporary interactive technologies. We begin by presenting what we feel to be the key challenges as yet unaddressed by conventional auditory display research. Subsequently, we propose a user-centered, acoustic ecology-informed, design method that we feel can be built upon to inform the design of sound for contemporary interactive technologies, thus tackling some of the challenges introduced. Our approach consists of three stages: firstly, encouraging designers and users to experience the original auditory environment, identifying the key sounds within that environment, and then summarizing this information into an 'earwitness account' that can be used as a prelude for informing the design of sonically enhanced technologies that may be used within similar environments. By trialing this method with undergraduate interactive media design students, we identify the methodological challenges involved in attempting to engage people, who are not necessarily 'sound professionals', with their existing auditory environments. We highlight the opportunities that arise and pitfalls that should be avoided when attempting to introduce such approaches within real-world design studies.","2008-10-20","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","83–92","","","","","","Sonic mapping","NordiCHI '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/WH6PBLER/Coleman et al. - 2008 - Sonic mapping towards engaging the user in the de.pdf","","","acoustic ecology; soundscapes; auditory environments; auditory interfaces; non-speech sound; user-centered design methods","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7JH8LKJ4","conferencePaper","2012","Yang, Tao; Ferati, Mexhid; Liu, Yikun; Rohani Ghahari, Romisa; Bolchini, Davide","Aural browsing on-the-go: listening-based back navigation in large web architectures","Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","978-1-4503-1015-4","","10.1145/2207676.2207715","https://dl.acm.org/doi/10.1145/2207676.2207715","Mobile web navigation requires highly-focused visual attention, which poses problems when it is inconvenient or distracting to continuously look at the screen (e.g., while walking). Aural interfaces support more eyes-free experiences, as users can primarily listen to the content and occasionally look at the device. Yet, designing aural information architectures remains a challenge. Specifically, back navigation is inefficient in the aural setting, as it forces users to listen to each previous page to retrieve the desired content. This paper introduces topic- and list-based back: two navigation strategies to enhance aural browsing. Both are manifest in Green-Savers Mobile (GSM), an aural mobile site. A study (N=29) compared both solutions to traditional back mechanisms. Our findings indicate that topic- and list-based back enable faster access to previous pages, improve the navigation experience and reduce perceived cognitive load. The proposed designs apply to a wide range of content-intensive, ubiquitous web systems.","2012-05-05","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","277–286","","","","","","Aural browsing on-the-go","CHI '12","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/J45R5LVC/Yang et al. - 2012 - Aural browsing on-the-go listening-based back nav.pdf","","","aural web; back navigation; info architecture; mobile web","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2FX8KJ3Y","conferencePaper","2018","Pradhan, Alisha; Mehta, Kanika; Findlater, Leah","""Accessibility Came by Accident"": Use of Voice-Controlled Intelligent Personal Assistants by People with Disabilities","Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","978-1-4503-5620-6","","10.1145/3173574.3174033","https://dl.acm.org/doi/10.1145/3173574.3174033","From an accessibility perspective, voice-controlled, home-based intelligent personal assistants (IPAs) have the potential to greatly expand speech interaction beyond dictation and screen reader output. To examine the accessibility of off-the-shelf IPAs (e.g., Amazon Echo) and to understand how users with disabilities are making use of these devices, we conducted two exploratory studies. The first, broader study is a content analysis of 346 Amazon Echo reviews that include users with disabilities, while the second study more specifically focuses on users with visual impairments, through interviews with 16 current users of home-based IPAs. Findings show that, although some accessibility challenges exist, users with a range of disabilities are using the Amazon Echo, including for unexpected cases such as speech therapy and support for caregivers. Richer voice-based applications and solutions to support discoverability would be particularly useful to users with visual impairments. These findings should inform future work on accessible voice-based IPAs.","2018-04-21","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","1–13","","","","","","""Accessibility Came by Accident""","CHI '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/6F9XHD9E/Pradhan et al. - 2018 - Accessibility Came by Accident Use of Voice-Con.pdf","","","accessibility; speech; conversational interfaces; disability; intelligent personal assistants","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PQISJYGD","conferencePaper","2019","Andrade, Ronny; Rogerson, Melissa J.; Waycott, Jenny; Baker, Steven; Vetere, Frank","Playing Blind: Revealing the World of Gamers with Visual Impairment","Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","978-1-4503-5970-2","","10.1145/3290605.3300346","https://dl.acm.org/doi/10.1145/3290605.3300346","Previous research on games for people with visual impairment (PVI) has focused on co-designing or evaluating specific games - mostly under controlled conditions. In this research, we follow a game-agnostic, ""in-the-wild"" approach, investigating the habits, opinions and concerns of PVI regarding digital games. To explore these issues, we conducted an online survey and follow-up interviews with gamers with VI (GVI). Dominant themes from our analysis include the particular appeal of digital games to GVI, the importance of social trajectories and histories of gameplay, the need to balance complexity and accessibility in both games targeted to PVI and mainstream games, opinions about the state of the gaming industry, and accessibility concerns around new and emerging technologies such as VR and AR. Our study gives voice to an underrepresented group in the gaming community. Understanding the practices, experiences and motivations of GVI provides a valuable foundation for informing development of more inclusive games.","2019-05-02","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","1–14","","","","","","Playing Blind","CHI '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/LIIX98A9/Andrade et al. - 2019 - Playing Blind Revealing the World of Gamers with .pdf","","","visual impairment; audiogames; digital games; empowerment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6WP8VR9U","conferencePaper","2022","Johansen, Stine S.; van Berkel, Niels; Fritsch, Jonas","Characterising Soundscape Research in Human-Computer Interaction","Designing Interactive Systems Conference","978-1-4503-9358-4","","10.1145/3532106.3533458","https://dl.acm.org/doi/10.1145/3532106.3533458","‘Soundscapes’ are an increasingly active topic in Human-Computer Interaction (HCI) and interaction design. From mapping acoustic environments through sound recordings to designing compositions as interventions, soundscapes appear as a recurring theme across a wide body of HCI research. Based on this growing interest, now is the time to explore the types of studies in which soundscapes provide a valuable lens to HCI research. In this paper, we review papers from conferences sponsored or co-sponsored by the ACM Special Interest Group on Computer-Human Interaction in which the term ’soundscape’ occurs. We analyse a total of 235 papers to understand the role of soundscapes as a research focus and identify untapped opportunities for soundscape research within HCI. We identify two common soundscape conceptualisations: (1) Acoustic environments and (2) Compositions, and describe what characterises studies into each concept and the hybrid forms that also occur. On the basis of this, we carve out a foundation for future soundscape research in HCI as a methodological anchor to form a common ground and support this growing research interest. Finally, we offer five recommendations for further research into soundscapes within HCI.","2022-06-13","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","1394–1417","","","","","","","DIS '22","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/6VDRWGZX/Johansen et al. - 2022 - Characterising Soundscape Research in Human-Comput.pdf","","","Soundscape; audio; literature review; sounds; theory","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GXA2BSPD","conferencePaper","2017","Thieme, Anja; Morrison, Cecily; Villar, Nicolas; Grayson, Martin; Lindley, Siân","Enabling Collaboration in Learning Computer Programing Inclusive of Children with Vision Impairments","Proceedings of the 2017 Conference on Designing Interactive Systems","978-1-4503-4922-2","","10.1145/3064663.3064689","https://dl.acm.org/doi/10.1145/3064663.3064689","We investigate how technology can support collaborative learning by children with mixed-visual abilities. Responding to a growing need for tools inclusive of children with vision impairments (VI) for the teaching of computer programing to novice learners, we explore Torino -- a physical programing language for teaching programing constructs and computational thinking to children age 7-11. We draw insights from 12 learning sessions with Torino that involved five pairs of children with vision ranging from blindness to full-sight. Our findings show how sense-making of the technology, collaboration, and learning were enabled through an interplay of system design, programing tasks and social interactions, and how this differed between the pairs. The paper contributes insights on the role of touch, audio and visual representations in designs inclusive of people with VI, and discusses the importance and opportunities provided through the 'social' in negotiations of accessibility, for learning, and for self-perceptions of ability and self-esteem.","2017-06-10","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","739–752","","","","","","","DIS '17","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/F7XS3BIG/Thieme et al. - 2017 - Enabling Collaboration in Learning Computer Progra.pdf","","","accessibility; education; visual impairment; collaboration; design for children; tactility; tangibility; visual disability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7EZ6KGMB","conferencePaper","2016","Fritsch, Jonas; Grönvall, Erik; Breinbjerg, Morten","Analyzing the aesthetics of participation of media architecture","Proceedings of the 3rd Conference on Media Architecture Biennale","978-1-4503-4749-5","","10.1145/2946803.2946807","https://dl.acm.org/doi/10.1145/2946803.2946807","This paper presents a theoretical framework for analyzing the aesthetics of participation of media architecture. The framework is based on a close reading of French philosopher Jacques Rancière and provides four points of emphasis: modes of sense perception, forms of engagement, community and emancipation. The framework is put to use in the analysis of three experimental media architectural projects; Ekkomaten/Echoes from Møllevangen, the coMotion Bench and FeltRadio. We discuss the findings from this analysis and outline future perspectives on how to develop and use the framework prospectively in the design of media architectural projects and other interactive environments.","2016-06-01","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","1–10","","","","","","","MAB","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/KPE9VERV/Fritsch et al. - 2016 - Analyzing the aesthetics of participation of media.pdf","","","media architecture; aesthetics of participation; experience philosophy; politics of sensation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IMU58RA5","conferencePaper","2014","Fritsch, Jonas; Breinbjerg, Morten; Jensen, Tue S.","Designing interactive listening situations","Proceedings of the 26th Australian Computer-Human Interaction Conference on Designing Futures: the Future of Design","978-1-4503-0653-9","","10.1145/2686612.2686618","https://dl.acm.org/doi/10.1145/2686612.2686618","This article presents the interactive sound installation Ekkomaten, a machine originally designed to let people explore an 18th century soundscape as part of a historical festival in Aarhus, Denmark. We present the design of the installation focusing on three core concerns when designing interactive listening situations; the physical interface, the site-specific soundscape and the affectively engaging listening experience. We then provide a detailed video analysis of the richness of use of the installation, focusing on the interaction and ways of listening facilitated by the setup. Based on this, we highlight the ways in which Ekkomaten has provided an interactive listening situation engaging people affectively and intellectually both in the exploration of sonified stories about the 18th century as well as of the installation in itself as an interactive machine for listening. Further, we reflect on important insights when designing interactive listening situations, critically reflect on the data and evaluation and outline a future experiment with the Ekkomaten infrastructure.","2014-12-02","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","31–40","","","","","","","OzCHI '14","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/9SSQ9CCL/Fritsch et al. - 2014 - Designing interactive listening situations.pdf","","","affective engagement; audio design; cultural heritage; interactive environments and installations; interactive sound design; site-specific design; urban computing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SW8FKX3V","conferencePaper","2020","Chuang, Yaliang","Designing the Expressivity of Multiple Smart Things for Intuitive and Unobtrusive Interactions","Proceedings of the 2020 ACM Designing Interactive Systems Conference","978-1-4503-6974-9","","10.1145/3357236.3395450","https://dl.acm.org/doi/10.1145/3357236.3395450","Connected products and systems are becoming popular, but they seldom provide direct and intuitive communication to the users. In this study, we applied Disney's animation principles to design the expressivity with LED lights and speakers commonly embedded in electronic devices. We explored the subtle transitions of brightness and controlled the timing to compose individual and system-level behaviors with multiple devices. The designs were evaluated and improved through three iterations. In the main study, we recruited 16 designer participants to investigate whether lights and sounds could be intuitively interpreted as what the system wanted to convey. The result shows that group light behaviors could evoke meanings that are highly similar to the intents of the system. When the acoustic accompaniments were provided, participants could better perceive the presence of devices. We concluded with six sets of light behaviors that are capable of expressing smart devices and systems' intents intuitively and unobtrusively.","2020-07-03","2023-07-06 05:44:24","2023-07-06 05:44:24","2023-07-05","2007–2019","","","","","","","DIS '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/PTUJU4VX/Chuang - 2020 - Designing the Expressivity of Multiple Smart Thing.pdf","","","situation awareness; feedback; feedforward; internet of things; direct interaction; semantic expression","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZHS8LHBD","conferencePaper","2020","Winters, R. Michael; Koziej, Stephanie","An auditory interface for realtime brainwave similarity in dyads","Proceedings of the 15th International Audio Mostly Conference","978-1-4503-7563-4","","10.1145/3411109.3411147","https://dl.acm.org/doi/10.1145/3411109.3411147","We present a case-study in the development of a""hyperscanning"" auditory interface that transforms realtime brainwave-similarity between interacting dyads into music. Our instrument extends reality in face-to-face communication with a musical stream reflecting an invisible socio-neurophysiological signal. This instrument contributes to the historical context of brain-computer interfaces (BCIs) applied to art and music, but is unique because it is contingent on the correlation between the brainwaves of the dyad, and because it conveys this information using entirely auditory feedback. We designed the instrument to be i) easy to understand, ii) relatable and iii) pleasant for members of the general public in an exhibition context. We present how this context and user group led to our choice of EEG hardware, inter-brain similarity metric, and our auditory mapping strategy. We discuss our experience following four public exhibitions, as well as future improvements to the instrument design and user experience.","2020-09-16","2023-07-06 05:49:55","2023-07-06 05:49:55","2023-07-05","261–264","","","","","","","AM '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/BRRCDXA7/Winters and Koziej - 2020 - An auditory interface for realtime brainwave simil.pdf","","","sonification; augmented reality; sound art; brain-computer interfaces; audio; neuroscience; social; sound interaction design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CSBTBKU2","conferencePaper","2008","Jagdish, Deepak; Sawhney, Rahul; Gupta, Mohit; Nangia, Shreyas","Sonic Grid: an auditory interface for the visually impaired to navigate GUI-based environments","Proceedings of the 13th international conference on Intelligent user interfaces","978-1-59593-987-6","","10.1145/1378773.1378824","https://dl.acm.org/doi/10.1145/1378773.1378824","This paper explores the prototype design of an auditory interface enhancement called the Sonic Grid that helps visually impaired users navigate GUI-based environments. The Sonic Grid provides an auditory representation of GUI elements embedded in a two-dimensional interface, giving a 'global' spatial context for use of auditory icons, ear-cons and speech feedback. This paper introduces the Sonic Grid, discusses insights gained through participatory design with members of the visually impaired community, and suggests various applications of the technique, including its use to ease the learning curve for using computers by the visually impaired.","2008-01-13","2023-07-06 05:49:55","2023-07-06 05:49:55","2023-07-05","337–340","","","","","","Sonic Grid","IUI '08","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/X749DG9C/Jagdish et al. - 2008 - Sonic Grid an auditory interface for the visually.pdf","","","accessibility; auditory interface; touch based devices","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CX8WHTM8","conferencePaper","2010","Dicke, Christina; Wolf, Katrin; Tal, Yaroslav","Foogue: eyes-free interaction for smartphones","Proceedings of the 12th international conference on Human computer interaction with mobile devices and services","978-1-60558-835-3","","10.1145/1851600.1851705","https://dl.acm.org/doi/10.1145/1851600.1851705","Graphical user interfaces for mobile devices have several drawbacks in mobile situations. In this paper, we present Foogue, an eyes-free interface that utilizes spatial audio and gesture input. Foogue does not require visual attention and hence does not divert visual attention from the task at hand. Foogue has two modes, which are designed to fit the usage patterns of mobile users. For user input we designed a gesture language build of a limited number of simple but also easy to differentiate gesture elements.","2010-09-07","2023-07-06 05:49:55","2023-07-06 05:49:55","2023-07-05","455–458","","","","","","Foogue","MobileHCI '10","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","/Users/minsik/Zotero/storage/USUVSVXK/Dicke et al. - 2010 - Foogue eyes-free interaction for smartphones.pdf","","","spatial audio; mobile; auditory interface; gesture interaction; 3D","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""