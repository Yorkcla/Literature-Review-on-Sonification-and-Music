"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"2RLFZ48I","journalArticle","2019","Sanyal, Shankha; Nag, Sayan; Banerjee, Archi; Sengupta, Ranjan; Ghosh, Dipak","Music of brain and music on brain: a novel EEG sonification approach","Cognitive Neurodynamics","","1871-4080, 1871-4099","10.1007/s11571-018-9502-4","http://link.springer.com/10.1007/s11571-018-9502-4","Can we hear the sound of our brain? Is there any technique which can enable us to hear the neuro-electrical impulses originating from the different lobes of brain? The answer to all these questions is YES. In this paper we present a novel method with which we can sonify the electroencephalogram (EEG) data recorded in ‘‘control’’ state as well as under the influence of a simple acoustical stimuli—a tanpura drone. The tanpura has a very simple construction yet the tanpura drone exhibits very complex acoustic features, which is generally used for creation of an ambience during a musical performance. Hence, for this pilot project we chose to study the nonlinear correlations between musical stimulus (tanpura drone as well as music clips) and sonified EEG data. Till date, there have been no study which deals with the direct correlation between a bio-signal and its acoustic counterpart and also tries to see how that correlation varies under the influence of different types of stimuli. This study tries to bridge this gap and looks for a direct correlation between music signal and EEG data using a robust mathematical microscope called Multifractal Detrended Cross Correlation Analysis (MFDXA). For this, we took EEG data of 10 participants in 2 min ‘‘control condition’’ (i.e. with white noise) and in 2 min ‘tanpura drone’ (musical stimulus) listening condition. The same experimental paradigm was repeated for two emotional music, ‘‘Chayanat’’ and ‘‘Darbari Kanada’’. These are well known Hindustani classical ragas which conventionally portray contrast emotional attributes, also verified from human response data. Next, the EEG signals from different electrodes were sonified and MFDXA technique was used to assess the degree of correlation (or the cross correlation coefficient cx) between the EEG signals and the music clips. The variation of cx for different lobes of brain during the course of the experiment provides interesting new information regarding the extraordinary ability of music stimuli to engage several areas of the brain significantly unlike any other stimuli (which engages specific domains only).","2019-02","2023-07-05 06:08:32","2023-07-19 23:45:33","2023-07-05 06:08:32","13-31","","1","13","","Cogn Neurodyn","Music of brain and music on brain","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/RQA9A9RR/Sanyal et al. - 2019 - Music of brain and music on brain a novel EEG son.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MZZKTPAL","bookSection","2018","Véron-Delor, Lauriane; Pinto, Serge; Eusebio, Alexandre; Velay, Jean-Luc; Danna, Jérémy","Music and Musical Sonification for the Rehabilitation of Parkinsonian Dysgraphia: Conceptual Framework","Music Technology with Swing","978-3-030-01691-3 978-3-030-01692-0","","","http://link.springer.com/10.1007/978-3-030-01692-0_21","Music has been shown to enhance motor control in patients with Parkinson’s disease (PD). Notably, musical rhythm is perceived as an external auditory cue that helps PD patients to better control movements. The rationale of such effects is that motor control based on auditory guidance would activate a compensatory brain network that minimizes the recruitment of the defective pathway involving the basal ganglia. Would associating music to movement improve its perception and control in PD? Musical sonification consists in modifying in real-time the playback of a preselected music according to some movement parameters. The validation of such a method is underway for handwriting in PD patients. When confirmed, this study will strengthen the clinical interest of musical sonification in motor control and (re)learning in PD.","2018","2023-07-05 06:08:32","2023-07-21 04:35:01","2023-07-05 06:08:32","312-326","","","11265","","","Music and Musical Sonification for the Rehabilitation of Parkinsonian Dysgraphia","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-01692-0_21","","/Users/minsik/Zotero/storage/9NVLLCUD/Véron-Delor et al. - 2018 - Music and Musical Sonification for the Rehabilitat.pdf","","","","Aramaki, Mitsuko; Davies, Matthew E. P.; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S3CRMK8N","journalArticle","2016","Walus, Bartlomiej P.; Pauletto, Sandra; Mason-Jones, Amanda","Sonification and music as support to the communication of alcohol-related health risks to young people: Study design and results","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-016-0220-0","http://link.springer.com/10.1007/s12193-016-0220-0","Excessive consumption of alcohol has been recognised as a significant risk factor impacting the health of young people. Effective communication of such risk is considered to be one key step to improve behaviour. We evaluated an innovative multimedia intervention that utilised audio (sonification—using sound to display data—and music) and interactivity to support the visual communication of alcohol health risk data. A 3-arm pilot experiment was undertaken. The trial measures included health knowledge, alcohol risk perception and user experience of the intervention. Ninetysix subjects participated in the experiment. At 1 month follow-up, alcohol knowledge and alcohol risk perception improved significantly in the whole sample. However, there was no difference between the intervention groups that experienced (1) visual presentation with interactivity (VI-Exp group) and, (2) visual presentation with audio (sonification and music) and interactivity (VAI-Exp group), when compared to the control group which experienced a (3) visual only presentation (V-Cont group). Participants reported enjoying the presentations and found them educational. The majority of participants indicated that the audio, music and sonification helped to convey the information well, and, although a larger sample size is needed to fully establish the effectiveness of the different interventions, this study provides a useful model for future similar studies.","2016-09","2023-07-05 06:08:32","2023-07-20 07:05:38","2023-07-05 06:08:32","235-246","","3","10","","J Multimodal User Interfaces","Sonification and music as support to the communication of alcohol-related health risks to young people","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/ILKJCIEL/Walus et al. - 2016 - Sonification and music as support to the communica.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4RHR52H3","journalArticle","2012","Huang, Chih-Fang; Lu, Hsiang-Pin; Ren, Jenny","Algorithmic approach to sonification of classical Chinese poetry","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-011-0856-4","http://link.springer.com/10.1007/s11042-011-0856-4","The classical Chinese poetry is a remarkable form of art in traditional Chinese character. However, it is difficult for people who are unfamiliar with ancient Chinese to experience the artistic content of the poetry. In this study, a sonification scheme, Tx2Ms (Text-to-Music), is proposed to extract the poetry features between lines in verses; moreover, dynamics and interval relations are modeled to map those features to the movement of multi-dimensional musical elements such as durations. This conversion is based on poetry intonation and acoustic analysis of the pronunciations of poems; and a stochastic compositional algorithm is created by applying a Markov chain. In addition, the best pentatonic mode for a specific poem is recommended according to the formants analysis. Therefore, the sonification of classical Chinese poetry not only provides a novel way for people to appreciate Chinese poetry but also enriches the state of mind and imagery in the delivery process, and the experiment results show that the proposed system is successfully accepted by most people.","2012-11","2023-07-05 06:08:32","2023-07-21 04:32:33","2023-07-05 06:08:32","489-518","","2","61","","Multimed Tools Appl","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/3PVKLA2M/Huang et al. - 2012 - Algorithmic approach to sonification of classical .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2QHW8T6Q","journalArticle","2020","Newbold, Joseph; Gold, Nicolas E.; Bianchi-Berthouze, Nadia","Movement sonification expectancy model: leveraging musical expectancy theory to create movement-altering sonifications","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-020-00322-2","http://link.springer.com/10.1007/s12193-020-00322-2","Abstract             When designing movement sonifications, their effect on people’s movement must be considered. Recent work has shown how real-time sonification can be designed to alter the way people move. However, the mechanisms through which these sonifications alter people’s expectations of their movement is not well explained. This is especially important when considering musical sonifications, to which people bring their own associations and musical expectation, and which can, in turn, alter their perception of the sonification. This paper presents a Movement Expectation Sonification Model, based on theories of motor-feedback and expectation, to explore how musical sonification can impact the way people perceive their movement. Secondly, we present a study that validates the predictions of this model by exploring how harmonic stability within sonification interacts with contextual cues in the environment to impact movement behaviour and perceptions. We show how musical expectancy can be built to either reward or encourage movement, and how such an effect is mediated through the presence of additional cues. This model offers a way for sonification designers to create movement sonifications that not only inform movement but can be used to encourage progress and reward successes.","2020-06","2023-07-05 06:08:32","2023-07-05 06:08:32","2023-07-05 06:08:32","153-166","","2","14","","J Multimodal User Interfaces","Movement sonification expectancy model","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/KKA2QX8L/Newbold et al. - 2020 - Movement sonification expectancy model leveraging.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MUYT32DV","bookSection","2008","Chemseddine, Maher; Noirhomme-Fraiture, Monique","Complex and Dynamic Data Representation by Sonification","Human-Computer Interaction Symposium","978-0-387-09677-3 978-0-387-09678-0","","","http://link.springer.com/10.1007/978-0-387-09678-0_18","So far, data representation has been based on visuals. The huge size of data verges on overuse of the visual capability. Thus, there is a need to reduce the almost exclusive use of visual techniques to represent data in order to increase our perception bandwidth. In this research, we aim to solve this problem by integrating the audio component. More precisely, we are interested in representing data using musical melodies. This paper presents a model for music elements based on human emotion, to express alert messages displayed by computer network monitoring.","2008","2023-07-05 06:08:32","2023-07-20 06:29:33","2023-07-05 06:08:32","195-200","","","272","","","","","","","","Springer US","Boston, MA","en","","","","","DOI.org (Crossref)","","ISSN: 1571-5736 Series Title: IFIP International Federation for Information Processing DOI: 10.1007/978-0-387-09678-0_18","","/Users/minsik/Zotero/storage/TDSDARHQ/Chemseddine and Noirhomme-Fraiture - 2008 - Complex and Dynamic Data Representation by Sonific.pdf","","","","Forbrig, Peter; Paternò, Fabio; Pejtersen, Annelise Mark","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IZLQIGIA","journalArticle","2012","Diniz, Nuno; Coussement, Pieter; Deweppe, Alexander; Demey, Michiel; Leman, Marc","An embodied music cognition approach to multilevel interactive sonification","Journal on Multimodal User Interfaces","","1783-8738","10.1007/s12193-011-0084-2","https://doi.org/10.1007/s12193-011-0084-2","In this paper, a new conceptual framework and related implementation for interactive sonification is introduced. The conceptual framework consists of a combination of three components, namely, gestalt-based electroacoustic composition techniques (sound), user and body-centered spatial exploration (body), and corporeal mediation technology (tools), which are brought together within an existing paradigm of embodied music cognition. The implementation of the conceptual framework is based on an iterative process that involves the development of several use cases. Through this methodology, it is possible to investigate new approaches for structuring and to interactively explore multivariable data through sound.","2012-05-01","2023-07-05 06:10:04","2023-07-05 06:10:04","2023-07-05 06:10:04","211-219","","3","5","","J Multimodal User Interfaces","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/KWERYVAV/Diniz et al. - 2012 - An embodied music cognition approach to multilevel.pdf","","","Sonification; Electroacoustic; Embodiment; Framework; Interaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RSZG7DPL","journalArticle","2012","Gena, Peter","Apropos sonification: a broad view of data as music and sound","AI & SOCIETY","","1435-5655","10.1007/s00146-011-0339-1","https://doi.org/10.1007/s00146-011-0339-1","Numbers have been identified with symbolic data forever. The profound association of both with acoustics, music, and sonic art from Pythagoras to current work is beyond reproach. Recently, sonification looks for ways to realize symbolic data (representing results or measurements) as well as “raw” data (signals, impulses, images, etc.) into compositions. In the strictest sense, everything in a computer is symbolic, that is, represented by 0s and 1s. In the arts, the digital age has broadened and enhanced the conceptual landscape not simply through its servitude to the creative process, but as its partner. However, there is a rich history of the use of data that no doubt has paved the way for many of today’s experiments including my own.","2012-05-01","2023-07-05 06:10:33","2023-07-05 06:10:33","2023-07-05 06:10:33","197-205","","2","27","","AI & Soc","Apropos sonification","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/3AWTXPUP/Gena - 2012 - Apropos sonification a broad view of data as musi.pdf","","","Algorithmic composition; DNA music; Music; Music history; Sonic art","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DABWYJ52","conferencePaper","2021","Polaczyk, Jakub; Croft, Katelyn; Cai, Yang","Compositional Sonification of Cybersecurity Data in a Baroque Style","Advances in Artificial Intelligence, Software and Systems Engineering","978-3-030-80624-8","","10.1007/978-3-030-80624-8_38","","Compositional sonification is a musical mapping algorithm that transforms a dataset into a soundscape for humans to hear numbers within a musical structure. In this study, we used three methods to sonify data: 1) a Baroque-style gestural mapping algorithm that emphasized timing and rhythm using an analog method of handwritten composition, 2) an electronic online pitch sequencer, and 3) the classical instrument Harp performing notated music. We investigated related sound mapping algorithms and the sensitivity of pattern representation of cybersecurity data, including the malware distribution network datasets. Our preliminary experiments showed that a Baroque-inspired timing element based on gestures plays a critical role in aural sonification. Using a sequencer with visualizers helped listeners grasp the patterns more effectively since they were able to watch and hear the results simultaneously. Performing the sonified data live on the harp helped make the compositions more relatable and thus connected with a broader audience. The harp also added a visual element which helped listeners identify patterns. We found that listeners were able to accurately identify the sonified data patterns in both the baroque-style analog handwritten compositions performed on harp and the compositions produced by the Online Sequencer. While non-musicians were able to answer questions about the data patterns they heard with a high percentage of accuracy, their results improved when visual elements were added.","2021","2023-07-05 06:12:24","2023-07-05 06:12:24","","304-312","","","","","","","Lecture Notes in Networks and Systems","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/7HF8V3K5/Polaczyk et al. - 2021 - Compositional Sonification of Cybersecurity Data i.pdf","","","Sonification; AI; Augmented reality; Baroque; Classical instruments; Classical music; Composition; Computer music; Computer virus; Creativity; Harp; Malware; Network; Sequencer","Ahram, Tareq Z.; Karwowski, Waldemar; Kalra, Jay","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7JXEFZDG","journalArticle","2012","Eacott, John","Flood Tide: sonification as musical performance—an audience perspective","AI & SOCIETY","","1435-5655","10.1007/s00146-011-0338-2","https://doi.org/10.1007/s00146-011-0338-2","The number of events and artifacts described as sonification has increased considerably in recent years with some works making a bridge between the representation of data and artistic expression. FloodTide which sonifies the flow of tidal water is such a work and has achieved a relatively high profile attracting good audiences for its 10 performances to date. It is not entirely obvious however what it is that attracts audiences and whether it is effective at representing the data being sonified. This paper aims to address these issues and is based on a discussion group in which these and other questions are considered.","2012-05-01","2023-07-05 06:12:56","2023-07-05 06:12:56","2023-07-05 06:12:56","189-195","","2","27","","AI & Soc","Flood Tide","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/HGWTT38M/Eacott - 2012 - Flood Tide sonification as musical performance—an.pdf","","","Sonification; Algorithmic composition; Live notation; Tide","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SRQSK2G9","journalArticle","2012","Varni, Giovanna; Dubus, Gaël; Oksanen, Sami; Volpe, Gualtiero; Fabiani, Marco; Bresin, Roberto; Kleimola, Jari; Välimäki, Vesa; Camurri, Antonio","Interactive sonification of synchronisation of motoric behaviour in social active listening to music with mobile devices","Journal on Multimodal User Interfaces","","1783-8738","10.1007/s12193-011-0079-z","https://doi.org/10.1007/s12193-011-0079-z","This paper evaluates three different interactive sonifications of dyadic coordinated human rhythmic activity. An index of phase synchronisation of gestures was chosen as coordination metric. The sonifications are implemented as three prototype applications exploiting mobile devices: Sync’n’Moog, Sync’n’Move, and Sync’n’Mood. Sync’n’Moog sonifies the phase synchronisation index by acting directly on the audio signal and applying a nonlinear time-varying filtering technique. Sync’n’Move intervenes on the multi-track music content by making the single instruments emerge and hide. Sync’n’Mood manipulates the affective features of the music performance. The three sonifications were also tested against a condition without sonification.","2012-05-01","2023-07-05 06:14:05","2023-07-05 06:14:05","2023-07-05 06:14:05","157-173","","3","5","","J Multimodal User Interfaces","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/HLWJPW9P/Varni et al. - 2012 - Interactive sonification of synchronisation of mot.pdf","","","Active music listening; Audio systems; Interactive sonification; Interactive systems; Sound and music computing; Synchronisation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P9QLLAFP","journalArticle","2019","Polo, Antonio; Sevillano, Xavier","Musical Vision: an interactive bio-inspired sonification tool to convert images into music","Journal on Multimodal User Interfaces","","1783-8738","10.1007/s12193-018-0280-4","https://doi.org/10.1007/s12193-018-0280-4","Musical Vision is a highly flexible, interactive and bio-inspired sonification tool that translates color images into harmonic polyphonic music by mimicking the human visual system in terms of its field of vision and photosensitive sensors. Putting the user at the center of the sonification process, Musical Vision allows the interactive design of fully configurable mappings between the color space and the MIDI instruments and audio pitch spaces to tailor the music rendering results to the application needs. Moreover, Musical Vision incorporates a harmonizer capable of introducing the necessary modifications to create melodies using harmonic chords. Above all else, Musical Vision is an extremely flexible system that the user can interactively configure to convert an image into either a few seconds or a several minutes long musical piece. Thus, it can be used, for instance, with trans-artistic purposes like the conversion of a painting into music, for augmenting vision with music, or for learning musical skills such as sol-fa. To evaluate the proposed sonification tool, we conducted a pilot user study, in which twelve volunteers were tested to interpret images containing geometric patterns from music rendered by Musical Vision. Results show that even those users with no musical education background were able to achieve nearly 70% accuracy in multiple choice tests after less than 25 min training. Moreover, users with some musical education were capable of accurately “drawing by ear” the images from no other stimuli than the sonifications.","2019-09-01","2023-07-05 06:14:31","2023-07-05 06:14:31","2023-07-05 06:14:31","231-243","","3","13","","J Multimodal User Interfaces","Musical Vision","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/MX9P5ZU3/Polo and Sevillano - 2019 - Musical Vision an interactive bio-inspired sonifi.pdf","","","Auditory user interface; MIDI; User-controlled sonification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7ZLIAE2D","journalArticle","2012","Gresham-Lancaster, Scot","Relationships of sonification to music and sound art","AI & SOCIETY","","1435-5655","10.1007/s00146-011-0337-3","https://doi.org/10.1007/s00146-011-0337-3","The definition of sonification has been reframed in recent years but remains somewhat in flux; the basic concepts and procedural flows have remained relatively unchanged. Recent definitions have focused on the objective the important uses of sonification in terms of scientific method. The full realization of the potential of the field must also include the craft and art of music composition. The author proposes examining techniques of sonification in a two-order framework: direct and procedural. The impact of new technologies and historical roots of that work argues that framing this broad topic should be in terms inclusive of scientific method and craftsmanship and art. The expressive use of sonic time-based data flows needs to be refined and expanded. The unexamined territory of how a broad-based population of listeners on a subjective, as well as objective level needs, have to be included in this new field.","2012-05-01","2023-07-05 06:14:59","2023-07-05 06:14:59","2023-07-05 06:14:59","207-212","","2","27","","AI & Soc","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/WYTPE9Z8/Gresham-Lancaster - 2012 - Relationships of sonification to music and sound a.pdf","","","Sonification; Composition; Sound Art","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EVFATR9U","journalArticle","2012","Polli, Andrea","Soundscape, sonification, and sound activism","AI & SOCIETY","","1435-5655","10.1007/s00146-011-0345-3","https://doi.org/10.1007/s00146-011-0345-3","In this article, the author will argue that the act of listening through public soundwalks and other formal and informal exercises builds environmental and social awareness and promotes changes in social and cultural practices. By examining the act of listening as an alternative pathway and comparing the research, writings, and creative work of leaders of the acoustic ecology movement (i.e., R. Murray Schafer, Hildegard Westerkamp, and Bernie Krause), the author hopes to shed light on these potentials. For purposes of comparison, projects that explore the sonification and audification of inaudible signals will be examined, including the work of Christina Kubisch. The process of audification and sonification of these signals will be examined in comparison to soundscape experiences in order to develop a theory of data sonification based on the soundscape. In order to build a community around the urban soundscape, in 2003, the author co-founded the New York Society of Acoustic Ecology. Through this endeavor, she co-created the ongoing NYSoundmap and Sound Seeker projects, which provide some practical research for this article. Thus, by comparing and contrasting theoretical writings with leading listening exercises, public soundwalks, soundscape-related brainstorming sessions, and presenting field recordings in various settings, new methodologies will be documented.","2012-05-01","2023-07-05 06:15:18","2023-07-05 06:15:18","2023-07-05 06:15:18","257-268","","2","27","","AI & Soc","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/6CWKMIEA/Polli - 2012 - Soundscape, sonification, and sound activism.pdf","","","Sonification; Acoustic ecology; Audification; Field recording; Soundscape; Soundwalks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WFFR33QR","journalArticle","2019","Maes, Pieter-Jan; Lorenzoni, Valerio; Six, Joren","The SoundBike: musical sonification strategies to enhance cyclists’ spontaneous synchronization to external music","Journal on Multimodal User Interfaces","","1783-8738","10.1007/s12193-018-0279-x","https://doi.org/10.1007/s12193-018-0279-x","The spontaneous tendency of people to synchronize their movements to music is a powerful mechanism useful for the development of strategies for tempo adaptation of simple repetitive movements. In the current article, we contribute to such strategies—applied to cycling—by introducing a new strategy based on the sonification of cyclists’ motor rhythm. For that purpose, we developed the SoundBike, a stationary bike equipped with sensors that allows interactive sonification of cyclists’ motor rhythm using two distinct but compatible sonification methods. One is based on the principle of step sequencers, which are frequently used for electronic music production. The other is based on the Kuramoto model, allowing automatic and continuous phase alignment of beat-annotated music pieces to cyclists’ motor rhythm, i.e., pedal cadence. Apart from an in-depth presentation of the technical aspects of the SoundBike, we present an experimental study in which we investigated whether the SoundBike could enhance spontaneous synchronization of cyclists to external music. The results of this experiment suggest that sonification of cyclists’ motor rhythm may increase their tendency to synchronize to external music, and helps to keep a more stable pedal cadence, compared to the condition of having external music only (without sonification). Although the results are preliminary and should be followed-up by additional experiments to become more conclusive, SoundBike seems anyhow a promising interactive sonification device to assist motor learning and adaptation in the field of sports and motor rehabilitation.","2019-09-01","2023-07-05 06:15:42","2023-07-05 06:15:42","2023-07-05 06:15:42","155-166","","3","13","","J Multimodal User Interfaces","The SoundBike","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/49V7UDG6/Maes et al. - 2019 - The SoundBike musical sonification strategies to .pdf","","","Movement tempo adaptation; Musical biofeedback; Reinforcement learning; Reward; Sensorimotor synchronization; Sonification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QMVQHAIG","journalArticle","2012","Joy, Jerome","What NMSAT says about sonification","AI & SOCIETY","","1435-5655","10.1007/s00146-011-0343-5","https://doi.org/10.1007/s00146-011-0343-5","This article presents a sample of references issuing directly from the existing NMSAT database. The method employed—that of systematically probing the database—reveals forms of sonification, but also hypothetical premises of sonification, covering the period from ancient times to the beginning of the twentieth century. The following are some of the categories of sonification that have emerged as a result of this search: Natural phenomenon & meteorology to sound (autophones); Image to sound; Text & communication to sound; Human & machine activities to sound (auditing); Localisation to sound (sonar); Architecture & geometry & abstract proportions to sound (scalization, transcription, & spatialization); Energy to sound; Human body to sound; Distance to sound (distance listening); Movement to sound (holophony, kynophony); and Interpreted observations to sound (naturalist music, transpositions & analogies, paraphrasing). The search also uncovered other principals and practices in the vicinity of sonification including: audification, auditing, auscultation, auralization, soniculation, transduction, mapping, earcons, auditory icons, sympathy, echometry, etc. It has been decided to summarise the results of « What NMSAT Says About Sonification » in this special issue of AI&Society, access to the unabridged version of article is available here: http://www.locusonus.org/sonification/.","2012-05-01","2023-07-05 06:16:59","2023-07-05 06:16:59","2023-07-05 06:16:59","233-244","","2","27","","AI & Soc","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/94RABVZJ/Joy - 2012 - What NMSAT says about sonification.pdf","","","Audio art; Database; Distance listening; Networked music; Networks; Sonification history; Timeline","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G9ATNJ9D","conferencePaper","2014","Zhang, Yuxi; Huang, Yifeng; Yue, Junwei; Zhang, Liqing","Sonification for EEG Frequency Spectrum and EEG-Based Emotion Features","Neural Information Processing","978-3-319-12643-2","","10.1007/978-3-319-12643-2_6","","Sonification is the use of representations of data through sound to convey information. It is particularly meaningful if the data are involved in time. This paper present a hybrid sonification method and aims to directly expressed the emotion hidden in the EEG signal through sound. The hybrid method mainly consists of two parts: (1) Frequency Mapping Representation (FMR) and (2) Emotion Feature Representation (EFR).","2014","2023-07-05 06:21:51","2023-07-05 06:21:51","","42-49","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/7EC4J4IN/Zhang et al. - 2014 - Sonification for EEG Frequency Spectrum and EEG-Ba.pdf","","","Emotion Feature; Motor Imagery; Music Generation; Music Piece; Note Generation Function","Loo, Chu Kiong; Yap, Keem Siah; Wong, Kok Wai; Beng Jin, Andrew Teoh; Huang, Kaizhu","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QMBKF2I3","journalArticle","2019","Wolf, KatieAnna E.; Fiebrink, Rebecca","Personalised interactive sonification of musical performance data","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-019-00294-y","http://link.springer.com/10.1007/s12193-019-00294-y","In this article, we describe methods and consequences for giving audience members interactive control over the real-time sonification of performer movement data in electronic music performance. We first briefly describe how to technically implement a musical performance in which each audience member can interactively construct and change their own individual sonification of performers’ movements, heard through headphones on a personal WiFi-enabled device, while also maintaining delay-free synchronization between performer movements and sound. Then, we describe two studies we conducted in the context of live musical performances with this technology. These studies have allowed us to examine how providing audience members with the ability to interactively sonify performer actions impacted their experiences, including their perceptions of their own role and engagement with the performance. These studies also allowed us to explore how audience members with different levels of expertise with sonification and sound, and different motivations for interacting, could be supported and influenced by different sonification interfaces. This work contributes to a better understanding of how providing interactive control over sonification may alter listeners’ experiences, of how to support everyday people in designing and using bespoke sonifications, and of new possibilities for musical performance and participation.","2019-09","2023-07-05 06:23:48","2023-07-20 07:05:49","2023-07-05 06:23:48","245-265","","3","13","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/4KCLJBR8/Wolf and Fiebrink - 2019 - Personalised interactive sonification of musical p.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JGDH3YRH","journalArticle","2022","Frid, Emma; Bresin, Roberto","Perceptual Evaluation of Blended Sonification of Mechanical Robot Sounds Produced by Emotionally Expressive Gestures: Augmenting Consequential Sounds to Improve Non-verbal Robot Communication","International Journal of Social Robotics","","1875-4791, 1875-4805","10.1007/s12369-021-00788-4","https://link.springer.com/10.1007/s12369-021-00788-4","Abstract             This paper presents two experiments focusing on perception of mechanical sounds produced by expressive robot movement and blended sonifications thereof. In the first experiment, 31 participants evaluated emotions conveyed by robot sounds through free-form text descriptions. The sounds were inherently produced by the movements of a NAO robot and were not specifically designed for communicative purposes. Results suggested no strong coupling between the emotional expression of gestures and how sounds inherent to these movements were perceived by listeners; joyful gestures did not necessarily result in joyful sounds. A word that reoccurred in text descriptions of all sounds, regardless of the nature of the expressive gesture, was “stress”. In the second experiment, blended sonification was used to enhance and further clarify the emotional expression of the robot sounds evaluated in the first experiment. Analysis of quantitative ratings of 30 participants revealed that the blended sonification successfully contributed to enhancement of the emotional message for sound models designed to convey frustration and joy. Our findings suggest that blended sonification guided by perceptual research on emotion in speech and music can successfully improve communication of emotions through robot sounds in auditory-only conditions.","2022-03","2023-07-05 06:23:48","2023-07-05 06:23:48","2023-07-05 06:23:48","357-372","","2","14","","Int J of Soc Robotics","Perceptual Evaluation of Blended Sonification of Mechanical Robot Sounds Produced by Emotionally Expressive Gestures","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/NMVAXPXX/Frid and Bresin - 2022 - Perceptual Evaluation of Blended Sonification of M.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TIPBA3QJ","bookSection","2008","Salter, Christopher L.; Baalman, Marije A. J.; Moody-Grigsby, Daniel","Between Mapping, Sonification and Composition: Responsive Audio Environments in Live Performance","Computer Music Modeling and Retrieval. Sense of Sounds","978-3-540-85034-2 978-3-540-85035-9","","","http://link.springer.com/10.1007/978-3-540-85035-9_17","This paper describes recent work on a large-scale, interactive theater performance entitled Schwelle as a platform to pose critical questions around the conception, design and implementation of what is commonly labeled responsive audio environments. The authors first discuss some principal issues in the design of responsive audio environments specifically within the domain of stage performance, addressing existing human-computer interaction paradigms and discussing three key areas: sensing, mapping and data sonification. Next, we discuss larger questions of composition in relation to these key areas, suggesting that potential strategies cross three different domains: mapping within algorithmic composition, data sonification techniques, and time-based evolutionary processes emerging from dynamical systems theory. We then examine in detail the recent work on Schwelle, which employs real time, distributed sensor data to drive a continuous dynamical systembased composition engine. The project’s conceptual and technical challenges are discussed as well as audience evaluation and feedback from the first presentation in Berlin in February 2007. This presentation lead the authors to re-iterate the design and build an additional state-system layer into the dynamical system in order to generate more perceivable sonic structures on both the macro as well as meso levels for the audience/listener. Finally, we conclude with a set of issues that may act as a framework for future research focused on compositional strategies for larger scale, distributed, networked-based sensor environments.","2008","2023-07-05 06:23:48","2023-07-19 23:49:26","2023-07-05 06:23:48","246-262","","","4969","","","Between Mapping, Sonification and Composition","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-85035-9_17","","/Users/minsik/Zotero/storage/HJ7IU9V4/Salter et al. - 2008 - Between Mapping, Sonification and Composition Res.pdf","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F6SKWD8H","bookSection","2021","Bardelli, Sandro; Ferretti, Claudia; Ludovico, Luca Andrea; Presti, Giorgio; Rinaldi, Maurizio","A Sonification of the zCOSMOS Galaxy Dataset","Culture and Computing. Interactive Cultural Heritage and Arts","978-3-030-77410-3 978-3-030-77411-0","","","https://link.springer.com/10.1007/978-3-030-77411-0_12","This paper proposes a sonification for zCOSMOS, an astronomical dataset that contains information about 20,000 galaxies. The goals of such an initiative are multiple: providing a sound-based description of the dataset in order to make hidden features emerge, hybridizing science with art in a cross-domain framework, and treating scientific data as cultural heritage to be preserved and enhanced, thus breaking down the barriers between scientists and the general audience. In the paper, both technical and artistic aspects of the sonification will be addressed. Finally, some relevant excerpts from the resulting sonification will be presented and discussed.","2021","2023-07-05 06:23:48","2023-07-19 23:53:59","2023-07-05 06:23:48","171-188","","","12794","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-77411-0_12","","/Users/minsik/Zotero/storage/8MV4ARUM/Bardelli et al. - 2021 - A Sonification of the zCOSMOS Galaxy Dataset.pdf","","","","Rauterberg, Matthias","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UGJJXZ32","journalArticle","2020","Aldana Blanco, Andrea Lorena; Grautoff, Steffen; Hermann, Thomas","ECG sonification to support the diagnosis and monitoring of myocardial infarction","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-020-00319-x","http://link.springer.com/10.1007/s12193-020-00319-x","Abstract                            This paper presents the design and evaluation of four sonification methods to support monitoring and diagnosis in Electrocardiography (ECG). In particular we focus on an ECG abnormality called ST-elevation which is an important indicator of a myocardial infarction. Since myocardial infarction represents a life-threatening condition it is of essential value to detect an ST-elevation as early as possible. As part of the evaluated sound designs, we propose two novel sonifications: (i)               Polarity sonification               , a continuous parameter-mapping sonification using a formant synthesizer and (ii)               Stethoscope sonification               , a combination of the ECG signal and a stethoscope recording. The other two designs, (iii) the               water ambience sonification               and the (iv)               morph sonification               , were presented in our previous work about ECG sonification (Aldana Blanco AL, Steffen G, Thomas H (2016) In: Proceedings of Interactive Sonification Workshop (ISon). Bielefeld, Germany). The study evaluates three components across the proposed sonifications (1) detection performance, meaning if participants are able to detect a transition from healthy to unhealthy states, (2) classification accuracy, that evaluates if participants can accurately classify the severity of the pathology, and (3) aesthetics and usability (pleasantness, informativeness and long-term listening). The study results show that the               polarity               design had the highest accuracy rates in the detection task whereas the               stethoscope sonification               obtained the better score in the classification assignment. Concerning aesthetics, the               water ambience sonification               was regarded as the most pleasant. Furthermore, we found a significant difference between sound/music experts and non-experts in terms of the error rates obtained in the detection task using the               morph sonification               and also in the classification task using the               stethoscope sonification               . Overall, the group of experts obtained lower error rates than the group of non-experts, which means that further training could improve accuracy rates and, particularly for designs that rely mainly on pitch variations, additional training is needed in the non-experts group.","2020-06","2023-07-05 06:23:48","2023-07-05 06:23:48","2023-07-05 06:23:48","207-218","","2","14","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/MCSVDJ7A/Aldana Blanco et al. - 2020 - ECG sonification to support the diagnosis and moni.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"926KCWKM","journalArticle","2021","Raglio, Alfredo; Panigazzi, Monica; Colombo, Roberto; Tramontano, Marco; Iosa, Marco; Mastrogiacomo, Sara; Baiardi, Paola; Molteni, Daniele; Baldissarro, Eleonora; Imbriani, Chiara; Imarisio, Chiara; Eretti, Laura; Hamedani, Mehrnaz; Pistarini, Caterina; Imbriani, Marcello; Mancardi, Gian Luigi; Caltagirone, Carlo","Hand rehabilitation with sonification techniques in the subacute stage of stroke","Scientific Reports","","2045-2322","10.1038/s41598-021-86627-y","https://www.nature.com/articles/s41598-021-86627-y","After a stroke event, most survivors suffer from arm paresis, poor motor control and other disabilities that make activities of daily living difficult, severely affecting quality of life and personal independence. This randomized controlled trial aimed at evaluating the efficacy of a music-based sonification approach on upper limbs motor functions, quality of life and pain perceived during rehabilitation. The study involved 65 subacute stroke individuals during inpatient rehabilitation allocated into 2 groups which underwent usual care dayweek) respectively of standard upper extremity motor rehabilitation or upper extremity treatment with sonification techniques. The Fugl-Meyer Upper Extremity Scale, Box and Block Test and the Modified Ashworth Scale were used to perform motor assessment and the McGill Quality of Life-it and the Numerical Pain Rating Scale to assess quality of life and pain. The assessment was performed at baseline, after 2 weeks, at the end of treatment and at follow-up (1 month after the end of treatment). Total scores of the Fugl-Meyer Upper Extremity Scale (primary outcome measure) and hand and wrist sub scores, manual dexterity scores of the affected and unaffected limb in the Box and Block Test, pain scores of the Numerical Pain Rating Scale (secondary outcomes measures) significantly improved in the sonification group compared to the standard of care group (time*group interaction < 0.05). Our findings suggest that music-based sonification sessions can be considered an effective standardized intervention for the upper limb in subacute stroke rehabilitation.","2021-03-31","2023-07-05 06:24:16","2023-07-05 06:24:16","2023-07-05 06:24:16","7237","","1","11","","Sci Rep","","","","","","","","en","2021 The Author(s)","","","","www.nature.com","","Number: 1 Publisher: Nature Publishing Group","","/Users/minsik/Zotero/storage/YUTB3P86/Raglio et al. - 2021 - Hand rehabilitation with sonification techniques i.pdf","","","Health care; Neurology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SXET4A4H","journalArticle","2012","Grond, Florian; Hermann, Thomas","Aesthetic strategies in sonification","AI & SOCIETY","","1435-5655","10.1007/s00146-011-0341-7","https://doi.org/10.1007/s00146-011-0341-7","Sound can be listened to in various ways and with different intentions. Multiple factors influence how and what we perceive when listening to sound. Sonification, the acoustic representation of data, is in essence just sound. It functions as sonification only if we make sure to listen attentively in order to access the abstract information it contains. This is difficult to accomplish since sound always calls the listener’s attention to concrete—whether natural or musical—points of references. Important aspects determining how we listen to sonification are discussed in this paper: elicited sounds, repeated sounds, conceptual sounds, technologically mediated sounds, melodic sounds, familiar sounds, multimodal sounds and vocal sounds. We discuss how these aspects help the listener engage with the sound, but also how they can become points of reference in and of themselves. The various sonic qualities employed in sonification can potentially open but also risk closing doors to the accessibility and perceptibility of the sonified data.","2012-05-01","2023-07-05 06:25:37","2023-07-05 06:25:37","2023-07-05 06:25:37","213-222","","2","27","","AI & Soc","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/GF389U77/Grond and Hermann - 2012 - Aesthetic strategies in sonification.pdf","","","Sonification; Aesthetics; Historic context","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U43J5F85","conferencePaper","2010","Moroni, Artemis; Manzolli, Jônatas","From Evolutionary Composition to Robotic Sonification","Applications of Evolutionary Computation","978-3-642-12242-2","","10.1007/978-3-642-12242-2_41","","A new approach is presented which integrates evolutionary computation and real world devices such as mobile robots and an omnidirectional vision system. Starting with an evolutionary composition system named JaVOX, a hybrid environment named AURAL evolved. In the AURAL, the behavior of mobile robots in an arena is applied as a compositional strategy. It uses trajectories produced by mobile robots to modify the fitness function of a real time sonic composition. The model is described, its evolutionary design and how the interaction between the real world devices was implemented. This research is oriented towards the study of automatic and semi-automatic processes of artistic production in the sound domain.","2010","2023-07-05 06:26:26","2023-07-05 06:26:26","","401-410","","","","","","","Lecture Notes in Computer Science","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/NY27QJAP/Moroni and Manzolli - 2010 - From Evolutionary Composition to Robotic Sonificat.pdf","","","Algorithmic composition; evolutionary computation; robotics; sonification","Di Chio, Cecilia; Brabazon, Anthony; Di Caro, Gianni A.; Ebner, Marc; Farooq, Muddassar; Fink, Andreas; Grahl, Jörn; Greenfield, Gary; Machado, Penousal; O’Neill, Michael; Tarantino, Ernesto; Urquhart, Neil","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B2JYG72S","conferencePaper","2014","Caldwell, Barrett S.; Viraldo, Jacob E.","Improving Control Room State Awareness through Complex Sonification Interfaces","Human Interface and the Management of Information. Information and Knowledge in Applications and Services","978-3-319-07863-2","","10.1007/978-3-319-07863-2_31","","Across a number of complex control room settings, there are concerns regarding operator information overload and alarm flooding. The evolution of control room technological capabilities has accelerated in recent years, due to drastic improvements in computer processing power, speed, and sensor integration. However, aging infrastructure and retiring senior operators in legacy control room systems such as chemical and power generation plants have begun to create opportunities for once-per-generation improvements in control room interface capabilities. Additional facilities, including power grid interconnection centers and computer network security monitoring centers, have created new generations of network operations control centers (NOCs). The authors’ work is emphasizing the development and application of audification and parameter mapping techniques to generate engineering-based principles for presenting state-based auditory information to plant or NOC operators. There are no current systematic engineering-based principles used to apply sonification to control rooms or engineering system states in a clearly standardized way. Our current work in this domain examines the critical parameters that control room operators recognize and monitor in order to get a “sense of the plant” in nominal, degrading, or hazardous states. Principles and parameters for implementing these sonification techniques for power plant and NOC contexts are presented and discussed.","2014","2023-07-05 06:26:47","2023-07-05 06:26:47","","317-323","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/Y6NIA8BN/Caldwell and Viraldo - 2014 - Improving Control Room State Awareness through Com.pdf","","","Audio Signal; Control Room; Nuisance Alarm; Power Generation Plant; State Awareness","Yamamoto, Sakae","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9XUV8PSM","journalArticle","2012","Jones, Stuart","Now? Towards a phenomenology of real time sonification","AI & SOCIETY","","1435-5655","10.1007/s00146-011-0342-6","https://doi.org/10.1007/s00146-011-0342-6","The author examines concepts of real time and real-time in relation to notions of perception and processes of sonification. He explores these relationships in three case studies and suggests that sonification can offer a form of reconciliation between ontology and phenomenology, and between ourselves and the flux we are part of.","2012-05-01","2023-07-05 06:27:33","2023-07-05 06:27:33","2023-07-05 06:27:33","223-231","","2","27","","AI & Soc","Now?","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/DH6BEU34/Jones - 2012 - Now Towards a phenomenology of real time sonifica.pdf","","","Sonification; Attention; Ontology; Perception; Phenomenology; Real time; Real-time","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YRPY3NN9","conferencePaper","2010","Vogt, Katharina; Pirrò, David; Kobenz, Ingo; Höldrich, Robert; Eckel, Gerhard","PhysioSonic - Evaluated Movement Sonification as Auditory Feedback in Physiotherapy","Auditory Display","978-3-642-12439-6","","10.1007/978-3-642-12439-6_6","","We detect human body movement interactively via a tracking system. This data is used to synthesize sound and transform sound files (music or text). A subject triggers and controls sound parameters with his or her movement within a pre-set range of motion. The resulting acoustic feedback enhances new modalities of perception and the awareness of the body movements. It is ideal for application in physiotherapy and other training contexts.","2010","2023-07-05 06:27:56","2023-07-05 06:27:56","","103-120","","","","","","","Lecture Notes in Computer Science","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/8I37BQMK/Vogt et al. - 2010 - PhysioSonic - Evaluated Movement Sonification as A.pdf","","","Auditory Feedback; Augmented Reality; Glenohumeral Joint; Shoulder Abduction; Shoulder Joint","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I6DUFMJF","journalArticle","2012","Knees, Peter; Pohle, Tim; Widmer, Gerhard","Sound/tracks: artistic real-time sonification of train journeys","Journal on Multimodal User Interfaces","","1783-8738","10.1007/s12193-011-0089-x","https://doi.org/10.1007/s12193-011-0089-x","We present an application of sonification in an artistic context, namely to augment the visual impressions of train journeys. While sonification and auditory displays are typically used as means to present data and to inform the user, our project sound/tracks aims at enhancing the visual experience of looking out of a moving train’s window at the passing landscape by adding a sound dimension. This allows for reflecting upon the visual impressions and deepening the state of contemplation. To this end, sound/tracks translates the perceived movement of the scenery and other visual impressions, such as passing trains, into music. The continuously changing view outside the window is captured with a camera and transformed into instantaneously played music. The application can be run on mobile phones with a built-in camera and on laptops with a Web-cam. The paper proposes and discusses different sonification approaches and presents different application scenarios.","2012-07-01","2023-07-05 06:29:29","2023-07-05 06:29:29","2023-07-05 06:29:29","87-93","","1","6","","J Multimodal User Interfaces","Sound/tracks","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/FGX4CY5C/Knees et al. - 2012 - Soundtracks artistic real-time sonification of t.pdf","","","Sonification; Mobile music generation; Train journey","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6LTDC8FF","journalArticle","2012","Barrass, Stephen","The aesthetic turn in sonification towards a social and cultural medium","AI & SOCIETY","","1435-5655","10.1007/s00146-011-0335-5","https://doi.org/10.1007/s00146-011-0335-5","The public release of datasets on the internet by government agencies, environmental scientists, political groups and many other organizations has fostered a social practice of data visualization. The audiences have expectations of production values commensurate with their daily experience of professional visual media. At the same time, access to this data has allowed visual designers and artists to apply their skills to what was previously a field dominated by scientists and engineers. The ‘aesthetic turn’ in data visualization has sparked debates between the new wave and older more scientifically grounded schools of thought on the topic. Sonification is not as well known or commonly practiced as visualization. But sound is a naturally affective, aesthetic and cultural medium. The extension of the aesthetic turn to sonification could transform this field from a scientific curiosity and engineering instrument into a popular mass medium. This paper proposes that a design approach can facilitate an aesthetic turn in sonification that integrates aesthetics and functionality by dissolving divisions between scientific and artistic methods. The first section applies the design perspective to the definition of sonification by replacing the linguistic concept of representation with non-verbal concept of functionality. The next section describes applications of the TaDa design method that raised aesthetic issues particular to sonification practice. The final section proposes a pragmatic aesthetics that distinguishes sonification from the auditory sciences and sonic arts. A design perspective may lead to a future where the general public tunes into pop sonifications for listening enjoyment as well as useful information about the world.","2012-05-01","2023-07-05 06:29:51","2023-07-05 06:29:51","2023-07-05 06:29:51","177-181","","2","27","","AI & Soc","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/P5SFUM3Y/Barrass - 2012 - The aesthetic turn in sonification towards a socia.pdf","","","Sonification; Data aesthetics; Information design; Popular culture","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TWSEYJ4Y","conferencePaper","2014","Parkinson, Adam; Tanaka, Atau","Making Data Sing: Embodied Approaches to Sonification","Sound, Music, and Motion","978-3-319-12976-1","","10.1007/978-3-319-12976-1_9","","We report on our experiences of two recent sonification pieces we have been involved in; composing for Peter Sinclair’s RoadMusic project, and creating a multi-speaker installation with sound artist Kaffe Matthews which sonifies the movement of sharks around the Galapagos islands.","2014","2023-07-05 06:39:23","2023-07-05 06:39:23","","151-160","","","","","","Making Data Sing","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/VBQ35YA5/Parkinson and Tanaka - 2014 - Making Data Sing Embodied Approaches to Sonificat.pdf","","","Sonification; Embodiment; Affect; Enaction","Aramaki, Mitsuko; Derrien, Olivier; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2G3YFMP2","bookSection","2019","Sanchez, Eric; Sanchez, Théophile","Let’s Listen to the Data: Sonification for Learning Analytics","Advances in Quantitative Ethnography","978-3-030-33231-0 978-3-030-33232-7","","","http://link.springer.com/10.1007/978-3-030-33232-7_16","This paper falls in the field of playing analytics. It deals with an empirical work dedicated to explore the potential of data sonification (i.e. the conversion of data into sound that reflects their objective properties or relations). Data sonification is proposed as an alternative to data visualization. We applied data sonification for the analysis of gameplays and players’ strategies during a session dedicated to game-based learning. The data of our study (digital traces) was collected from 200 pre-service teachers who played Tamagocours, an online collaborative multiplayer game dedicated to learn the rules (ie. copyright) that comply with the policies for the use of digital resources in an educational context. For one typical individual (parangon) for each of the 5 categories of players, the collected digital traces were converted into an audio format so that the actions that they performed become listenable. A specific software, SOnification of DAta for Learning Analytics (SODA4LA), was developed for this purpose. The first results show that different features of the data can be recognized from data listening. These results also enable for the identification of different parameters that should be taken into account for the sonification of diachronic data. We consider that this study open new perspectives for playing analytics. Thus we advocate for new research aiming at exploring the potential of data sonification for the analysis of complex and diachronic datasets in the field of educational sciences.","2019","2023-07-05 06:41:23","2023-07-19 11:12:57","2023-07-05 06:41:23","189-198","","","1112","","","Let’s Listen to the Data","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Communications in Computer and Information Science DOI: 10.1007/978-3-030-33232-7_16","","/Users/minsik/Zotero/storage/2TCK385L/Sanchez and Sanchez - 2019 - Let’s Listen to the Data Sonification for Learnin.pdf","","","","Eagan, Brendan; Misfeldt, Morten; Siebert-Evenstone, Amanda","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UAUS8448","bookSection","2013","Elgendi, Mohamed; Rebsamen, Brice; Cichocki, Andrzej; Vialatte, Francois; Dauwels, Justin","Real-Time Wireless Sonification of Brain Signals","Advances in Cognitive Neurodynamics (III)","978-94-007-4791-3 978-94-007-4792-0","","","https://link.springer.com/10.1007/978-94-007-4792-0_24","In this paper, an alternative representation of EEG is investigated, in particular, translation of EEG into sound; patterns in the EEG then correspond to sequences of notes. The aim is to provide an alternative tool for analysing and exploring brain signals, e.g., for diagnosis of neurological diseases. Specifically, a system is proposed that transforms EEG signals, recorded by a wireless headset, into sounds in real-time. In order to assess the resulting representation of EEG as sounds, the proposed sonification system is applied to EEG signals of Alzheimer’s (AD) patients and healthy age-matched control subjects (recorded by a high-quality wired EEG system). Fifteen volunteers were asked to classify the sounds generated from the EEG of 5 AD patients and 5 healthy subjects; the volunteers labeled most sounds correctly, in particular, an overall sensitivity and specificity of 93.3% and 97.3% respectively was obtained, suggesting that the sound sequences generated by the sonification system contain relevant information about EEG signals and underlying brain activity.","2013","2023-07-05 06:41:23","2023-07-19 11:10:15","2023-07-05 06:41:23","175-181","","","","","","","","","","","Springer Netherlands","Dordrecht","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-94-007-4792-0_24","","/Users/minsik/Zotero/storage/M7WRZR8D/Elgendi et al. - 2013 - Real-Time Wireless Sonification of Brain Signals.pdf","","","","Yamaguchi, Yoko","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LK36ZABP","journalArticle","2017","Dyer, J. F.; Stapleton, P.; Rodger, M. W. M.","Advantages of melodic over rhythmic movement sonification in bimanual motor skill learning","Experimental Brain Research","","0014-4819, 1432-1106","10.1007/s00221-017-5047-8","http://link.springer.com/10.1007/s00221-017-5047-8","n important question for skill acquisition is whether and how augmented feedback can be designed to improve the learning of complex skills. Auditory information triggered by learners’ actions, movement sonification, can enhance learning of a complex bimanual coordination skill, specifically polyrhythmic bimanual shape tracing. However, it is not clear whether the coordination of polyrhythmic sequenced movements is enhanced by auditory-specified timing information alone or whether more complex sound mappings, such as melodic sonification, are necessary. Furthermore, while short-term retention of bimanual coordination performance has been shown with movement sonification training, longer term retention has yet to be demonstrated. In the present experiment, participants learned to trace a diamond shape with one hand while simultaneously tracing a triangle with the other to produce a sequenced 4:3 polyrhythmic timing pattern. Two groups of participants received real-time auditory feedback during training: melodic sonification (individual movements triggered a separate note of a melody) and rhythmic sonification (each movement triggered a percussive sound), while a third control group received no augmented feedback. Task acquisition and performance in immediate retention were superior in the melodic sonification group as compared to the rhythmic sonification and control group. In a 24-h retention phase, a decline in performance in the melodic sonification group was reversed by brief playback of the target pattern melody. These results show that melodic sonification of movement can provide advantages over augmented feedback which only provides timing information by better structuring the sequencing of timed actions, and also allow recovery of complex target patterns of movement after training. These findings have important implications for understanding the role of augmented perceptual information in skill learning, as well as its application to real-world training or rehabilitation scenarios.","2017-10","2023-07-05 06:41:23","2023-07-20 00:05:08","2023-07-05 06:41:23","3129-3140","","10","235","","Exp Brain Res","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/4J6NBL9Q/Dyer et al. - 2017 - Advantages of melodic over rhythmic movement sonif.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3BV5ABI5","bookSection","2014","Jeon, Myounghoon; Smith, Michael T.; Walker, James W.; Kuhl, Scott A.","Constructing the Immersive Interactive Sonification Platform (iISoP)","Distributed, Ambient, and Pervasive Interactions","978-3-319-07787-1 978-3-319-07788-8","","","http://link.springer.com/10.1007/978-3-319-07788-8_32","For decades, researchers have spurred research on sonification, the use of non-speech audio to convey information [1]. With ‘interaction’ and ‘user experience’ being pervasive, interactive sonification [2], an emerging interdisciplinary area, has been introduced and its role and importance have rapidly increased in the auditory display community. From this background, we have devised a novel platform, “iISoP” (immersive Interactive Sonification Platform) for location, movement, and gesture-based interactive sonification research, by leveraging the existing Immersive Visualization Studio (IVS) at Michigan Tech. Projects in each developmental phase and planned research are discussed with a focus on “design research” and “interactivity”.","2014","2023-07-05 06:41:23","2023-07-19 23:58:01","2023-07-05 06:41:23","337-348","","","8530","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-07788-8_32","","/Users/minsik/Zotero/storage/Z3ZR3FLB/Jeon et al. - 2014 - Constructing the Immersive Interactive Sonificatio.pdf","","","","Streitz, Norbert; Markopoulos, Panos","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Kobsa, Alfred; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Doug; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y63M8222","journalArticle","2019","Frid, Emma; Elblaus, Ludvig; Bresin, Roberto","Interactive sonification of a fluid dance movement: an exploratory study","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-018-0278-y","http://link.springer.com/10.1007/s12193-018-0278-y","In this paper we present three different experiments designed to explore sound properties associated with fluid movement: (1) an experiment in which participants adjusted parameters of a sonification model developed for a fluid dance movement, (2) a vocal sketching experiment in which participants sketched sounds portraying fluid versus nonfluid movements, and (3) a workshop in which participants discussed and selected fluid versus nonfluid sounds. Consistent findings from the three experiments indicated that sounds expressing fluidity generally occupy a lower register and has less high frequency content, as well as a lower bandwidth, than sounds expressing nonfluidity. The ideal sound to express fluidity is continuous, calm, slow, pitched, reminiscent of wind, water or an acoustic musical instrument. The ideal sound to express nonfluidity is harsh, non-continuous, abrupt, dissonant, conceptually associated with metal or wood, unhuman and robotic. Findings presented in this paper can be used as design guidelines for future applications in which the movement property fluidity is to be conveyed through sonification.","2019-09","2023-07-05 06:41:23","2023-07-20 06:59:08","2023-07-05 06:41:23","181-189","","3","13","","J Multimodal User Interfaces","Interactive sonification of a fluid dance movement","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/L7SQJ5M6/Frid et al. - 2019 - Interactive sonification of a fluid dance movement.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8MX67HT3","bookSection","2009","Kildal, Johan","Aspects of Auditory Perception and Cognition for Usable Display Resolution in Data Sonification","Human-Computer Interaction – INTERACT 2009","978-3-642-03654-5 978-3-642-03655-2","","","http://link.springer.com/10.1007/978-3-642-03655-2_52","Sonification of data via the mapping of values to frequency of sound is an auditory data analysis technique commonly used to display graph information. The goal for any form of graph is to display numerical information with accuracy and neutrality while exploiting perceptual and cognitive processes. Conveying information in frequency of sound is subject to aspects of pitch perception, largely overlooked to date, that can influence these properties of auditory graphing. This paper identifies some of these aspects and describes potential design limitations and opportunities derived from the musical nature of auditory data representations.","2009","2023-07-05 06:41:23","2023-07-20 06:28:35","2023-07-05 06:41:23","467-470","","","5726","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-03655-2_52","","/Users/minsik/Zotero/storage/PMTUSZDX/Kildal - 2009 - Aspects of Auditory Perception and Cognition for U.pdf","","","","Gross, Tom; Gulliksen, Jan; Kotzé, Paula; Oestreicher, Lars; Palanque, Philippe; Prates, Raquel Oliveira; Winckler, Marco","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U93G3DYK","journalArticle","2021","Martin, Edward J.; Meagher, Thomas R.; Barker, Daniel","Using sound to understand protein sequence data: new sonification algorithms for protein sequences and multiple sequence alignments","BMC Bioinformatics","","1471-2105","10.1186/s12859-021-04362-7","https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-021-04362-7","Abstract                            Background               The use of sound to represent sequence data—sonification—has great potential as an alternative and complement to visual representation, exploiting features of human psychoacoustic intuitions to convey nuance more effectively. We have created five parameter-mapping sonification algorithms that aim to improve knowledge discovery from protein sequences and small protein multiple sequence alignments. For two of these algorithms, we investigated their effectiveness at conveying information. To do this we focussed on subjective assessments of user experience. This entailed a focus group session and survey research by questionnaire of individuals engaged in bioinformatics research.                                         Results               For single protein sequences, the success of our sonifications for conveying features was supported by both the survey and focus group findings. For protein multiple sequence alignments, there was limited evidence that the sonifications successfully conveyed information. Additional work is required to identify effective algorithms to render multiple sequence alignment sonification useful to researchers. Feedback from both our survey and focus groups suggests future directions for sonification of multiple alignments: animated visualisation indicating the column in the multiple alignment as the sonification progresses, user control of sequence navigation, and customisation of the sound parameters.                                         Conclusions               Sonification approaches undertaken in this work have shown some success in conveying information from protein sequence data. Feedback points out future directions to build on the sonification approaches outlined in this paper. The effectiveness assessment process implemented in this work proved useful, giving detailed feedback and key approaches for improvement based on end-user input. The uptake of similar user experience focussed effectiveness assessments could also help with other areas of bioinformatics, for example in visualisation.","2021-12","2023-07-05 06:41:23","2023-07-05 06:41:23","2023-07-05 06:41:23","456","","1","22","","BMC Bioinformatics","Using sound to understand protein sequence data","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/ET57DLSC/Martin et al. - 2021 - Using sound to understand protein sequence data n.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I7XHBE4B","journalArticle","2017","Dyer, John; Stapleton, Paul; Rodger, Matthew","Transposing musical skill: sonification of movement as concurrent augmented feedback enhances learning in a bimanual task","Psychological Research","","0340-0727, 1430-2772","10.1007/s00426-016-0775-0","http://link.springer.com/10.1007/s00426-016-0775-0","Concurrent feedback provided during acquisition can enhance performance of novel tasks. The ‘guidance hypothesis’ predicts that feedback provision leads to dependence and poor performance in its absence. However, appropriately structured feedback information provided through sound (‘sonification’) may not be subject to this effect. We test this directly using a rhythmic bimanual shape-tracing task in which participants learned to move at a 4:3 timing ratio. Sonification of movement and demonstration was compared to two other learning conditions: (1) Sonification of task demonstration alone and (2) completely silent practice (control). Sonification of movement emerged as the most effective form of practice, reaching significantly lower error scores than control. Sonification of solely the demonstration, which was expected to benefit participants by perceptually unifying task requirements, did not lead to better performance than control. Good performance was maintained by participants in the Sonification condition in an immediate retention test without feedback, indicating that the use of this feedback can overcome the guidance effect. On a 24-h retention test, performance had declined and was equal between groups. We argue that this and similar findings in the feedback literature are best explained by an ecological approach to motor skill learning which places available perceptual information at the highest level of importance.","2017-07","2023-07-05 06:41:23","2023-07-21 04:55:56","2023-07-05 06:41:23","850-862","","4","81","","Psychological Research","Transposing musical skill","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/MCNFGW3Y/Dyer et al. - 2017 - Transposing musical skill sonification of movemen.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KIR6M7J3","journalArticle","2023","Toffa, O. K.; Mignotte, M.","Dataset and semantic based-approach for image sonification","Multimedia Tools and Applications","","1573-7721","10.1007/s11042-022-12914-z","https://doi.org/10.1007/s11042-022-12914-z","This paper presents an image-audio dataset and a mid-level image sonification system that strives to help visually impaired users understand the semantic content of an image and access visual information via a combination of semantic audio and an easily decodable audio generated in real time, both triggered by sliding, taping, holding actions when the users explore the image on a touch screen or with a pointer. Firstly, we segmented the original image using a label fusion model and based on the user position in the image, a sonified signal is generated using musical notes and meaningful visual information within the active region like the color and the luminance, then the gradient and the texture. Secondly, we integrated the semantic understanding of the image into our model using DeepLab semantic segmentation of the image and created a dataset of audio and images aligned on the 20 classes of the PASCAL VOC 2012 dataset. The dataset of images are organized based on color, gradient, texture for low-level sonification and on semantic content with sounds for mid-level sonification. Thirdly, in order to provide both types of information in a complementary way, the slide, tap and hold actions of a touch screen are incorporated in the model. The semantic audio providing a brief description of the visual object is played on slide action, the generated signal with color details of the object on the tap action, gradient and texture of the object on hold action. Finally, we validated our sonification model on the provided dataset during a pilot study and the subjects were generally able to identify the objects in the image, the color of the objects and even provide a general description of the scene of the image. Our system could be useful to visually impaired persons in a photo sharing application using a smartphone or for painting art description in a digital museum.","2023-01-01","2023-07-05 06:44:22","2023-07-05 06:44:22","2023-07-05 06:44:22","1505-1518","","1","82","","Multimed Tools Appl","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/3UCEFJA2/Toffa and Mignotte - 2023 - Dataset and semantic based-approach for image soni.pdf","","","Sonification; Auditory feedback; Image accessibility; Touch screen; Visually impaired","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BQRM58EN","conferencePaper","2019","Rogozinsky, Gleb G.; Lyzhinkin, Konstantin; Egorova, Anna; Podolsky, Dmitry","Distributed Software Hardware Solution for Complex Network Sonification","Language, Music and Computing","978-3-030-05594-3","","10.1007/978-3-030-05594-3_12","","The paper presents the hardware software solution for sonification of complex networks and systems. The sonification expands the possibilities of an analysis of complex information through using the human hearing. Auditory displays allow reducing the operator’s workload and better detection of specific features and patterns in the data. The proposed sonification complex consists of two main parts. Data source located in the LO ZNIIS generates data that describes current state of a network or complex system, accumulates and redirects it. The parametric sonification layer located in the SUT converts the information into forms suitable for creating new audio environment, and represents the data as relevant timbral classes.","2019","2023-07-05 06:44:45","2023-07-05 06:44:45","","149-160","","","","","","","Communications in Computer and Information Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","","","","Sonification; Big data; Csound","Eismont, Polina; Mitrenina, Olga; Pereltsvaig, Asya","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RQVQLT39","conferencePaper","2021","Goudarzi, Visda","Exploring a Taxonomy of Interaction in Interactive Sonification Systems","Human Interaction, Emerging Technologies and Future Applications III","978-3-030-55307-4","","10.1007/978-3-030-55307-4_22","","This paper explores a variety of existing interactive sonification systems in the context of interactive sound art. In design of interactive sonification from technological standpoint, the stress is put on studying the usability and functionality of the systems. We explore the focus towards creative aspects of interaction in both technology development and sound creation stages. In some artistic sonifications, the control is in the hand of the technology creators, in some others in the hand of the artists, and sometimes in the hand of the performers or the audience members. The numerous relations and interactions between performers, composers, technologists, data domain scientists, environment and audiences make it difficult to classify the complex phenomenon of interactive sonification. Some challenges in such systems are the ownership of technical and aesthetic components, balancing engagement and interaction among different stakeholders (domain scientist, designer, composer, spectator, etc.) and encouraging audience engagement.","2021","2023-07-05 06:45:57","2023-07-05 06:45:57","","140-145","","","","","","","Advances in Intelligent Systems and Computing","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","","","","Sonification; Interactive sonification; Auditory display; Human computer interaction; Parameter mapping","Ahram, Tareq; Taiar, Redha; Langlois, Karine; Choplin, Arnaud","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G8Y3KNXM","conferencePaper","2010","Grond, Florian; Hermann, Thomas; Verfaille, Vincent; Wanderley, Marcelo M.","Methods for Effective Sonification of Clarinetists’ Ancillary Gestures","Gesture in Embodied Communication and Human-Computer Interaction","978-3-642-12553-9","","10.1007/978-3-642-12553-9_15","","We present the implementation of two different sonifications methods of ancillary gestures from clarinetists. The sonifications are data driven from the clarinetist’s posture which is captured with a VICON motion tracking system. The first sonification method is based on the velocities of the tracking markers, the second method involves a principal component analysis as a data preprocessing step. Further we develop a simple complementary visual display with a similar information content to match the sonification. The effect of the two sonifications with respect to the movement perception is studied in an experiment where test subjects annotate the clarinetists performance represented by various combinations of the resulting uni- and multimodal displays.","2010","2023-07-05 06:46:23","2023-07-05 06:46:23","","171-181","","","","","","","Lecture Notes in Computer Science","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/NGTL3XIJ/Grond et al. - 2010 - Methods for Effective Sonification of Clarinetists.pdf","","","sonification; 3D movement data; ancillary gestures; multimodal displays","Kopp, Stefan; Wachsmuth, Ipke","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GPL5NHMU","conferencePaper","2010","Ramakrishnan, Chandrasekhar","Sonification and Information Theory","Auditory Display","978-3-642-12439-6","","10.1007/978-3-642-12439-6_7","","We apply tools from the mathematical theory of information to the problem of sonification design. This produces entropy sonification, a technique for sonification, as well as a framework for analyzing and understanding sonifications in general.","2010","2023-07-05 06:47:43","2023-07-05 06:47:43","","121-142","","","","","","","Lecture Notes in Computer Science","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/8W6DI7BS/Ramakrishnan - 2010 - Sonification and Information Theory.pdf","","","Granular Synthesis; Information Theory; Interaction Design; Sonification Design","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NT7FBCKS","conferencePaper","2022","Vishnevsky, Andrey; Abbas, Nadezda","Sonification of Information Security Incidents in an Organization Using a Multistep Cooperative Game Model","Information Systems and Technologies","978-3-031-04826-5","","10.1007/978-3-031-04826-5_30","","This work is devoted to the development of computer attacks detection tool with a sound interface. Information security tools transmit visual signals to characterize the behavior of violators, but various types of conflict processes could be expressed by the plot of musical compositions. This approach could be used to encode the interaction of a protective computer system with an attacker in a harmonious way. In presented work, as an example of the conflict situation, was used a stochastic multistep cooperative game between the units of the attacked organization. An attempt to express audibly the stability of the cooperative game is made. It was realized with the help of the musical harmony of several voices of musical instruments. The program code for calculating the positional consistency of the proposed game-theoretic model and fragments of musical compositions for voicing the state of the protected organization are also proposed. Combining the basics of musical composition with game-theoretic modeling could offer a number of new possibilities for creating an ergonomic auditory human-computer interfaces.","2022","2023-07-05 06:48:25","2023-07-05 06:48:25","","306-314","","","","","","","Lecture Notes in Networks and Systems","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/9KWZDGZ6/Vishnevsky and Abbas - 2022 - Sonification of Information Security Incidents in .pdf","","","Sonification; Music; Visually impaired; Assistive technologies; Computer attacks; Training","Rocha, Alvaro; Adeli, Hojjat; Dzemyda, Gintautas; Moreira, Fernando","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CUJ2GM5H","conferencePaper","2015","Jeon, Myounghoon; Landry, Steven; Ryan, Joseph D.; Walker, James W.","Technologies Expand Aesthetic Dimensions: Visualization and Sonification of Embodied Penwald Drawings","Arts and Technology","978-3-319-18836-2","","10.1007/978-3-319-18836-2_9","","Even though defining art gets more and more difficult, reintegrating art and technology seems to be a clear trend. The present paper aims to show how technologies can expand aesthetic dimensions of art works. Michigan Tech researchers collaborated with a world-renowned artist, Tony Orrico in the immersive virtual environment. While he performed, multiple cameras tracked his body movements and physiological devices logged his biosignals (respiration, heart rate, etc.). Then, the system translated the data into visualization and sonification. Incremental aesthetic dimensions (representation-performance, 2d−3d, outside-inside) obtained based on this art-technology collaboration are discussed with research in progress.","2015","2023-07-05 06:49:19","2023-07-05 06:49:19","","69-76","","","","","","Technologies Expand Aesthetic Dimensions","Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","","","","Interactive sonification; Aesthetic computing; Digital aesthetics; Embodied drawing; Performing arts; Visualization","Brooks, Anthony Lewis; Ayiter, Elif; Yazicigil, Onur","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D9QD8HZH","journalArticle","2019","Rönnberg, Niklas","Sonification supports perception of brightness contrast","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-019-00311-0","http://link.springer.com/10.1007/s12193-019-00311-0","In complex visual representations, there are several possible challenges for the visual perception that might be eased by adding sound as a second modality (i.e. sonification). It was hypothesized that sonification would support visual perception when facing challenges such as simultaneous brightness contrast or the Mach band phenomena. This hypothesis was investigated with an interactive sonification test, yielding objective measures (accuracy and response time) as well as subjective measures of sonification benefit. In the test, the participant’s task was to mark the vertical pixel line having the highest intensity level. This was done in a condition without sonification and in three conditions where the intensity level was mapped to different musical elements. The results showed that there was a benefit of sonification, with higher accuracy when sonification was used compared to no sonification. This result was also supported by the subjective measurement. The results also showed longer response times when sonification was used. This suggests that the use and processing of the additional information took more time, leading to longer response times but also higher accuracy. There were no differences between the three sonification conditions.","2019-12","2023-07-05 06:51:38","2023-07-20 06:56:56","2023-07-05 06:51:38","373-381","","4","13","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/D3CEBA5Y/Rönnberg - 2019 - Sonification supports perception of brightness con.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2VYJUSZX","bookSection","2015","Walker, James; Smith, Michael T.; Jeon, Myounghoon","Interactive Sonification Markup Language (ISML) for Efficient Motion-Sound Mappings","Human-Computer Interaction: Interaction Technologies","978-3-319-20915-9 978-3-319-20916-6","","","http://link.springer.com/10.1007/978-3-319-20916-6_36","Despite rapid growth of research on auditory display and sonification mapping per se, there has been little effort on efficiency or accessibility of the mapping process. In order to expedite variations on sonification research configurations, we have developed the Interactive Sonification Markup Language (ISML). ISML is designed within the context of the Immersive Interactive Sonification Platform (iISoP) at Michigan Technological University. We present an overview of the system, the motivation for developing ISML, and the time savings realized through its development. We then discuss the features of ISML and its accompanying graphical editor, and conclude by summarizing the system’s feature development and future plans for its further enhancement. ISML is expected to decrease repetitive development tasks for multiple research studies and to increase accessibility to diverse sonification researchers who do not have programming experience.","2015","2023-07-05 06:51:38","2023-07-20 06:30:06","2023-07-05 06:51:38","385-394","","","9170","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-20916-6_36","","/Users/minsik/Zotero/storage/ZUDWX2M3/Walker et al. - 2015 - Interactive Sonification Markup Language (ISML) fo.pdf","","","","Kurosu, Masaaki","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZQSPG7MZ","journalArticle","2021","Plaisier, Heleen; Meagher, Thomas R.; Barker, Daniel","DNA sonification for public engagement in bioinformatics","BMC Research Notes","","1756-0500","10.1186/s13104-021-05685-7","https://bmcresnotes.biomedcentral.com/articles/10.1186/s13104-021-05685-7","Abstract                            Objective               Visualisation methods, primarily color-coded representation of sequence data, have been a predominant means of representation of DNA data. Algorithmic conversion of DNA sequence data to sound—sonification—represents an alternative means of representation that uses a different range of human sensory perception. We propose that sonification has value for public engagement with DNA sequence information because it has potential to be entertaining as well as informative. We conduct preliminary work to explore the potential of DNA sequence sonification in public engagement with bioinformatics. We apply a simple sonification technique for DNA, in which each DNA base is represented by a specific note. Additionally, a beat may be added to indicate codon boundaries or for musical effect. We report a brief analysis from public engagement events we conducted that featured this method of sonification.                                         Results               We report on use of DNA sequence sonification at two public events. Sonification has potential in public engagement with bioinformatics, both as a means of data representation and as a means to attract audience to a drop-in stand. We also discuss further directions for research on integration of sonification into bioinformatics public engagement and education.","2021-12","2023-07-05 06:51:38","2023-07-05 06:51:38","2023-07-05 06:51:38","273","","1","14","","BMC Res Notes","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/FREXN75R/Plaisier et al. - 2021 - DNA sonification for public engagement in bioinfor.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LEYNHUM5","bookSection","2017","Sousa, Luís; Pinto, António","MuSec: Sonification of Alarms Generated by a SIEM","Ambient Intelligence– Software and Applications – 8th International Symposium on Ambient Intelligence (ISAmI 2017)","978-3-319-61117-4 978-3-319-61118-1","","","http://link.springer.com/10.1007/978-3-319-61118-1_5","The information generated by a network monitoring system is overwhelming. Monitoring is imperative but very difficult to accomplish due to several reasons. More so for the case of non tech-savvy home users. Security Information Event Management applications generate alarms that correlate multiple occurrences on the network. These events are classified accordingly to their risk. An application that allows the sonification of events generated by a Security Information Event Management can facilitate the security monitoring of a home network by a less tech-savvy user by allowing him to just listen to the result of the sonification of such events.","2017","2023-07-05 06:51:38","2023-07-19 11:28:40","2023-07-05 06:51:38","32-39","","","615","","","MuSec","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Advances in Intelligent Systems and Computing DOI: 10.1007/978-3-319-61118-1_5","","/Users/minsik/Zotero/storage/68FWBI8B/Sousa and Pinto - 2017 - MuSec Sonification of Alarms Generated by a SIEM.pdf","","","","De Paz, Juan F.; Julián, Vicente; Villarrubia, Gabriel; Marreiros, Goreti; Novais, Paulo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YDGXY444","bookSection","2021","Denjean, Sébastien; Kronland-Martinet, Richard; Roussarie, Vincent; Ystad, Sølvi","Zero-Emission Vehicles Sonification Strategy Based on Shepard-Risset Glissando","Perception, Representations, Image, Sound, Music","978-3-030-70209-0 978-3-030-70210-6","","","http://link.springer.com/10.1007/978-3-030-70210-6_46","In this paper we present a sonification strategy developed for electric vehicles aiming to synthetize a new engine sound to enhance the driver’s dynamic perception of his vehicle. We chose to mimic the internal combustion engine (ICE) noise by informing the driver through pitch variations. However, ICE noise pitch variations are correlated to the engine’s rotations per minute (RPM) and its dynamics is covered within a limited vehicle speed range. In order to inform the driver with a significant pitch variation throughout the full vehicle speed range, we based our sonification strategy on the Shepard-Risset glissando. These illusory infinite ascending/descending sounds enable to represent accelerations with significant pitch variations for an unlimited range of speeds. In a way, we stay within the metaphor of ICE noise with unheard gearshifts. We tested this sonification strategy in a perceptual test in a driving simulator and showed that the mapping of this acoustical feedback affects the drivers’ perception of vehicle dynamics.","2021","2023-07-05 06:51:38","2023-07-21 04:45:13","2023-07-05 06:51:38","709-724","","","12631","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-70210-6_46","","/Users/minsik/Zotero/storage/YMQKYYI7/Denjean et al. - 2021 - Zero-Emission Vehicles Sonification Strategy Based.pdf","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Aramaki, Mitsuko","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5DE4YYA5","journalArticle","2012","Kronland-Martinet, Richard; Ystad, Sølvi; Aramaki, Mitsuko","High-level control of sound synthesis for sonification processes","AI & SOCIETY","","0951-5666, 1435-5655","10.1007/s00146-011-0340-8","http://link.springer.com/10.1007/s00146-011-0340-8","Methods of sonification based on the design and control of sound synthesis is presented in this paper. The semiotics of isolated sounds was evidenced by performing fundamental studies using a combined acoustical and brain imaging (event-related potentials) approach. The perceptual cues (which are known as invariants) responsible for the evocations elicited by the sounds generated by impacts, moving sound sources, dynamic events and vehicles (car-door closing and car engine noise) were then identified based on physical and perceptual considerations. Lastly, some examples of the high-level control of a synthesis process simulating immersive 3-D auditory scenes, interacting objects and evoked dynamics are presented.","2012-05","2023-07-05 06:51:38","2023-07-19 11:22:28","2023-07-05 06:51:38","245-255","","2","27","","AI & Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QR6E7M4D","journalArticle","2012","Bresin, Roberto; Hermann, Thomas; Hunt, Andy","Interactive sonification","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-012-0095-7","http://link.springer.com/10.1007/s12193-012-0095-7","","2012-05","2023-07-05 06:51:38","2023-07-05 06:51:38","2023-07-05 06:51:38","85-86","","3-4","5","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/L9TASIT8/Bresin et al. - 2012 - Interactive sonification.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TQECEVAU","journalArticle","2019","Niewiadomski, Radoslaw; Mancini, Maurizio; Cera, Andrea; Piana, Stefano; Canepa, Corrado; Camurri, Antonio","Does embodied training improve the recognition of mid-level expressive movement qualities sonification?","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-018-0284-0","http://link.springer.com/10.1007/s12193-018-0284-0","This research is a part of a broader project exploring how movement qualities can be recognized by means of the auditory channel: can we perceive an expressive full-body movement quality by means of its interactive sonification? The paper presents a sonification framework and an experiment to evaluate if embodied sonic training (i.e., experiencing interactive sonification of your own body movements) increases the recognition of such qualities through the auditory channel only, compared to a non-embodied sonic training condition. We focus on the sonification of two mid-level movement qualities: fragility and lightness. We base our sonification models, described in the first part, on the assumption that specific compounds of spectral features of a sound can contribute to the cross-modal perception of a specific movement quality. The experiment, described in the second part, involved 40 participants divided into two groups (embodied sonic training vs. no training). Participants were asked to report the level of lightness and fragility they perceived in 20 audio stimuli generated using the proposed sonification models. Results show that (1) both expressive qualities were correctly recognized from the audio stimuli, (2) a positive effect of embodied sonic training was observed for fragility but not for lightness. The paper is concluded by the description of the artistic performance that took place in 2017 in Genoa (Italy), in which the outcomes of the presented experiment were exploited.","2019-09","2023-07-05 06:51:38","2023-07-20 07:03:37","2023-07-05 06:51:38","191-203","","3","13","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/FPG6QPWL/Niewiadomski et al. - 2019 - Does embodied training improve the recognition of .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TZP2G3XK","conferencePaper","2022","Baratè, Adriano; Ludovico, Luca A.; Motola, Alessia; Presti, Giorgio","Augmentation of a Virtual Exhibition of Paintings Through Sonification","The Future of Heritage Science and Technologies: ICT and Digital Heritage","978-3-031-20302-2","","10.1007/978-3-031-20302-2_28","","In this paper, we will take into consideration a virtual exhibition of paintings and focus on one of the possibilities offered by a virtual approach: the implementation of additional features aiming at the augmentation of users’ experience. Specifically, we will propose a framework to sonify paintings by introducing virtual audio sources in the pictures. The reproduction of spatialized sound during the vision of the painting is expected to induce sensory enhancement. Audio materials can be introduced either to reinforce the experience, as for the natural sounds of a landscape, or to deliver new meanings, thus aiming at Gesamtkunstwerk, i.e. an all-embracing art form. After presenting the state of the art about virtual exhibitions and the use of sonification in virtual reality, this paper will introduce a publicly available prototype of sound-augmented experience. The project, implemented in Unity, represents a pilot study to be further developed with a back-end area to let the user configure her/his own exhibition space, showcase selected paintings, and add sound sources in custom positions of the artworks.","2022","2023-07-05 06:52:46","2023-07-05 06:52:46","","380-392","","","","","","","Communications in Computer and Information Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/ZN4Q8SNX/Baratè et al. - 2022 - Augmentation of a Virtual Exhibition of Paintings .pdf","","","Sonification; Unity; Virtual reality; Visual arts","Furferi, Rocco; Governi, Lapo; Volpe, Yary; Seymour, Kate; Pelagotti, Anna; Gherardini, Francesco","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PW44C267","conferencePaper","2006","Rutkowski, Tomasz M.; Vialatte, Francois; Cichocki, Andrzej; Mandic, Danilo P.; Barros, Allan Kardec","Auditory Feedback for Brain Computer Interface Management – An EEG Data Sonification Approach","Knowledge-Based Intelligent Information and Engineering Systems","978-3-540-46544-7","","10.1007/11893011_156","","An auditory feedback for Brain Computer Interface (BCI) applications is proposed. This is achieved based on the so-called sonification of the mental states of humans, captured by Electro-Encephalogram (EEG) recordings. Two time-frequency signal decomposition techniques, the Bump Modelling and Empirical Mode Decomposition (EMD), are used to map the EEG recordings onto musical scores. This auditory feedback proves to have extremely high potential in the development of on-line BCI interfaces. Examples based on the responses from visual stimuli support the analysis.","2006","2023-07-05 06:53:04","2023-07-05 06:53:04","","1232-1239","","","","","","","Lecture Notes in Computer Science","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/T8IAS8R9/Rutkowski et al. - 2006 - Auditory Feedback for Brain Computer Interface Man.pdf","","","Auditory Feedback; Brain Computer Interface; Empirical Mode Decomposition; Instantaneous Frequency; Intrinsic Mode Function","Gabrys, Bogdan; Howlett, Robert J.; Jain, Lakhmi C.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TA9Q947Q","journalArticle","2015","Dubus, Gaël; Bresin, Roberto","Exploration and evaluation of a system for interactive sonification of elite rowing","Sports Engineering","","1460-2687","10.1007/s12283-014-0164-0","https://doi.org/10.1007/s12283-014-0164-0","In recent years, many solutions based on interactive sonification have been introduced for enhancing sport training. Few of them have been assessed in terms of efficiency or design. In a previous study, we performed a quantitative evaluation of four models for the sonification of elite rowing in a non-interactive context. For the present article, we conducted on-water experiments to investigate the effects of some of these models on two kinematic quantities: stroke rate value and fluctuations in boat velocity. To this end, elite rowers interacted with discrete and continuous auditory displays in two experiments. A method for computing an average rowing cycle is introduced, together with a measure of velocity fluctuations. Participants answered to questionnaires and interviews to assess the degree of acceptance of the different models and to reveal common trends and individual preferences. No significant effect of sonification could be determined in either of the two experiments. The measure of velocity fluctuations was found to depend linearly on stroke rate. Participants provided feedback about their aesthetic preferences and functional needs during interviews, allowing us to improve the models for future experiments to be conducted over longer periods.","2015-03-01","2023-07-05 06:53:39","2023-07-05 06:53:39","2023-07-05 06:53:39","29-41","","1","18","","Sports Eng","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/C4PX5WXW/Dubus and Bresin - 2015 - Exploration and evaluation of a system for interac.pdf","","","Sonification; Auditory display; Evaluation; Interactive; Rowing; Sonic interaction; Sport","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GZYJIRSC","conferencePaper","2014","Danna, Jérémy; Paz-Villagrán, Vietminh; Capel, Annabelle; Pétroz, Céline; Gondre, Charles; Pinto, Serge; Thoret, Etienne; Aramaki, Mitsuko; Ystad, Sølvi; Kronland-Martinet, Richard; Velay, Jean-Luc","Movement Sonification for the Diagnosis and the Rehabilitation of Graphomotor Disorders","Sound, Music, and Motion","978-3-319-12976-1","","10.1007/978-3-319-12976-1_16","","The dynamic features of sounds make them particularly appropriate for assessing the spatiotemporal characteristics of movements. Furthermore, sounds can inform about the correctness of an ongoing movement without directly interfering with the visual and proprioceptive feedback. Finally, because of their playful characteristics, sounds are potentially effective for motivating writers in particular need of any writing assistance. By associating relevant sounds to the specific variables of handwriting movement, the present chapter aimed at reporting how supplementary auditory information allows an examiner (teacher or therapist) to assess the movement quality from his/her hearing. Furthermore, a writer could also improve his/her movement from this real-time auditory feedback. Sonification of some movement characteristics would be a relevant tool for the diagnosis and the rehabilitation of some developmental disabilities (e.g. dysgraphia) or acquired disorders (e.g. Parkinson’s disease).","2014","2023-07-05 06:55:14","2023-07-05 06:55:14","","246-255","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/KDDMRCVS/Danna et al. - 2014 - Movement Sonification for the Diagnosis and the Re.pdf","","","Auditory feedback; Dysgraphia; Kinematics; Motor control; Parkinson’s disease","Aramaki, Mitsuko; Derrien, Olivier; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U89TVI7Y","conferencePaper","2010","Constantinescu, Angela; Müller, Karin","Sonification of ASCII Circuit Diagrams","Computers Helping People with Special Needs","978-3-642-14097-6","","10.1007/978-3-642-14097-6_16","","Graphics and diagrams can be made available to blind users mainly in two ways: via speech descriptions or tactile printing. However, both approaches require help from a sighted, well instructed third party. We propose here a fast and inexpensive alternative to making graphics accessible to blind people, using sound and a corpus of ASCII graphics. The goal is to outline the challenges in sonifying ASCII diagrams in such a way that the semantics of the graphics is successfully brought to their users. Additionally, the users should have the possibility to perform the sonifications themselves. The concept is exemplified using circuit diagrams.","2010","2023-07-05 06:56:02","2023-07-05 06:56:02","","89-91","","","","","","","Lecture Notes in Computer Science","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/G478LWS9/Constantinescu and Müller - 2010 - Sonification of ASCII Circuit Diagrams.pdf","","","Sonification; accessibility; ASCII graphics; blind","Miesenberger, Klaus; Klaus, Joachim; Zagler, Wolfgang; Karshmer, Arthur","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A8RL7Y64","journalArticle","2023","Kacem, Amal; Zbiss, Khalil; Watta, Paul; Mohammadi, Alireza","Wave space sonification of the folding pathways of protein molecules modeled as hyper-redundant robotic mechanisms","Multimedia Tools and Applications","","1573-7721","10.1007/s11042-023-15385-y","https://doi.org/10.1007/s11042-023-15385-y","Investigation of the folding pathways of protein molecules plays a key role in studying diseases such as Alzheimer’s and designing viral drugs at the molecular level. Despite recent advances in visualization techniques, effective sonification (i.e., non-speech auditory representation) of large datasets associated with protein folding pathways is still an open question. This paper investigates the problem of sonification of protein folding pathway datasets by using the wave space sonification (WSS) framework due to Hermann (2018). In particular, this paper utilizes the powerful WSS framework to develop a sonification methodology for the dihedral angle folding trajectories of protein molecules, which are modeled as hyper-redundant robotic mechanisms with many rigid nano-linkages. As an example, the developed sonification methodology is applied to a protein molecule backbone chain with a dihedral angle space of dimension 82, where a canonical wave space function based on a sum-of-sinusoids with conformation-dependent frequencies and a sample-based wave space function based on Mozart’s Alla Turca are utilized for sonification of the folding trajectories of this peptide chain.","2023-05-30","2023-07-05 06:56:54","2023-07-05 06:56:54","2023-07-05 06:56:54","","","","","","Multimed Tools Appl","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/CXJHJMGR/Kacem et al. - 2023 - Wave space sonification of the folding pathways of.pdf","","","Sonification; Hyper-redundant robots; Protein folding; Wave Space Sonification (WSS)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HBT3QKLE","journalArticle","2012","Gresham-Lancaster, Scot","Waveguide synthesis for sonification of distributed sensor arrays","AI & SOCIETY","","1435-5655","10.1007/s00146-011-0357-z","https://doi.org/10.1007/s00146-011-0357-z","A decade of work is outlined based on the use of sensors on plants that are used to change the parameters of a fixed rotation of overlapping pitches. The use of waveguide, physical modeling synthesis, allows the repeated music figures to be changed in timbral space in real time in a discernable set of ongoing parameter mapping from a large data set being generated by various biological and atmospheric sensors.","2012-05-01","2023-07-05 06:57:14","2023-07-05 06:57:14","2023-07-05 06:57:14","289-292","","2","27","","AI & Soc","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/HNE4YATI/Gresham-Lancaster - 2012 - Waveguide synthesis for sonification of distribute.pdf","","","Sonification; Self-organization; Sensor network; Sound art; Waveguide","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6MB95IJ5","bookSection","2023","Torresan, Christian; Bernardes, Gilberto; Caetano, Elsa; Restivo, Teresa","The Singing Bridge: Sonification of a Stress-Ribbon Footbridge","ArtsIT, Interactivity and Game Creation","978-3-031-28992-7 978-3-031-28993-4","","","https://link.springer.com/10.1007/978-3-031-28993-4_25","Stress-ribbon footbridges are often prone to excessive vibrations induced by environmental phenomena (e.g., wind and rain) and human actions (e.g., walking and jumping) and their liveliness is strongly associated with their slenderness. In earlier studies, multiple dynamic responses of a stress-ribbon footbridge were observed on the campus of the Faculty of Engineering of the University of Porto (FEUP) in Portugal. Although extreme vibrations have never been reported, vertical oscillations are clearly perceptible under pedestrian excitement. While monitoring the bridge, the technology revealed physical phenomena that are invisible to humans. This project aims to adopt sonification techniques as a compositional tool to create a sonic manifestation that shows the dynamic response of the bridge. In this study, two different sonification techniques (audification and parameter mapping) were used to extrapolate the same phenomena using different strategies. For what concerns sound synthesis, the first technique used an FM synthesizer, while the second one used external VSTs and generative approaches to create a compelling musical sonification. In order to evaluate the proposed sonification techniques’ reliability, an online listening test was conducted to assess the three main dimensions of a collected dataset: the number of people crossing the bridge, their walking speed, and the steadiness of their pace. Respondents were required to complete both a blind test and one after a short training to assess the intuitiveness and reliability of both methods. According to the results, it is clear that the training significantly improves the participants’ accuracy in identifying the correct categories. In fact, almost all values have increased after the short training. Therefore, this suggests that the success of both sonification techniques could improve significantly with deeper training. Additionally, the overall trend shows parameter mapping sonification as a more intuitive and precise technique than audification.","2023","2023-07-05 06:59:22","2023-07-19 11:35:53","2023-07-05 06:59:22","359-373","","","479","","","The Singing Bridge","","","","","Springer Nature Switzerland","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering DOI: 10.1007/978-3-031-28993-4_25","","/Users/minsik/Zotero/storage/ILXQC5BD/Torresan et al. - 2023 - The Singing Bridge Sonification of a Stress-Ribbo.pdf","","","","Brooks, Anthony L.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B2K4HGKF","bookSection","2010","Saranti, Anna; Eckel, Gerhard; Pirrò, David","Quantum Harmonic Oscillator Sonification","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_10","","2010","2023-07-05 06:59:22","2023-07-05 06:59:22","2023-07-05 06:59:22","184-201","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_10","","/Users/minsik/Zotero/storage/C79X4IYL/Saranti et al. - 2010 - Quantum Harmonic Oscillator Sonification.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6U2QKPTA","bookSection","2007","Daunys, Gintautas; Lauruska, Vidas","Sonification System of Maps for Blind","Universal Access in Human-Computer Interaction. Ambient Interaction","978-3-540-73280-8 978-3-540-73281-5","","","http://link.springer.com/10.1007/978-3-540-73281-5_37","Presentation of graphical information is very important for blind. This information will help blind better understand surrounding world. The developed system is devoted for investigation of graphical information by blind user using a digitiser. SVG language with additional elements is used for describing of maps. Non-speech sounds are used to transfer information about colour. Alerting sound signal is issued near two regions boundary.","2007","2023-07-05 06:59:22","2023-07-21 05:08:34","2023-07-05 06:59:22","349-352","","","4555","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73281-5_37","","/Users/minsik/Zotero/storage/2NCAS8ZZ/Daunys and Lauruska - 2007 - Sonification System of Maps for Blind.pdf","","","","Stephanidis, Constantine","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6R73UGAK","journalArticle","2012","Dubus, Gaël","Evaluation of four models for the sonification of elite rowing","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0085-1","http://link.springer.com/10.1007/s12193-011-0085-1","Many aspects of sonification represent potential benefits for the practice of sports. Taking advantage of the characteristics of auditory perception, interactive sonification offers promising opportunities for enhancing the training of athletes. The efficient learning and memorizing abilities pertaining to the sense of hearing, together with the strong coupling between auditory and sensorimotor systems, make the use of sound a natural field of investigation in quest of efficiency optimization in individual sports at a high level. This study presents an application of sonification to elite rowing, introducing and evaluating four sonification models. The rapid development of mobile technology capable of efficiently handling numerical information offers new possibilities for interactive auditory display. Thus, these models have been developed under the specific constraints of a mobile platform, from data acquisition to the generation of a meaningful sound feedback. In order to evaluate the models, two listening experiments have then been carried out with elite rowers. Results show a good ability of the participants to efficiently extract basic characteristics of the sonified data, even in a non-interactive context. Qualitative assessment of the models highlights the need for a balance between function and esthetics in interactive sonification design. Consequently, particular attention on usability is required for future displays to become widespread.","2012-05","2023-07-05 06:59:22","2023-07-20 06:53:58","2023-07-05 06:59:22","143-156","","3-4","5","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/4HHYGB4L/Dubus - 2012 - Evaluation of four models for the sonification of .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"62PVCZ4I","bookSection","2009","Olivetti Belardinelli, Marta; Federici, Stefano; Delogu, Franco; Palmiero, Massimiliano","Sonification of Spatial Information: Audio-Tactile Exploration Strategies by Normal and Blind Subjects","Universal Access in Human-Computer Interaction. Intelligent and Ubiquitous Interaction Environments","978-3-642-02709-3 978-3-642-02710-9","","","http://link.springer.com/10.1007/978-3-642-02710-9_62","On the basis of a meta-analysis of existing literature about sonification technologies, new experimental results on audio-tactile exploration strategies of georeferenced sonificated data by sighted and blind subjects are presented, discussing: technology suitability, subjects’ performances, accessibility and usability in the user/technology interaction.","2009","2023-07-05 06:59:22","2023-07-21 05:10:23","2023-07-05 06:59:22","557-563","","","5615","","","Sonification of Spatial Information","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-02710-9_62","","/Users/minsik/Zotero/storage/DGA6GELV/Olivetti Belardinelli et al. - 2009 - Sonification of Spatial Information Audio-Tactile.pdf","","","","Stephanidis, Constantine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S3N6QJXU","bookSection","2007","Ag. Ibrahim, Ag. Asri; Hunt, Andy","An HCI Model for Usability of Sonification Applications","Task Models and Diagrams for Users Interface Design","978-3-540-70815-5 978-3-540-70816-2","","","http://link.springer.com/10.1007/978-3-540-70816-2_18","Sonification is a representation of data using sounds with the intention of communication and interpretation. The process and technique of converting the data into sound is called the sonification technique. One or more techniques might be required by a sonification application. However, sonification techniques are not always suitable for all kinds of data, and often custom techniques are used - where the design is tailored to the domain and nature of the data as well as the users’ required tasks within the application. Therefore, it is important to assure the usability of the technique for the specific domain application being developed. This paper describes a new HCI Model for usability of sonification applications. It consists of two other models, namely the Sonification Application (SA) Model and User Interpretation Construction (UIC) Model. The SA model will be used to explain the application from the designer’s point of view. The UIC Model will be used to explain what the user might perceive and understand.","2007","2023-07-05 06:59:22","2023-07-21 05:03:49","2023-07-05 06:59:22","245-258","","","4385","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-70816-2_18","","","","","","Coninx, Karin; Luyten, Kris; Schneider, Kevin A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q6MGP8H7","journalArticle","2019","Lorenzoni, Valerio; Van Den Berghe, Pieter; Maes, Pieter-Jan; De Bie, Tijl; De Clercq, Dirk; Leman, Marc","Design and validation of an auditory biofeedback system for modification of running parameters","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-018-0283-1","http://link.springer.com/10.1007/s12193-018-0283-1","Real-time auditory feedback during sports activities is becoming increasingly popular in view of opportunities for monitoring and movement (re)training in ecological environments. However, the design of an effective feedback strategy is difficult. In this paper, we present a methodical approach to the design of an auditory feedback strategy for running gait modification of recreational runners, using distortion of a musical baseline. First tests were conducted to select the best performing auditory distortion signal in terms of clarity and level perception, and to derive the relative perception curve. This was found to be pink noise with an exponential response curve. Further tests were carried out to determine the just noticeable difference of this signal in actual running conditions. Finally, validation tests were performed to examine if the real-time auditory biofeedback, combined with music, could alter the runner’s steps per minute (SPM) during treadmill-based running. The results show that our sonification strategy can alter the mean running SPM in a clear and non-disturbing way, and that our noise-based continuous feedback approach performs better than standard verbal instructions. Even though some of the participants did not respond effectively to the feedback, a large majority of the participants rated the feedback system as pleasant and indicated that they would use such system to improve their running style.","2019-09","2023-07-05 06:59:22","2023-07-20 07:01:36","2023-07-05 06:59:22","167-180","","3","13","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HJE4PQ5I","bookSection","2002","Hermann, Thomas; Nölker, Claudia; Ritter, Helge","Hand Postures for Sonification Control","Gesture and Sign Language in Human-Computer Interaction","978-3-540-43678-2 978-3-540-47873-7","","","http://link.springer.com/10.1007/3-540-47873-6_32","Sonification is a rather new technique in human-computer interaction which addresses auditory perception. In contrast to speech interfaces, sonification uses non-verbal sounds to present information. The most common sonification technique is parameter mapping where for each data point a sonic event is generated whose acoustic attributes are determined from data values by a mapping function. For acoustic data exploration, this mapping must be adjusted or manipulated by the user. We propose the use of hand postures as a particularly natural and intuitive means of parameter manipulation for this data exploration task. As a demonstration prototype we developed a hand posture recognition system for gestural controlling of sound. The presented implementation applies artificial neural networks for the identification of continuous hand postures from camera images and uses a real-time sound synthesis engine. In this paper, we present our system and first applications of the gestural control of sounds. Techniques to apply gestures to control sonification are proposed and sound examples are given.","2002","2023-07-05 06:59:22","2023-07-20 00:08:01","2023-07-05 06:59:22","307-316","","","2298","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-47873-6_32","","","","","","Wachsmuth, Ipke; Sowa, Timo","Goos, G.; Hartmanis, J.; Van Leeuwen, J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GKQEVKJ9","bookSection","2014","Lockton, Dan; Bowden, Flora; Brass, Clare; Gheerawo, Rama","Powerchord: Towards Ambient Appliance-Level Electricity Use Feedback through Real-Time Sonification","Ubiquitous Computing and Ambient Intelligence. Personalisation and User Adapted Services","978-3-319-13101-6 978-3-319-13102-3","","","http://link.springer.com/10.1007/978-3-319-13102-3_10","Feedback on energy use mainly uses visual, numerical interfaces. This paper introduces an alternative: energy sonification, turning real-time electricity use data from appliances into ambient sound. Powerchord, a work in progress prototype developed through co-creation with householders, is detailed.","2014","2023-07-05 06:59:22","2023-07-21 05:08:16","2023-07-05 06:59:22","48-51","","","8867","","","Powerchord","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-13102-3_10","","","","","","Hervás, Ramón; Lee, Sungyoung; Nugent, Chris; Bravo, José","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IVE495T5","journalArticle","2017","Ferraro-Petrillo, Umberto","Using the audio of 8-bit video games to monitor web marketing campaigns","Multimedia Systems","","0942-4962, 1432-1882","10.1007/s00530-016-0509-6","http://link.springer.com/10.1007/s00530-016-0509-6","Monitoring the performance of a web marketing campaign is usually a long-lasting, low-effort but distracting task, where a user repeatedly glances at some sort of visual analytics tools to check whether the campaign is going well. In this paper, we explore an alternative approach for this task, where the performance of a web marketing campaign is monitored through sonification, using the soundset of popular 8-bit arcade video games. On one hand, sonification would allow a user to be constantly informed about the current state of the campaign without being distracted. On the other hand, the sound metaphors coming from popular 8-bit arcade video games would be able to convey information about the status of the campaign in a simple and effective way (i.e., if the sonification of a campaign resembles the audio of a successful game session, then the campaign is going well). We investigated this idea by developing a prototype system for the sonification of the behavior of a web server activity through a configurable set of sound metaphors. We then analyzed the effectiveness of our approach by conducting a simple experimental study. This was done, first, by sonifying the progress of a given web marketing campaign using the soundset of two popular 8-bit video games: Super Mario Bros and Bubble Bobble. The outcoming soundtrack was then used in a controlled setting to assess the performance of a group of 20 participants listening to our soundtrack under different work conditions.","2017-07","2023-07-05 06:59:22","2023-07-21 04:31:28","2023-07-05 06:59:22","469-484","","4","23","","Multimedia Systems","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PTMK8EDN","bookSection","2000","Soddell, Fran; Soddell, Jacques","Microbes and Music","PRICAI 2000 Topics in Artificial Intelligence","978-3-540-67925-7 978-3-540-44533-3","","","http://link.springer.com/10.1007/3-540-44533-1_76","L-systems are string rewriting mechanisms used to create images of complex organisms from a simple set of an axiom and production rules. They have also been used to create music. This study developed a Musical Instrument Digital Interface interpretation suitable for applying to strings generated by L-systems that had been previously developed to model the growth of filamentous microbes (fungi and bacteria). The resulting sound files helped distinguish between organisms with different growth rates, provided some insight into the temporal differences among stages of growth, and also resulted in interesting musical pieces.","2000","2023-07-05 06:59:22","2023-07-21 04:55:06","2023-07-05 06:59:22","767-777","","","1886","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-44533-1_76","","","","","","Mizoguchi, Riichiro; Slaney, John","Goos, G.; Hartmanis, J.; Van Leeuwen, J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LN7XK4RN","bookSection","2014","Braund, Edward; Miranda, Eduardo","Music with Unconventional Computing: A System for Physarum Polycephalum Sound Synthesis","Sound, Music, and Motion","978-3-319-12975-4 978-3-319-12976-1","","","https://link.springer.com/10.1007/978-3-319-12976-1_11","The field of computer music is evolving in tandem with advances in computer science. Our research is interested in how the developing field of unconventional computation may provide new pathways for music and music technologies. In this paper we present the development of a system for harnessing the biological computing substrate Physarum Polycephalum for sonification. Physarum Polycephalum is a large single cell with a myriad of diploid nuclei, which moves like a giant amoeba in its pursuit for food. The organism is amorphous, and although without a brain or any serving centre of control, can respond to the environmental conditions that surround it.","2014","2023-07-05 06:59:22","2023-07-21 05:01:17","2023-07-05 06:59:22","175-189","","","8905","","","Music with Unconventional Computing","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-12976-1_11","","","","","","Aramaki, Mitsuko; Derrien, Olivier; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GHY279NQ","journalArticle","2016","Stahl, Benjamin; Thoshkahna, Balaji","Design and evaluation of the effectiveness of a sonification technique for real time heart-rate data","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-016-0218-7","http://link.springer.com/10.1007/s12193-016-0218-7","This article is motivated by the question “Can a sonification system that provides continuous auditory heart rate feedback help stabilize an athlete’s heart rate at a given target heart rate while exercising?” The sonification system uses a Polar H7 heart rate sensor to measure the heart rate of the athlete and an iOS device for its processing and display. We implemented several sonification approaches, of which two were tested in both a unimodal and an audiovisual context in comparison to a purely visual feedback and to not having any feedback. The system’s objective performance and multiple subjective usability aspects were evaluated in an experiment with 16 subjects. The experiment has to be considered a pilot study because the exercising conditions were artificial. The subjects were exercising on an indoor cycle and could focus their visual sense on the visual display all the time. It was found that all of the feedback methods could convey information to the athlete and were therefore clearly superior to not having any feedback. The failure of showing a supremacy of the multimodal methods over the purely visual one can be reasoned by the fact that the testing conditions were artificial and could therefore not show the advantages of auditory/audiovisual feedback due to limited bandwidth of the visual channel. The conclusions we make about the design and evaluation of such sonification systems can be considered a useful starting point for further work in this field.","2016-09","2023-07-05 06:59:22","2023-07-20 07:04:22","2023-07-05 06:59:22","207-219","","3","10","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AYPIHIAJ","bookSection","2010","Schaffert, Nina; Mattes, Klaus; Effenberg, Alfred O.","A Sound Design for Acoustic Feedback in Elite Sports","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_8","Sound (acoustic information) is the naturally evocative, audible result of kinetic events. Humans interact with the world by the everyday experience of listening to perceive and interpret the environment. Elite athletes, especially, rely on sport specific sounds for feedback about successful (or unsuccessful) movements. Visualization plays the dominant role in technique analysis, but the limitations of visual observation (of time related events) compared with auditory perception, which represents information with a clearer time-resolution, mean that acoustic displays offer a promising alternative to visual displays. Sonification, as acoustic representation of information, offers an abundance of applications in elite sports for monitoring, observing movement and detecting changes therein. Appropriate sound is needed to represent specific movement patterns. This article presents conceptual considerations for a sound design to fulfill the specific purpose of movement optimization that would be acceptable to elite athletes, with first practical experience with elite athletes in rowing.","2010","2023-07-05 06:59:22","2023-07-19 11:38:26","2023-07-05 06:59:22","143-165","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_8","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R7ZACSXJ","conferencePaper","2023","Fink, Thomas; Akdag Salah, Alkim Almila","Extending the Visual Arts Experience: Sonifying Paintings with AI","Artificial Intelligence in Music, Sound, Art and Design","978-3-031-29956-8","","10.1007/978-3-031-29956-8_7","","Sonification of visual information is a relatively new research line that aims to create a new way to access and experience visual displays, especially for the visually impaired. When applied to artworks, sonification needs to translate the aesthetic experience as well. This is attempted via a handful studies in the literature, where most of the transformation and music generation is done manually, or only by using the low level visual features of artworks. In this paper, we present a sonification model that uses both low level and high level features such as color, edge information, saliency, object and scene detection to create a pleasant and descriptive sonification of artworks with the use of a fully automatic pipeline. The results of the model are tested via interviews done with experts in music theory and generative music models. We found a high agreement among experts for the evaluation of a small set of sonified paintings. Addition of high level features such as sounds extracted from the scene played a big role in this. Among the challenges observed during the interviews was the need to add emotion and mood information as well as semantic information to the sonification in order to create more descriptive melodies and sounds. The complexity and ambiguity of the visual information generated the most disagreement among experts both in their interpretation of the paintings as well as their sonifications.","2023","2023-07-05 07:02:24","2023-07-05 07:02:24","","100-116","","","","","","Extending the Visual Arts Experience","Lecture Notes in Computer Science","","","","Springer Nature Switzerland","Cham","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/P8MVVUII/Fink and Akdag Salah - 2023 - Extending the Visual Arts Experience Sonifying Pa.pdf","","","Artwork sonification; Generative music models; High level visual feature sonification; Low level visual feature sonification","Johnson, Colin; Rodríguez-Fernández, Nereida; Rebelo, Sérgio M.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E84DTQLY","journalArticle","2021","Iber, Michael; Lechner, Patrik; Jandl, Christian; Mader, Manuel; Reichmann, Michael","Auditory augmented process monitoring for cyber physical production systems","Personal and Ubiquitous Computing","","1617-4909, 1617-4917","10.1007/s00779-020-01394-3","https://link.springer.com/10.1007/s00779-020-01394-3","Abstract             We describe two proof-of-concept approaches on the sonification of estimated operation states and conditions focusing on two scenarios: a laboratory setup of a manipulated 3D printer and an industrial setup focusing on the operations of a punching machine. The results of these studies form the basis for the development of an “intelligent” noise protection headphone as part of Cyber Physical Production Systems which provides auditorily augmented information to machine operators and enables radio communication between them. Further application areas are implementations in control rooms (equipped with multi-channel loudspeaker systems) and utilization for training purposes. As a first proof-of-concept, the data stream of error probability estimations regarding partly manipulated 3D printing processes were mapped to three sonification models, providing evidence about momentary operation states. The neural network applied indicates a high accuracy (> 93%) of the error estimation distinguishing between normal and manipulated operation states. None of the manipulated states could be identified by listening. An auditory augmentation, or sonification of these error estimations, provides a considerable benefit to process monitoring. For a second proof-of-concept, setup operations of a punching machine were recorded. Since all operations were apparently flawlessly executed, and there were no errors to be reported, we focused on the identification of operation phases. Each phase of a punching process could be algorithmically distinguished at an estimated probability rate of > 94%. In the auditory display, these phases were represented by different instrumentations of a musical piece in order to allow users to differentiate between operations auditorily.","2021-08","2023-07-05 07:04:22","2023-07-05 07:04:22","2023-07-05 07:04:22","691-704","","4","25","","Pers Ubiquit Comput","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/WGEB3NRY/Iber et al. - 2021 - Auditory augmented process monitoring for cyber ph.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VXWMM39L","journalArticle","2017","Temple, Mark D.","An auditory display tool for DNA sequence analysis","BMC Bioinformatics","","1471-2105","10.1186/s12859-017-1632-x","http://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-017-1632-x","Background: DNA Sonification refers to the use of an auditory display to convey the information content of DNA sequence data. Six sonification algorithms are presented that each produce an auditory display. These algorithms are logically designed from the simple through to the more complex. Three of these parse individual nucleotides, nucleotide pairs or codons into musical notes to give rise to 4, 16 or 64 notes, respectively. Codons may also be parsed degenerately into 20 notes with respect to the genetic code. Lastly nucleotide pairs can be parsed as two separate frames or codons can be parsed as three reading frames giving rise to multiple streams of audio. Results: The most informative sonification algorithm reads the DNA sequence as codons in three reading frames to produce three concurrent streams of audio in an auditory display. This approach is advantageous since start and stop codons in either frame have a direct affect to start or stop the audio in that frame, leaving the other frames unaffected. Using these methods, DNA sequences such as open reading frames or repetitive DNA sequences can be distinguished from one another. These sonification tools are available through a webpage interface in which an input DNA sequence can be processed in real time to produce an auditory display playable directly within the browser. The potential of this approach as an analytical tool is discussed with reference to auditory displays derived from test sequences including simple nucleotide sequences, repetitive DNA sequences and coding or non-coding genes. Conclusion: This study presents a proof-of-concept that some properties of a DNA sequence can be identified through sonification alone and argues for their inclusion within the toolkit of DNA sequence browsers as an adjunct to existing visual and analytical tools.","2017-12","2023-07-05 07:04:22","2023-07-21 07:35:47","2023-07-05 07:04:22","221","","1","18","","BMC Bioinformatics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/U7FL4KIX/Temple - 2017 - An auditory display tool for DNA sequence analysis.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JQI847XD","journalArticle","2021","Datta, Prerit; Namin, Akbar Siami; Jones, Keith S.; Hewett, Rattikorn","Warning users about cyber threats through sounds","SN Applied Sciences","","2523-3963, 2523-3971","10.1007/s42452-021-04703-4","https://link.springer.com/10.1007/s42452-021-04703-4","Abstract               This paper reports a formative evaluation of auditory representations of cyber security threat indicators and cues, referred to as sonifications, to warn users about cyber threats. Most Internet browsers provide visual cues and textual warnings to help users identify when they are at risk. Although these alarming mechanisms are very effective in informing users, there are certain situations and circumstances where these alarming techniques are unsuccessful in drawing the user’s attention: (1) security warnings and features (e.g., blocking out malicious Websites) might overwhelm a typical Internet user and thus the users may overlook or ignore visual and textual warnings and, as a result, they might be targeted, (2) these visual cues are inaccessible to certain users such as those with visual impairments. This work is motivated by our previous work of the use of sonification of security warnings to users who are visually impaired. To investigate the usefulness of sonification in general security settings, this work uses real Websites instead of simulated Web applications with sighted participants. The study targets sonification for three different types of security threats: (1) phishing, (2) malware downloading, and (3) form filling. The results show that on average 58% of the participants were able to correctly remember what the sonification conveyed. Additionally, about 73% of the participants were able to correctly identify the threat that the sonification represented while performing tasks using real Websites. Furthermore, the paper introduces “CyberWarner”, a sonification sandbox that can be installed on the Google Chrome browser to enable auditory representations of certain security threats and cues that are designed based on several URL heuristics.                                         Article highlights                                                                        It is feasible to develop sonified cyber security threat indicators that users intuitively understand with minimal experience and training.                                                           Users are more cautious about malicious activities in general. However, when navigating real Websites, they are less informed. This might be due to the appearance of the navigating Websites or the overwhelming issues when performing tasks.                                                           Participants’ qualitative responses indicate that even when they did not remember what the sonification conveyed, the sonification was able to capture the user’s attention and take safe actions in response.","2021-07","2023-07-05 07:04:22","2023-07-05 07:04:22","2023-07-05 07:04:22","714","","7","3","","SN Appl. Sci.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/6R8LP8ZW/Datta et al. - 2021 - Warning users about cyber threats through sounds.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HBG9IBUQ","bookSection","2015","Braund, Edward; Miranda, Eduardo","Music with Unconventional Computing: Towards a Step Sequencer from Plasmodium of Physarum Polycephalum","Evolutionary and Biologically Inspired Music, Sound, Art and Design","978-3-319-16497-7 978-3-319-16498-4","","","https://link.springer.com/10.1007/978-3-319-16498-4_2","The field of computer music has evolved in tandem with advances made in computer science. We are interested in how the developing field of unconventional computation may provide new pathways for music and related technologies. In this paper, we outline our initial work into harnessing the behaviour of the biological computing substrate Physarum polycephalum for a musical step sequencer. The plasmodium of Physarum polycephalum is an amorphous unicellular organism, which moves like a giant amoeba as it navigates its environment for food. Our research manipulates the organism’s route-efficient propagation characteristics in order to create a growth environment for musical/sound arrangement. We experiment with this device in two different scenarios: sample triggering and MIDI note triggering using sonification techniques.","2015","2023-07-05 07:04:22","2023-07-20 00:02:31","2023-07-05 07:04:22","15-26","","","9027","","","Music with Unconventional Computing","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-16498-4_2","","","","","","Johnson, Colin; Carballal, Adrian; Correia, João","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ACAXDZY8","journalArticle","2019","Ezquerro, L.; Simón, J. L.","Geomusic as a New Pedagogical and Outreach Resource: Interpreting Geoheritage with All the Senses","Geoheritage","","1867-2477, 1867-2485","10.1007/s12371-019-00364-3","http://link.springer.com/10.1007/s12371-019-00364-3","The scientific, rational approach to the knowledge of Earth can be complemented and enhanced with an emotional approach by means of arts. Sonification of sedimentary series, by converting distinct lithology, facies or geochemical parameters into notes, and bed thickness into duration of sounds, provides a new viewpoint on both their sequential features and the cultural meaning of geoheritage. A total of 14 musical compositions have been achieved according to that procedure, based on successions of diverse ages and sedimentary environments within the Iberian Peninsula. Some of these successions exhibit cyclic features that have been analyzed by a number of authors. Cyclostratigraphy shows how certain sedimentary patterns can reveal climatic oscillations related to periodic variations of Earth orbital cycles. Geomusic elaborated from sonification of such sedimentary cycles could be therefore linked with Music of the Spheres postulated by Pythagoras in ancient Greece. Its hidden message deals with asking for a New Culture of Earth, for a renewed, friendly relationship with our planet. Its applied development could extend to soundtracks of scientific documentaries, background music at museums or geoparks, or performances at outreach events, or as a motivating factor in Earth Sciences learning.","2019-09","2023-07-05 07:04:22","2023-07-20 00:07:51","2023-07-05 07:04:22","1187-1198","","3","11","","Geoheritage","Geomusic as a New Pedagogical and Outreach Resource","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7TTZ9UCZ","bookSection","2010","Worrall, David","Using Sound to Identify Correlations in Market Data","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_11","Despite intensive study, a comprehensive understanding of the structure of capital market trading data remains elusive. The one known application of audification to market price data reported in 1990 that it was difficult to interpret the results probably because the market does not resonate according to acoustic laws. This paper reports on some techniques for transforming the data so it does resonate; so audification can be used as a means of identifying autocorrelation in capital market trading data. Also reported are some experiments in which the data is sonified using a homomorphic modulation technique. The results obtained indicate that the technique may have a wider application to other similarly structured time-series data.","2010","2023-07-05 07:04:22","2023-07-19 11:39:55","2023-07-05 07:04:22","202-218","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_11","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5CU9DKV3","journalArticle","2014","Wu, Dan; Li, Chao-yi; Liu, Jie; Lu, Jing; Yao, De-zhong","Scale-free brain ensemble modulated by phase synchronization","Journal of Zhejiang University SCIENCE C","","1869-1951, 1869-196X","10.1631/jzus.C1400199","http://link.springer.com/10.1631/jzus.C1400199","To listen to brain activity as a piece of music, we proposed the scale-free brainwave music (SFBM) technology, which could translate the scalp electroencephalogram (EEG) into music notes according to the power law of both EEG and music. In the current study, this methodology was further extended to a musical ensemble of two channels. First, EEG data from two selected channels are translated into musical instrument digital interface (MIDI) sequences, where the EEG parameters modulate the pitch, duration, and volume of each musical note. The phase synchronization index of the two channels is computed by a Hilbert transform. Then the two MIDI sequences are integrated into a chorus according to the phase synchronization index. The EEG with a high synchronization index is represented by more consonant musical intervals, while the low index is expressed by inconsonant musical intervals. The brain ensemble derived from real EEG segments illustrates differences in harmony and pitch distribution during the eyes-closed and eyes-open states. Furthermore, the scale-free phenomena exist in the brainwave ensemble. Therefore, the scale-free brain ensemble modulated by phase synchronization is a new attempt to express the EEG through an auditory and musical way, and it can be used for EEG monitoring and bio-feedback.","2014-10","2023-07-05 07:04:22","2023-07-20 06:50:20","2023-07-05 07:04:22","821-831","","10","15","","J. Zhejiang Univ. - Sci. C","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F774G35J","journalArticle","2023","Esau-Held, Margarita; Marsh, Andrew; Krauß, Veronika; Stevens, Gunnar","“Foggy sounds like nothing” — enriching the experience of voice assistants with sonic overlays","Personal and Ubiquitous Computing","","1617-4909, 1617-4917","10.1007/s00779-023-01722-3","https://link.springer.com/10.1007/s00779-023-01722-3","Abstract                            Although Voice Assistants are ubiquitously available for some years now, the interaction is still monotonous and utilitarian. Sound design offers conceptual and methodological research to design auditive interfaces. Our work aims to complement and supplement voice interaction with               sonic overlays               to enrich the user experience. Therefore, we followed a user-centered design process to develop a sound library for weather forecasts based on empirical results from a user survey of associative mapping. After analyzing the data, we created audio clips for seven weather conditions and evaluated the perceived combination of sound and speech with 15 participants in an interview study. Our findings show that supplementing speech with soundscapes is a promising concept that communicates information and induces emotions with a positive affect for the user experience of Voice Assistants. Besides a novel design approach and a collection of sound overlays, we provide four design implications to support voice interaction designers.","2023-06-06","2023-07-05 07:04:22","2023-07-05 07:04:22","2023-07-05 07:04:22","","","","","","Pers Ubiquit Comput","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/VPVGXWHF/Esau-Held et al. - 2023 - “Foggy sounds like nothing” — enriching the experi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UT8X597G","journalArticle","2023","Alfaro-Contreras, María; Iñesta, José M.; Calvo-Zaragoza, Jorge","Optical music recognition for homophonic scores with neural networks and synthetic music generation","International Journal of Multimedia Information Retrieval","","2192-6611, 2192-662X","10.1007/s13735-023-00278-5","https://link.springer.com/10.1007/s13735-023-00278-5","Abstract                            The recognition of patterns that have a time dependency is common in areas like speech recognition or natural language processing. The equivalent situation in image analysis is present in tasks like text or video recognition. Recently, Convolutional Recurrent Neural Networks (CRNN) have been broadly applied to solve these tasks in an end-to-end fashion with successful performance. However, its application to Optical Music Recognition (OMR) is not so straightforward due to the presence of different elements sharing the same horizontal position, disrupting the linear flow of the timeline. In this paper, we study the ability of the state-of-the-art CRNN approach to learn codes that represent this disruption in homophonic scores. In our experiments, we study the lower bounds in the recognition task of real scores when the models are trained with synthetic data. Two relevant conclusions are drawn: (1) Our serialized ways of encoding the music content are appropriate for CRNN-based OMR; (2) the learning process is possible with synthetic data, but there exists a               glass ceiling               when recognizing real sheet music.","2023-06","2023-07-05 07:04:22","2023-07-05 07:04:22","2023-07-05 07:04:22","12","","1","12","","Int J Multimed Info Retr","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/5VZKX4QY/Alfaro-Contreras et al. - 2023 - Optical music recognition for homophonic scores wi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BN8ACU4I","bookSection","2021","Buongiorno Nardelli, Marco","MUSICNTWRK: Data Tools for Music Theory, Analysis and Composition","Perception, Representations, Image, Sound, Music","978-3-030-70209-0 978-3-030-70210-6","","","http://link.springer.com/10.1007/978-3-030-70210-6_14","We present the API for MUSICNTWRK, a python library for pitch class set and rhythmic sequences classification and manipulation, the generation of networks in generalized music and sound spaces, deep learning algorithms for timbre recognition, and the sonification of arbitrary data. The software is freely available under GPL 3.0 and can be downloaded at www.musicntwrk.com.","2021","2023-07-05 07:04:22","2023-07-21 04:44:57","2023-07-05 07:04:22","190-215","","","12631","","","MUSICNTWRK","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-70210-6_14","","/Users/minsik/Zotero/storage/ESYUANFS/Buongiorno Nardelli - 2021 - MUSICNTWRK Data Tools for Music Theory, Analysis .pdf","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Aramaki, Mitsuko","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BB8Y5T8G","bookSection","2013","Wu, Dan; Yao, Dezhong","Chorus from the Two Brain hemispheres with Chinese Pentatonic Scale","World Congress on Medical Physics and Biomedical Engineering May 26-31, 2012, Beijing, China","978-3-642-29304-7 978-3-642-29305-4","","","https://link.springer.com/10.1007/978-3-642-29305-4_114","To listen to brain activity as a piece of music, we proposed a scale-free brainwave music (SFBM) technology, which translates the scalp EEG into music notes according to the power law of both EEG and music. In this paper, the methodology was further extended to chorus music of two channels from the two hemispheres. First, EEG data from two channels symmetrically located on the left and right hemispheres are translated into MIDI sequences by SFBM, respectively, where the EEG parameters modulate the pitch, duration and volume of each music note. Then, the two sequences are filtered into a chorus of the Chinese pentatonic scale or the Western major scale. The resulted Chinese and western music of different sleep stages illustrate distinct differences in harmony, and the music with Chinese pentatonic scale sounds more harmonious.","2013","2023-07-05 07:04:22","2023-07-21 05:15:24","2023-07-05 07:04:22","430-433","","","39","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: IFMBE Proceedings DOI: 10.1007/978-3-642-29305-4_114","","","","","","Long, Mian","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9LGN5W9E","bookSection","2023","Riber, Adrián García; Serradilla, Francisco","AI-rmonies of the Spheres","Artificial Intelligence in Music, Sound, Art and Design","978-3-031-29955-1 978-3-031-29956-8","","","https://link.springer.com/10.1007/978-3-031-29956-8_9","Thanks to the efforts and cooperation of the international community, nowadays it is possible to analyze astronomical data captured by the observatories and telescopes of major space agencies around the world from a personal computer. The development of virtual observatory technology (VO), and the standardization of the formats it uses, allow professional and amateur astronomers to access astronomical data and images through internet with relative ease. Immersed in this environment of global accessibility, this article presents an astronomical data-driven unsupervised music composition system based on Deep Learning, aimed at offering an automatic and objective review on the classical topic of the Harmonies of the Spheres. The system explores the MILES stellar library from the Spanish Virtual Observatory (SVO) using a variational autoencoder architecture to cross-match its stellar spectra via Pitch-Class Set Theory with a music score generated by a LSTM with attention neural network in the style of late-renaissance music.","2023","2023-07-05 07:04:22","2023-07-19 11:34:11","2023-07-05 07:04:22","132-147","","","13988","","","","","","","","Springer Nature Switzerland","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-031-29956-8_9","","","","","","Johnson, Colin; Rodríguez-Fernández, Nereida; Rebelo, Sérgio M.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MCKXWSH9","journalArticle","2012","Sinclair, Peter","Living with alarms: the audio environment in an intensive care unit","AI & SOCIETY","","0951-5666, 1435-5655","10.1007/s00146-011-0344-4","http://link.springer.com/10.1007/s00146-011-0344-4","This article treats the use of sonification in Percy Military Training Hospital’s intensive care unit, through an interview with Anaesthetist Professor Bruno Debien. It starts with a description of the environment completed by some technical information concerning the equipment. This is followed by a commented transcription of the interview with Bruno Debien and concludes with reflections on the nature of audio alarms and their relation to different modes of listening.","2012-05","2023-07-05 07:04:22","2023-07-19 11:23:55","2023-07-05 07:04:22","269-276","","2","27","","AI & Soc","Living with alarms","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/D26YF9TF/Sinclair - 2012 - Living with alarms the audio environment in an in.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D464JRQI","bookSection","2023","Ocampo, Rodolfo; Andres, Josh; Schmidt, Adrian; Pegram, Caroline; Shave, Justin; Hill, Charlton; Wright, Brendan; Bown, Oliver","Using GPT-3 to Achieve Semantically Relevant Data Sonificiation for an Art Installation","Artificial Intelligence in Music, Sound, Art and Design","978-3-031-29955-1 978-3-031-29956-8","","","https://link.springer.com/10.1007/978-3-031-29956-8_14","Large Language Models such as GPT-3 exhibit generative language capabilities with multiple potential applications in creative practice. In this paper, we present a method for data sonification that employs the GPT-3 model to create semantically relevant mappings between artificial intelligence-generated natural language descriptions of data, and human-generated descriptions of sounds. We implemented this method in a public art installation to generate a soundscape based on data from different systems. While common sonification approaches rely on arbitrary mappings between data values and sonic values, our approach explores the use of language models to achieve a mapping not via values but via meaning. We find our approach is a useful tool for musification practice and demonstrates a new application of generative language models in creative new media arts practice. We show how different prompts influence data to sound mappings, and highlight that matching the embeddings of texts of different lengths produces undesired behavior.","2023","2023-07-05 07:04:22","2023-07-19 11:33:47","2023-07-05 07:04:22","212-227","","","13988","","","","","","","","Springer Nature Switzerland","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-031-29956-8_14","","","","","","Johnson, Colin; Rodríguez-Fernández, Nereida; Rebelo, Sérgio M.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HBA5MMQF","bookSection","2018","O’Brien, Benjamin; Juhas, Brett; Bieńkiewicz, Marta; Pruvost, Laurent; Buloup, Frank; Bringnoux, Lionel; Bourdin, Christophe","Considerations for Developing Sound in Golf Putting Experiments","Music Technology with Swing","978-3-030-01691-3 978-3-030-01692-0","","","http://link.springer.com/10.1007/978-3-030-01692-0_23","This chapter presents the core interests and challenges of using sound for learning motor skills and describes the development of sonification techniques for three separate golf-putting experiments. These studies are part of the ANR SoniMove project, which aims to develop new Human Machine Interfaces (HMI) that provide gestural control of sound in the areas of sports and music. After a brief introduction to sonification and sound-movement studies, the following addresses the ideas and sound synthesis techniques developed for each experiment.","2018","2023-07-05 07:04:22","2023-07-21 04:34:09","2023-07-05 07:04:22","338-358","","","11265","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-01692-0_23","","","","","","Aramaki, Mitsuko; Davies, Matthew E. P.; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WQ8YIKCI","journalArticle","2013","Wu, Dan; Li, Chao-Yi; Yao, De-Zhong","An ensemble with the chinese pentatonic scale using electroencephalogram from both hemispheres","Neuroscience Bulletin","","1673-7067, 1995-8218","10.1007/s12264-013-1334-y","http://link.springer.com/10.1007/s12264-013-1334-y","To listen to brain activity as a piece of music, we previously proposed scale-free brainwave music (SFBM) technology, which translated the scalp electroencephalogram (EEG) into musical notes according to the power law of both the EEG and music. In this study, the methodology was further extended to ensemble music on two channels from the two hemispheres. EEG data from two channels symmetrically located on the left and right hemispheres were translated into MIDI sequences by SFBM, and the EEG parameters modulated the pitch, duration and volume of each note. Then, the two sequences were filtered into an ensemble with two voices: the pentatonic scale (traditional Chinese music) or the heptatonic scale (standard Western music). We demonstrated differences in harmony between the two scales generated at different sleep stages, with the pentatonic scale being more harmonious. The harmony intervals of this brain ensemble at various sleep stages followed the power law. Compared with the heptatonic scale, it was easier to distinguish the different stages using the pentatonic scale. These results suggested that the hemispheric ensemble can represent brain activity by variations in pitch, tempo and harmony. The ensemble with the pentatonic scale sounds more consonant, and partially reflects the relations of the two hemispheres. This can be used to distinguish the different states of brain activity and provide a new perspective on EEG analysis.","2013-10","2023-07-05 07:06:20","2023-07-21 04:38:41","2023-07-05 07:06:20","581-587","","5","29","","Neurosci. Bull.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/YEZKCMMI/Wu et al. - 2013 - An ensemble with the chinese pentatonic scale usin.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UH7MIMDF","bookSection","2005","Beilharz, Kirsty","Responsive Sensate Environments: Past and Future Directions","Computer Aided Architectural Design Futures 2005","978-1-4020-3460-2","","","http://link.springer.com/10.1007/1-4020-3698-1_34","This paper looks at ways in which recent developments in sensing technologies and gestural control of data in 3D space provide opportunities to interact with information. Social and spatial data, the utilisation of space, flows of people and dense abstract data lend themselves to visual and auditory representation to enhance our understanding of socio-spatial patterns. Mapping information to visualisation and sonification leads to gestural interaction with information representation, dissolving the visibility and tangibility of traditional computational interfaces and hardware. The purpose of this integration of new technologies is to blur boundaries between computational and spatial interaction and to transform building spaces into responsive, intelligent interfaces for display and information access.","2005","2023-07-05 07:06:20","2023-07-19 23:47:41","2023-07-05 07:06:20","361-370","","","","","","Responsive Sensate Environments","","","","","Springer-Verlag","Berlin/Heidelberg","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/1-4020-3698-1_34","","","","","","Martens, Bob; Brown, Andre","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7MSM425N","bookSection","2012","Tulilaulu, Aurora; Paalasmaa, Joonas; Waris, Mikko; Toivonen, Hannu","Sleep Musicalization: Automatic Music Composition from Sleep Measurements","Advances in Intelligent Data Analysis XI","978-3-642-34155-7 978-3-642-34156-4","","","http://link.springer.com/10.1007/978-3-642-34156-4_36","We introduce data musicalization as a novel approach to aid analysis and understanding of sleep measurement data. Data musicalization is the process of automatically composing novel music, with given data used to guide the process. We present Sleep Musicalization, a methodology that reads a signal from state-of-the-art mattress sensor, uses highly non-trivial data analysis methods to measure sleep from the signal, and then composes music from the measurements. As a result, Sleep Musicalization produces music that reflects the user’s sleep during a night and complements visualizations of sleep measurements. The ultimate goal is to help users improve their sleep and well-being. For practical use and later evaluation of the methodology, we have built a public web service at http://sleepmusicalization.net for users of the sleep sensors.","2012","2023-07-05 07:06:20","2023-07-19 11:12:20","2023-07-05 07:06:20","392-403","","","7619","","","Sleep Musicalization","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-34156-4_36","","","","","","Hollmén, Jaakko; Klawonn, Frank; Tucker, Allan","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"79RZBEIW","journalArticle","2019","De Amicis, Raffaele; Riggio, Mariapaola; Shahbaz Badr, Arash; Fick, Jason; Sanchez, Christopher A.; Prather, Eric Andrew","Cross-reality environments in smart buildings to advance STEM cyberlearning","International Journal on Interactive Design and Manufacturing (IJIDeM)","","1955-2513, 1955-2505","10.1007/s12008-019-00546-x","http://link.springer.com/10.1007/s12008-019-00546-x","Real time data associated with the Building Information Model plays a critical role in the interpretation of the built environment, which is particularly relevant as an increasing number of education facilities and institutions promote sustainable engineering practices and monitoring data available to the public. However, it is challenging for non-technical audiences to fully comprehend or use information concealed in scientific data related to the performance of structures and materials. It is especially difficult for them to connect these concepts to physical contexts and phenomena. In this paper, we present how cross-reality paradigms in Architecture, Engineering, and Construction, coupled with multimodal representation techniques, enhance data literacy in both professionals and laypeople alike. In particular, we present the design of a learning environment where cutting-edge holographic interfaces and display technologies are combined with sonified and visual data to create a more immersive environment for data analysis and exploration, empowering users with situated data awareness and new ways of understanding real-time data.","2019-03","2023-07-05 07:06:20","2023-07-20 06:46:14","2023-07-05 07:06:20","331-348","","1","13","","Int J Interact Des Manuf","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VHIS7AN2","journalArticle","2022","Reh, Julia; Schmitz, Gerd; Hwang, Tong-Hun; Effenberg, Alfred O.","Loudness affects motion: asymmetric volume of auditory feedback results in asymmetric gait in healthy young adults","BMC Musculoskeletal Disorders","","1471-2474","10.1186/s12891-022-05503-6","https://bmcmusculoskeletdisord.biomedcentral.com/articles/10.1186/s12891-022-05503-6","Abstract                            Background               The potential of auditory feedback for motor learning in the rehabilitation of various diseases has become apparent in recent years. However, since the volume of auditory feedback has played a minor role so far and its influence has hardly been considered, we investigate the volume effect of auditory feedback on gait pattern and gait direction and its interaction with pitch.                                         Methods                                Thirty-two healthy young participants were randomly divided into two groups: Group 1 (                 n                  = 16) received a high pitch (150-250 Hz) auditory feedback; group 2 (                 n                  = 16) received a lower pitch (95-112 Hz) auditory feedback. The feedback consisted of a real-time sonification of the right and left foot ground contact. After an initial condition (no auditory feedback and full vision), both groups realized a 30-minute habituation period followed by a 30-minute asymmetry period. At any condition, the participants were asked to walk blindfolded and with auditory feedback towards a target at 15 m distance and were stopped 5 m before the target. Three different volume conditions were applied in random order during the habituation period: loud, normal, and quiet. In the subsequent asymmetry period, the three volume conditions baseline, right quiet and left quiet were applied in random order.                                                        Results               In the habituation phase, the step width from the loud to the quiet condition showed a significant interaction of volume*pitch with a decrease at high pitch (group 1) and an increase at lower pitch (group 2) (group 1: loud 1.02 ± 0.310, quiet 0.98 ± 0.301; group 2: loud 0.95 ± 0.229, quiet 1.11 ± 0.298). In the asymmetry period, a significantly increased ground contact time on the side with reduced volume could be found (right quiet: left foot 0.988 ± 0.033, right foot 1.003 ± 0.040, left quiet: left foot 1.004 ± 0.036, right foot 1.002 ± 0.033).                                         Conclusions               Our results suggest that modifying the volume of auditory feedback can be an effective way to improve gait symmetry. This could facilitate gait therapy and rehabilitation of hemiparetic and arthroplasty patients, in particular if gait improvement based on verbal corrections and conscious motor control is limited.","2022-12","2023-07-05 07:06:20","2023-07-05 07:06:20","2023-07-05 07:06:20","586","","1","23","","BMC Musculoskelet Disord","Loudness affects motion","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/BXW4RHS9/Reh et al. - 2022 - Loudness affects motion asymmetric volume of audi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"63YJ6T4F","bookSection","2005","Hepting, Daryl H.; Gerhard, David","Collaborative Computer-Aided Parameter Exploration for Music and Animation","Computer Music Modeling and Retrieval","978-3-540-24458-5 978-3-540-31807-1","","","http://link.springer.com/10.1007/978-3-540-31807-1_13","Although many artists have worked to create associations between music and animation, this has traditionally be done by developing one to suit the pre-existing other, as in visualization or sonification. The approach we employ in this work is to enable the simultaneous development of both music and sound from a common and rather generic central parameter variation, which may simply indicate a structure for periodic repetitions. This central parameter variation is then simultaneously mapped to appropriate musical and graphical variables by the musician and the animator, thereby contributing their own interpretations. The result of this mapping is then rendered in an intermediate form where music and animation are allowed to iteratively influence each other. The main piece of software in this development is the system which allows exploration of parameter mappings. The software interface allows both musician and animator to meaningfully experiment with the other’s mappings since the interface permits access in a common form, without requiring additional skills to interpret.","2005","2023-07-05 07:06:20","2023-07-19 23:48:23","2023-07-05 07:06:20","158-172","","","3310","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-31807-1_13","","/Users/minsik/Zotero/storage/YC8Q93MG/Hepting and Gerhard - 2005 - Collaborative Computer-Aided Parameter Exploration.pdf","","","","Wiil, Uffe Kock","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MVHTF4XH","journalArticle","2022","Hamilton-Fletcher, Giles; Alvarez, James; Obrist, Marianna; Ward, Jamie","SoundSight: a mobile sensory substitution device that sonifies colour, distance, and temperature","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-021-00376-w","https://link.springer.com/10.1007/s12193-021-00376-w","Abstract                            Depth, colour, and thermal images contain practical and actionable information for the blind. Conveying this information through alternative modalities such as audition creates new interaction possibilities for users as well as opportunities to study neuroplasticity. The ‘SoundSight’ App (               www.SoundSight.co.uk               ) is a smartphone platform that allows 3D position, colour, and thermal information to directly control thousands of high-quality sounds in real-time to create completely unique and responsive soundscapes for the user. Users can select the specific sensor input and style of auditory output, which can be based on anything—tones, rainfall, speech, instruments, or even full musical tracks. Appropriate default settings for image-sonification are given by designers, but users still have a fine degree of control over the timing and selection of these sounds. Through utilising smartphone technology with a novel approach to sonification, the SoundSight App provides a cheap, widely accessible, scalable, and flexible sensory tool. In this paper we discuss common problems encountered with assistive sensory tools reaching long-term adoption, how our device seeks to address these problems, its theoretical background, its technical implementation, and finally we showcase both initial user experiences and a range of use case scenarios for scientists, artists, and the blind community.","2022-03","2023-07-05 07:06:20","2023-07-05 07:06:20","2023-07-05 07:06:20","107-123","","1","16","","J Multimodal User Interfaces","SoundSight","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/EQ7F6NWH/Hamilton-Fletcher et al. - 2022 - SoundSight a mobile sensory substitution device t.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PIY2463J","bookSection","2005","Graham, M. Ian; Karahalios, Karrie","ChatAmp: Talking with Music and Text","Human-Computer Interaction - INTERACT 2005","978-3-540-28943-2 978-3-540-31722-7","","","http://link.springer.com/10.1007/11555261_84","Current systems for synchronous, text-based communication offer more varied interactions than e-mail, but cannot easily convey non-verbal or emotional information in an unobtrusive and intuitive manner. In this report we introduce ChatAmp, a new chat system which incorporates music as a central part of social interaction. Music is used in order to create an unobtrusive ambient soundscape that gives information about conversational activity and emotion using changes to instrument behavior. This soundscape acts as a peripheral channel to let a multitasking user monitor the conversation while focused elsewhere without being interrupted by jarring alert sounds. By combining this with non-sequential visualization which groups all of a user's activity in his area of the screen, ChatAmp provides ""at-a-glance"" information through both auditory and visual channels. Informal user tests support the effectiveness of integrating music and conversation in achieving the goals above and suggest directions for further research.","2005","2023-07-05 07:06:20","2023-07-20 05:55:29","2023-07-05 07:06:20","978-981","","","3585","","","ChatAmp","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11555261_84","","/Users/minsik/Zotero/storage/QJDG9UD3/Graham and Karahalios - 2005 - ChatAmp Talking with Music and Text.pdf","","","","Costabile, Maria Francesca; Paternò, Fabio","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GCYZISXM","bookSection","2019","Mannone, Maria","Have Fun with Math and Music!","Mathematics and Computation in Music","978-3-030-21391-6 978-3-030-21392-3","","","http://link.springer.com/10.1007/978-3-030-21392-3_33","If abstraction makes mathematics strong, it often makes it also hard to learn, if not discouraging. If math pedagogy suffers from the lack of engaging strategies, the pedagogy of mathematical music theory must deal with the additional difficulty of double fields and double vocabulary. However, games and interdisciplinary references in a STEAM framework can help the learner break down complex concepts into essential ideas, and gain interest and motivation to approach advanced topics. Here we present some general considerations, followed by two examples which may be applied in a high-school or early college level course. The first is a musical application of a Rubik’s cube, the CubeHarmonic, to approach group theory and combinatorics jointly with musical chords; the second is an application of category theory to investigate simple musical variations together with transformations on a visual shape.","2019","2023-07-05 07:06:20","2023-07-21 04:28:28","2023-07-05 07:06:20","379-382","","","11502","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-21392-3_33","","","","","","Montiel, Mariana; Gomez-Martin, Francisco; Agustín-Aquino, Octavio A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N5KTKV6Q","bookSection","2015","Escalante, Juan Manuel","The Sound Digestive System: A Strategy for Music and Sound Composition","Evolutionary and Biologically Inspired Music, Sound, Art and Design","978-3-319-16497-7 978-3-319-16498-4","","","https://link.springer.com/10.1007/978-3-319-16498-4_7","Sound Digestive System is an audio visual project that uses the digestive system processes into algorithmic sound composition. This project proposes different strategies to bring bio-data, translations and interpretations of living processes into the sound domain, thus generating an artistic result based on scientific data.","2015","2023-07-05 07:06:20","2023-07-20 00:04:20","2023-07-05 07:06:20","71-77","","","9027","","","The Sound Digestive System","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-16498-4_7","","","","","","Johnson, Colin; Carballal, Adrian; Correia, João","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"55LW796Z","journalArticle","2023","Jeanne, Rudy; Piton, Timothy; Minjoz, Séphora; Bassan, Nicolas; Le Chenechal, Morgan; Semblat, Antoine; Hot, Pascal; Kibleur, Astrid; Pellissier, Sonia","Gut-Brain Coupling and Multilevel Physiological Response to Biofeedback Relaxation After a Stressful Task Under Virtual Reality Immersion: A Pilot Study","Applied Psychophysiology and Biofeedback","","1090-0586, 1573-3270","10.1007/s10484-022-09566-y","https://link.springer.com/10.1007/s10484-022-09566-y","Human physiological reactions to the environment are coordinated by the interactions between brain and viscera. In particular, the brain, heart, and gastrointestinal tract coordinate with each other to provide physiological equilibrium by involving the central, autonomic, and enteric nervous systems. Recent studies have demonstrated an electrophysiological coupling between the gastrointestinal tract and the brain (gut-brain axis) under resting-state conditions. As the gut-brain axis plays a key role in individual stress regulation, we aimed to examine modulation of gut-brain coupling through the use of an overwhelming and a relaxing module as a first step toward modeling of the underlying mechanisms. This study was performed in 12 participants who, under a virtual reality environment, performed a 9-min cognitive stressful task followed by a 9-min period of relaxation. Brain activity was captured by electroencephalography, autonomic activities by photoplethysmography, and electrodermal and gastric activities by electrogastrography. Results showed that compared with the stressful task, relaxation induced a significant decrease in both tonic and phasic sympathetic activity, with an increase in brain alpha power and a decrease in delta power. The intensity of gut-brain coupling, as assessed by the modulation index of the phase-amplitude coupling between the normogastric slow waves and the brain alpha waves, decreased under the relaxation relative to the stress condition. These results highlight the modulatory effect of biofeedback relaxation on gut-brain coupling and suggest noninvasive multilevel electrophysiology as a promising way to investigate the mechanisms underlying gut-brain coupling in physiological and pathological situations.","2023-03","2023-07-05 07:06:20","2023-07-19 11:31:56","2023-07-05 07:06:20","109-125","","1","48","","Appl Psychophysiol Biofeedback","Gut-Brain Coupling and Multilevel Physiological Response to Biofeedback Relaxation After a Stressful Task Under Virtual Reality Immersion","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GA53KICV","journalArticle","2017","Vets, T.; Nijs, L.; Lesaffre, M.; Moens, B.; Bressan, F.; Colpaert, P.; Lambert, P.; Van De Walle, R.; Leman, M.","Gamified music improvisation with BilliArT: a multimodal installation with balls","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-016-0224-9","http://link.springer.com/10.1007/s12193-016-0224-9","This paper presents the concept and the realisation of an interactive multimedia installation, called BilliArT, together with an explorative user study conducted on the data gathered during a public exhibition of the installation. The study concerns functional properties of the installation (e.g. usability, design quality) and subjective qualities of the sonic and visual feedback of the installation. The installation consists in a collaborative environment based on the carambole billiards game, which allows the users to engage in a user-driven machine-based jazz-inspired music improvisation, augmented with visual feedback. The installation is designed to promote the interaction among the users and the billiard game, stimulating the motivation to engage in the game by balancing predictable and unpredictable output, and reinforcing the feeling of reward, irrespective of their level of musical training. BilliArT introduces a new framework for expressive interaction related to the concepts of motivation and reward. The exploratory study proved the ability of the installation to activate the users’ sense of aesthetic reward, leading to a more active and satisfactory engagement in the game. Future studies may exploit these results to the advantage of the world of the arts, as well as of studies in human-computer interaction, interface design, and cultural heritage preservation.","2017-03","2023-07-05 07:06:20","2023-07-20 07:05:17","2023-07-05 07:06:20","25-38","","1","11","","J Multimodal User Interfaces","Gamified music improvisation with BilliArT","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6Y5HXUTF","journalArticle","2013","Schmitz, Gerd; Mohammadi, Bahram; Hammer, Anke; Heldmann, Marcus; Samii, Amir; Münte, Thomas F; Effenberg, Alfred O","Observation of sonified movements engages a basal ganglia frontocortical network","BMC Neuroscience","","1471-2202","10.1186/1471-2202-14-32","https://bmcneurosci.biomedcentral.com/articles/10.1186/1471-2202-14-32","Abstract                            Background               Producing sounds by a musical instrument can lead to audiomotor coupling, i.e. the joint activation of the auditory and motor system, even when only one modality is probed. The sonification of otherwise mute movements by sounds based on kinematic parameters of the movement has been shown to improve motor performance and perception of movements.                                         Results               Here we demonstrate in a group of healthy young non-athletes that congruently (sounds match visual movement kinematics) vs. incongruently (no match) sonified breaststroke movements of a human avatar lead to better perceptual judgement of small differences in movement velocity. Moreover, functional magnetic resonance imaging revealed enhanced activity in superior and medial posterior temporal regions including the superior temporal sulcus, known as an important multisensory integration site, as well as the insula bilaterally and the precentral gyrus on the right side. Functional connectivity analysis revealed pronounced connectivity of the STS with the basal ganglia and thalamus as well as frontal motor regions for the congruent stimuli. This was not seen to the same extent for the incongruent stimuli.                                         Conclusions               We conclude that sonification of movements amplifies the activity of the human action observation system including subcortical structures of the motor loop. Sonification may thus be an important method to enhance training and therapy effects in sports science and neurological rehabilitation.","2013-12","2023-07-05 07:06:20","2023-07-05 07:06:20","2023-07-05 07:06:20","32","","1","14","","BMC Neurosci","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/VPNL4XSX/Schmitz et al. - 2013 - Observation of sonified movements engages a basal .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZM86MAVQ","bookSection","2015","Al-Rifaie, Asmaa Majid; Al-Rifaie, Mohammad Majid","Generative Music with Stochastic Diffusion Search","Evolutionary and Biologically Inspired Music, Sound, Art and Design","978-3-319-16497-7 978-3-319-16498-4","","","https://link.springer.com/10.1007/978-3-319-16498-4_1","This paper introduces an approach for using a swarm intelligence algorithm, Stochastic Diffusion Search (SDS) – inspired by one species of ants, Leptothorax acervorum – in order to generate music from plain text. In this approach , SDS is adapted in such a way to vocalise the agents, to hear their “chit-chat” . While the generated music depends on the input text, the algorithm’s search capability in locating the words in the input text is reflected in the duration and dynamic of the resulting musical notes. In other words, the generated music depends on the behaviour of the algorithm and the communication between its agents. This novel approach, while staying loyal to the original input text, when run each time, ‘vocalises’ the input text in varying ‘flavours’.","2015","2023-07-05 07:06:20","2023-07-20 00:02:19","2023-07-05 07:06:20","1-14","","","9027","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-16498-4_1","","/Users/minsik/Zotero/storage/NA4IGSFI/Al-Rifaie and Al-Rifaie - 2015 - Generative Music with Stochastic Diffusion Search.pdf","","","","Johnson, Colin; Carballal, Adrian; Correia, João","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I63M742Y","journalArticle","2022","De Prisco, Roberto; Zaccagnino, Rocco","Creative DNA computing: splicing systems for music composition","Soft Computing","","1432-7643, 1433-7479","10.1007/s00500-022-06828-z","https://link.springer.com/10.1007/s00500-022-06828-z","Abstract                            Splicing systems are a form of DNA computing as they mimic the recombination process among DNA molecules. This work discusses the use of splicing systems to build automatic tools for reproducing human beings’               creativity               , in the context of               automatic music composition               . More specifically, this work describes three general splicing system approaches for automatic music composition, and their application to two specific cases, namely composing 4-voice music and composing Jazz solos in a given style. Examples of music composed by the systems are presented.","2022-09","2023-07-05 07:10:08","2023-07-05 07:10:08","2023-07-05 07:10:08","9689-9706","","18","26","","Soft Comput","Creative DNA computing","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/7C4YAS62/De Prisco and Zaccagnino - 2022 - Creative DNA computing splicing systems for music.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8QK42WLT","journalArticle","2018","Gupta, Ashish; Bhushan, Braj; Behera, Laxmidhar","Short-term enhancement of cognitive functions and music: A three-channel model","Scientific Reports","","2045-2322","10.1038/s41598-018-33618-1","https://www.nature.com/articles/s41598-018-33618-1","Abstract             Short-term effects of music stimulus on enhancement of cognitive functions in human brain are documented, however the underlying neural mechanisms in these cognitive effects are not well investigated. In this study, we have attempted to decipher the mechanisms involved in alterations of neural networks that lead to enhanced cognitive effects post-exposure to music. We have investigated the changes in Electroencephalography (EEG) power and functional connectivity of alpha band in resting state of the brain after exposure to Indian classical music. We have quantified the changes in functional connectivity by phase coherence, phase delay, and phase slope index analyses. Spatial mapping of functional connectivity dynamics thus obtained, on brain networks revealed reduced information flow in long-distance connections between frontal and parietal cortex, and between other cortical regions underpinning intelligence. Analyses also showed increased power in the prefrontal and occipital cortex. With these findings, we have developed a stimulus-mechanism-end effect based neuro-cognitive model that explains the music induced cognitive enhancement by a three-channel framework - (1) enhanced global efficiency of brain, (2) enhanced local neural efficiency at the prefrontal lobe, and (3) increased sustained attention. Results signify that music directly affects the cognitive system and leads to improved brain efficiency through well-defined mechanisms.","2018-10-19","2023-07-05 07:10:08","2023-07-05 07:10:08","2023-07-05 07:10:08","15528","","1","8","","Sci Rep","Short-term enhancement of cognitive functions and music","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/US8GZGCQ/Gupta et al. - 2018 - Short-term enhancement of cognitive functions and .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WEE2I62G","bookSection","2007","DeWitt, Anna; Bresin, Roberto","Sound Design for Affective Interaction","Affective Computing and Intelligent Interaction","978-3-540-74888-5 978-3-540-74889-2","","","http://link.springer.com/10.1007/978-3-540-74889-2_46","Different design approaches contributed to what we see today as the prevalent design paradigm for Human Computer Interaction; though they have been mostly applied to the visual aspect of interaction. In this paper we presented a proposal for sound design strategies that can be used in applications involving affective interaction. For testing our approach we propose the sonification of the Affective Diary, a digital diary with focus on emotions, affects, and bodily experience of the user. We applied results from studies in music and emotion to sonic interaction design. This is one of the first attempts introducing different physics-based models for the real-time complete sonification of an interactive user interface in portable devices.","2007","2023-07-05 07:10:08","2023-07-19 11:13:13","2023-07-05 07:10:08","523-533","","","4738","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-74889-2_46","","","","","","Paiva, Ana C. R.; Prada, Rui; Picard, Rosalind W.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MDK4XEQA","bookSection","2018","Effenberg, Alfred O.; Hwang, Tong-Hun; Ghai, Shashank; Schmitz, Gerd","Auditory Modulation of Multisensory Representations","Music Technology with Swing","978-3-030-01691-3 978-3-030-01692-0","","","http://link.springer.com/10.1007/978-3-030-01692-0_20","Motor control and motor learning as well as interpersonal coordination are based on motor perception and emergent perceptuomotor representations. At least in early stages motor learning and interpersonal coordination are emerging heavily on visual information in terms of observing others and transforming the information into internal representations to guide owns behavior. With progressing learning, also other perceptual modalities are added when a new motor pattern is established by repeated physical exercises. In contrast to the vast majority of publications on motor learning and interpersonal coordination referring to a certain perceptual modality here we look at the perceptual system as a unitary system coordinating and unifying the information of all involved perceptual modalities. The relation between perceptual streams of different modalities, the intermodal processing and multisensory integration of information as a basis for motor control and learning will be the main focus of this contribution. Multi-/intermodal processing of perceptual streams results in multimodal representations and opens up new approaches to support motor learning and interpersonal coordination: Creating an additional perceptual stream adequately auditory movement information can be generated suitable to be integrated with information of other modalities and thereby modulating the resulting perceptuomotor representations without the need of attention and higher cognition. Here, the concept of a movement defined real-time acoustics is used to serve the auditory system in terms of an additional movement-auditory stream. Before the computational approach of kinematic real-time sonification is finally described, a special focus is directed to the level of adaptation modules of the internal models. Furthermore, this concept is compared with different approaches of additional acoustic movement information. Moreover, a perspective of this approach is given in a broad spectrum of new applications of supporting motor control and learning in sports and motor rehabilitation as well as a broad spectrum of joint action and interpersonal coordination between humans but also concerning human-robotinteraction.","2018","2023-07-05 07:10:08","2023-07-21 04:33:53","2023-07-05 07:10:08","284-311","","","11265","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-01692-0_20","","/Users/minsik/Zotero/storage/AKCCYUSI/Effenberg et al. - 2018 - Auditory Modulation of Multisensory Representation.pdf","","","","Aramaki, Mitsuko; Davies, Matthew E. P.; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LDP95RLH","bookSection","2017","Parseihian, Gaëtan; Bourdin, Christophe; Bréjard, Vincent; Kronland-Martinet, Richard","Increasing Pleasantness and Security Using 3D-Sound Design in Public Transport","Bridging People and Sound","978-3-319-67737-8 978-3-319-67738-5","","","http://link.springer.com/10.1007/978-3-319-67738-5_9","A collaborative project aiming to improve the bus trip with auditory information is presented in this paper. This project took place in Marseille and involved several laboratories from Aix-Marseille University and the Marseille transit operator. This project consists in the study of three fundamental actions of the sound on the bus’ passengers: designing sound announcement to inform passengers of the next stop in a playful and intuitive way, brightening up the route with spatialized soundscapes to increase the trips pleasantness, and using sound to alert passengers of emergency braking. For that purpose, a high quality multi channel sound spatialization system was integrated in the bus and a sonification software based on geolocation was designed. The overall concepts of this project are first presented, then the integration of the sound spatialization system and the implementation of the sonification software are described. Finally, an evaluation method of passengers satisfaction is discussed and first results of a laboratory experiment are presented.","2017","2023-07-05 07:10:08","2023-07-19 23:37:01","2023-07-05 07:10:08","150-168","","","10525","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-67738-5_9","","","","","","Aramaki, Mitsuko; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EX348NLM","bookSection","2023","Hoy, Rory; Van Nort, Doug","Networked and Collaborative Musical Play Amongst Humans and Virtual Biological Agents in Locus Diffuse","Music in the AI Era","978-3-031-35381-9 978-3-031-35382-6","","","https://link.springer.com/10.1007/978-3-031-35382-6_9","Locus Diffuse is a networked multi-user instrument populated by a simulated slime mold and four human players. Mimicking the biological behavior of slime mold and establishing a virtual living network between player nodes, the system sonifies interaction along these connections. Participants use a browser based interface to play the multi-user instrument, and access an accompanying stream for audio and visual output of the system. Player responses from various play sessions are reported, and discussed relative to the concept of sonic ecosystems. These responses demonstrate distinct frames of focus employed by participants in regard to human/machine and inter-human collaboration, including perceived interaction of sound sources and agent behavior, perceived interaction through personal connection to agents, and differing perceptions of an aural vs visual understanding of the system.","2023","2023-07-05 07:10:08","2023-07-21 04:33:43","2023-07-05 07:10:08","94-110","","","13770","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-031-35382-6_9","","","","","","Aramaki, Mitsuko; Hirata, Keiji; Kitahara, Tetsuro; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9VK6AAP6","bookSection","2015","Maureira, Marcello A. Gómez; Rombout, Lisa E.","The Vocal Range of Movies - Sonifying Gender Representation in Film","Entertainment Computing - ICEC 2015","978-3-319-24588-1 978-3-319-24589-8","","","http://link.springer.com/10.1007/978-3-319-24589-8_54","Research has shown that in contemporary movies, male characters consistently outnumber female characters. In recent years, the number of speaking roles identified as female has declined or remained stable. Guidelines like the Bechdel and Mako Mori test have emerged as a method of evaluating gender representation in film. In this study, a more abstract and experiential form of evaluation is proposed. The per-segment sonification of the assigned gender of a character and the amount of lines they have in that segment of the script creates an audio file, showcasing the gender-representation in the movie dynamically. Two focus groups, one specifically consisting of young filmmakers, have expressed their interest in this form of movie-sonification. Expressed wishes for additional features and other suggested improvements are taken into consideration for the creation of the next prototype.","2015","2023-07-05 07:10:08","2023-07-19 23:59:47","2023-07-05 07:10:08","545-550","","","9353","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-24589-8_54","","/Users/minsik/Zotero/storage/AUWVMGL3/Maureira and Rombout - 2015 - The Vocal Range of Movies - Sonifying Gender Repre.pdf","","","","Chorianopoulos, Konstantinos; Divitini, Monica; Baalsrud Hauge, Jannicke; Jaccheri, Letizia; Malaka, Rainer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5IZCUKH3","bookSection","2007","Raś, Zbigniew W.; Zhang, Xin; Lewis, Rory","MIRAI: Multi-hierarchical, FS-Tree Based Music Information Retrieval System","Rough Sets and Intelligent Systems Paradigms","978-3-540-73450-5 978-3-540-73451-2","","","http://link.springer.com/10.1007/978-3-540-73451-2_10","With the fast booming of online music repositories, there is a need for content-based automatic indexing which will help users to find their favorite music objects in real time. Recently, numerous successful approaches on musical data feature extraction and selection have been proposed for instrument recognition in monophonic sounds. Unfortunately, none of these methods can be successfully applied to polyphonic sounds. Identification of music instruments in polyphonic sounds is still difficult and challenging, especially when harmonic partials are overlapping with each other. This has stimulated the research on music sound separation and new features development for content-based automatic music information retrieval. Our goal is to build a cooperative query answering system (QAS), for a musical database, retrieving from it all objects satisfying queries like ”find all musical pieces in pentatonic scale with a viola and piano where viola is playing for minimum 20 seconds and piano for minimum 10 seconds”. We use the database of musical sounds, containing almost 4000 sounds taken from the MUMs (McGill University Master Samples), as a vehicle to construct several classifiers for automatic instrument recognition. Classifiers showing the best performance are adopted for automatic indexing of musical pieces by instruments. Our musical database has an FS-tree (Frame Segment Tree) structure representation. The cooperativeness of QAS is driven by several hierarchical structures used for classifying musical instruments.","2007","2023-07-05 07:10:08","2023-07-21 04:59:19","2023-07-05 07:10:08","80-89","","","4585","","","MIRAI","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73451-2_10","","","","","","Kryszkiewicz, Marzena; Peters, James F.; Rybinski, Henryk; Skowron, Andrzej","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XH9Q5TYL","journalArticle","2013","Sigrist, Roland; Rauter, Georg; Riener, Robert; Wolf, Peter","Augmented visual, auditory, haptic, and multimodal feedback in motor learning: A review","Psychonomic Bulletin & Review","","1069-9384, 1531-5320","10.3758/s13423-012-0333-8","http://link.springer.com/10.3758/s13423-012-0333-8","It is generally accepted that augmented feedback, provided by a human expert or a technical display, effectively enhances motor learning. However, discussion of the way to most effectively provide augmented feedback has been controversial. Related studies have focused primarily on simple or artificial tasks enhanced by visual feedback. Recently, technical advances have made it possible also to investigate more complex, realistic motor tasks and to implement not only visual, but also auditory, haptic, or multimodal augmented feedback. The aim of this review is to address the potential of augmented unimodal and multimodal feedback in the framework of motor learning theories. The review addresses the reasons for the different impacts of feedback strategies within or between the visual, auditory, and haptic modalities and the challenges that need to be overcome to provide appropriate feedback in these modalities, either in isolation or in combination. Accordingly, the design criteria for successful visual, auditory, haptic, and multimodal feedback are elaborated.","2013-02","2023-07-05 07:10:08","2023-07-21 04:56:17","2023-07-05 07:10:08","21-53","","1","20","","Psychon Bull Rev","Augmented visual, auditory, haptic, and multimodal feedback in motor learning","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/VJ57FFJZ/Sigrist et al. - 2013 - Augmented visual, auditory, haptic, and multimodal.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N53XRLFZ","bookSection","2014","Mikami, Tomoya; Takano, Kosuke","A Music Search System for Expressive Music Performance Learning","Human Interface and the Management of Information. Information and Knowledge in Applications and Services","978-3-319-07862-5 978-3-319-07863-2","","","http://link.springer.com/10.1007/978-3-319-07863-2_9","In this paper, we present a music search system that focuses on performance style to cultivate a pupil’s expressive performance of music. The system allows pupils to learn the performance style to be mastered by obtaining both model and non-model content. By browsing non-model content that is similar to the quality of a pupil’s performance, the pupil can quickly identify his/her areas that require improvement. In addition, the pupil can improve his/her performance skill by repeatedly imitating the models. We evaluate the capabilities of our music search system regarding the extraction of performance style from a classical music source and the precision of the music search results for performance style.","2014","2023-07-05 07:10:08","2023-07-20 05:53:34","2023-07-05 07:10:08","80-89","","","8522","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-07863-2_9","","","","","","Yamamoto, Sakae","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Kobsa, Alfred; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Doug; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MGQRMCHP","journalArticle","2012","Grond, Florian; Hermann, Thomas","Singing function: Exploring auditory graphs with a vowel based sonification","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0068-2","http://link.springer.com/10.1007/s12193-011-0068-2","We present in this paper SingingFunction, a vowel-based sonification strategy for mathematical functions. Within the research field of auditory graphs as representation of scalar functions, we focus in SingingFunction on important aspects of sound design, which allow to better distinguish function shapes as auditory gestalts. SingingFunction features the first vowel-based synthesis for function sonification, and allows for a seamless integration of higher derivatives of the function into a single sound stream. We present further the results of a psycho physical experiment, where we compare the effectiveness of function sonifications based on either mapping only f′(x), or including hierarchically further information about the first derivatives f′(x), or the second derivative f″(x). Further we look at interactivity as an important factor and report interesting effects across all 3 sonification methods by comparing interactive explorations versus simple playback of sonified functions. Finally, we discuss SingingFunction within the context of existing function sonifications, and possible evaluation methods.","2012-05","2023-07-05 07:10:08","2023-07-20 06:59:17","2023-07-05 07:10:08","87-95","","3-4","5","","J Multimodal User Interfaces","Singing function","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T7FVVRS8","journalArticle","2016","Metatla, Oussama; Martin, Fiore; Parkinson, Adam; Bryan-Kinns, Nick; Stockman, Tony; Tanaka, Atau","Audio-haptic interfaces for digital audio workstations: A participatory design approach","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-016-0217-8","http://link.springer.com/10.1007/s12193-016-0217-8","We examine how auditory displays, sonification and haptic interaction design can support visually impaired sound engineers, musicians and audio production specialists access to digital audio workstation. We describe a user-centred approach that incorporates various participatory design techniques to help make the design process accessible to this population of users. We also outline the audio-haptic designs that results from this process and reflect on the benefits and challenges that we encountered when applying these techniques in the context of designing support for audio editing.","2016-09","2023-07-05 07:10:08","2023-07-20 07:02:45","2023-07-05 07:10:08","247-258","","3","10","","J Multimodal User Interfaces","Audio-haptic interfaces for digital audio workstations","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/HZCF4N3X/Metatla et al. - 2016 - Audio-haptic interfaces for digital audio workstat.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VHL52WWM","journalArticle","2020","Temple, Mark D.","Real-time audio and visual display of the Coronavirus genome","BMC Bioinformatics","","1471-2105","10.1186/s12859-020-03760-7","https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-03760-7","Abstract                            Background               This paper describes a web based tool that uses a combination of sonification and an animated display to inquire into the SARS-CoV-2 genome. The audio data is generated in real time from a variety of RNA motifs that are known to be important in the functioning of RNA. Additionally, metadata relating to RNA translation and transcription has been used to shape the auditory and visual displays. Together these tools provide a unique approach to further understand the metabolism of the viral RNA genome. This audio provides a further means to represent the function of the RNA in addition to traditional written and visual approaches.                                         Results               Sonification of the SARS-CoV-2 genomic RNA sequence results in a complex auditory stream composed of up to 12 individual audio tracks. Each auditory motive is derived from the actual RNA sequence or from metadata. This approach has been used to represent transcription or translation of the viral RNA genome. The display highlights the real-time interaction of functional RNA elements. The sonification of codons derived from all three reading frames of the viral RNA sequence in combination with sonified metadata provide the framework for this display. Functional RNA motifs such as transcription regulatory sequences and stem loop regions have also been sonified. Using the tool, audio can be generated in real-time from either genomic or sub-genomic representations of the RNA. Given the large size of the viral genome, a collection of interactive buttons has been provided to navigate to regions of interest, such as cleavage regions in the polyprotein, untranslated regions or each gene. These tools are available through an internet browser and the user can interact with the data display in real time.                                         Conclusion               The auditory display in combination with real-time animation of the process of translation and transcription provide a unique insight into the large body of evidence describing the metabolism of the RNA genome. Furthermore, the tool has been used as an algorithmic based audio generator. These audio tracks can be listened to by the general community without reference to the visual display to encourage further inquiry into the science.","2020-12","2023-07-05 07:10:08","2023-07-05 07:10:08","2023-07-05 07:10:08","431","","1","21","","BMC Bioinformatics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/XH6RRUA5/Temple - 2020 - Real-time audio and visual display of the Coronavi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DZ43YGE7","journalArticle","2023","Samara, Mohammed; Deriche, Mohamed; Al-Sadah, Jihad; Osais, Yahya","Design and Implementation of a Real-Time Color Recognition System for the Visually Impaired","Arabian Journal for Science and Engineering","","2193-567X, 2191-4281","10.1007/s13369-022-07506-w","https://link.springer.com/10.1007/s13369-022-07506-w","There is a growing interest in developing Computer-Based Assistive Technology (CAT) systems able to help the Visually Impaired (VI) in their daily needs and integrate well within society. One aspect that has not been well addressed is helping the visually impaired is the identification of colors for daily activities. Color recognition and perception is very important in interacting with the society and the surrounding environment. This paper presents a proof-of-concept design of a real-time embedded system that can help the visually impaired recognize colors, interact, and take decisions based on their perception of colors. Our approach is based on conveying color information, from the full color space, using a unique set of synthesized sound signals. The hardware part of the system is a pen-like device, which can detect color and generate a language-independent auditory signal representing the HSV values of the identified color. Numerous experiments have been performed using the new system with both the visually impaired and some blindfolded (BLD) participants. The system was proven to be very efficient in relation to training time and leads to high accuracy in color detection, classification, and matching tests. These experiments confirmed that the developed sonification scheme is effective yet simple in achieving color perception for the visually impaired. The proof-of-concept achieves about 93% recognition accuracy using off-the-shelf components, it is cheap to implement, robust, and requires a much shorter time for training when compared to existing systems.","2023-05","2023-07-05 07:10:08","2023-07-19 11:32:22","2023-07-05 07:10:08","6783-6796","","5","48","","Arab J Sci Eng","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P5A6EN86","journalArticle","2020","Abboud, Ralph; Tekli, Joe","Integration of nonparametric fuzzy classification with an evolutionary-developmental framework to perform music sentiment-based analysis and composition","Soft Computing","","1432-7643, 1433-7479","10.1007/s00500-019-04503-4","http://link.springer.com/10.1007/s00500-019-04503-4","Over the past years, several approaches have been developed to create algorithmic music composers. Most existing solutions focus on composing music that appears theoretically correct or interesting to the listener. However, few methods have targeted sentiment-based music composition: generating music that expresses human emotions. The few existing methods are restricted in the spectrum of emotions they can express (usually to two dimensions: valence and arousal) as well as the level of sophistication of the music they compose (usually monophonic, following translation-based, predefined templates or heuristic textures). In this paper, we introduce a new algorithmic framework for autonomous music sentiment-based expression and composition, titled MUSEC, that perceives an extensible set of six primary human emotions (e.g., anger, fear, joy, love, sadness, and surprise) expressed by a MIDI musical file and then composes (creates) new polyphonic (pseudo) thematic, and diversified musical pieces that express these emotions. Unlike existing solutions, MUSEC is: (i) a hybrid crossover between supervised learning (SL, to learn sentiments from music) and evolutionary computation (for music composition, MC), where SL serves at the fitness function of MC to compose music that expresses target sentiments, (ii) extensible in the panel of emotions it can convey, producing pieces that reflect a target crisp sentiment (e.g., love) or a collection of fuzzy sentiments (e.g., 65% happy, 20% sad, and 15% angry), compared with crisp-only or two-dimensional (valence/arousal) sentiment models used in existing solutions, (iii) adopts the evolutionary-developmental model, using an extensive set of specially designed music-theoretic mutation operators (trille, staccato, repeat, compress, etc.), stochastically orchestrated to add atomic (individual chord-level) and thematic (chord pattern-level) variability to the composed polyphonic pieces, compared with traditional evolutionary solutions producing monophonic and non-thematic music. We conducted a large battery of tests to evaluate MUSEC’s effectiveness and efficiency in both sentiment analysis and composition. It was trained on a specially constructed set of 120 MIDI pieces, including 70 sentiment-annotated pieces: the first significant dataset of sentiment-labeled MIDI music made available online as a benchmark for future research in this area. Results are encouraging and highlight the potential of our approach in different application domains, ranging over music information retrieval, music composition, assistive music therapy, and emotional intelligence.","2020-07","2023-07-05 07:10:08","2023-07-21 05:00:26","2023-07-05 07:10:08","9875-9925","","13","24","","Soft Comput","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XWSSB7L5","journalArticle","2022","Yang, Tiancheng; Nazir, Shah","A comprehensive overview of AI-enabled music classification and its influence in games","Soft Computing","","1432-7643, 1433-7479","10.1007/s00500-022-06734-4","https://link.springer.com/10.1007/s00500-022-06734-4","With the development and advancement of information technology, artificial intelligence (AI) and machine learning (ML) are applied in every sector of life. Among these applications, music is one of them which has gained attention in the last couple of years. The music industry is revolutionized with AIbased innovative and intelligent techniques. It is very convenient for composers to compose music of high quality using these technologies. Artificial intelligence and Music (AIM) is one of the emerging fields used to generate and manage sounds for different media like the Internet, games, etc. Sounds in the games are very effective and can be made more attractive by implementing AI approaches. The quality of sounds in the game directly impacts the productivity and experience of the player. With computer-assisted technologies, the game designers can create sounds for different scenarios or situations like horror and suspense and provide gamer information. The practical and productive audio of a game can guide visually impaired people during other events in the game. For the better creation and composition of music, good quality of knowledge about musicology is essential. Due to AIM, there are a lot of intelligent and interactive tools available for the efficiency and effective learning of music. The learners can be provided with a very reliable and interactive environment based on artificial intelligence. The current study has considered presenting a detailed overview of the literature available in the area of research. The study has demonstrated literature analysis from various perspectives, which will become evidence for researchers to devise novel solutions in the field.","2022-08","2023-07-05 07:10:08","2023-07-21 05:00:40","2023-07-05 07:10:08","7679-7693","","16","26","","Soft Comput","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/8GPIYF78/Yang and Nazir - 2022 - A comprehensive overview of AI-enabled music class.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A2VVJLD2","journalArticle","2007","Ardito, C.; Costabile, M. F.; De Angeli, A.; Pittarello, F.","Navigation help in 3D worlds: some empirical evidences on use of sound","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-006-0060-0","http://link.springer.com/10.1007/s11042-006-0060-0","The concept of Interaction Locus (IL) has been introduced to help the users to orient, navigate, and identify relevant interaction areas in 3D Virtual Environments (VEs). The IL is a multimodal concept: it adds to the 3D visual scene parallel information channels that are perceived by other senses. In particular, the IL emphasizes the role of music as a navigation aid in a VE. This paper reports three user-evaluations of different IL enriched virtual worlds, and in particular of the role of the IL auditory component. Results suggest that audio in 3D plays not only an aesthetic role, which the users greatly appreciate, but also a functional role simplifying navigation and helping the users to recognise scenes in the environment. Such a functional role however is subordinated to a proper understanding of the link between music and virtual space. While these experiments refer to desktop virtual reality environments, their findings are general enough to inform the design of navigational tools for other segments of the mixed reality domain.","2007-05","2023-07-05 07:10:08","2023-07-21 04:31:37","2023-07-05 07:10:08","201-216","","2","33","","Multimed Tools Appl","Navigation help in 3D worlds","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JQS9AY9J","journalArticle","2019","Ghai, Shashank; Ghai, Ishan","Effects of (music-based) rhythmic auditory cueing training on gait and posture post-stroke: A systematic review & dose-response meta-analysis","Scientific Reports","","2045-2322","10.1038/s41598-019-38723-3","https://www.nature.com/articles/s41598-019-38723-3","Gait dysfunctions are common post-stroke. Rhythmic auditory cueing has been widely used in gait rehabilitation for movement disorders. However, a consensus regarding its influence on gait and postural recovery post-stroke is still warranted. A systematic review and meta-analysis was performed to analyze the effects of auditory cueing on gait and postural stability post-stroke. Nine academic databases were searched according to PRISMA guidelines. The eligibility criteria for the studies were a) studies were randomized controlled trials or controlled clinical trials published in English, German, Hindi, Punjabi or Korean languages b) studies evaluated the effects of auditory cueing on spatiotemporal gait and/or postural stability parameters post-stroke c) studies scored ≥4 points on the PEDro scale. Out of 1,471 records, 38 studies involving 968 patients were included in this present review. The review and meta-analyses revealed beneficial effects of training with auditory cueing on gait and postural stability. A training dosage of 20–45 minutes session, for 3–5 times a week enhanced gait performance, dynamic postural stability i.e. velocity (Hedge’s g: 0.73), stride length (0.58), cadence (0.75) and timed-up and go test (−0.76). This review strongly recommends the incorporation of rhythmic auditory cueing based training in gait and postural rehabilitation, post-stroke.","2019-02-18","2023-07-05 07:10:23","2023-07-05 07:10:23","2023-07-05 07:10:23","2183","","1","9","","Sci Rep","Effects of (music-based) rhythmic auditory cueing training on gait and posture post-stroke","","","","","","","en","2019 The Author(s)","","","","www.nature.com","","Number: 1 Publisher: Nature Publishing Group","","/Users/minsik/Zotero/storage/C3CKX3LW/Ghai and Ghai - 2019 - Effects of (music-based) rhythmic auditory cueing .pdf","","","Rehabilitation; Stroke","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"64V9BRGX","bookSection","2014","Sterkenburg, Jason; Jeon, Myounghoon; Plummer, Christopher","Auditory Emoticons: Iterative Design and Acoustic Characteristics of Emotional Auditory Icons and Earcons","Human-Computer Interaction. Advanced Interaction Modalities and Techniques","978-3-319-07229-6 978-3-319-07230-2","","","http://link.springer.com/10.1007/978-3-319-07230-2_60","In recent decades there has been an increased interest in sonification research. Two commonly used sonification techniques, auditory icons and earcons, have been the subject of a lot of study. However, despite this there has been relatively little research investigating the relationship between these sonification techniques and emotions and affect. Additionally, despite their popularity, auditory icons and earcons are often treated separately and are rarely compared directly in studies. The current paper shows iterative design procedures to create emotional auditory icons and earcons. The ultimate goal of the study is to compare auditory icons and earcons in their ability to represent emotional states. The results show that there are some strong user preferences both within sonification categories and between sonfication categories. The implications and extensions of this work are discussed.","2014","2023-07-05 07:11:51","2023-07-20 06:30:48","2023-07-05 07:11:51","633-640","","","8511","","","Auditory Emoticons","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-07230-2_60","","","","","","Kurosu, Masaaki","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Kobsa, Alfred; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Doug; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JF5HR37I","bookSection","2010","Sobue, Shin-ichi; Araki, Hiroshi; Tazawa, Seiichi; Noda, Hirotomo; Kamiya, Izumi; Yamamoto, Aya; Fujita, Takeo; Higashiizumi, Ichiro; Okumura, Hayato","An Application of Lunar GIS with Visualized and Auditory Japan’s Lunar Explorer “Kaguya” Data","Advanced Techniques in Computing Sciences and Software Engineering","978-90-481-3659-9 978-90-481-3660-5","","","https://link.springer.com/10.1007/978-90-481-3660-5_27","This paper describes an application of a geographical information system with visualized and sonification lunar remote sensing data provided by Japan’s lunar explorer (SELENE “KAGUYA”). Web based GIS is a very powerful tool which lunar scientists can use to visualize and access remote sensing data with other geospatial information. We discuss enhancement of the pseudo-colored visual map presentation of lunar topographical altimetry data derived from LALT and the map of the data to several sound parameters (Interval, harmony, and tempo). This paper describes an overview of this GIS with a sonification system, called “Moonbell”.","2010","2023-07-05 07:11:51","2023-07-19 11:09:41","2023-07-05 07:11:51","159-163","","","","","","","","","","","Springer Netherlands","Dordrecht","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-90-481-3660-5_27","","","","","","Elleithy, Khaled","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AZ7X7EPY","journalArticle","2020","Groß-Vogt, Katharina; Frank, Matthias; Höldrich, Robert","Focused Audification and the optimization of its parameters","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-019-00317-8","http://link.springer.com/10.1007/s12193-019-00317-8","Abstract             We present a sonification method which we call Focused Audification (FA; previously: Augmented Audification) that allows to expand pure audification in a flexible way. It is based on a combination of single-side-band modulation and a pitch modulation of the original data stream. Based on two free parameters, the sonification’s frequency range is adjustable to the human hearing range and allows to interactively zoom into the data set at any scale. The parameters have been adjusted in a multimodal experiment on cardiac data by laypeople. Following from these results we suggest a procedure for parameter optimization to achieve an optimal listening range for any data set, adjusted to human speech.","2020-06","2023-07-05 07:11:51","2023-07-05 07:11:51","2023-07-05 07:11:51","187-198","","2","14","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/W2UIQ4QV/Groß-Vogt et al. - 2020 - Focused Audification and the optimization of its p.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8LEFTREU","bookSection","2014","Taibbi, Marzia; Bernareggi, Cristian; Gerino, Andrea; Ahmetovic, Dragan; Mascetti, Sergio","AudioFunctions: Eyes-Free Exploration of Mathematical Functions on Tablets","Computers Helping People with Special Needs","978-3-319-08595-1 978-3-319-08596-8","","","http://link.springer.com/10.1007/978-3-319-08596-8_84","It is well known that mathematics presents a number of hindrances to visually impaired students. In case of function graphs, for example, several assistive solutions have been proposed to enhance their accessibility. Unfortunately, both hardware tools (e.g., tactile paper) and existing software applications cannot guarantee, at the same time, a clear understanding of the graph and a full autonomous study. In this paper we present AudioFunctions, an iPad app that adopts three sonification techniques to convey information about the function graph. Our experimental evaluation, conducted with 7 blind people, clearly highlights that, by using AudioFunctions, students have a better understanding of the graph than with tactile paper and existing software solutions.","2014","2023-07-05 07:11:51","2023-07-19 23:53:17","2023-07-05 07:11:51","537-544","","","8547","","","AudioFunctions","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-08596-8_84","","","","","","Miesenberger, Klaus; Fels, Deborah; Archambault, Dominique; Peňáz, Petr; Zagler, Wolfgang","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2SN3MXP7","bookSection","2005","Cahen, Roland; Rodet, Xavier; Lambert, Jean-Philippe","Sound Navigation in PHASE Installation: Producing Music as Performing a Game Using Haptic Feedback","Virtual Storytelling. Using Virtual Reality Technologies for Storytelling","978-3-540-30511-8 978-3-540-32285-6","","","http://link.springer.com/10.1007/11590361_5","Sound Navigation consists in browsing through different sound objects and sound generators situated within a virtual world including virtual spatialized sound and visual scenes, to perform a musical trajectory and composition. In the PHASE Project installation, the 3D virtual world resembles the surface of a vinyl disk, magnified so that one can see the groove and move a “needle” (the “reading head”) in it and out of it to read the disk. Another such “needle” (the “writing head”) can “write” music in the groove. A part of the game is a pursuit between the writing head and the reading head handled by the player. Different musical devices have been implemented. Most of them have a haptic behavior. The scenario is fully related to the musical metaphor and aims to give equivalent pleasure to contemplative as well as to competitive players.","2005","2023-07-05 07:11:51","2023-07-21 05:15:07","2023-07-05 07:11:51","41-50","","","3805","","","Sound Navigation in PHASE Installation","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11590361_5","","","","","","Subsol, Gérard","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8KMT8ZD7","bookSection","2008","Young, Michael","NN Music: Improvising with a ‘Living’ Computer","Computer Music Modeling and Retrieval. Sense of Sounds","978-3-540-85034-2 978-3-540-85035-9","","","http://link.springer.com/10.1007/978-3-540-85035-9_23","This paper proposes attributes of a living computer music, the product of a live algorithm. It illustrates how these attributes can inform creative design with reference to a real-time system for solo performer-machine collaboration, Neural Network Music, and the PQƒ framework proposed for live algorithms. Improvisation is treated as a classification problem at a high level of musical behaviour which can be measured statistically and train a multilayer perceptron neural network. Network outputs shape a stochastic-based synthesis engine. Mappings are covertly assigned, revisited by both player and machine as a performance develops. As the timing and choice of mapping is unknown, both participants are invited to learn and adapt to a responsive sonic environment which is created afresh on each performance. This offers a novel real-time application of feed-forward neural networks and a challenging, creative technological platform for freely improvised music.","2008","2023-07-05 07:11:51","2023-07-19 23:50:06","2023-07-05 07:11:51","337-350","","","4969","","","NN Music","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-85035-9_23","","/Users/minsik/Zotero/storage/SNLRSZA3/Young - 2008 - NN Music Improvising with a ‘Living’ Computer.pdf","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T87CT5DS","bookSection","2008","Tanaka, Atau","Visceral Mobile Music Systems","Transdisciplinary Digital Art. Sound, Vision and the New Screen","978-3-540-79485-1 978-3-540-79486-8","","","http://link.springer.com/10.1007/978-3-540-79486-8_15","This paper describes a second-generation mobile music system that adds qualities of physical interaction to previous participative, networked, multi-user systems. We call upon traditions in interactive sensor music instrument building to inform this process. The resulting system underscores its dual personal/community context awareness with a technique of hybrid audio display. This allows the system to exhibit qualities of reflexive social translucence providing a view of the group all while giving each member of the group a responsive sense of agency. The visceral mobile music system was tested in a theatre environment with manageable location tracking and creative non-musical test subjects. The combination of musical practice and interaction design establish artistic creativity as an important component of the research process.","2008","2023-07-05 07:11:51","2023-07-21 05:07:39","2023-07-05 07:11:51","155-170","","","7","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Communications in Computer and Information Science DOI: 10.1007/978-3-540-79486-8_15","","","","","","Adams, Randy; Gibson, Steve; Arisona, Stefan Müller","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U2RIZDDA","bookSection","2016","Braund, Edward; Miranda, Eduardo R.","BioComputer Music: Generating Musical Responses with Physarum polycephalum-Based Memristors","Music, Mind, and Embodiment","978-3-319-46281-3 978-3-319-46282-0","","","http://link.springer.com/10.1007/978-3-319-46282-0_26","This paper introduces BioComputer Music, an experimental one piano duet between pianist and plasmodial slime mould Physarum polycephalum. This piece harnesses a system we have been developing, which we call BioComputer. BioComputer consists of an analogue circuit that encompasses components grown from the biological computing substrate Physarum polycephalum. Our system listens to the pianist and uses the memristive characteristics of Physarum polycephalum to generate a musical response that it plays through electromagnets placed on the strings of the piano. Such electromagnets set the strings into vibration, producing a distinctive timbre. Physarum polycephalum is an amorphous unicellular organism that has been discovered to exhibit memristive qualities. The memristor changes its resistance according to the amount of charge that has previously flown through. In this paper, we introduce the general concepts, technology and musical composition behind the BioComputer Music piece. We also discuss our rationale for using Physarum polycephalum .","2016","2023-07-05 07:11:51","2023-07-21 04:35:10","2023-07-05 07:11:51","405-419","","","9617","","","BioComputer Music","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-46282-0_26","","/Users/minsik/Zotero/storage/B7URZW7I/Braund and Miranda - 2016 - BioComputer Music Generating Musical Responses wi.pdf","","","","Kronland-Martinet, Richard; Aramaki, Mitsuko; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y9Q2AV7Y","bookSection","2013","Funk, Mathias; Hengeveld, Bart; Frens, Joep; Rauterberg, Matthias","Aesthetics and Design for Group Music Improvisation","Distributed, Ambient, and Pervasive Interactions","978-3-642-39350-1 978-3-642-39351-8","","","http://link.springer.com/10.1007/978-3-642-39351-8_40","Performing music as a group—improvised or from sheet music—is an intensive and immersive interaction activity that bears its own aesthetics. Players in such a setting are usually skilled in playing an instrument up to the level where they do not need to focus on the “operation” of the instrument, but can instead focus on higher-level feedback loops, e.g., between players in their section or the entire group. Novel technology can capitalize on these higherlevel feedback loops through the creation of interactive musical instruments that stimulate playing in groups (collaborative music rather than parallel music). However, making this experience accessible to fresh or novice players involves two challenges: how to design (1) musical instruments for such a setting and experience, and (2) instrument support that extends the interaction between players to their instruments. This allows to interact not only via their instrument with other human players, but directly with other instruments, producing a much richer and more intertwined musical experience. The paper shows results from a class of design students and reports on the lessons learned.","2013","2023-07-05 07:11:51","2023-07-19 23:57:50","2023-07-05 07:11:51","368-377","","","8028","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-39351-8_40","","/Users/minsik/Zotero/storage/XINN6B9Z/Funk et al. - 2013 - Aesthetics and Design for Group Music Improvisatio.pdf","","","","Streitz, Norbert; Stephanidis, Constantine","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AANSA4NP","journalArticle","2017","Raglio, Alfredo; Zaliani, Alberto; Baiardi, Paola; Bossi, Daniela; Sguazzin, Cinzia; Capodaglio, Edda; Imbriani, Chiara; Gontero, Giulia; Imbriani, Marcello","Active music therapy approach for stroke patients in the post-acute rehabilitation","Neurological Sciences","","1590-1874, 1590-3478","10.1007/s10072-017-2827-7","http://link.springer.com/10.1007/s10072-017-2827-7","Guidelines in stroke rehabilitation recommend the use of a multidisciplinary approach. Different approaches and techniques with music are used in the stroke rehabilitation to improve motor and cognitive functions but also psychological outcomes. In this randomized controlled pilot trial, relational active music therapy approaches were tested in the post-acute phase of disease. Thirty-eight hospitalized patients with ischemic and hemorrhagic stroke were recruited and allocated in two groups. The experimental group underwent the standard of care (physiotherapy and occupational therapy daily sessions) and relational active music therapy treatments. The control group underwent the standard of care only. Motor functions and psychological aspects were assessed before and after treatments. Music therapy process was also evaluated using a specific rating scale. All groups showed a positive trend in quality of life, functional and disability levels, and gross mobility. The experimental group showed a decrease of anxiety and, in particular, of depression (p = 0.016). In addition, the strength of non-dominant hand (grip) significantly increased in the experimental group (p = 0.041). Music therapy assessment showed a significant improvement over time of non-verbal and sonorous-music relationships. Future studies, including a greater number of patients and follow-up evaluations, are needed to confirm promising results of this study.","2017-05","2023-07-05 07:11:51","2023-07-21 04:37:34","2023-07-05 07:11:51","893-897","","5","38","","Neurol Sci","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W9LUS8N2","journalArticle","2020","Wilson, Rebekah","Aesthetic and technical strategies for networked music performance","AI & SOCIETY","","0951-5666, 1435-5655","10.1007/s00146-020-01099-4","http://link.springer.com/10.1007/s00146-020-01099-4","Networked music is no longer a future genre: the global quarantine event of 2020 launched the concept of performing together over the Internet into the mainstream. While the demand for performing at a distance may be a new imperative, musicians find themselves faced with technological and performative processes that do not appear to be suitable for performing music together online, due particularly to network latency which disrupts the ability for musicians to synchronize. The research presented in this paper investigates and challenges the reasons why networked music is not readily embraced by musicians and describes how that might change, by way of interviews with practitioners and an in-depth review of the technical constraints. Limitations that might cause frustration are in fact shown to have creative strategies that give rise to aesthetic approaches, distinct to the platform. By exploiting the constraints, in tandem with developing technology designed specifically for remote performance, aesthetic implications arise that not only accommodate the inconveniences of latency and acoustic feedback but can help us adapt and transform how we engage in real-time online, towards a future where we can imagine performing together over even more dramatic distances such as high-latency, low-bandwidth locations outside of urban areas—or even over galactic distances.","2020-11-18","2023-07-05 07:11:51","2023-07-19 11:28:29","2023-07-05 07:11:51","","","","","","AI & Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/KCQU4KW8/Wilson - 2020 - Aesthetic and technical strategies for networked m.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SM5CGKG8","bookSection","2005","Arifi, Vlora; Clausen, Michael; Kurth, Frank; Müller, Meinard","Score-PCM Music Synchronization Based on Extracted Score Parameters","Computer Music Modeling and Retrieval","978-3-540-24458-5 978-3-540-31807-1","","","http://link.springer.com/10.1007/978-3-540-31807-1_15","In this paper we present algorithms for the automatic time-synchronization of score-, MIDI- or PCM-data streams which represent the same polyphonic piano piece. In contrast to related approaches, we compute the actual alignment by using note parameters such as onset times and pitches. Working in a score-like domain has advantages in view of the efficiency and accuracy: due to the expressiveness of score-like parameters only a small number of such features is sufficient to solve the synchronization task. To obtain a score-like representation from the waveform-based PCM-data streams we use a preprocessing step to extract note parameters. In this we use the concept of novelty curves for onset detection and multirate filter banks in combination with note templates for pitch extraction. Also the data streams in MIDI- and score-format have to be suitably preprocessed. In particular, we suggest a data format which handles possible ambiguities such as trills or arpeggios by introducing the concept of fuzzy-notes. Further decisive ingredients of our approach are carefully designed cost functions in combination with an appropriate notion of alignment which is more flexible than the classical DTW concept. Our synchronization algorithms have been tested on a variety of classical polyphonic piano pieces recorded on MIDI- and standard acoustic pianos or taken from CD-recordings.","2005","2023-07-05 07:11:51","2023-07-19 23:48:04","2023-07-05 07:11:51","193-210","","","3310","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-31807-1_15","","","","","","Wiil, Uffe Kock","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SEZWGCYV","bookSection","2008","Amatriain, Xavier; Castellanos, Jorge; Höllerer, Tobias; Kuchera-Morin, JoAnn; Pope, Stephen T.; Wakefield, Graham; Wolcott, Will","Experiencing Audio and Music in a Fully Immersive Environment","Computer Music Modeling and Retrieval. Sense of Sounds","978-3-540-85034-2 978-3-540-85035-9","","","http://link.springer.com/10.1007/978-3-540-85035-9_27","The UCSB Allosphere is a 3-story-high spherical instrument in which virtual environments and performances can be experienced in full immersion. The space is now being equipped with high-resolution active stereo projectors, a 3D sound system with several hundred speakers, and with tracking and interaction mechanisms. The Allosphere is at the same time multimodal, multimedia, multi-user, immersive,andinteractive. This novel and unique instrument will be used for research into scientific visualization/auralization and data exploration, and as a research environment for behavioral and cognitive scientists. It will also serve as a research and performance space for artists exploring new forms of art. In particular, the Allosphere has been carefully designed to allow for immersive music and aural applications. In this paper, we give an overview of the instrument, focusing on the audio subsystem. We give the rationale behind some of the design decisions and explain the different techniques employed in making the Allosphere a truly generalpurpose immersive audiovisual lab and stage. Finally, we present first results and our experiences in developing and using the Allosphere in several prototype projects.","2008","2023-07-05 07:11:51","2023-07-19 23:48:58","2023-07-05 07:11:51","380-400","","","4969","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-85035-9_27","","/Users/minsik/Zotero/storage/CZCTJIB6/Amatriain et al. - 2008 - Experiencing Audio and Music in a Fully Immersive .pdf","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HVW5KR3M","bookSection","2023","Partesotti, Elena; Hebling, Eduardo D.; Moroni, Artemis S.; Antunes, Micael; Da Silva, César P.; Dezotti, Cássio G.; Manzolli, Jônatas","Analysis of Affective Behavior in the Artistic Installation Moviescape","ArtsIT, Interactivity and Game Creation","978-3-031-28992-7 978-3-031-28993-4","","","https://link.springer.com/10.1007/978-3-031-28993-4_23","The purpose of this paper is to study the correlation between attention and affective response when a user interacts with the artistic installation MovieScape - an Extended Immersive Digital Music Instrument. We analyzed the affective modulation in ten subjects. For that, we applied Affective Slider and audio-segmentation on the recorded performances of a Circumplex Space (a 2D Circumplex Model of Affect) to apply quantitative metrics and divide the interaction in Explorative and Contemplative states. In our paper we propose this as a unifying conceptual framework for analysing affective behavior within interactive installation such as MS. Overall, the analyses present an increase of Arousal after the interaction of the subjects within the environment, and a correlation between the Explorative interaction and the increment of Arousal which confirms the improvement in attention. Eventually, an inductive mechanism of expectancy occurred. These conclusions open up for potential in designing EIDMIs for therapeutic and pedagogical purposes. Also, these findings may also prove that subjects reached Creative Empowerment while becoming aware of MovieScape.","2023","2023-07-05 07:11:51","2023-07-19 11:35:32","2023-07-05 07:11:51","327-345","","","479","","","","","","","","Springer Nature Switzerland","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering DOI: 10.1007/978-3-031-28993-4_23","","","","","","Brooks, Anthony L.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6IBHBW7F","journalArticle","2011","Yuan, Bei; Folmer, Eelke; Harris, Frederick C.","Game accessibility: a survey","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-010-0189-5","http://link.springer.com/10.1007/s10209-010-0189-5","Over the last three decades, video games have evolved from a pastime into a force of change that is transforming the way people perceive, learn about, and interact with the world around them. In addition to entertainment, games are increasingly used for other purposes such as education or health. Despite this increased interest, a significant number of people encounter barriers when playing games due to a disability. Accessibility problems may include the following: (1) not being able to receive feedback; (2) not being able to determine in-game responses; (3) not being able to provide input using conventional input devices. This paper surveys the current state-of-the-art in research and practice in the accessibility of video games and points out relevant areas for future research. A generalized game interaction model shows how a disability affects ones ability to play games. Estimates are provided on the total number of people in the United States whose ability to play games is affected by a disability. A large number of accessible games are surveyed for different types of impairments, across several game genres, from which a number of high- and low-level accessibility strategies are distilled for game developers to inform their design.","2011-03","2023-07-05 07:11:51","2023-07-21 05:13:08","2023-07-05 07:11:51","81-100","","1","10","","Univ Access Inf Soc","Game accessibility","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WASBZ2GF","bookSection","2001","Van Scoy, Frances L.; Kawai, Takamitsu; Darrah, Marjorie; Rash, Connie","Haptic display of mathematical functions for teaching mathematics to students with vision disabilities: design and proof of concept","Haptic Human-Computer Interaction","978-3-540-42356-0 978-3-540-44589-0","","","http://link.springer.com/10.1007/3-540-44589-7_4","The design and initial implementation of a system for constructing a haptic model of a mathematical function for exploration using a PHANToM are described. A user types the mathematical function as a Fortran arithmetic expression and the system described here carves the trace of the function onto a virtual block of balsa wood. Preliminary work in generating music which describes the function has begun.","2001","2023-07-05 07:11:51","2023-07-20 00:16:57","2023-07-05 07:11:51","31-40","","","2058","","","Haptic display of mathematical functions for teaching mathematics to students with vision disabilities","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-44589-7_4","","","","","","Brewster, Stephen; Murray-Smith, Roderick","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZMIEFRNM","bookSection","2018","Manzolli, Jonatas; Moroni, Artemis; Valarini, Guilherme A.","SELFHOOD: An Evolutionary and Interactive Experience Synthesizing Images and Sounds","Music Technology with Swing","978-3-030-01691-3 978-3-030-01692-0","","","http://link.springer.com/10.1007/978-3-030-01692-0_41","The SELFHOOD installation was conceived aiming to instigate a reflection on the self through a practical and interactive experience. A representation of each participant is created in a form of a cloud of points and a sound drone, suggesting their selves. The dynamics of the visitors’ movements is sonified in such way that colours and sound textures are fused in a surrounding hexaphonic system. CromaCrono≈, the system for immersive improvisation that produces digitally synthesized sounds in real time, is described. Philosophical concepts concerning notions of the Self are presented. We propose that the notion of Presence can be induced by virtual and/or physical sources of stimulation governed by a number of principles that underlie human experience, creativity, and discovery. The methodological point of view is that the notion of Presence indicates that there are essential inputs for the construction of self-referral agents.","2018","2023-07-05 07:13:09","2023-07-21 04:34:01","2023-07-05 07:13:08","625-636","","","11265","","","SELFHOOD","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-01692-0_41","","","","","","Aramaki, Mitsuko; Davies, Matthew E. P.; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LFRIR8VP","bookSection","2018","Chamberlain, Alan; Bødker, Mads; De Roure, David; Willcox, Pip; Emsley, Iain; Malizia, Alessio","A Landscape of Design: Interaction, Interpretation and the Development of Experimental Expressive Interfaces","Human-Computer Interaction. Theories, Methods, and Human Issues","978-3-319-91237-0 978-3-319-91238-7","","","https://link.springer.com/10.1007/978-3-319-91238-7_3","This short paper presents the initial research insights of an ongoing research project that focuses upon understanding the role of landscape, its use as a resource for designing interfaces for musical expression, and as a tool for leveraging ethnographic understandings about space, place, design and musical expression. We briefly discuss the emerging research and reasoning behind our approach, the site that we are focusing on, our participatory methodology and conceptual designs. This innovative research is envisaged as something that can engage and interest the conference participants, encourage debate and act as an exploratory platform, which will in turn inform our research, practice and design.","2018","2023-07-05 07:13:09","2023-07-20 06:33:04","2023-07-05 07:13:08","24-34","","","10901","","","A Landscape of Design","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-91238-7_3","","/Users/minsik/Zotero/storage/A76A8PCZ/Chamberlain et al. - 2018 - A Landscape of Design Interaction, Interpretation.pdf","","","","Kurosu, Masaaki","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M7IDTVCU","bookSection","2007","Korhonen, Hannu; Holm, Jukka; Heikkinen, Mikko","Utilizing Sound Effects in Mobile User Interface Design","Human-Computer Interaction – INTERACT 2007","978-3-540-74794-9 978-3-540-74796-3","","","http://link.springer.com/10.1007/978-3-540-74796-3_27","The current generation of mobile devices is capable of producing polyphonic sounds, has enough processing power for real-time signal processing, and much better sound quality than their predecessors. The importance of audio is increasing as we are moving towards multimodal user interfaces where audio is one of the major components. In this paper, we present new ways of using audio feedback more efficiently and intelligently in mobile user interfaces by utilizing real-time signal processing. To test the ideas in practice, a prototype calendar application was implemented. We arranged a one week field trial to validate the design ideas. The results indicate that sound effects are capable of passing information to the user in some extent, but they are more useful in impressing the user and making existing audio feedback sound better.","2007","2023-07-05 07:13:09","2023-07-20 05:55:58","2023-07-05 07:13:08","283-296","","","4662","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-74796-3_27","","/Users/minsik/Zotero/storage/INP5M5VW/Korhonen et al. - 2007 - Utilizing Sound Effects in Mobile User Interface D.pdf","","","","Baranauskas, Cécilia; Palanque, Philippe; Abascal, Julio; Barbosa, Simone Diniz Junqueira","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A6LYQH6E","journalArticle","2019","Ziemer, Tim; Schultheis, Holger","Psychoacoustic auditory display for navigation: an auditory assistance system for spatial orientation tasks","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-018-0282-2","http://link.springer.com/10.1007/s12193-018-0282-2","A psychoacoustic auditory display for navigation is presented. Interactive sonification guides users to an invisible target location in two-dimensional space. Orthogonal spatial dimensions are mapped to perceptual auditory qualities that are orthogonal as well. The psychoacoustic auditory display could serve as an alternative or complement to conventional assistance systems for vehicle or airplane control, or for minimally invasive surgery. The approach is evaluated by an experiment, which compares the performance of 18 participants approaching (i) a visually presented target (ii) an invisible target guided by sound. Results demonstrate that users are able to integrate the sonified information to find the right angle and distance, or to segregate both spatial axes and interpret one at a time. Auditory navigation takes significantly longer than visual navigation, but path lengths are not significantly different.","2019-09","2023-07-05 07:13:09","2023-07-20 07:06:16","2023-07-05 07:13:08","205-218","","3","13","","J Multimodal User Interfaces","Psychoacoustic auditory display for navigation","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UUBL5HN9","journalArticle","2009","Vézien, J. M.; Ménélas, B.; Nelson, J.; Picinali, L.; Bourdot, P.; Ammi, M.; Katz, B. F. G.; Burkhardt, J. M.; Pastur, L.; Lusseyran, F.","Multisensory VR exploration for computer fluid dynamics in the CoRSAIRe project","Virtual Reality","","1359-4338, 1434-9957","10.1007/s10055-009-0134-1","http://link.springer.com/10.1007/s10055-009-0134-1","In the last 30 years, the evolution of digital data processing in terms of processing power, storage capacity, and algorithmic efficiency in the simulation of physical phenomena has allowed the emergence of the discipline known as computational fluid dynamics or CFD. More recently, virtual reality (VR) systems have proven an interesting alternative to conventional user interfaces, in particular, when exploring complex and massive datasets, such as those encountered in scientific visualization applications. Unfortunately, all too often, VR technologies have proven unsatisfactory in providing a true added value compared to standard interfaces, mostly because insufficient attention was given to the activity and needs of the intended user audience. The present work focuses on the design of a multimodal VR environment dedicated to the analysis of non-stationary flows in CFD. Specifically, we report on the identification of relevant strategies of CFD exploration coupled to adapted VR data representation and interaction techniques. Three different contributions will be highlighted. First, we show how placing the CFD expert user at the heart of the system is accomplished through a formalized analysis of work activity and through system evaluation. Second, auditory outputs providing analysis of time-varying phenomena in a spatialized virtual environment are introduced and evaluated. Finally, specific haptic feedbacks are designed and evaluated to enhance classical visual data exploration of CFD simulations.","2009-12","2023-07-05 07:13:09","2023-07-21 05:14:46","2023-07-05 07:13:08","257-271","","4","13","","Virtual Reality","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KXDQN7YX","bookSection","2013","Carroll, Dustin; Chakraborty, Suranjan; Lazar, Jonathan","Designing Accessible Visualizations: The Case of Designing a Weather Map for Blind Users","Universal Access in Human-Computer Interaction. Design Methods, Tools, and Interaction Techniques for eInclusion","978-3-642-39187-3 978-3-642-39188-0","","","http://link.springer.com/10.1007/978-3-642-39188-0_47","Major strides have been made to improve the accessibility of textbased documents for blind users, however, visualizations still remain largely inaccessible. The AISP framework represents an attempt to streamline the design process by aligning the information seeking behaviors of a blind user with those of a sighted user utilizing auditory feedback. With the recent popularity of touch-based devices, and the overwhelming success of the talking tactile tablet, we therefore suggest that the AISP framework be extended to include the sense of touch. This research-in-progress paper proposes such an extended design framework, MISD. In addition, the article also presents the preliminary work done in designing an accessible weather map based on our theory-driven design. A discussion and an outline of future work conclude the manuscript.","2013","2023-07-05 07:13:09","2023-07-21 05:09:59","2023-07-05 07:13:08","436-445","","","8009","","","Designing Accessible Visualizations","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-39188-0_47","","/Users/minsik/Zotero/storage/6ULQ8FLP/Carroll et al. - 2013 - Designing Accessible Visualizations The Case of D.pdf","","","","Stephanidis, Constantine; Antona, Margherita","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6FZAYGZE","bookSection","2021","Villeneuve, Jérôme; Leonard, James; Tache, Olivier","Instruments and Sounds as Objects of Improvisation in Collective Computer Music Practice","Perception, Representations, Image, Sound, Music","978-3-030-70209-0 978-3-030-70210-6","","","http://link.springer.com/10.1007/978-3-030-70210-6_41","This paper presents the authors’ first attempt at a new (and unexpected) exercise: that of observing, contextualising and problematising their own collective Computer Music experiences. After two years practising emergent collective improvisation in private and public settings, which has led the authors to fundamentally reconsider both individual and collective musical creation, came the desire to methodologically deconstruct this process - one that they never anticipated and, until now, had never formalised. By starting from the very notions or performance and improvisation in the context of Computer Music, and crossing prolific literature on these topics with humble observations from their own experience, the authors then elaborate on what appears to them as the most enticing perspective of this creative context: the systematic improvisation of both their tools and sounds in an unique flow.","2021","2023-07-05 07:13:09","2023-07-21 04:45:41","2023-07-05 07:13:08","636-654","","","12631","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-70210-6_41","","/Users/minsik/Zotero/storage/38KARUHQ/Villeneuve et al. - 2021 - Instruments and Sounds as Objects of Improvisation.pdf","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Aramaki, Mitsuko","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"85EBJ78N","bookSection","2011","Castagna, Riccardo; Chiolerio, Alessandro; Margaria, Valentina","Music Translation of Tertiary Protein Structure: Auditory Patterns of the Protein Folding","Applications of Evolutionary Computation","978-3-642-20519-4 978-3-642-20520-0","","","http://link.springer.com/10.1007/978-3-642-20520-0_22","We have translated genome-encoded protein sequence into musical notes and created a polyphonic harmony taking in account its tertiary structure. We did not use a diatonic musical scale to obtain a pleasant sound, focusing instead on the spatial relationship between aminoacids closely placed in the 3-dimensional protein folding. In this way, the result is a musical translation of the real morphology of the protein, that opens the challenge to bring musical harmony rules into the proteomic research field.","2011","2023-07-05 07:13:09","2023-07-19 11:30:30","2023-07-05 07:13:08","214-222","","","6625","","","Music Translation of Tertiary Protein Structure","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-20520-0_22","","","","","","Di Chio, Cecilia; Brabazon, Anthony; Di Caro, Gianni A.; Drechsler, Rolf; Farooq, Muddassar; Grahl, Jörn; Greenfield, Gary; Prins, Christian; Romero, Juan; Squillero, Giovanni; Tarantino, Ernesto; Tettamanzi, Andrea G. B.; Urquhart, Neil; Uyar, A. Şima","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AYVKD65W","journalArticle","2021","Mandanici, Marcella; Di Filippo, Roberto; Delle Monache, Stefano","The Discovery of Interactive Spaces: Learning by Design in High School Music Technology Classes","Technology, Knowledge and Learning","","2211-1662, 2211-1670","10.1007/s10758-020-09464-4","https://link.springer.com/10.1007/s10758-020-09464-4","This paper describes an educational experience realized in the form of extracurricular workshops involving music technology students of the “V. Gambara” music high school in Brescia (Italy). By means of a participatory prototyping experience, the project aimed at fostering the students awareness and understanding of technological means and their utility . The Discovery of Interactive Spaces project focuses on motion tracking technologies in connection with sound and visual production, as means to provoke reflections on their cultural and societal impact on social utility and inclusion, and artistic expression. To this end, students proposed design concepts, and prototyped sonic interactive experiences. The Discovery of Interactive Spaces is framed within the broader themes of computational thinking and creativity, learning by design, and technology awareness. These themes represent the pillars of technological citizenship, which is considered crucial for the twenty-first century student.","2021-12","2023-07-05 07:13:09","2023-07-21 05:04:09","2023-07-05 07:13:08","1131-1151","","4","26","","Tech Know Learn","The Discovery of Interactive Spaces","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3CMFLQRP","journalArticle","2014","Thaut, Michael H.; McIntosh, Gerald C.","Neurologic Music Therapy in Stroke Rehabilitation","Current Physical Medicine and Rehabilitation Reports","","2167-4833","10.1007/s40141-014-0049-y","http://link.springer.com/10.1007/s40141-014-0049-y","Based on insights from brain research in music, neurologic music therapy (NMT) has been established as a new model for music in therapy and medicine. Standardized clinical interventions are based on clusters of research evidence and established learning principles in motor, speech/language, and cognitive training. The research support for NMT in stroke rehabilitation has been growing rapidly over the past 20 years. This paper will review research data and clinical applications for neurorehabilitation in the speech/language, cognitive and sensorimotor domains.","2014-06","2023-07-05 07:13:09","2023-07-19 23:54:34","2023-07-05 07:13:08","106-113","","2","2","","Curr Phys Med Rehabil Rep","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/XCPCPR5A/Thaut and McIntosh - 2014 - Neurologic Music Therapy in Stroke Rehabilitation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RNHZ7Y5D","bookSection","2006","Havryliv, Mark; Narushima, Terumi","Metris: A Game Environment for Music Performance","Computer Music Modeling and Retrieval","978-3-540-34027-0 978-3-540-34028-7","","","http://link.springer.com/10.1007/11751069_9","Metris is a version of the Tetris game that uses a player’s musical response to control game performance. The game is driven by two factors: traditional game design and the player’s individual sense of music and sound. Metris uses tuning principles to determine relationships between pitch and the timbre of the sounds produced. These relationships are represented as bells synchronised with significant events in the game. Key elements of the game design control a musical environment based on just intonation tuning. This presents a scenario where the game design is enhanced by a user's sense of sound and music. Conventional art music is subverted by responses to simple design elements in a popular game.","2006","2023-07-05 07:13:09","2023-07-19 23:48:15","2023-07-05 07:13:08","101-109","","","3902","","","Metris","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11751069_9","","/Users/minsik/Zotero/storage/92UAY6H9/Havryliv and Narushima - 2006 - Metris A Game Environment for Music Performance.pdf","","","","Kronland-Martinet, Richard; Voinier, Thierry; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HW97J364","journalArticle","2011","Van Der Vlist, Bram; Bartneck, Christoph; Mäueler, Sebastian","moBeat: Using Interactive Music to Guide and Motivate Users During Aerobic Exercising","Applied Psychophysiology and Biofeedback","","1090-0586, 1573-3270","10.1007/s10484-011-9149-y","http://link.springer.com/10.1007/s10484-011-9149-y","An increasing number of people are having trouble staying fit and maintaining a healthy bodyweight because of lack of physical activity. Getting people to exercise is crucial. However, many struggle with developing healthy exercising habits, due to hurdles like having to leave the house and the boring character of endurance exercising. In this paper, we report on a design project that explores the use of audio to motivate and provide feedback and guidance during exercising in a home environment. We developed moBeat, a system that provides intensity-based coaching while exercising, giving real-time feedback on training pace and intensity by means of interactive music. We conducted a within-subject comparison between our moBeat system and a commercially available heart rate watch. With moBeat, we achieved a comparable success rate: our system has a significant, positive influence on intrinsic motivation and attentional focus, but we did not see significant differences with regard to either perceived exertion or effectiveness. Although promising, future research is needed.","2011-06","2023-07-05 07:13:09","2023-07-19 11:32:14","2023-07-05 07:13:08","135-145","","2","36","","Appl Psychophysiol Biofeedback","moBeat","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/9IMYTPS6/Van Der Vlist et al. - 2011 - moBeat Using Interactive Music to Guide and Motiv.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y7CKLC5V","bookSection","2009","Kuuskankare, Mika; Laurson, Mikael","Strategies and Methods for Creating Symbolic Electroacoustic Scores in ENP","Computer Music Modeling and Retrieval. Genesis of Meaning in Sound and Music","978-3-642-02517-4 978-3-642-02518-1","","","http://link.springer.com/10.1007/978-3-642-02518-1_19","In this paper we explore the potential of Expressive Notation Package (ENP) in representing electroacoustic scores. We use the listening score by Rainer Wehinger–originally created for György Ligeti’s electronic piece Articulation–as reference material. Our objective is to recreate a small excerpt form the score using an ENP tool called Expression Designer (ED). ED allows the user to create new graphical expressions with programmable behavior. In this paper we aim to demonstrate how a collection of specific graphic symbols found in Articulation can be implemented using ED. We also discuss how this information can be later manipulated and accessed for the needs of a sonic realization, for example.","2009","2023-07-05 07:13:09","2023-07-19 23:48:47","2023-07-05 07:13:08","262-271","","","5493","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-02518-1_19","","","","","","Ystad, Sølvi; Kronland-Martinet, Richard; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MHLK7JK3","journalArticle","2010","Pettey, Gary; Bracken, Cheryl Campanella; Rubenking, Bridget; Buncher, Michael; Gress, Erika","Telepresence, soundscapes and technological expectation: putting the observer into the equation","Virtual Reality","","1359-4338, 1434-9957","10.1007/s10055-009-0148-8","http://link.springer.com/10.1007/s10055-009-0148-8","In an experiment exploring the impact of sound on sensations of telepresence, 126 participants watched a video clip using either headphones or speakers. The results illustrate that sound is an important factor in stimulating telepresence responses in audiences. Interactions between soundscape and screen size were also revealed. A traverse interaction between aural/visual congruency and sound­ scapes was evident. A second data set of 102 participants was collected to illuminate the effect of technological expectation that emerged in the first study. Expectations had been mentioned in other studies, and the data support the notion that people have an expectation of the techno­ logical quality of a presentation. The results suggest that examining expectations could assist in future conceptual­ izations of telepresence.","2010-03","2023-07-05 07:13:09","2023-07-21 05:14:37","2023-07-05 07:13:08","15-25","","1","14","","Virtual Reality","Telepresence, soundscapes and technological expectation","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/PZ6VZCUQ/Pettey et al. - 2010 - Telepresence, soundscapes and technological expect.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IV5P4DDX","journalArticle","2022","Ley-Flores, Judith; Alshami, Eslam; Singh, Aneesha; Bevilacqua, Frédéric; Bianchi-Berthouze, Nadia; Deroy, Ophelia; Tajadura-Jiménez, Ana","Effects of pitch and musical sounds on body-representations when moving with sound","Scientific Reports","","2045-2322","10.1038/s41598-022-06210-x","https://www.nature.com/articles/s41598-022-06210-x","Abstract             The effects of music on bodily movement and feelings, such as when people are dancing or engaged in physical activity, are well-documented—people may move in response to the sound cues, feel powerful, less tired. How sounds and bodily movements relate to create such effects? Here we deconstruct the problem and investigate how different auditory features affect people’s body-representation and feelings even when paired with the same movement. In three experiments, participants executed a simple arm raise synchronised with changing pitch in simple tones (Experiment 1), rich musical sounds (Experiment 2) and within different frequency ranges (Experiment 3), while we recorded indirect and direct measures on their movement, body-representations and feelings. Changes in pitch influenced people’s general emotional state as well as the various bodily dimensions investigated—movement, proprioceptive awareness and feelings about one’s body and movement. Adding harmonic content amplified the differences between ascending and descending sounds, while shifting the absolute frequency range had a general effect on movement amplitude, bodily feelings and emotional state. These results provide new insights in the role of auditory and musical features in dance and exercise, and have implications for the design of sound-based applications supporting movement expression, physical activity, or rehabilitation.","2022-02-17","2023-07-05 07:13:09","2023-07-05 07:13:09","2023-07-05 07:13:08","2676","","1","12","","Sci Rep","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/YA6BKVBA/Ley-Flores et al. - 2022 - Effects of pitch and musical sounds on body-repres.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QTFEESTG","bookSection","2009","Eslambochilar, Parisa; Buchanan, George; Loizides, Fernando","Hear It Is: Enhancing Rapid Document Browsing with Sound Cues","Research and Advanced Technology for Digital Libraries","978-3-642-04345-1 978-3-642-04346-8","","","http://link.springer.com/10.1007/978-3-642-04346-8_9","Document navigation has become increasingly commonplace as the use of electronic documents has grown. Speed–Dependent Automatic Zooming (SDAZ) is one popular method for providing rapid movement within a digital text. However, there is evidence that details of the document are overlooked as the pace of navigation rises. We produced a document reader software where sound is used to complement the visual cues that a user searches for visually. This software was then evaluated in a user study that provides strong supportive evidence that non-visual cues can improve user performance in visual seeking tasks.","2009","2023-07-05 07:13:09","2023-07-21 04:56:56","2023-07-05 07:13:08","75-86","","","5714","","","Hear It Is","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-04346-8_9","","/Users/minsik/Zotero/storage/8MM4HKL8/Eslambochilar et al. - 2009 - Hear It Is Enhancing Rapid Document Browsing with.pdf","","","","Agosti, Maristella; Borbinha, José; Kapidakis, Sarantos; Papatheodorou, Christos; Tsakonas, Giannis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3JEC6EQP","bookSection","2003","Tzanetakis, George","Musescape: A Tool for Changing Music Collections into Libraries","Research and Advanced Technology for Digital Libraries","978-3-540-40726-3 978-3-540-45175-4","","","http://link.springer.com/10.1007/978-3-540-45175-4_38","Increases in hard disk capacity and audio compression technology have enabled the storage of large collections of music on personal computers and portable devices. As an example a portable device with 20 Gigabytes of storage can hold up to 4000 songs in compressed audio format. Currently the only way of structuring these collections is using a file system hierarchy which allows very limited forms of searching and retrieval. These limitations are even more pronounced in the case of portable devices where there is less screen real estate and user attention is limited compared to a personal computer. Musescape is a prototype tool for organizing and interacting with large music collections in audio format with specific emphasis on portable devices. It provides a variety of automatic and manual ways to organize and interact with large music collections using a consistent continuous audio feedback user interface for browsing, searching and annotating. Using this system a user can convert an unstructured or partially structured collection of music with limited retrieval capabilities into a music library with enhanced functionality.","2003","2023-07-05 07:15:22","2023-07-21 04:57:06","2023-07-05 07:15:22","412-421","","","2769","","","Musescape","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-45175-4_38","","","","","","Koch, Traugott; Sølvberg, Ingeborg Torvik","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y5GP3YZ5","journalArticle","2023","Sørensen, Vibeke; Lansing, J. Stephen","Art, technology and the Internet of Living Things","AI & SOCIETY","","0951-5666, 1435-5655","10.1007/s00146-023-01667-4","https://link.springer.com/10.1007/s00146-023-01667-4","Intelligence augmentation was one of the original goals of computing. Artificial Intelligence (AI) inherits this project and is at the leading edge of computing today. Computing can be considered an extension of brain and body, with mathematical prowess and logic fundamental to the infrastructure of computing. Multimedia computing—sensing, analyzing, and translating data to and from visual images, animation, sound and music, touch and haptics, as well as smell—is based on our human senses and is now commonplace. We use data visualization and sonification, as well as data mining and analysis, to sort through the complexity and vast volume of data coming from the world inside and around us. It helps us ‘see’ in new ways. We can think of this capacity as a new kind of “digital glasses”. The Internet of Living Things (IOLT) is potentially an even more profound extension of ourselves to the world: a network of electronic devices embedded into objects, but now with subcutaneous, ingestible devices, and embedded sensors that include people and other living things. Like the Internet of Things (IOT), living things are connected; we call those connections “ecology”. As the IOT becomes increasingly synonymous with the IOLT, the question of ethics that is at the centre of aesthetics and the arts will move to the forefront of our experience of and regard for the world in and around us.","2023-05-16","2023-07-05 07:15:22","2023-07-19 11:27:01","2023-07-05 07:15:22","","","","","","AI & Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/N2JX3SUK/Sørensen and Lansing - 2023 - Art, technology and the Internet of Living Things.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L6XECBUF","bookSection","2021","Sørensen, Vibeke; Lansing, J. Stephen; Thummanapalli, Nagaraju","Voyages Along the Star Paths: Capturing Calendrical Cycles from Kauai to Bali","Culture and Computing. Interactive Cultural Heritage and Arts","978-3-030-77410-3 978-3-030-77411-0","","","https://link.springer.com/10.1007/978-3-030-77411-0_20","Systems for the representation of temporal cycles play a vital role in all cultures, but they seldom figure prominently in studies of heritage. Anthropologists are often frustrated by he lumping together and dismissal of nonwestern concepts of time as merely “cyclical time”, interpreted as changelessness or the absence of progress. Here we compare two of the most complex and sophisticated calendrical systems known to anthropology, from the islands of Bali and Hawaii. The sheer complexity of these concepts and their intimate relationship to astronomical phenomena make them very difficult to compare using the scholar’s traditional toolkit of text and images. But they are admirably suited to immersive digital media. As well as facilitating descriptive exposition, real-time computer animation, music programming and other digital technology opens new avenues for research on the relationship of the abstract structure of calendrical systems to polyrhythms in music and other aspects of the phenomenology of time consciousness.","2021","2023-07-05 07:15:22","2023-07-19 23:54:15","2023-07-05 07:15:22","296-317","","","12794","","","Voyages Along the Star Paths","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-77411-0_20","","","","","","Rauterberg, Matthias","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HRUH2SFX","journalArticle","2022","Robinson, Frederic Anthony; Bown, Oliver; Velonaki, Mari","Designing Sound for Social Robots: Candidate Design Principles","International Journal of Social Robotics","","1875-4791, 1875-4805","10.1007/s12369-022-00891-0","https://link.springer.com/10.1007/s12369-022-00891-0","Abstract             How can we use sound and music to create rich and engaging human-robot interactions? A growing body of HRI research explores the many ways in which sound affects human-robot interactions and although some studies conclude with tentative design recommendations, there are, to our knowledge, no generalised design recommendations for the robot sound design process. We address this gap by first investigating sound design frameworks in the domains of product sound design and film sound to see whether practices and concepts from these areas contain actionable insights for the creation of robot sound. We then present three case studies, detailed examinations of the sound design of commercial social robots Cozmo and Vector, Jibo, and Kuri, facilitated by expert interviews with the robots’ sound designers. Combining insights from the design frameworks and case studies, we propose nine candidate design principles for robot sound which provide (1) a design-oriented perspective on robot sound that may inform future research, and (2) actionable guidelines for designers, engineers and decision-makers aiming to use sound to create richer and more refined human-robot interactions.","2022-08","2023-07-05 07:15:22","2023-07-05 07:15:22","2023-07-05 07:15:22","1507-1525","","6","14","","Int J of Soc Robotics","Designing Sound for Social Robots","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/XGRH4S6L/Robinson et al. - 2022 - Designing Sound for Social Robots Candidate Desig.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4XM5U3Q8","bookSection","2014","Houix, Olivier; Misdariis, Nicolas; Susini, Patrick; Bevilacqua, Frédéric; Gutierrez, Florestan","Sonically Augmented Artifacts: Design Methodology Through Participatory Workshops","Sound, Music, and Motion","978-3-319-12975-4 978-3-319-12976-1","","","https://link.springer.com/10.1007/978-3-319-12976-1_2","Participatory workshops have been organized within the framework of the ANR project Legos that concerns gesture-sound interactive systems. These workshops addressed both theoretical issues and experimentation with prototypes. The first goal was to stimulate new ideas related to the control of everyday objects using sound feedback, and then, to create and experiment with new sonic augmented objects. The second aim was educational. We investigated how sonic interaction design can be introduced to people without backgrounds in sound and music. We present in this article an overview of three workshops. The first workshop focused on the analysis and the possible sonification of everyday objects. New usage scenarios were obtained and tested. The second workshop focused on sound metaphor, questioning the relationship between sound and gesture. The last one was a workshop organized during a summer school for students. During these workshops, we experimented a cycle of design process: analysis, creation and testing.","2014","2023-07-05 07:15:22","2023-07-21 05:01:46","2023-07-05 07:15:22","20-40","","","8905","","","Sonically Augmented Artifacts","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-12976-1_2","","","","","","Aramaki, Mitsuko; Derrien, Olivier; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MBI93HXF","bookSection","2001","Harding, Chris; Kakadiaris, Ioannis A.; Casey, John F.; Bowen Loftin, R.","A Case Study in Multi-Sensory Investigation of Geoscientific Data","Data Visualization 2001","978-3-211-83674-3 978-3-7091-6215-6","","","http://link.springer.com/10.1007/978-3-7091-6215-6_2","In this paper, we report our ongoing research into multi-sensory investigation of geoscientific data. Our Geoscientific Data Investigation System (GDIS) integrates three-dimensional, interactive computer graphics, touch (haptics) and real-time sonification into a multi-sensory Virtual Environment. GDIS has been used to investigate geological structures on the high-resolution bathymetry data from the Mid-Atlantic Ridge. Haptic force feedback was used to precisely digitize line features on threedimensional morphology and to feel surface properties via varying friction settings; additional, overlapping data can be perceived via sound (sonification). We also report on the results of a psycho-acoustic study about the absolute recognition of sound signals, and on the actual feedback that we have received from a number of geoscientists during a recent major geoscience conference.","2001","2023-07-05 07:15:22","2023-07-19 23:54:45","2023-07-05 07:15:22","3-14","","","","","","","","","","","Springer Vienna","Vienna","en","","","","","DOI.org (Crossref)","","Series Title: Eurographics DOI: 10.1007/978-3-7091-6215-6_2","","","","","","Ebert, David S.; Favre, Jean M.; Peikert, Ronald","Hansmann, W.; Purgathofer, W.; Sillion, F.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R2HUFYQG","bookSection","2007","Ohyama, Kisa; Itoh, Takayuki","DIVA: An Automatic Music Arrangement Technique Based on Impression of Images","Smart Graphics","978-3-540-73213-6 978-3-540-73214-3","","","http://link.springer.com/10.1007/978-3-540-73214-3_16","This poster reports our first approach for automatic music arrangement based on the impression of input images. Given a digital image and keywords of objects shot in the image, the technique selects a rhythm pattern associated from the keywords and color distribution of the image. As a preprocessing, the technique first provides sample colors, images, and keywords to users, and then collects the feedback of selection of rhythm patterns associated from them. The technique then leads equations to calculate the association of rhythm pattern from arbitrary input images. Finally, the technique automatically calculates the association scores of all prepared rhythm patterns from the images, and provides music arranged applying the associated rhythm pattern.","2007","2023-07-05 07:15:22","2023-07-21 04:59:47","2023-07-05 07:15:22","178-181","","","4569","","","DIVA","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73214-3_16","","","","","","Butz, Andreas; Fisher, Brian; Krüger, Antonio; Olivier, Patrick; Owada, Shigeru","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ENFELAZA","bookSection","2020","Baird, Alice; Song, Meishu; Schuller, Björn","Interaction with the Soundscape: Exploring Emotional Audio Generation for Improved Individual Wellbeing","Artificial Intelligence in HCI","978-3-030-50333-8 978-3-030-50334-5","","","http://link.springer.com/10.1007/978-3-030-50334-5_15","Ourdailyinteractionwiththesoundscapeisinflux,and complexnaturalsoundcombinationshaveshowntohaveadverseimplicationsonuserexperience.Acomputationalapproachtostabilisethe sonicenvironment,tailoredtoauser’scurrentaffectivestatemayprove beneficialinavarietyofscenarios,includingworkplaceefficiency,and exercise.Herein,wepresentinitialperceptiontestresults,fromarudimentaryapproachforsoundscapeaugmentationutilisingchromaticfeaturesonification.Resultsshowthatarousalandvalancedimensionsof emotioncanbealteredthroughaugmentationofthreeclassesofnaturalsoundscape,namely‘mechanical’,‘nature’,and‘human’.Proceeding thisweoutlineapossibleapproachforanaffectiveaudio-basedrecognitionandgenerationsystem,inwhichusers(eitherindividuallyorasa groupwithinaspecificenvironment)areprovidedwithan augmentation oftheircurrentsoundscape,asameansofimprovingwellbeing.","2020","2023-07-05 07:15:22","2023-07-19 11:33:15","2023-07-05 07:15:22","229-242","","","12217","","","Interaction with the Soundscape","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-50334-5_15","","/Users/minsik/Zotero/storage/78LHSWVB/Baird et al. - 2020 - Interaction with the Soundscape Exploring Emotion.pdf","","","","Degen, Helmut; Reinerman-Jones, Lauren","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PGZ3IK5Y","journalArticle","2016","Hinterberger, Thilo; Fürnrohr, Elena","The Sensorium: Psychophysiological Evaluation of Responses to a Multimodal Neurofeedback Environment","Applied Psychophysiology and Biofeedback","","1090-0586, 1573-3270","10.1007/s10484-016-9332-2","http://link.springer.com/10.1007/s10484-016-9332-2","The Sensorium is a multimodal neurofeedback environment that reflects a person’s physiological state by presenting physiological signals via orchestral sounds from a speaker and multi-coloured lights projected onto a white surface. The software manages acquisition, real-time processing, storage, and sonification of various physiological signals such as the electroencephalogram (EEG) or electrocardiogram (ECG). Each of the 36 participants completed 6 interventional conditions consisting of three different Sensorium-phases with EEG and ECG feedback, a mindfulness meditation, a guided body scan exercise, and a Pseudo-Sensorium using pre-recorded data that did not reflect the subject’s own physiology. During all phases EEG, ECG, skin conductance, and respiration were recorded. A feedback questionnaire assessed the participants’ subjective reports of changes in well-being, perception, and life-spirit. The results indicate that the Sensorium sessions were not statistically inferior compared to their corresponding active control conditions with respect to improvements in subjective reports concerning well-being and perception. Additionally, the Sensorium was rated as being a more extraordinary experience, as compared to meditation. During the Sensorium conditions the EEG showed lower levels of theta2 (7–8.5 Hz), alpha (9–12 Hz) and beta (12.5–25 Hz) activity. Since participants reported benefit from the Sensorium experience regardless of any prior experience with meditation, we propose this novel method of meditative and extraordinary self-experience to be utilized as a modern alternative to more traditional forms of meditation.","2016-09","2023-07-05 07:15:22","2023-07-19 11:31:45","2023-07-05 07:15:22","315-329","","3","41","","Appl Psychophysiol Biofeedback","The Sensorium","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7GKD9JHL","bookSection","2021","Craveirinha, Rui; Pereira, Luís Lucas; Seiça, Mariana; Roque, Licínio","Aesthetic Perspectives on Computational Media Design","Entertainment Computing – ICEC 2021","978-3-030-89393-4 978-3-030-89394-1","","","https://link.springer.com/10.1007/978-3-030-89394-1_42","The main goal of this workshop is to open a collaborative space for reflection and debate on how Aesthetics can inform the design of computational media, in an attempt to grasp the multiplicity of aesthetic dimensions that can influence the reception, experience, and interpretation of such sociotechnical objects and experiences. We will approach the goal from the entertainment computing field where such concerns are of vital influence .","2021","2023-07-05 07:15:22","2023-07-20 00:00:06","2023-07-05 07:15:22","482-488","","","13056","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-89394-1_42","","","","","","Baalsrud Hauge, Jannicke; C. S. Cardoso, Jorge; Roque, Licínio; Gonzalez-Calero, Pedro A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2C36NNHZ","bookSection","2014","Gerino, Andrea; Alabastro, Nicolò; Bernareggi, Cristian; Ahmetovic, Dragan; Mascetti, Sergio","MathMelodies: Inclusive Design of a Didactic Game to Practice Mathematics","Computers Helping People with Special Needs","978-3-319-08595-1 978-3-319-08596-8","","","http://link.springer.com/10.1007/978-3-319-08596-8_88","Tablet computers are becoming a common tool to support learning since primary school. Indeed, many didactic applications are already available on online stores. Most of these applications engage the child by immersing the educational purpose of the software within an entertaining environment, often in the form of a game with sophisticated graphic and interaction. Unfortunately, this makes most of these applications inaccessible to visually impaired children. In this contribution we present MathMelodies, an iPad application that supports math learning in primary school and that is designed to be accessible also to visually impaired children. We describe the main challenges we faced during the development of this didactic application that is both engaging and accessible. The application, currently publicly available, is collecting enthusiastic reviews from teachers, who often contribute with precious insight for improving the solution.","2014","2023-07-05 07:15:22","2023-07-19 23:51:58","2023-07-05 07:15:22","564-571","","","8547","","","MathMelodies","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-08596-8_88","","","","","","Miesenberger, Klaus; Fels, Deborah; Archambault, Dominique; Peňáz, Petr; Zagler, Wolfgang","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BT3PTYK2","journalArticle","2014","Han, Yoon Chung; Han, Byeong-jun","Virtual pottery: a virtual 3D audiovisual interface using natural hand motions","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-013-1382-3","http://link.springer.com/10.1007/s11042-013-1382-3","In this paper, we present our approach towards designing and implementing a virtual 3D sound sculpting interface that creates audiovisual results using hand motions in real time. In the interface “Virtual Pottery,” we use the metaphor of pottery creation in order to adopt the natural hand motions to 3D spatial sculpting. Users can create their own pottery pieces by changing the position of their hands in real time, and also generate 3D sound sculptures based on pre-existing rules of music composition. The interface of Virtual Pottery can be categorized by shape design and camera sensing type. This paper describes how we developed the two versions of Virtual Pottery and implemented the technical aspects of the interfaces. Additionally, we investigate the ways of translating hand motions into musical sound. The accuracy of the detection of hand motions is crucial for translating natural hand motions into virtual reality. According to the results of preliminary evaluations, the accuracy of both motion-capture tracking system and portable depth sensing camera is as high as the actual data. We carried out user studies, which took into account information about the two exhibitions along with the various ages of users. Overall, Virtual Pottery serves as a bridge between the virtual environment and traditional art practices, with the consequence that it can lead to the cultivation of the deep potential of virtual musical instruments and future art education programs.","2014-11","2023-07-05 07:15:22","2023-07-21 04:32:23","2023-07-05 07:15:22","917-933","","2","73","","Multimed Tools Appl","Virtual pottery","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"489PA4FK","bookSection","2019","Torres-Cardona, Hector Fabio; Aguirre-Grisales, Catalina; Castro-Londoño, Victor Hugo; Rodriguez-Sotelo, Jose Luis","Interpolation, a Model for Sound Representation Based on BCI","Augmented Cognition","978-3-030-22418-9 978-3-030-22419-6","","","http://link.springer.com/10.1007/978-3-030-22419-6_34","Brain state control has been well established in the area of Brain-computer interfaces over the last decades in which the active applications allow controlling external devices consciously. The purpose of this study was to develop a real-time graphical sound representation system based on an interaction design that allows navigating through the motor imagery cognitive task in a bidimensional plane. This representation was developed using the OpenBCI EEG acquisition system in order to record the necessary information which was sent and processed in Max/MSP software. The system operates under a metaphorical Graphical User Interface (GUI) programmed in Processing. The system was tested through an experiment under controlled conditions in which six professional musicians participated. From the experimental results, it was found that all participants achieved different control levels associated to their static and dynamic response with an average of 26.73% and 73.27% respectively.","2019","2023-07-05 07:15:22","2023-07-19 11:40:02","2023-07-05 07:15:22","471-483","","","11580","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-22419-6_34","","","","","","Schmorrow, Dylan D.; Fidopiastis, Cali M.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RGNZMV52","bookSection","2014","Boyer, Eric O.; Pyanet, Quentin; Hanneton, Sylvain; Bevilacqua, Frédéric","Learning Movement Kinematics with a Targeted Sound","Sound, Music, and Motion","978-3-319-12975-4 978-3-319-12976-1","","","https://link.springer.com/10.1007/978-3-319-12976-1_14","This study introduces an experiment designed to analyze the sensorimotor adaptation to a motion-based sound synthesis system. We investigated a sound-oriented learning task, namely to reproduce a targeted sound. The motion of a small handheld object was used to control a sound synthesizer. The object angular velocity was measured by a gyroscope and transmitted in real time wirelessly to the sound system. The targeted sound was reached when the motion matched a given reference angular velocity profile with a given accuracy. An incorrect velocity profile produced either a noisier sound or a sound with a louder high harmonic, depending on the sign of the velocity error. The results showed that the participants were generally able to learn to reproduce sounds very close to the targeted sound. A corresponding motor adaptation was also found to occur, at various degrees, in most of the participants when the profile is altered.","2014","2023-07-05 07:15:22","2023-07-21 05:01:09","2023-07-05 07:15:22","218-233","","","8905","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-12976-1_14","","/Users/minsik/Zotero/storage/QCTS6D7A/Boyer et al. - 2014 - Learning Movement Kinematics with a Targeted Sound.pdf","","","","Aramaki, Mitsuko; Derrien, Olivier; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HLKWXKUS","bookSection","2020","Polydorou, Doros; Ben-Tal, Oded; Damsma, Atser; Schlichting, Nadine","VR: Time Machine","Human-Computer Interaction. Human Values and Quality of Life","978-3-030-49064-5 978-3-030-49065-2","","","http://link.springer.com/10.1007/978-3-030-49065-2_21","Time Machine is an immersive Virtual Reality installation that explains – in simple terms – the Striatal Beat Frequency (SBF) model of time perception. The installation was created as a collaboration between neuroscientists within the field of time perception along with a team of digital designers and audio composers/engineers. This paper outlines the process, as well as the lessons learned, while designing the virtual reality experience that aims to simplify a complex idea to a novice audience. The authors describe in detail the process of creating the world, the user experience mechanics and the methods of placing information in the virtual place in order to enhance the learning experience. The work was showcased at the 4th International Conference on Time Perspective, where the authors collected feedback from the audience. The paper concludes with a reflection on the work and some suggestions for the next iteration of the project.","2020","2023-07-05 07:15:22","2023-07-20 06:32:05","2023-07-05 07:15:22","294-306","","","12183","","","VR","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-49065-2_21","","/Users/minsik/Zotero/storage/WLSJEXQI/Polydorou et al. - 2020 - VR Time Machine.pdf","","","","Kurosu, Masaaki","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z6T8V3BK","journalArticle","2022","Lau, Chng Wei; Qu, Zhonglin; Draper, Daniel; Quan, Rosa; Braytee, Ali; Bluff, Andrew; Zhang, Dongmo; Johnston, Andrew; Kennedy, Paul J.; Simoff, Simeon; Nguyen, Quang Vinh; Catchpoole, Daniel","Virtual reality for the observation of oncology models (VROOM): immersive analytics for oncology patient cohorts","Scientific Reports","","2045-2322","10.1038/s41598-022-15548-1","https://www.nature.com/articles/s41598-022-15548-1","Abstract             The significant advancement of inexpensive and portable virtual reality (VR) and augmented reality devices has re-energised the research in the immersive analytics field. The immersive environment is different from a traditional 2D display used to analyse 3D data as it provides a unified environment that supports immersion in a 3D scene, gestural interaction, haptic feedback and spatial audio. Genomic data analysis has been used in oncology to understand better the relationship between genetic profile, cancer type, and treatment option. This paper proposes a novel immersive analytics tool for cancer patient cohorts in a virtual reality environment, virtual reality to observe oncology data models. We utilise immersive technologies to analyse the gene expression and clinical data of a cohort of cancer patients. Various machine learning algorithms and visualisation methods have also been deployed in VR to enhance the data interrogation process. This is supported with established 2D visual analytics and graphical methods in bioinformatics, such as scatter plots, descriptive statistical information, linear regression, box plot and heatmap into our visualisation. Our approach allows the clinician to interrogate the information that is familiar and meaningful to them while providing them immersive analytics capabilities to make new discoveries toward personalised medicine.","2022-07-05","2023-07-05 07:15:22","2023-07-05 07:15:22","2023-07-05 07:15:22","11337","","1","12","","Sci Rep","Virtual reality for the observation of oncology models (VROOM)","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/H49VLSZV/Lau et al. - 2022 - Virtual reality for the observation of oncology mo.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6A59AYXC","journalArticle","2016","Tordini, Francesco; Bregman, Albert S.; Cooperstock, Jeremy R.","Prioritizing foreground selection of natural chirp sounds by tempo and spectral centroid","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-016-0223-x","http://link.springer.com/10.1007/s12193-016-0223-x","Salience shapes the involuntary perception of a sound scene into foreground and background. Auditory interfaces, such as those used in continuous process monitoring, rely on the prominence of those sounds that are perceived as foreground. We propose to distinguish between the salience of sound events and that of streams, and introduce a paradigm to study the latter using repetitive patterns of natural chirps. Since streams are the sound objects populating the auditory scene, we suggest the use of global descriptors of perceptual dimensions to predict their salience, and hence, the organization of the objects into foreground and background. However, there are many possible independent features that can be used to describe sounds. Based on the results of two experiments, we suggest a parsimonious interpretation of the rules guiding foreground formation: after loudness, tempo and brightness are the dimensions that have higher priority. Our data show that, under equal-loudness conditions, patterns with fast tempo and lower brightness tend to emerge and that the interaction between tempo and brightness in foreground selection seems to increase with task difficulty. We propose to use the relations we uncovered as the underpinnings for a computational model of foreground selection, and also, as design guidelines for stream-based sonification applications.","2016-09","2023-07-05 07:15:22","2023-07-20 07:05:05","2023-07-05 07:15:22","221-234","","3","10","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PUE9A7LD","bookSection","2016","Rodrigues, Ana; Costa, Ernesto; Cardoso, Amílcar; Machado, Penousal; Cruz, Tiago","Evolving L-Systems with Musical Notes","Evolutionary and Biologically Inspired Music, Sound, Art and Design","978-3-319-31007-7 978-3-319-31008-4","","","http://link.springer.com/10.1007/978-3-319-31008-4_13","Over the years researchers have been interested in devising computational approaches for music and image generation. Some of the approaches rely on generative rewriting systems like L-systems. More recently, some authors questioned the interplay of music and images, that is, how we can use one type to drive the other. In this paper we present a new method for the algorithmic generations of images that are the result of a visual interpretation of an L-system. The main novelty of our approach is based on the fact that the L-system itself is the result of an evolutionary process guided by musical elements. Musical notes are decomposed into elements – pitch, duration and volume in the current implementation – and each of them is mapped into corresponding parameters of the L-system – currently line length, width, color and turning angle. We describe the architecture of our system, based on a multi-agent simulation environment, and show the results of some experiments that provide support to our approach.","2016","2023-07-05 07:18:09","2023-07-20 00:04:28","2023-07-05 07:18:09","186-201","","","9596","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-31008-4_13","","","","","","Johnson, Colin; Ciesielski, Vic; Correia, João; Machado, Penousal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JKN5X9ZE","journalArticle","2008","Fornari, José; Maia, Adolfo; Manzolli, Jônatas","Soundscape Design Through Evolutionary Engines","Journal of the Brazilian Computer Society","","0104-6500, 1678-4804","10.1007/BF03192564","https://journal-bcs.springeropen.com/articles/10.1007/BF03192564","Abstract                            Two implementations of an Evolutionary Sound Synthesis method using the Interaural Time Difference (ITD) and psychoacoustic descriptors are presented here as a way to develop criteria for fitness evaluation. We also explore a relationship between adaptive sound evolution and three soundscape characteristics: keysounds, key-signals and sound-marks. Sonic Localization Field is defined using a sound attenuation factor and ITD azimuth angle, respectively (I               i               , L               i               ). These pairs are used to build Spatial Sound Genotypes (SSG) and they are extracted from a waveform population set. An explanation on how our model was initially written in MATLAB is followed by a recent Pure Data (Pd) implementation. It also elucidates the development and use of: parametric scores, a triplet of psychoacoustic descriptors and the correspondent graphical user interface.","2008-09","2023-07-05 07:18:09","2023-07-05 07:18:09","2023-07-05 07:18:09","51-64","","3","14","","J Braz Comp Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/862GIVWD/Fornari et al. - 2008 - Soundscape Design Through Evolutionary Engines.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A4MK7PGM","bookSection","2017","Yairi, Ikuko Eguchi","Designing Interfaces to Make Information More Tangible for Visually Impaired People","Universal Access in Human–Computer Interaction. Designing Novel Interactions","978-3-319-58702-8 978-3-319-58703-5","","","https://link.springer.com/10.1007/978-3-319-58703-5_27","This paper introduces our two research projects. One is to propose the graphic representation method with touch and sound as the universal designed touch screen interface for visually impaired people. Another is to investigate the good design of the collaborative work environment of the visually impaired. The proposed graphic representation method and interfaces are basic techniques for developing plug-ins which help blind people to use ordinary mass-produced computer devices with touch screens, such as smartphones and iPads. Our idea is so simple that musical scales enable users to trace graphics by their fingers and to memorize their position on the touch screen. Our recent progress including digital textbook application for visually impaired children is also reported. To investigate the design of the collaborative work environment, we have developed the collaborative music composition application with a tangible interface using daily goods that would attract the attention of both visually impaired and sighted people, and to induce collaborative communication among them. After evaluating this application, we focused our interest on the moment in which the visually impaired people are having fun, and on the factor of the excitement and concentration. This paper introduces our experimental system, which is a shooting game application without visual information, to investigate the factor of the excitement and concentration of the collaboration between visually impaired people. Recent analysis results of the collaboration are reported.","2017","2023-07-05 07:18:09","2023-07-21 05:10:42","2023-07-05 07:18:09","366-378","","","10278","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-58703-5_27","","","","","","Antona, Margherita; Stephanidis, Constantine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y2BXJBYR","journalArticle","2022","Sorensen, Vibeke; Lansing, John Stephen; Thummanapalli, Nagaraju; Cambria, Erik","Mood of the Planet: Challenging Visions of Big Data in the Arts","Cognitive Computation","","1866-9956, 1866-9964","10.1007/s12559-020-09766-w","https://link.springer.com/10.1007/s12559-020-09766-w","Mood of the Planet is an interactive physical-digital sculpture that has as its center-piece a large “arch” or “doorway” that emits colored light and sound as a form of visualization and sonification of the changing, live emotions expressed by people all around the Earth. It is the product of several disciplines, including the arts, computer science, linguistics and psychology. In particular, we use artificial intelligence to collect and analyze social media data and extract emotions from these using a brain-inspired and psychologically motivated emotion categorization model. Such emotions are then translated into colors and sounds that the audience can experience while passing through the arch. Feedback from the audience proved the Mood of the Planet to provide a more accurate, personal and tangible experience about the data-emotions dichotomy.","2022-01","2023-07-05 07:18:09","2023-07-19 23:45:24","2023-07-05 07:18:09","310-321","","1","14","","Cogn Comput","Mood of the Planet","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/5ZA2PWYI/Sorensen et al. - 2022 - Mood of the Planet Challenging Visions of Big Dat.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"95TML2SX","bookSection","2014","Holtzman, Benjamin; Candler, Jason; Turk, Matthew; Peter, Daniel","Seismic Sound Lab: Sights, Sounds and Perception of the Earth as an Acoustic Space","Sound, Music, and Motion","978-3-319-12975-4 978-3-319-12976-1","","","https://link.springer.com/10.1007/978-3-319-12976-1_10","We construct a representation of earthquakes and global seismic waves through sound and animated images. The seismic wave field is the ensemble of elastic waves that propagate through the planet after an earthquake, emanating from the rupture on the fault. The sounds are made by time compression (i.e. speeding up) of seismic data with minimal additional processing. The animated images are renderings of numerical simulations of seismic wave propagation in the globe. Synchronized sounds and images reveal complex patterns and illustrate numerous aspects of the seismic wave field. These movies represent phenomena occurring far from the time and length scales normally accessible to us, creating a profound experience for the observer. The multi-sensory perception of these complex phenomena may also bring new insights to researchers.","2014","2023-07-05 07:18:09","2023-07-21 05:01:30","2023-07-05 07:18:09","161-174","","","8905","","","Seismic Sound Lab","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-12976-1_10","","","","","","Aramaki, Mitsuko; Derrien, Olivier; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VB74FRL5","journalArticle","2005","Zotkin, Dmitry N.; Chi, Taishih; Shamma, Shihab A.; Duraiswami, Ramani","Neuromimetic Sound Representation for Percept Detection and Manipulation","EURASIP Journal on Advances in Signal Processing","","1687-6180","10.1155/ASP.2005.1350","https://asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.1350","The acoustic wave received at the ears is processed by the human auditory system to separate different sounds along the intensity, pitch, and timbre dimensions. Conventional Fourier-based signal processing, while endowed with fast algorithms, is unable to easily represent a signal along these attributes. In this paper, we discuss the creation of maximally separable sounds in auditory user interfaces and use a recently proposed cortical sound representation, which performs a biomimetic decomposition of an acoustic signal, to represent and manipulate sound for this purpose. We briefly overview algorithms for obtaining, manipulating, and inverting a cortical representation of a sound and describe algorithms for manipulating signal pitch and timbre separately. The algorithms are also used to create sound of an instrument between a “guitar” and a “trumpet.” Excellent sound quality can be achieved if processing time is not a concern, and intelligible signals can be reconstructed in reasonable processing time (about ten seconds of computational time for a one-second signal sampled at 8 kHz). Work on bringing the algorithms into the real-time processing domain is ongoing.","2005-12","2023-07-05 07:18:09","2023-07-20 00:01:28","2023-07-05 07:18:09","486137","","9","2005","","EURASIP J. Adv. Signal Process.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/6PHT4UWJ/Zotkin et al. - 2005 - Neuromimetic Sound Representation for Percept Dete.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JTEBYB2B","bookSection","2006","Frauenberger, C; Stockman, T; Putz, V; Höldrich, R","Design Patterns for Auditory Displays","People and Computers XIX — The Bigger Picture","978-1-84628-192-1 978-1-84628-249-2","","","http://link.springer.com/10.1007/1-84628-249-7_30","This paper proposes the use of patterns in the design process for auditory displays and /or interfaces realized in other modalities. We introduce a meta-domain in which user interfaces can be designed using these patterns without determining their means of realization. The mode-independent description of such interfaces can then be used to create the real interface maintaining the strengths of the different interaction channels. While this work is focused on how this approach can be applied on auditory displays, we keep in mind that the approach shall be applicable on other interaction modalities equally. The development of a set of mode independent interaction patterns is shown along with descriptions of their representations in the auditory domain. A real world application was chosen to evaluate the approach; Microsoft Explorer was analysed, described through the mode independent interaction patterns and transformed into the auditory domain making extensive use of 3D audio rendering techniques. The result, a file manager created in a virtual audio environment, was evaluated with sighted persons as well as visually impaired and blind participants showing the feasibility and usability of the approach.","2006","2023-07-05 07:18:09","2023-07-21 04:42:20","2023-07-05 07:18:09","473-488","","","","","","","","","","","Springer London","London","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/1-84628-249-7_30","","","","","","McEwan, Tom; Gulliksen, Jan; Benyon, David","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T9FV3STF","journalArticle","2021","Woods, Carl T.; Araújo, Duarte; Davids, Keith; Rudd, James","From a Technology That Replaces Human Perception–Action to One That Expands It: Some Critiques of Current Technology Use in Sport","Sports Medicine - Open","","2199-1170, 2198-9761","10.1186/s40798-021-00366-y","https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-021-00366-y","Abstract                            Information technology has been integrated into most areas of sport, providing new insights, improving the efficiency of operational processes, and offering unique opportunities for exploration and inquiry. While acknowledging this positive impact, this paper explores whether sufficient consideration has been directed towards what technology risks detracting from the learning and developmental experiences of its users. Specifically, viewed through the philosophical lens of the device paradigm, and considering a more ecological account of technological implementation, we discuss how technology               use               in sport could subtly disengage educators and applied sports scientists from performance environments. Insights gleaned from such an ecological account of technology implementation could lead sports science and educational teams to ask and reflect on tough questions of current practice: i.e.               has too much control been given to technological devices to ‘solve’ problems and communicate knowledge (about) in sport? Has technology improved the skills of players and performance staff? Or are performance staff at risk of becoming over-reliant on technology, and as a result, reducing the value of experiential knowledge (of) and intuition?               Questions like these should be asked if technological devices, purported to support aspects of practice, are continually integrated into the sporting landscape.","2021-12","2023-07-05 07:18:09","2023-07-05 07:18:09","2023-07-05 07:18:09","76","","1","7","","Sports Med - Open","From a Technology That Replaces Human Perception–Action to One That Expands It","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/JR2K3IPF/Woods et al. - 2021 - From a Technology That Replaces Human Perception–A.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CYGUN2QM","journalArticle","2022","Ochsner, Beate; Spöhrer, Markus; Stock, Robert","Rethinking Assistive Technologies: Users, Environments, Digital Media, and App-Practices of Hearing","NanoEthics","","1871-4757, 1871-4765","10.1007/s11569-020-00381-5","https://link.springer.com/10.1007/s11569-020-00381-5","Abstract                            Against the backdrop of an aging world population increasingly affected by a diverse range of abilities and disabilities as well as the rise of ubiquitous computing and digital app cultures, this paper questions how mobile technologies mediate between heterogeneous environments and sensing beings. To approach the current technological manufacturing of the senses, two lines of thought are of importance: First, there is a need to critically reflect upon the concept of assistive technologies (AT) as artifacts providing tangible solutions for a specific disability. Second, the conventional distinction between user and environment requires a differentiated consideration. This contribution will first review James Gibson’s concept of “affordances” and modify this approach by introducing theories and methods of Science and Technology Studies (STS) and Actor-Network Theory (ANT). Then, we present two case studies where we explore the relations between recent “assistive” app technologies and human sensory perception. As hearing and seeing are key in this regard, we concentrate on two specific media technologies: ReSound LINX               2               , a hearing aid which allows for direct connect (via Bluetooth) with iPhone, iPad, or iPod Touch, and Camassia, an IOS app for sonic wayfinding for blind people. We emphasize the significance of dis-/abling practices for manufacturing novel forms of hearing and seeing and drawing on sources like promotional materials by manufacturers, ads, or user testimonials and reviews. Our analysis is interested in the reciprocal relationships between users and their socio-technical and media environments. By and large, this contribution will provide crucial insights into the contemporary entanglement of algorithm-driven technologies, daily practices, and sensing subjects: the production of techno-sensory arrangements.","2022-04","2023-07-05 07:18:09","2023-07-05 07:18:09","2023-07-05 07:18:09","65-79","","1","16","","Nanoethics","Rethinking Assistive Technologies","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/P2XK89IR/Ochsner et al. - 2022 - Rethinking Assistive Technologies Users, Environm.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FXJL5UBP","journalArticle","2008","Evreinova, Tatiana V.; Evreinov, Grigori; Raisamo, Roope","A camera-joystick for sound-augmented non-visual navigation and target acquisition: a case study","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-007-0109-5","http://link.springer.com/10.1007/s10209-007-0109-5","This paper presents the results of a comparative study of user input with a camera-joystick and a manual joystick used in a target acquisition task when neither targets nor pointer could be perceived visually. The camera-joystick is an input technique in which each on-screen item is accessible from the center with a predefined vector of head motion. Absolute pointing was implemented with an acceleration factor of 1.7 and a moving average on 5 detected head positions. The underlying assumption was that, in order to provide a robust input for blind users, the interaction technique has to be based on perceptually well-discriminated human movements, which compose a basic framework of an accessible virtual workspace demanding minimum external auxiliary cues. The target spots, having a diameter of 35 mm and a distance between the centers of adjacent spots of 60 mm, were arranged in a rectangular grid of 5 rows by 5 columns. The targets were captured from a distance of 600 mm. The results have shown that the camera input is a promising technique for non-visual human–computer interaction. The subjects demonstrated, more than twice, better performance in the target acquisition task with the camera-joystick versus the manual joystick. All the participants reported that the camera-joystick was a robust and preferable input technique when visual information was not available. Blind interaction techniques could be significantly further improved allowing a user-dependent activation of the navigational cues to better coordinate feedbacks with exploratory behavior.","2008-09","2023-07-05 07:18:09","2023-07-21 05:11:23","2023-07-05 07:18:09","129-144","","3","7","","Univ Access Inf Soc","A camera-joystick for sound-augmented non-visual navigation and target acquisition","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KZXJ3CVL","journalArticle","2016","Lech, Michal; Kostek, Bozena; Czyzewski, Andrzej","Multimedia polysensory integration training system dedicated to children with educational difficulties","Journal of Intelligent Information Systems","","0925-9902, 1573-7675","10.1007/s10844-015-0390-3","http://link.springer.com/10.1007/s10844-015-0390-3","This paper aims at presenting a multimedia system providing polysensory training for pupils with educational difficulties. The particularly interesting aspect of the system lies in the sonic interaction with image projection in which sounds generated lead to stimulation of a particular part of the human brain. The system architecture, video processing methods, therapeutic exercises and guidelines for children’s interaction with the system are presented. Results of pupils’ improvements after several weeks of exercising with the system are provided. The outcome of this study suggests that learning and developing through the interactive method helped to improve children’s spatial orientation skills.","2016-12","2023-07-05 07:18:09","2023-07-20 06:47:34","2023-07-05 07:18:09","531-552","","3","47","","J Intell Inf Syst","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/SEMICDAM/Lech et al. - 2016 - Multimedia polysensory integration training system.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H8AU69I9","journalArticle","2017","Jakus, Grega; Stojmenova, Kristina; Tomažič, Sašo; Sodnik, Jaka","A system for efficient motor learning using multimodal augmented feedback","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-016-3774-7","http://link.springer.com/10.1007/s11042-016-3774-7","Numerous studies have established that using various forms of augmented feedback improves human motor learning. In this paper, we present a system that enables real-time analysis of motion patterns and provides users with objective information on their performance of an executed set of motions. This information can be used to identify individual segments of improper motion early in the learning process, thus preventing improperly learned motion patterns that can be difficult to correct once fully learned. The primary purpose of the proposed system is to serve as a general tool in the research on impact of different feedback modalities on the process of motor learning, for example, in sports or rehabilitation. The key advantages of the system are high-speed and high-accuracy tracking, as well as its flexibility, as it supports various types of feedback (auditory and visual, concurrent or terminal). The practical application of the proposed system is demonstrated through the example of learning a golf swing.","2017-10","2023-07-05 07:18:09","2023-07-21 04:32:55","2023-07-05 07:18:09","20409-20421","","20","76","","Multimed Tools Appl","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z53T4A3K","bookSection","2019","Mannone, Maria; Favali, Federico","Categories, Musical Instruments, and Drawings: A Unification Dream","Mathematics and Computation in Music","978-3-030-21391-6 978-3-030-21392-3","","","http://link.springer.com/10.1007/978-3-030-21392-3_5","The mathematical formalism of category theory allows to investigate musical structures at both low and high levels, performance practice (with musical gestures) and music analysis. Mathematical formalism can also be used to connect music with other disciplines such as visual arts. In our analysis, we extend former studies on category theory applied to musical gestures, including musical instruments and playing techniques. Some basic concepts of categories may help navigate within the complexity of several branches of contemporary music research, giving it a unitarian character. Such a ‘unification dream,’ that we can call ‘cARTegory theory,’ also includes metaphorical references to topos theory.","2019","2023-07-05 07:18:09","2023-07-21 04:28:36","2023-07-05 07:18:09","59-72","","","11502","","","Categories, Musical Instruments, and Drawings","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-21392-3_5","","","","","","Montiel, Mariana; Gomez-Martin, Francisco; Agustín-Aquino, Octavio A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5IKV8DBT","bookSection","2022","Rhodes, Chris; Allmendinger, Richard; Climent, Ricardo","Classifying Biometric Data for Musical Interaction Within Virtual Reality","Artificial Intelligence in Music, Sound, Art and Design","978-3-031-03788-7 978-3-031-03789-4","","","https://link.springer.com/10.1007/978-3-031-03789-4_25","Since 2015, commercial gestural interfaces have widened accessibility for researchers and artists to use novel Electromyographic (EMG) biometric data. EMG data measures musclar amplitude and allows us to enhance Human-Computer Interaction (HCI) through providing natural gestural interaction with digital media. Virtual Reality (VR) is an immersive technology capable of simulating the real world and abstractions of it. However, current commercial VR technology is not equipped to process and use biometric information. Using biometrics within VR allows for better gestural detailing and use of complex custom gestures, such as those found within instrumental music performance, compared to using optical sensors for gesture recognition in current commercial VR equipment. However, EMG data is complex and machine learning must be used to employ it. This study uses a Myo armband to classify four custom gestures in Wekinator and observe their prediction accuracies and representations (including or omitting signal onset) to compose music within VR. Results show that specific regression and classification models, according to gesture representation type, are the most accurate when classifying four music gestures for advanced music HCI in VR. We apply and record our results, showing that EMG biometrics are promising for future interactive music composition systems in VR.","2022","2023-07-05 07:18:09","2023-07-19 11:34:01","2023-07-05 07:18:09","385-400","","","13221","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-031-03789-4_25","","/Users/minsik/Zotero/storage/AIF88GPN/Rhodes et al. - 2022 - Classifying Biometric Data for Musical Interaction.pdf","","","","Martins, Tiago; Rodríguez-Fernández, Nereida; Rebelo, Sérgio M.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DIQA6TNM","bookSection","2022","Kania, Damian; Szurmik, Tomasz; Bibrowicz, Karol; Romaniszyn-Kania, Patrycja; Czak, Mirosław; Mańka, Anna; Rosiak, Maria; Turner, Bruce; Pollak, Anita; Mitas, Andrzej W.","The Effect of Therapeutic Commands on the Teaching of Maintaining Correct Static Posture","Information Technology in Biomedicine","978-3-031-09134-6 978-3-031-09135-3","","","https://link.springer.com/10.1007/978-3-031-09135-3_33","The article presents the results of a preliminary study analy-sing the physiological parameters obtained during exercises that teach the patient’s correct body posture while sitting. Electrodermal activity (EDA), blood volume pulse (BVP), and electromyographic (EMG) signals were recorded and analysed during the training process for position shaping. A music preference and musicality questionnaire was carried out before the study. The JAWS questionnaire was completed twice by the respondent, before and after exercises. The physiotherapists provided instructions with respect to the stimulation of the autonomic nervous system, observed in EDA, heart rate and the subsequent motor units. While performing the exercises, the subjects felt positive emotions, which can be perceived as a positive experience for the probands and suggests their willingness to learn and maintain correct body posture while sitting. The sonification of the therapist’s commands and their sonic emotional content is further researched.","2022","2023-07-05 07:18:09","2023-07-20 06:34:24","2023-07-05 07:18:09","393-405","","","1429","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Advances in Intelligent Systems and Computing DOI: 10.1007/978-3-031-09135-3_33","","","","","","Pietka, Ewa; Badura, Pawel; Kawa, Jacek; Wieclawek, Wojciech","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6JBPVVA4","journalArticle","2020","Rozé, Jocelyn; Aramaki, Mitsuko; Kronland-Martinet, Richard; Ystad, Sølvi","Cellists’ sound quality is shaped by their primary postural behavior","Scientific Reports","","2045-2322","10.1038/s41598-020-70705-8","https://www.nature.com/articles/s41598-020-70705-8","During the last 20 years, the role of musicians’ body movements has emerged as a central question in instrument practice: Why do musicians make so many postural movements, for instance, with their torsos and heads, while playing musical instruments? The musical significance of such ancillary gestures is still an enigma and therefore remains a major pedagogical challenge, since one does not know if these movements should be considered essential embodied skills that improve musical expressivity. Although previous studies established clear connections between musicians’ body movements and musical structures (particularly for clarinet, piano or violin performances), no evidence of direct relationships between body movements and the quality of the produced timbre has ever been found. In this study, focusing on the area of bowed-string instruments, we address the problem by showing that cellists use a set of primary postural directions to develop fluid kinematic bow features (velocity, acceleration) that prevent the production of poor quality (i.e., harsh, shrill, whistling) sounds. By comparing the body-related angles between normal and posturally constrained playing situations, our results reveal that the chest rotation and vertical inclination made by cellists act as coordinative support for the kinematics of the bowing gesture. These findings support the experimental works of Alexander, especially those that showed the role of head movements with respect to the upper torso (the so-called primary control) in ensuring the smooth transmission of fine motor control in musicians all the way to the produced sound. More generally, our research highlights the importance of focusing on this fundamental postural sense to improve the quality of human activities across different domains (music, dance, sports, rehabilitation, working positions, etc.).","2020-08-17","2023-07-05 07:19:05","2023-07-05 07:19:05","2023-07-05 07:19:05","13882","","1","10","","Sci Rep","","","","","","","","en","2020 The Author(s)","","","","www.nature.com","","Number: 1 Publisher: Nature Publishing Group","","/Users/minsik/Zotero/storage/VUV6HSIR/Rozé et al. - 2020 - Cellists’ sound quality is shaped by their primary.pdf","","","Acoustics; Skeleton","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C57BXEDK","journalArticle","2006","Barrass, Stephen; Barrass, Tim","Musical creativity in collaborative virtual environments","Virtual Reality","","1359-4338, 1434-9957","10.1007/s10055-006-0043-5","http://link.springer.com/10.1007/s10055-006-0043-5","A review of musical creativity in collaborative virtual environments (CVE) shows recurring interaction metaphors that tend from precise control of individual parameters to higher level gestural influence over whole systems. Musical performances in CVE also show a consistent re-emergence of a unique form of collaboration called “melding” in which individual virtuosity is subsumed to that of the group. Based on these observations, we hypothesized that CVE could be a medium for creating new forms of music, and developed the audiovisual augmented reality system (AVIARy) to explore higher level metaphors for composing spatial music in CVE. This paper describes the AVIARy system, the initial experiments with interaction metaphors, and the application of the system to develop and stage a collaborative musical performance at a sound art concert. The results from these experiments indicate that CVE can be a medium for new forms of musical creativity and distinctive forms of music.","2006-09-27","2023-07-05 07:21:00","2023-07-21 05:14:03","2023-07-05 07:21:00","149-157","","2","10","","Virtual Reality","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V2CEYTXK","journalArticle","2022","Wu, Yongmeng; Bryan-Kinns, Nick; Zhi, Jinyi","Exploring visual stimuli as a support for novices’ creative engagement with digital musical interfaces","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-022-00393-3","https://link.springer.com/10.1007/s12193-022-00393-3","Visual materials are a widely used tool for stimulating creativity. This paper explores the potential for visual stimuli to support novices’ creative engagement with multimodal digital musical interfaces. An empirical study of 24 participants was conducted to compare the effect of abstract and literal forms of graphical scores on novices’ creative engagement, and whether being informed or uninformed about meanings of symbols in the score had any impact on creative engagement. The results suggest that abstract visual stimuli can provide an effective scaffold for creative engagement when participants are not informed about their design. It was found that providing information about visual stimuli has both advantages and disadvantages, depending largely on the stimuli’s visual style. Being informed about the meaning of a literal visual stimuli helped participants in making interpretations and gaining inspiration, whereas having information about abstract stimuli led to frustration. Qualitative data indicates that both forms of visual stimuli support creative engagement but at different stages of a creative process, and a descriptive model is presented to explain this. The findings highlight the benefits of visual stimuli in supporting creative engagement in the process of music making – a multimodal interaction domain typically involving few or no visual activities.","2022-09","2023-07-05 07:21:00","2023-07-20 07:05:59","2023-07-05 07:21:00","343-356","","3","16","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2Q3DHWJU","bookSection","2011","Merer, Adrien; Ystad, Sølvi; Kronland-Martinet, Richard; Aramaki, Mitsuko","Abstract Sounds and Their Applications in Audio and Perception Research","Exploring Music Contents","978-3-642-23125-4 978-3-642-23126-1","","","http://link.springer.com/10.1007/978-3-642-23126-1_12","Recognition of sound sources and events is an important process in sound perception and has been studied in many research domains. Conversely sounds that cannot be recognized are not often studied except by electroacoustic music composers. Besides, considerations on recognition of sources might help to address the problem of stimulus selection and categorization of sounds in the context of perception research. This paper introduces what we call abstract sounds with the existing musical background and shows their relevance for different applications.","2011","2023-07-05 07:21:00","2023-07-20 00:05:42","2023-07-05 07:21:00","176-187","","","6684","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-23126-1_12","","/Users/minsik/Zotero/storage/VJMSD29S/Merer et al. - 2011 - Abstract Sounds and Their Applications in Audio an.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W9XWQJVC","bookSection","2011","Makeig, Scott; Leslie, Grace; Mullen, Tim; Sarma, Devpratim; Bigdely-Shamlo, Nima; Kothe, Christian","First Demonstration of a Musical Emotion BCI","Affective Computing and Intelligent Interaction","978-3-642-24570-1 978-3-642-24571-8","","","http://link.springer.com/10.1007/978-3-642-24571-8_61","Development of EEG-based brain computer interface (BCI) methods has largely focused on creating a communication channel for subjects with intact cognition but profound loss of motor control from stroke or neurodegenerative disease that allows such subjects to communicate by spelling out words on a personal computer. However, other important human communication channels may also be limited or unavailable for handicapped subjects -- direct non-linguistic emotional communication by gesture, vocal prosody, facial expression, etc.. We report and examine a first demonstration of a musical ‘emotion BCI’ in which, as one element of a live musical performance, an able-bodied subject successfully engaged the electronic delivery of an ordered sequence of five music two-tone bass frequency drone sounds by imaginatively re-experiencing the human feeling he had spontaneously associated with the sound of each drone sound during training sessions. The EEG data included activities of both brain and non-brain sources (scalp muscles, eye movements). Common Spatial Pattern classification gave 84% correct pseudo-online performance and 5-of-5 correct classification in live performance. Re-analysis of the training session data including only the brain EEG sources found by multiple-mixture Amica ICA decomposition achieved five-class classification accuracy of 59-70%, confirming that different voluntary emotion imagination experiences may be associated with distinguishable brain source EEG dynamics.","2011","2023-07-05 07:21:00","2023-07-19 11:13:24","2023-07-05 07:21:00","487-496","","","6975","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-24571-8_61","","/Users/minsik/Zotero/storage/IMIB4MXK/Makeig et al. - 2011 - First Demonstration of a Musical Emotion BCI.pdf","","","","D’Mello, Sidney; Graesser, Arthur; Schuller, Björn; Martin, Jean-Claude","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RX97YFC5","bookSection","2017","Arango, Julián Jaramillo","Sound Interaction Design and Creation in the Context of Urban Space","Bridging People and Sound","978-3-319-67737-8 978-3-319-67738-5","","","http://link.springer.com/10.1007/978-3-319-67738-5_8","This paper reports current theoretical and creative results of a postdoctoral research study entitled Sound Design for Urban Spaces. It focuses on the design process of novel audio devices and on encouraging people in transit through the city to explore technology-empowered listening strategies. In this paper, the practice of urban sound design will be raised and some conceptual contributions concerning listening, acoustic analysis and sonic interaction will be discussed. We will extract some design insights that have been inspiring in the creation process of the Smartphone Ensemble, The AirQ Jacket and Lumina Nocte. These projects have been got under way along with MA students from the Design and Creation program at the Caldas University in Manizales, Colombia.","2017","2023-07-05 07:21:00","2023-07-19 23:36:53","2023-07-05 07:21:00","137-149","","","10525","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-67738-5_8","","","","","","Aramaki, Mitsuko; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JWNSGHSY","bookSection","2017","Sinclair, Peter; Cahen, Roland; Tanant, Jonathan; Gena, Peter","New Atlantis: Audio Experimentation in a Shared Online World","Bridging People and Sound","978-3-319-67737-8 978-3-319-67738-5","","","http://link.springer.com/10.1007/978-3-319-67738-5_14","Computer games and virtual worlds are ""traditionally"" visually orientated, and their audio dimension often secondary. In this paper we will describe New Atlantis a virtual world that aims to put sound first. We will describe the motivation, the history and the development of this FrancoAmerican project and the serendipitous use made of the distance between partner structures. We explain the overall architecture of the world and discuss the reasons for certain key structural choices. New Atlantis' first aim is to provide a platform for audio-graphic design and practice, for students as well as artists and researchers, engaged in higher education art or media curricula. We describe the integration of student’s productions through workshops and exchanges and discuss and the first public presentations of NA that took place from January 2016. Finally we will unfold perspectives for future research and the further uses of New Atlantis.","2017","2023-07-05 07:21:00","2023-07-19 23:37:15","2023-07-05 07:21:00","229-246","","","10525","","","New Atlantis","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-67738-5_14","","/Users/minsik/Zotero/storage/94G8V58N/Sinclair et al. - 2017 - New Atlantis Audio Experimentation in a Shared On.pdf","","","","Aramaki, Mitsuko; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UUN6243X","bookSection","2011","Riedenklau, Eckard; Hermann, Thomas; Ritter, Helge","Saving and Restoring Mechanisms for Tangible User Interfaces through Tangible Active Objects","Human-Computer Interaction. Interaction Techniques and Environments","978-3-642-21604-6 978-3-642-21605-3","","","http://link.springer.com/10.1007/978-3-642-21605-3_12","In this paper we present a proof of concept for saving and restoring mechanisms for Tangible User Interfaces (TUIs). We describe our actuated Tangible Active Objects (TAOs) and explain the design which allows equal user access to a dial-based fully tangible actuated menu metaphor. We present a new application extending an existing TUI for interactive sonification of process data with saving and restoring mechanisms and we outline another application proposal for family therapists.","2011","2023-07-05 07:21:00","2023-07-20 06:32:42","2023-07-05 07:21:00","110-118","","","6762","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-21605-3_12","","/Users/minsik/Zotero/storage/7443DGDV/Riedenklau et al. - 2011 - Saving and Restoring Mechanisms for Tangible User .pdf","","","","Jacko, Julie A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KLU2NE9F","bookSection","2007","Taylor, Robyn; Kazakevich, Maryia; Boulanger, Pierre; Garcia, Manuel; Bischof, Walter F.","Multi-modal Interface for Fluid Dynamics Simulations Using 3–D Localized Sound","Smart Graphics","978-3-540-73213-6 978-3-540-73214-3","","","http://link.springer.com/10.1007/978-3-540-73214-3_17","Multi-modal capabilities can be added to a simulation system in order to enhance data comprehension. We describe a system for adding sonification capabilities to a real-time computational fluid dynamics (CFD) simulator. Our system uses Max/MSP modules to add sonic properties to CFD solutions. The enhancements described in this paper allow users to locate sound sources in a 3–D environment using stereo auditory cues to identify data features.","2007","2023-07-05 07:21:00","2023-07-21 04:59:55","2023-07-05 07:21:00","182-187","","","4569","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73214-3_17","","/Users/minsik/Zotero/storage/H5BA33JS/Taylor et al. - 2007 - Multi-modal Interface for Fluid Dynamics Simulatio.pdf","","","","Butz, Andreas; Fisher, Brian; Krüger, Antonio; Olivier, Patrick; Owada, Shigeru","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XAXJBXM7","bookSection","2016","Mascetti, Sergio; Rossetti, Chiara; Gerino, Andrea; Bernareggi, Cristian; Picinali, Lorenzo; Rizzi, Alessandro","Towards a Natural User Interface to Support People with Visual Impairments in Detecting Colors","Computers Helping People with Special Needs","978-3-319-41263-4 978-3-319-41264-1","","","http://link.springer.com/10.1007/978-3-319-41264-1_23","A mobile application that detects an item’s color is potentially very useful for visually impaired people. However, users could run into difficulties when centering the target item in the mobile device camera field of view. To address this problem, in this contribution we propose a mobile application that detects the color of the item pointed by the user with one finger. In its current version, the application requires the user to wear a marker on the finger used for pointing. A preliminary evaluation conducted with blind users confirms the usefulness of the application, and encourages further development.","2016","2023-07-05 07:21:00","2023-07-19 23:52:14","2023-07-05 07:21:00","171-178","","","9758","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-41264-1_23","","/Users/minsik/Zotero/storage/7T9TQ6SU/Mascetti et al. - 2016 - Towards a Natural User Interface to Support People.pdf","","","","Miesenberger, Klaus; Bühler, Christian; Penaz, Petr","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YPCMAJ59","bookSection","2019","Sakhardande, Prabodh; Joshi, Anirudha; Jadhav, Charudatta; Joshi, Manjiri","Comparing User Performance on Parallel-Tone, Parallel-Speech, Serial-Tone and Serial-Speech Auditory Graphs","Human-Computer Interaction – INTERACT 2019","978-3-030-29380-2 978-3-030-29381-9","","","http://link.springer.com/10.1007/978-3-030-29381-9_16","Visualization techniques such as bar graphs and pie charts let sighted users quickly understand and explore numerical data. These techniques remain by and large inaccessible for visually impaired users. Even when these are made accessible, they remain slow and cumbersome, and not as useful as they might be to sighted users. Previous research has studied two methods of improving perception and speed of navigating auditory graphs - using non-speech audio (such as tones) instead of speech to communicate data and using two audio streams in parallel instead of in series. However, these studies were done in the early 2000s and speech synthesis techniques have improved considerably in recent times, as has the familiarity of visually impaired users with smartphones and speech systems. We systematically compare user performance on four modes that can be used for the generation of auditory graphs: parallel-tone, parallel-speech, serialtone, and serial-speech. We conducted two within-subjects studies - one with 20 sighted users and the other with 20 visually impaired users. Each user group performed point estimation and point comparison tasks with each technique on two sizes of bar graphs. We assessed task time, errors and user preference. We found that while tone was faster than speech, speech was more accurate than tone. The parallel modality was faster than serial modality and visually impaired users were faster than their sighted counterparts. Further, users showed a strong personal preference towards the serial-speech technique. To the best of our knowledge, this is the first empirical study that systematically compares these four techniques.","2019","2023-07-05 07:21:00","2023-07-20 06:29:25","2023-07-05 07:21:00","247-266","","","11746","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-29381-9_16","","/Users/minsik/Zotero/storage/FI8GS2KP/Sakhardande et al. - 2019 - Comparing User Performance on Parallel-Tone, Paral.pdf","","","","Lamas, David; Loizides, Fernando; Nacke, Lennart; Petrie, Helen; Winckler, Marco; Zaphiris, Panayiotis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NF9GB98Q","bookSection","2012","Paalasmaa, Joonas; Murphy, David J.; Holmqvist, Ove","Analysis of Noisy Biosignals for Musical Performance","Advances in Intelligent Data Analysis XI","978-3-642-34155-7 978-3-642-34156-4","","","http://link.springer.com/10.1007/978-3-642-34156-4_23","Biosignal sensors are now small, affordable, and wireless. We desire to include such sensors (e.g. heart rate, respiration, acceleration) in a live musical performance, which sets requirements on the reliability and variability of the data. Unfortunately the raw signals from such devices are unable to meet these requirements. We contribute our solutions for overcoming the shortcomings of these sensors in two parts. The first is an online data processing and analysis system, including on-line generative models that describe the signals but add consistency. The second is the end-to-end system for capturing wireless signal data for the analysis system and integrating the resulting output into a popular digital audio workstation in a very flexible manner conducive to live performance. We also explore the role of “analysis supervisor”—a member of the performing act who ensures that the results of biosignal analysis fall within the desired ranges to contribute to the music effectively.","2012","2023-07-05 07:21:00","2023-07-19 11:12:11","2023-07-05 07:21:00","241-252","","","7619","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-34156-4_23","","","","","","Hollmén, Jaakko; Klawonn, Frank; Tucker, Allan","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6PF6KEW8","bookSection","2009","Suzuki, Kenji","Embodied Sound Media Technology for the Enhancement of the Sound Presence","Human-Computer Interaction. Novel Interaction Methods and Techniques","978-3-642-02576-1 978-3-642-02577-8","","","http://link.springer.com/10.1007/978-3-642-02577-8_82","In this paper, the paradigms of Embodied Sound Media (ESM) technology are described with several case studies. The ESM is designed to formalize a musical sound-space based on the conversion of free human movement into sounds. This technology includes the measurement of human motion, processing, acoustic conversion and output. The first idea was to introduce direct and intuitive sound feedbacks within the context of not only embodied interaction between humans and devices but also social interaction among humans. The developed system is a sort of active aid for an embodied performance that allows the users to get feedback for emotional stimuli in terms of sound surrounding the users. The overviews of several devices developed in this scenario and the potential applications to physical fitness, exercise, entertainment, assistive technology and rehabilitation are also addressed.","2009","2023-07-05 07:21:00","2023-07-20 06:32:55","2023-07-05 07:21:00","745-751","","","5611","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-02577-8_82","","/Users/minsik/Zotero/storage/NGQEEDSV/Suzuki - 2009 - Embodied Sound Media Technology for the Enhancemen.pdf","","","","Jacko, Julie A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2Z8NDBVP","bookSection","2008","Vy, Quoc V.; Mori, Jorge A.; Fourney, David W.; Fels, Deborah I.","EnACT: A Software Tool for Creating Animated Text Captions","Computers Helping People with Special Needs","978-3-540-70539-0 978-3-540-70540-6","","","http://link.springer.com/10.1007/978-3-540-70540-6_87","Music in captioning is often represented by only its title and/or a music note. This representation provides little to no information of the intended effect or emotion of the music. In this paper, we present a software tool that was created to enable users to mark emotions in a script or lyrics and then render those marks into animated text for display as captions. A pilot study was conducted to collect initial responses to, preferences and understanding of the animated lyrics of one song by a deaf and hard of hearing audience. Participants were able to identify the animated lyrics as belonging to a song and found that the animations helped them understand the portrayed emotions. They also identified the shaking style of animation portraying fear as least preferable.","2008","2023-07-05 07:21:00","2023-07-19 23:53:40","2023-07-05 07:21:00","609-616","","","5105","","","EnACT","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-70540-6_87","","","","","","Miesenberger, Klaus; Klaus, Joachim; Zagler, Wolfgang; Karshmer, Arthur","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QDZ38DTX","bookSection","2010","Valle, Andrea; Lombardo, Vincenzo; Schirosa, Mattia","Simulating the Soundscape through an Analysis/Resynthesis Methodology","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_17","This paper presents a graph-based system for the dynamic generation of soundscapes and its implementation in an application that allows for an interactive, real-time exploration of the resulting soundscapes. The application can be used alone, as a pure sonic exploration device, but can also be integrated into a virtual reality engine. In this way, the soundcape can be acoustically integrated in the exploration of an architectonic/urbanistic landscape. The paper is organized as follows: after taking into account the literature on soundscape, we provide a formal definition of the concept; then, a model is introduced, and finally, we describe a software application together with a case-study.","2010","2023-07-05 07:21:00","2023-07-19 11:38:50","2023-07-05 07:21:00","330-357","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_17","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5QMF9SNP","bookSection","2010","Gygi, Brian; Shafiro, Valeriy","From Signal to Substance and Back: Insights from Environmental Sound Research to Auditory Display Design","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_16","","2010","2023-07-05 07:21:00","2023-07-05 07:21:00","2023-07-05 07:21:00","306-329","","","5954","","","From Signal to Substance and Back","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_16","","/Users/minsik/Zotero/storage/RWBXT8ZF/Gygi and Shafiro - 2010 - From Signal to Substance and Back Insights from E.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M3CJCR8B","bookSection","2014","Neidlinger, Kristin; Ju, Wendy","Sound Bending – Talking Bodies Quantum Sound Suits","Design, User Experience, and Usability. User Experience Design for Diverse Interaction Platforms and Environments","978-3-319-07625-6 978-3-319-07626-3","","","http://link.springer.com/10.1007/978-3-319-07626-3_56","The QuantumSound Suits are an innovative technological solution for creating sounds from movement. Made in collaboration with contortionists, a multidisciplinary team designed custom body-painted silicone suits embedded with flexible sensors. A healing sound artist mapped the tones of the eleven sensors to movement, animating the physical motion and providing sonic feedback of body’s position. This is an exploration of real-time movement notation and human activity recognition of body location in space.","2014","2023-07-05 07:21:00","2023-07-19 23:55:39","2023-07-05 07:21:00","598-605","","","8518","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-07626-3_56","","/Users/minsik/Zotero/storage/TRVLALV4/Neidlinger and Ju - 2014 - Sound Bending – Talking Bodies Quantum Sound Suits.pdf","","","","Marcus, Aaron","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Kobsa, Alfred; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Doug; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VUHVMM9U","journalArticle","2021","Kolel-Veetil, Manoj; Sen, Ayusman; Buehler, Markus J.","Surface adhesion of viruses and bacteria: Defend only and/or vibrationally extinguish also?! A perspective","MRS Advances","","2059-8521","10.1557/s43580-021-00079-0","https://link.springer.com/10.1557/s43580-021-00079-0","Coronaviruses COVID-19, SARS-CoV and NL63 use spikes in their corona to bind to angiotensin converting enzyme 2 (ACE2) sites on cytoskeletal membranes of host cells to deliver their viral payload. While groups such as disulfides in ACE2’s zinc metallopeptidase, and also in COVID-19’s spikes, facilitate such binding, it is worth exploring how similar complementary sites on materials such as polymers, metals, ceramics, fabrics, and biomaterials promote binding of viruses and bacteria and how they could be further engineered to prevent bioactivity, or to act as agents to collect viral payloads in filters or similar devices. In that vein, this article offers a perspective on novel tools and approaches for chemically and topologically modifying most utilitarian surfaces via defensive topological vibrational engineering to either prevent such adhesion or to enhance adhesion and elicit vibrational characteristics/’musical signatures’ from the surfaces so that the structure of the binding sites of viruses and bacteria is permanently altered and/or their cellular machinery is permanently disabled by targeted chemical transformations.","2021-06","2023-07-05 07:21:00","2023-07-21 04:30:22","2023-07-05 07:21:00","355-361","","13","6","","MRS Advances","Surface adhesion of viruses and bacteria","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/4UKRKN7T/Kolel-Veetil et al. - 2021 - Surface adhesion of viruses and bacteria Defend o.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9ZI6JH3A","bookSection","2010","Wersényi, György","Auditory Representations of a Graphical User Interface for a Better Human-Computer Interaction","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_5","As part of a project to improve human computer interaction mostly for blind users, a survey with 50 blind and 100 sighted users included a questionnaire about their user habits during everyday use of personal computers. Based on their answers, the most important functions and applications were selected and results of the two groups were compared. Special user habits and needs of blind users are described. The second part of the investigation included collecting of auditory representations (auditory icons, spearcons etc.), mapping with visual information and evaluation with the target groups. Furthermore, a new design method for auditory events and class was introduced, called “auditory emoticons”. These use non-verbal human voice samples to represent additional emotional content. Blind and sighted users evaluated different auditory representations for the selected events, including spearcons for different languages. Auditory icons using environmental, familiar sounds as well emoticons are received very well, whilst spearcons seem to be redundant except menu navigation for blind users.","2010","2023-07-05 07:21:00","2023-07-19 11:39:45","2023-07-05 07:21:00","80-102","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_5","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T56HM7KS","bookSection","2003","Bennett, David J.","Effects of Navigation and Position on Task When Presenting Diagrams to Blind People uUsing Sound","Diagrammatic Representation and Inference","978-3-540-43561-7 978-3-540-46037-4","","","http://link.springer.com/10.1007/3-540-46037-3_19","This paper questions how we could and should present diagrams to blind people using computer-generated sound. Using systems that present information about one part of the diagram at a time, rather than the whole, leads to two problems. The first problem is how to present information so that users can integrate the information into a coherent overall picture. The second is how to select the area to be presented. This is looked at by using a system that presents graphs representing central heating system schematics. The system presents information by user choice through either a hierarchical split of information and navigation system, or a connection oriented split of information and navigation system. Further, we have a split as to whether a simple system of presenting location of nodes is used, or not. Tasks, classified as being based on hierarchical information or connection-based information, were set using the system and the effect of the different models was recorded. It was found that there was a match of task to navigation system, but that presentation of position had no discernable effect.","2003","2023-07-05 07:23:08","2023-07-19 23:56:09","2023-07-05 07:23:08","161-175","","","2317","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-46037-3_19","","","","","","Hegarty, Mary; Meyer, Bernd; Narayanan, N. Hari","Goos, G.; Hartmanis, J.; Van Leeuwen, J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9USIX3JA","journalArticle","2012","Grond, Florian","Safety Certificate: an audification performance of high-speed trains","AI & SOCIETY","","0951-5666, 1435-5655","10.1007/s00146-011-0351-5","http://link.springer.com/10.1007/s00146-011-0351-5","Safety Certificate is a musical performance based on sensor data from high-speed trains. The original purpose of this data is to provide a basis for the assessments of the mechanical aspects of train safety. In this performance, the data, which represents dynamical processes below the audible range, are converted into sound through audification. The sound that is generated live during the performance is manipulated through the Manta control interface, which allows for the convenient layering of 48 different timbres. Safety Certificate was premiered at Seconde Nature in Aix-en-Provence in March 2010 during the Sonification symposium–What, Where, How, Why, organized by Locus Sonus. The following short article gives details about the data, the audification technique, use of the control interface, and the musical structure of the performance.","2012-05","2023-07-05 07:23:08","2023-07-19 11:20:41","2023-07-05 07:23:08","293-295","","2","27","","AI & Soc","Safety Certificate","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7RXV8FBY","bookSection","2019","Colombo, Roberto; Mazzone, Alessandra; Delconte, Carmen; Raglio, Alfredo","Patient Motivation and Rewarding to Maximize Outcome: A Sensory Perspective","Converging Clinical and Engineering Research on Neurorehabilitation III","978-3-030-01844-3 978-3-030-01845-0","","","http://link.springer.com/10.1007/978-3-030-01845-0_42","Motivation is an important topic in rehabilitation and frequently used as a determinant of rehabilitation outcome. Several factors can influence patient motivation and so improve exercise adherence. This paper presents some techniques to improve patient motivation and maximize outcome during technology-assisted rehabilitation. In particular, we present some examples of multimodal augmented feedback. This approach provide feedback on the performance during training through the stimulation of different sensory channels such as vision, audition, proprioception etc. It is believed to have several advantages over unimodal feedback and it can promote the patient’s motivation and maximize outcome.","2019","2023-07-05 07:23:08","2023-07-19 23:53:49","2023-07-05 07:23:08","213-217","","","21","","","Patient Motivation and Rewarding to Maximize Outcome","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Biosystems & Biorobotics DOI: 10.1007/978-3-030-01845-0_42","","","","","","Masia, Lorenzo; Micera, Silvestro; Akay, Metin; Pons, José L.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GLNAQEXS","journalArticle","2017","Braund, Edward; Miranda, Eduardo Reck","On Building Practical Biocomputers for Real-world Applications: Receptacles for Culturing Slime Mould Memristors and Component Standardisation","Journal of Bionic Engineering","","1672-6529, 2543-2141","10.1016/S1672-6529(16)60386-4","http://link.springer.com/10.1016/S1672-6529(16)60386-4","Our application of bionic engineering is novel: we are interested in developing hybrid hardware-wetware systems for music. This paper introduces receptacles for culturing Physarum polycephalum-based memristors that are highly accessible to the creative practitioner. The myxomycete Physarum polycephalum is an amorphous unicellular organism that has been found to exhibit memristive properties. Such a discovery has potential to allow us to move towards engineering electrical systems that encompass Physarum polycephalum components. To realise this potential, it is necessary to address some of the constraints associated with harnessing living biological entities in systems for real-time application. Within the paper, we present 3D printed receptacles designed to standardise both the production of components and memristive observations. Subsequent testing showed a significant decrease in growth time, increased lifespan, and superior similarity in component-to-component responses. The results indicate that our receptacle design may provide means of implementing hybrid electrical systems for music technology.","2017-03","2023-07-05 07:23:08","2023-07-20 06:46:29","2023-07-05 07:23:08","151-162","","1","14","","J Bionic Eng","On Building Practical Biocomputers for Real-world Applications","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/AI2GKMV2/Braund and Miranda - 2017 - On Building Practical Biocomputers for Real-world .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U2YUAAAP","bookSection","2021","Spagnol, Gabriela Salim; Ling, Li Hui; Li, Li Min; Manzolli, Jônatas","A Proposal of Emotion Evocative Sound Compositions for Therapeutic Purposes","Perception, Representations, Image, Sound, Music","978-3-030-70209-0 978-3-030-70210-6","","","http://link.springer.com/10.1007/978-3-030-70210-6_26","Recognition and understanding of emotions is a path for self healing. We have worked with Mandalas of Emotions, derived from the Traditional Chinese Medicine (TCM), as a complementary therapy. In this paper, we present the conceptual framework related to the creation of sound collages for the five elements of TCM and assessment of these compositions by experienced holistic therapists. Results present quantitative data, according to scales for relaxation, arousal and valence, and qualitative data from transcription and analysis of the recorded responses of volunteers. In our study, the most common perceptions were warmth, irritation, peace and fear. The innovation of this proposal may stimulate further research on emotion-evoking sounds, and in sound composition.","2021","2023-07-05 07:23:08","2023-07-21 04:45:31","2023-07-05 07:23:08","396-408","","","12631","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-70210-6_26","","","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Aramaki, Mitsuko","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3NGN6G6Q","bookSection","2013","Conan, Simon; Aramaki, Mitsuko; Kronland-Martinet, Richard; Ystad, Sølvi","Intuitive Control of Rolling Sound Synthesis","From Sounds to Music and Emotions","978-3-642-41247-9 978-3-642-41248-6","","","http://link.springer.com/10.1007/978-3-642-41248-6_6","This paper presents a rolling sound synthesis model which can be intuitively controlled. To propose this model, different aspects of the rolling phenomenon are explored : physical modeling, perceptual attributes and signal morphology. A source-filter model for rolling sounds synthesis is presented with associated intuitive controls.","2013","2023-07-05 07:23:08","2023-07-20 00:05:52","2023-07-05 07:23:08","99-109","","","7900","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-41248-6_6","","/Users/minsik/Zotero/storage/J8ZM9YPV/Conan et al. - 2013 - Intuitive Control of Rolling Sound Synthesis.pdf","","","","Aramaki, Mitsuko; Barthet, Mathieu; Kronland-Martinet, Richard; Ystad, Sølvi","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JGP4VDNK","bookSection","2016","Rickman, Jordan; Tanenbaum, Theresa Jean","GeoPoetry: Designing Location-Based Combinatorial Electronic Literature Soundtracks for Roadtrips","Interactive Storytelling","978-3-319-48278-1 978-3-319-48279-8","","","https://link.springer.com/10.1007/978-3-319-48279-8_8","In this paper we present GeoPoetry, a location-based work of electronic literature that generates poetic language and dynamic soundtracks for road-trips that reflect the mood of people in the surrounding area. GeoPoetry takes recent nearby geotagged Twitter data and generates strings of combinatorial poetry from them using simple Markov-chain text generation. It also performs a sentiment analysis on the local Twitter traffic, which it uses to seed a playlist on Spotify, using a simple model of affect. The result is a sonic reflection of the social geography traversed by the user that responds to its situatedness in both space and time. GeoPoetry participates in a long tradition of public and locative artwork which has the potential to inspire exciting new works of interactive narrative.","2016","2023-07-05 07:23:08","2023-07-20 06:37:09","2023-07-05 07:23:08","85-96","","","10045","","","GeoPoetry","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-48279-8_8","","","","","","Nack, Frank; Gordon, Andrew S.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UKNB87AE","journalArticle","2017","Mathew, Justin Dan; Huot, Stéphane; Katz, Brian F. G.","Survey and implications for the design of new 3D audio production and authoring tools","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-017-0245-z","http://link.springer.com/10.1007/s12193-017-0245-z","3D audio production tools vary from lowlevel programming libraries to higher-level user interfaces that are used across a wide range of applications today. However, many of these user interfaces are underdeveloped with limited functionality, forcing users to resort to ad hoc solutions with other tools or programming languages. Identifying these limitations and custom methods are needed to inform the development of new user interfaces. Towards this end, an on-line survey was conducted with current practitioners to gather ethnographic information on their tools, methods, and opinions. Results of the survey identified specific methods and limitations regarding Audio Rendering, Visual Feedback, Functionality, and Workflow Integration. These results also shed light on three basic tasks that have to be performed interactively with 3D audio production tools: Defining the Rendering Space, Creation and Manipulation of Audio Objects, and Use of Feedback. This classification helps organize the creative needs for 3D audio tools that address issues within the workflow and low-level functionality of systems.","2017-09","2023-07-05 07:23:08","2023-07-20 07:02:21","2023-07-05 07:23:08","277-287","","3","11","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/J369T77E/Mathew et al. - 2017 - Survey and implications for the design of new 3D a.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QI476AR6","journalArticle","2018","Ghai, Shashank; Ghai, Ishan; Schmitz, Gerd; Effenberg, Alfred O.","Effect of rhythmic auditory cueing on parkinsonian gait: A systematic review and meta-analysis","Scientific Reports","","2045-2322","10.1038/s41598-017-16232-5","https://www.nature.com/articles/s41598-017-16232-5","Abstract             The use of rhythmic auditory cueing to enhance gait performance in parkinsonian patients’ is an emerging area of interest. Different theories and underlying neurophysiological mechanisms have been suggested for ascertaining the enhancement in motor performance. However, a consensus as to its effects based on characteristics of effective stimuli, and training dosage is still not reached. A systematic review and meta-analysis was carried out to analyze the effects of different auditory feedbacks on gait and postural performance in patients affected by Parkinson’s disease. Systematic identification of published literature was performed adhering to PRISMA guidelines, from inception until May 2017, on online databases; Web of science, PEDro, EBSCO, MEDLINE, Cochrane, EMBASE and PROQUEST. Of 4204 records, 50 studies, involving 1892 participants met our inclusion criteria. The analysis revealed an overall positive effect on gait velocity, stride length, and a negative effect on cadence with application of auditory cueing. Neurophysiological mechanisms, training dosage, effects of higher information processing constraints, and use of cueing as an adjunct with medications are thoroughly discussed. This present review bridges the gaps in literature by suggesting application of rhythmic auditory cueing in conventional rehabilitation approaches to enhance motor performance and quality of life in the parkinsonian community.","2018-01-11","2023-07-05 07:23:08","2023-07-05 07:23:08","2023-07-05 07:23:08","506","","1","8","","Sci Rep","Effect of rhythmic auditory cueing on parkinsonian gait","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/JWMVXCDW/Ghai et al. - 2018 - Effect of rhythmic auditory cueing on parkinsonian.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BHKHTFNQ","bookSection","2010","Brazil, Eoin","A Review of Methods and Frameworks for Sonic Interaction Design: Exploring Existing Approaches","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_3","This article presents a review of methods and frameworks focused on the early conceptual design of sonic interactions. The aim of the article is to provide novice and expert designers in human computer interaction an introduction to sonic interaction design, to auditory displays, and to the methods used to design the sounds and interactions. A range of the current best practices are analysed. These are discussed with regard to the key methods and concepts, by providing examples from existing work in the field. A complementary framework is presented to highlight how these methods can be used together by an auditory display designer at the early conceptual design stage. These methods are reflected upon and provides a closing discussion on the future directions of research that can be explored using these approaches.","2010","2023-07-05 07:23:08","2023-07-19 11:36:54","2023-07-05 07:23:08","41-67","","","5954","","","A Review of Methods and Frameworks for Sonic Interaction Design","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_3","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IMULEYKI","journalArticle","2020","Rocchesso, Davide; Mannone, Maria","A quantum vocal theory of sound","Quantum Information Processing","","1570-0755, 1573-1332","10.1007/s11128-020-02772-9","https://link.springer.com/10.1007/s11128-020-02772-9","Abstract                            Concepts and formalism from acoustics are often used to exemplify quantum mechanics. Conversely, quantum mechanics could be used to achieve a new perspective on acoustics, as shown by Gabor studies. Here, we focus in particular on the study of human voice, considered as a probe to investigate the world of sounds. We present a theoretical framework that is based on               observables               of vocal production, and on some               measurement apparati               that can be used both for analysis and synthesis. In analogy to the description of spin states of a particle, the quantum-mechanical formalism is used to describe the relations between the fundamental states associated with phonetic labels such as phonation, turbulence, and supraglottal myoelastic vibrations. The intermingling of these states, and their temporal evolution, can still be interpreted in the Fourier/Gabor plane, and effective extractors can be implemented. The bases for a quantum vocal theory of sound, with implications in sound analysis and design, are presented.","2020-09","2023-07-05 07:23:08","2023-07-05 07:23:08","2023-07-05 07:23:08","292","","9","19","","Quantum Inf Process","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/KURYRRVZ/Rocchesso and Mannone - 2020 - A quantum vocal theory of sound.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7ZV97AQW","journalArticle","2019","De Leo-Winkler, M. A.; Wilson, G.; Green, W.; Chute, L.; Henderson, E.; Mitchell, T.","The Vibrating Universe: Astronomy for the Deaf","Journal of Science Education and Technology","","1059-0145, 1573-1839","10.1007/s10956-018-9761-1","http://link.springer.com/10.1007/s10956-018-9761-1","The Deaf have often been overlooked when designing informal STEM education and public outreach activities. Astronomers at UC Riverside and teachers at the California School for the Deaf, Riverside (CSDR), have designed an astronomy workshop aimed specifically for the Deaf using the school’s on-site sound lab. We have used astronomy for this workshop because the field has a significant edge over other sciences to act as portal for K-12 engagement in science given the imagery it presents, the answers it offers to grand questions, and its interdisciplinary nature. The workshop is an unconventional activity that excites the students and provides a positive experience in astronomy, based on knowledge that they already acquired beforehand in the classroom. Our workshop uses electromagnetic emissions, enhanced sounds and sonification processes of cosmic phenomena that have low frequencies and sufficiently distinguishable patterns which are delivered to students through a specialized designed sound lab for the Deaf. Storytelling paired with videos and images are used to give understandable meaning to the sounds of the Universe. Positive feedback was collected from over 80 students who participated in our workshop. Our activity can be reproduced elsewhere to further engage the Deaf community in science.","2019-06","2023-07-05 07:23:08","2023-07-20 06:49:40","2023-07-05 07:23:08","222-230","","3","28","","J Sci Educ Technol","The Vibrating Universe","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/2ZH73UBN/De Leo-Winkler et al. - 2019 - The Vibrating Universe Astronomy for the Deaf.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SVGTKTKY","journalArticle","2020","Magnusson, Charlotte; Rassmus-Gröhn, Kirsten; Rydeman, Bitte","Developing a mobile activity game for stroke survivors—lessons learned","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-020-00342-y","https://link.springer.com/10.1007/s12193-020-00342-y","Abstract             Persons who have survived a stroke might lower the risk of having recurrent strokes by adopting a healthier lifestyle with more exercise. One way to promote exercising is by fitness or exergame apps for mobile phones. Health and fitness apps are used by a significant portion of the consumers, but these apps are not targeted to stroke survivors, who may experience cognitive limitations (like fatigue and neglect), have problems with mobility due to hemiplegia, and balance problems. We outline the design process, implementation and user involvement in the design of an exergame app that is specifically targeted to stroke survivors, and present the lessons learned during the design process.","2020-09","2023-07-05 07:23:08","2023-07-05 07:23:08","2023-07-05 07:23:08","303-312","","3","14","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/WMA9WA22/Magnusson et al. - 2020 - Developing a mobile activity game for stroke survi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YLH36GPD","journalArticle","2022","Canessa, Enrique","Wave-like behaviour in (0,1) binary sequences","Scientific Reports","","2045-2322","10.1038/s41598-022-18360-z","https://www.nature.com/articles/s41598-022-18360-z","Abstract                            A comprehensive study of the properties of finite (0,1) binary systems from the mathematical viewpoint of quantum theory is presented. This is a quantum-inspired extension of the GenomeBits model to characterize observed genome sequences, where a complex wavefunction                                                   $$\psi _{n}$$                                                               ψ                       n                                                                                       is considered as an analogous probability measure and it is related to an alternating (0,1) binary series having independent distributed terms. The real and imaginary spectrum of                                                   $$\psi _{n}$$                                                               ψ                       n                                                                                       vs.               the nucleotide base positions display characteristic features of sound waves. This approach represents a novel perspective for identifying and “observing” emergent properties of genome sequences in the form of wavefunctions via superposition states. The motivation is to develop a simple algorithm to perform wave calculations from binary sequences and to apply these wave functions to sonification.","2022-08-17","2023-07-05 07:23:08","2023-07-05 07:23:08","2023-07-05 07:23:08","13971","","1","12","","Sci Rep","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/H2AXEUN2/Canessa - 2022 - Wave-like behaviour in (0,1) binary sequences.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DYE7PWM6","journalArticle","2012","Bakker, Saskia; Van Den Hoven, Elise; Eggen, Berry","Knowing by ear: leveraging human attention abilities in interaction design","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0062-8","http://link.springer.com/10.1007/s12193-011-0062-8","In a world in which intelligent technologies are integrated in everyday objects and environments, users are at risk of being overburdened with information and interaction possibilities. Calm technology therefore aims at designing interactions that may reside in the periphery of the user’s attention and only shift to the center of the attention when required. However, for such designs to be effective, a detailed understanding of human attention abilities is needed. In this paper, we therefore present a qualitative study on the everyday periphery of the attention. As we expected, we found that sound plays a major role in this, which supports our approach to use interactive sonification as an interaction style for peripheral interaction. We present a range of rich examples of everyday situations that lay out the design space for peripheral interaction and support these findings by describing three initial designs that use interactive sonification for peripheral interaction.","2012-05","2023-07-05 07:23:08","2023-07-20 06:50:41","2023-07-05 07:23:08","197-209","","3-4","5","","J Multimodal User Interfaces","Knowing by ear","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/X399P5R7/Bakker et al. - 2012 - Knowing by ear leveraging human attention abiliti.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9IEG444D","journalArticle","2012","Katz, Brian F. G.; Kammoun, Slim; Parseihian, Gaëtan; Gutierrez, Olivier; Brilhault, Adrien; Auvray, Malika; Truillet, Philippe; Denis, Michel; Thorpe, Simon; Jouffrais, Christophe","NAVIG: augmented reality guidance system for the visually impaired: Combining object localization, GNSS, and spatial audio","Virtual Reality","","1359-4338, 1434-9957","10.1007/s10055-012-0213-6","http://link.springer.com/10.1007/s10055-012-0213-6","Navigating complex routes and finding objects of interest are challenging tasks for the visually impaired. The project NAVIG (Navigation Assisted by artificial VIsion and GNSS) is directed toward increasing personal autonomy via a virtual augmented reality system. The system integrates an adapted geographic information system with different classes of objects useful for improving route selection and guidance. The database also includes models of important geolocated objects that may be detected by real-time embedded vision algorithms. Object localization (relative to the user) may serve both global positioning and sensorimotor actions such as heading, grasping, or piloting. The user is guided to his desired destination through spatialized semantic audio rendering, always maintained in the head-centered reference frame. This paper presents the overall project design and architecture of the NAVIG system. In addition, details of a new type of detection and localization device are presented. This approach combines a bio-inspired vision system that can recognize and locate objects very quickly and a 3D sound rendering system that is able to perceptually position a sound at the location of the recognized object. This system was developed in relation to guidance directives developed through participative design with potential users and educators for the visually impaired.","2012-11","2023-07-05 07:23:08","2023-07-21 05:14:20","2023-07-05 07:23:08","253-269","","4","16","","Virtual Reality","NAVIG","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DBMQDUIX","journalArticle","2021","Takabatake, Kazuhiko; Kunii, Naoto; Nakatomi, Hirofumi; Shimada, Seijiro; Yanai, Kei; Takasago, Megumi; Saito, Nobuhito","Musical Auditory Alpha Wave Neurofeedback: Validation and Cognitive Perspectives","Applied Psychophysiology and Biofeedback","","1090-0586, 1573-3270","10.1007/s10484-021-09507-1","https://link.springer.com/10.1007/s10484-021-09507-1","Abstract             Neurofeedback through visual, auditory, or tactile sensations improves cognitive functions and alters the activities of daily living. However, some people, such as children and the elderly, have difficulty concentrating on neurofeedback for a long time. Constant stressless neurofeedback for a long time may be achieved with auditory neurofeedback using music. The primary purpose of this study was to clarify whether music-based auditory neurofeedback increases the power of the alpha wave in healthy subjects. During neurofeedback, white noise was superimposed on classical music, with the noise level inversely correlating with normalized alpha wave power. This was a single-blind, randomized control crossover trial in which 10 healthy subjects underwent, in an assigned order, normal and random feedback (NF and RF), either of which was at least 4 weeks long. Cognitive functions were evaluated before, between, and after each neurofeedback period. The secondary purpose was to assess neurofeedback-induced changes in cognitive functions. A crossover analysis showed that normalized alpha-power was significantly higher in NF than in RF; therefore, music-based auditory neurofeedback facilitated alpha wave induction. A composite category-based analysis of cognitive functions revealed greater improvements in short-term memory in subjects whose alpha-power increased in response to NF. The present study employed a long period of auditory alpha neurofeedback and achieved successful alpha wave induction and subsequent improvements in cognitive functions. Although this was a pilot study that validated a music-based alpha neurofeedback system for healthy subjects, the results obtained are encouraging for those with difficulty in concentrating on conventional alpha neurofeedback.             Trial registration: 2018077NI, date of registration: 2018/11/27","2021-12","2023-07-05 07:23:08","2023-07-05 07:23:08","2023-07-05 07:23:08","323-334","","4","46","","Appl Psychophysiol Biofeedback","Musical Auditory Alpha Wave Neurofeedback","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/QD2LCQ6E/Takabatake et al. - 2021 - Musical Auditory Alpha Wave Neurofeedback Validat.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K54MBR6J","bookSection","2017","Mauceri, Frank; Majercik, Stephen M.","A Swarm Environment for Experimental Performance and Improvisation","Computational Intelligence in Music, Sound, Art and Design","978-3-319-55749-6 978-3-319-55750-2","","","http://link.springer.com/10.1007/978-3-319-55750-2_13","This paper describes Swarm Performance and Improvisation (Swarm-PI), a real-time computer environment for music improvisation that uses swarm algorithms to control sound synthesis and to mediate interactions with a human performer. Swarm models are artificial, multi-agent systems where the organized movements of large groups are the result of simple, local rules between individuals. Swarms typically exhibit self-organization and emergent behavior. In Swarm-PI, multiple acoustic descriptors from a live audio feed generate parameters for an independent swarm among multiple swarms in the same space, and each swarm is used to synthesize a stream of sound using granular sampling. This environment demonstrates the effectiveness of using swarms to model human interactions typical to group improvisation and to generate organized patterns of synthesized sound.","2017","2023-07-05 07:23:08","2023-07-19 23:47:33","2023-07-05 07:23:08","190-200","","","10198","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-55750-2_13","","","","","","Correia, João; Ciesielski, Vic; Liapis, Antonios","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J4ZVZ9GW","bookSection","2014","Rodrigues, Mailis G.; Wanderley, Marcelo M.; Lopes, Paulo F.","Intonaspacio: A Digital Musical Instrument for Exploring Site-Specificities in Sound","Sound, Music, and Motion","978-3-319-12975-4 978-3-319-12976-1","","","https://link.springer.com/10.1007/978-3-319-12976-1_24","The integration of space as a parameter in the composition of an art work as been relegated to a secondary role. Site-specific art is a branch of the visual arts whose main goal is to fuse space in the art work, i.e., the piece belongs to the space where it is placed and its meaning is lost once it is removed. There’s an idea of bi-directionality beneath the conception of the work, where space defines the perception of the piece and the piece interferes in the perception of space. In music and especially in sound art, there are some examples of site-specific works, but they are sparse and mainly centered on the idea of installation. Site-specificity in sound is an open and not yet fully explored field. Our research purposes a new digital musical instrument (DMI) Intonaspacio, which allows the access to the sound of the room, and the integration of it in the music in real time. Intonaspacio provides the performer with tools to create site-specific sound, i.e., to integrate space as part of the creative work. In this paper we present Intonaspacio, focusing attention on the design of the physical interface. Up until now we have designed two versions of Intonaspacio. From the first version to the second one, we have modified the material of the frame, which led us to reconsider some of our previous decisions on the choice of sensors and their placement. We also designed different mappings that present several approaches to the site-specific question.","2014","2023-07-05 07:23:08","2023-07-21 05:02:01","2023-07-05 07:23:08","393-415","","","8905","","","Intonaspacio","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-12976-1_24","","","","","","Aramaki, Mitsuko; Derrien, Olivier; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PY44H8GV","bookSection","2001","McCormack, Jon","Eden: An Evolutionary Sonic Ecosystem","Advances in Artificial Life","978-3-540-42567-0 978-3-540-44811-2","","","http://link.springer.com/10.1007/3-540-44811-X_13","This paper describes an Artificial Life system for music composition. An evolving ecology of sonic entities populate a virtual world and compete for limited resources. Part of their genetic representation permits the creatures to make and listen to sounds. Complex musical and sonic relationships can develop as the creatures use sound to aid in their survival and mating prospects.","2001","2023-07-05 07:23:08","2023-07-19 11:10:01","2023-07-05 07:23:08","133-142","","","2159","","","Eden","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-44811-X_13","","","","","","Kelemen, Jozef; Sosík, Petr","Goos, G.; Hartmanis, J.; Van Leeuwen, J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VEB28LYZ","bookSection","2021","Delle Monache, Stefano; Rocchesso, Davide","Exploring Design Cognition in Voice-Driven Sound Sketching and Synthesis","Perception, Representations, Image, Sound, Music","978-3-030-70209-0 978-3-030-70210-6","","","http://link.springer.com/10.1007/978-3-030-70210-6_30","Conceptual design and communication of sonic ideas are critical, and still unresolved aspects of current sound design practices, especially when teamwork is involved. Design cognition studies in the visual domain represent a valuable resource to look at, to better comprehend the reasoning of designers when they approach a sound-based project. A design exercise involving a team of professional sound designers is analyzed, and discussed in the framework of the Function-BehaviorStructure ontology of design. The use of embodied sound representations of concepts fosters team-building and a more effective communication, in terms of shared mental models.","2021","2023-07-05 07:27:40","2023-07-21 04:45:04","2023-07-05 07:27:40","465-480","","","12631","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-70210-6_30","","/Users/minsik/Zotero/storage/NHMUMHQV/Delle Monache and Rocchesso - 2021 - Exploring Design Cognition in Voice-Driven Sound S.pdf","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Aramaki, Mitsuko","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F8E5HD4W","bookSection","2023","Navarro-Cáceres, Juan José; Mendes, André Sales; Blas, Hector Sánchez San; González, Gabriel Villarrubia; Navarro-Cáceres, María","MusicFactory: Application of a Convolutional Neural Network for the Generation of Soundscapes from Images","New Trends in Disruptive Technologies, Tech Ethics and Artificial Intelligence","978-3-031-14858-3 978-3-031-14859-0","","","https://link.springer.com/10.1007/978-3-031-14859-0_14","A soundscape is a sound description of a concrete environment. Therefore, the soundscapes are always connected to a visual component, as it might capture sounds from an urban city, a countryside, or a domestic place. In this work, we present a system that generate soundscapes from images. Firstly, we recognize some objects in the image. In a second step the system searches the most adequate sounds according to the entities identified in the picture. Finally, a soundscape is synthesized by combining the short sound files found. The results obtained according to the subjective evaluation are promising and encouraging to deepen our research in the soundscape generation.","2023","2023-07-05 07:27:40","2023-07-21 04:39:06","2023-07-05 07:27:40","156-164","","","1430","","","MusicFactory","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Advances in Intelligent Systems and Computing DOI: 10.1007/978-3-031-14859-0_14","","","","","","De La Iglesia, Daniel H.; De Paz Santana, Juan F.; López Rivero, Alfonso J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EWQYEYH6","journalArticle","2011","Miranda, Eduardo R.; Adamatzky, Andrew; Jones, Jeff","Sounds synthesis with slime mould of Physarum Polycephalum","Journal of Bionic Engineering","","1672-6529, 2543-2141","10.1016/S1672-6529(11)60016-4","http://link.springer.com/10.1016/S1672-6529(11)60016-4","Physarum polycephalum is a huge single cell with thousands of nuclei, which behaves like a giant amoeba. During its foraging behaviour this plasmodium produces electrical activity corresponding to different physiological states. We developed a method to render sounds from such electrical activity and thus represent spatio-temporal behaviour of slime mould in a form apprehended by humans. We show to control behaviour of slime mould to shape it towards reproduction of required range of sounds.","2011-06","2023-07-05 07:27:40","2023-07-20 06:46:38","2023-07-05 07:27:40","107-113","","2","8","","J Bionic Eng","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/UFBD3KCV/Miranda et al. - 2011 - Sounds synthesis with slime mould of Physarum Poly.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3856TWWF","bookSection","2013","Rinaldi, Claudia; Santic, Marco; Pomante, Luigi; Graziosi, Fabio","Exploiting Latest Technologies for RF Sounding’s Evolution","Arts and Technology","978-3-642-37981-9 978-3-642-37982-6","","","http://link.springer.com/10.1007/978-3-642-37982-6_5","In this paper we present the most recent technological innovations introduced into the artistic installation we called RF Sounding, keep on maintaining our fundamental goals: creating an artistic installation that can be used for educational purposes as well. Indeed we have been inspired by the impossible human dream of flying that we reasoned on the acoustic dimension. We decided to make the inaudible, audible by a translation in the audio bandwidth of signals coming from cellular networks. We thus want to provide the user, entering the specifically defined area, with awareness of radio frequency signals characterizing the cellular networks band. With respect to the prototype presented in previous papers we finally exploit the information coming from a spectrum analyser, thus taking into account also the whole uplink, and position data acquired from a Microsoft Kinect in order to realize localization inside the equipped area, without the need for the users to wear an active device.","2013","2023-07-05 07:27:40","2023-07-19 11:35:24","2023-07-05 07:27:40","33-40","","","116","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering DOI: 10.1007/978-3-642-37982-6_5","","","","","","De Michelis, Giorgio; Tisato, Francesco; Bene, Andrea; Bernini, Diego","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PABNE37B","bookSection","2014","Laffineur, Ludovic; Degeest, Alexandra; Frisson, Christian; Giot, Rudi","Interactive Network Installation","Intelligent Technologies for Interactive Entertainment","978-3-319-08188-5 978-3-319-08189-2","","","http://link.springer.com/10.1007/978-3-319-08189-2_20","The work discussed is this paper deals with a interactive installation to monitor the network flow in a artistic way. The system is developed in C++ grabs packets using LibPCap, analyses them at low level (e.g. packet length) and also provides high-level information (e.g. port number). This new approach is based more on the network flow analysis than on network services analysis. The software communicates with ©Resolume Avenue and ©Reaktor through OSC protocol. ©Resolume Avenue is a software for Video Jockey (VJ) purposes and ©Reaktor is a modular software music studio developed by Native Instrument. Users can actively take part to an interactive audiovisual exhibition system using their mobile device to send e-mails, listen to a web radio, surf on a website, read RSS feeds, in short, the experience begins once visitors exchange data with the network.","2014","2023-07-05 07:27:40","2023-07-20 06:36:04","2023-07-05 07:27:40","140-143","","","136","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering DOI: 10.1007/978-3-319-08189-2_20","","","","","","Reidsma, Dennis; Choi, Insook; Bargar, Robin","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PQ822AV4","journalArticle","2010","Roma, Gerard; Janer, Jordi; Kersten, Stefan; Schirosa, Mattia; Herrera, Perfecto; Serra, Xavier","Ecological Acoustics Perspective for Content-Based Retrieval of Environmental Sounds","EURASIP Journal on Audio, Speech, and Music Processing","","1687-4714, 1687-4722","10.1155/2010/960863","http://asmp.eurasipjournals.com/content/2010/1/960863","In this paper we present a method to search for environmental sounds in large unstructured databases of user-submitted audio, using a general sound events taxonomy from ecological acoustics. We discuss the use of Support Vector Machines to classify sound recordings according to the taxonomy and describe two use cases for the obtained classification models: a content-based web search interface for a large audio database and a method for segmenting field recordings to assist sound design.","2010","2023-07-05 07:27:40","2023-07-21 07:39:39","2023-07-05 07:27:40","1-11","","","2010","","EURASIP Journal on Audio, Speech, and Music Processing","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/IYKDSVIJ/Roma et al. - 2010 - Ecological Acoustics Perspective for Content-Based.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7WT5TM5X","journalArticle","2009","Lugmayr, Artur; Risse, Thomas; Stockleben, Bjoern; Laurila, Kari; Kaario, Juha","Semantic ambient media—an introduction","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-009-0282-z","http://link.springer.com/10.1007/s11042-009-0282-z","The medium is the message! And the message was literacy, media democracy and music charts. Mostly one single distinguishable medium such as TV, the Web, the radio, or books transmitted the message. Now in the age of ubiquitous and pervasive computing, where information flows through a plethora of distributed interlinked media—what is the message ambient media will tell us? What does semantic mean in this context? Which experiences will it open to us? What is content in the age of ambient media? Ambient media are embedded throughout the natural environment of the consumer—in his home, in his car, in restaurants, and on his mobile device. Predominant sample services are smart wallpapers in homes, location based services, RFID based entertainment services for children, or intelligent homes. The goal of this article is to define semantic ambient media and discuss the contributions to the Semantic Ambient Media Experience (SAME) workshop, which was held in conjunction with the ACM Multimedia conference in Vancouver in 2008. The results of the workshop can be found on: www.ambientmediaassociation.org.","2009-09","2023-07-05 07:27:40","2023-07-21 04:33:10","2023-07-05 07:27:40","337-359","","3","44","","Multimed Tools Appl","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TXTC9EMU","journalArticle","1992","Rasmussen, Susan J.","Reflections on tamazai, a Tuareg idiom of suffering","Culture, Medicine and Psychiatry","","0165-005X, 1573-076X","10.1007/BF00052154","http://link.springer.com/10.1007/BF00052154","In this essay, I explore ways in which the theatrical and the medical are inextricably mixed and affect each other, in the Nigerien Tuareg idiom of tamazai. Tamazai is locally-described as “an illness of the heart and soul, not curable by Koranic verses,” but by exorcism of spirits. A case study and analysis of healing rituals demonstrate how this idiom communicates women's relationships and empowers dramatic framing.","1992-09","2023-07-05 07:27:40","2023-07-19 23:54:24","2023-07-05 07:27:40","337-365","","3","16","","Cult Med Psych","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XDTT797K","bookSection","2011","Eigenfeldt, Arne; Pasquier, Philippe","A Sonic Eco-System of Self-Organising Musical Agents","Applications of Evolutionary Computation","978-3-642-20519-4 978-3-642-20520-0","","","http://link.springer.com/10.1007/978-3-642-20520-0_29","We present a population of autonomous agents that exist within a sonic eco-system derived from real-time analysis of live audio. In this system, entitled Coming Together: Shoals, agents search for food consisting of CataRT unit analyses, which, when found, are consumed through granulation. Individual agents are initialised with random synthesis parameters, but communicate these parameters to agents in local neighborhoods. Agents form social networks, and converge their parameters within these networks, thereby creating unified grain streams. Separate gestures thus emerge through the self-organisation of the population.","2011","2023-07-05 07:27:40","2023-07-19 11:30:38","2023-07-05 07:27:40","283-292","","","6625","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-20520-0_29","","","","","","Di Chio, Cecilia; Brabazon, Anthony; Di Caro, Gianni A.; Drechsler, Rolf; Farooq, Muddassar; Grahl, Jörn; Greenfield, Gary; Prins, Christian; Romero, Juan; Squillero, Giovanni; Tarantino, Ernesto; Tettamanzi, Andrea G. B.; Urquhart, Neil; Uyar, A. Şima","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MPGV8ETA","journalArticle","2004","Coward, Sean W.; Stevens, Catherine J.","Extracting Meaning from Sound: Nomic Mappings, Everyday Listening, and Perceiving Object Size from Frequency","The Psychological Record","","0033-2933, 2163-3452","10.1007/BF03395478","http://link.springer.com/10.1007/BF03395478","In developing a theoretical framework for the field of ecological acoustics, Gaver (1993b) distinguished between the experience of musical listening (perceiving sounds) and everyday listening (perceiving sources of sounds). Within the everyday listening experience, Gaver (1993a) proposed that the frequency of an object results from, and therefore specifies, the size of that object. The relation in which frequency and object size stand to one another is an example of a nomic mapping. A symbolic mapping involves the pairing of unrelated dimensions and, relative to a nomic mapping, requires an additional step in recognition and learning. Using a perceptual identification task, an experiment investigated the hypothesis that nomic mappings are identified more easily than symbolic mappings. It was predicted that the advantage manifests only during the everyday listening experience, and that the initially superior recognition of nomic mappings is equaled by symbolic mappings after extended exposure. The results provided support for the hypotheses. Theoretical implications of the differential recognition of nomic and symbolic mappings are discussed, together with practical applications of nomic relations.","2004-07","2023-07-05 07:27:40","2023-07-21 05:07:20","2023-07-05 07:27:40","349-364","","3","54","","Psychol Rec","Extracting Meaning from Sound","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VTELXM9N","bookSection","2020","Moesgaard, Frederik; Hulgaard, Lasse; Bødker, Mads","Involving Users in Sound Design","Design, User Experience, and Usability. Interaction Design","978-3-030-49712-5 978-3-030-49713-2","","","http://link.springer.com/10.1007/978-3-030-49713-2_28","Sound plays an important role in our well-being, our experience of the world around us and our understanding of products, services and interactions. Sound affects our sense of place, and it can modulate our feelings, agency and attention. In a world of increasingly ubiquitous digital technologies, sound may prove a valuable resource for sense making as well as experience- and UX design. Yet the possibilities and challenges of user participation in sound design processes are not well understood. This paper reports on a pilot study examining how participants can be involved in different phases of a sound design process. The results and reflections aim to help researchers and designers in an effort to better understand some of the dynamics of moving from a largely expert driven approach to sound design towards a more user-oriented and participatory approaches.","2020","2023-07-05 07:27:40","2023-07-19 23:55:29","2023-07-05 07:27:40","405-425","","","12200","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-49713-2_28","","","","","","Marcus, Aaron; Rosenzweig, Elizabeth","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J3AQ6VNY","journalArticle","2021","Feng, Feng; Li, Puhong; Stockman, Tony","Exploring crossmodal perceptual enhancement and integration in a sequence-reproducing task with cognitive priming","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-020-00326-y","https://link.springer.com/10.1007/s12193-020-00326-y","Abstract             Crossmodal correspondence, a perceptual phenomenon which has been extensively studied in cognitive science, has been shown to play a critical role in people’s information processing performance. However, the evidence has been collected mostly based on strictly-controlled stimuli and displayed in a noise-free environment. In real-world interaction scenarios, background noise may blur crossmodal effects that designers intend to leverage. More seriously, it may induce additional crossmodal effects, which can be mutually exclusive to the intended one, leading to unexpected distractions from the task at hand. In this paper, we report two experiments designed to tackle these problems with cognitive priming techniques. The first experiment examined how to enhance the perception of specific crossmodal stimuli, namely pitch–brightness and pitch–elevation stimuli. The second experiment investigated how people perceive and respond to crossmodal stimuli that were mutually exclusive. Results showed that first, people’s crossmodal perception was affected by cognitive priming, though the effect varies according to the combination of crossmodal stimuli and the types of priming material. Second, when two crossmodal stimuli are mutually exclusive, priming on only the dominant one (Pitch–elevation) lead to improved performance. These results can help inform future design of multisensory systems by presenting details of how to enhance crossmodal information with cognitive priming.","2021-03","2023-07-05 07:27:40","2023-07-05 07:27:40","2023-07-05 07:27:40","45-59","","1","15","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/GN7QDXU5/Feng et al. - 2021 - Exploring crossmodal perceptual enhancement and in.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S4EE49DQ","bookSection","2004","Nicholson, Mark; Vickers, Paul","Pen-Based Gestures: An Approach to Reducing Screen Clutter in Mobile Computing","Mobile Human-Computer Interaction - MobileHCI 2004","978-3-540-23086-1 978-3-540-28637-0","","","http://link.springer.com/10.1007/978-3-540-28637-0_30","Mobile computing is an area of high growth despite having some serious design issues. It is difficult to increase the size of the screen because of the device’s physical constraints. Consequently, as mobile applications have incorporated more functionality, screen clutter has increased. One method of reducing clutter is to remove visual controls and use pen-based gestures instead. We describe a cinema listing application for a Palm OS device that implements pen-based gestures as the main input method. Two methods are used to communicate the options available on each screen: audio cues and small visual prompts. Preliminary results suggest that buttons can be removed from the screen without detriment to task accuracy or user performance.","2004","2023-07-05 07:27:40","2023-07-21 04:30:13","2023-07-05 07:27:40","320-324","","","3160","","","Pen-Based Gestures","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-28637-0_30","","","","","","Brewster, Stephen; Dunlop, Mark","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3MLNBFVM","bookSection","2020","Rajko, Jessica J.","Designing Palpable Data Representations","HCI International 2020 - Late Breaking Papers: User Experience Design and Case Studies","978-3-030-60113-3 978-3-030-60114-0","","","http://link.springer.com/10.1007/978-3-030-60114-0_32","This paper discusses a multisensory approach to data representation with a specific focus on haptic media. In this, I provide a philosophical and methodological overview of my design process informed by the following themes and topics: 1) the haptic subject; 2) touch as political; 3) co-formed knowledge; and 4) arts-based research methods. The overview is further contextualized by a thorough analysis of collaborative work Vibrant Lives, a 4-year project that includes a suite of unique, custom-designed, vibrotactile interfaces that give audiences a real-time experience of their own personal data output. I continue my analysis by sharing observations from a series of workshops I conducted with haptified archive data. In conclusion, I reflect on issues of user ethics, agency, and control when designing touch-based experiences of data in a multisensory installation setting.","2020","2023-07-05 07:27:40","2023-07-20 05:49:55","2023-07-05 07:27:40","464-480","","","12423","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-60114-0_32","","","","","","Stephanidis, Constantine; Marcus, Aaron; Rosenzweig, Elizabeth; Rau, Pei-Luen Patrick; Moallem, Abbas; Rauterberg, Matthias","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7S8NHLUN","bookSection","2003","Goldstein, Mikael; Öquist, Gustav; Björk, Staffan","Evaluating Sonified Rapid Serial Visual Presentation: An Immersive Reading Experience on a Mobile Device","Universal Access Theoretical Perspectives, Practice, and Experience","978-3-540-00855-2 978-3-540-36572-3","","","http://link.springer.com/10.1007/3-540-36572-9_39","Can the addition of sound enhance the reading experience on small screens when using Rapid Serial Visual Presentation (RSVP) for dynamic text presentation? In this paper we introduce Sonified RSVP and report findings from a usability evaluation where the experience of reading texts enhanced with nomic auditory icons was evaluated. At a comfortable pace 12 subjects read long Swedish texts of equal difficulty with and without the addition of sound on a handheld device. Reading speed (M≈217 wpm) and comprehension (M≈58% correct) did not differ significantly between the two conditions. The evaluation revealed a rather high task load for both conditions but no significant differences. However, the subjective rating of Immersion was rated significantly higher for the Sonified condition. Causes, implications and directions for further work are discussed based on these findings.","2003","2023-07-05 07:27:40","2023-07-21 05:13:24","2023-07-05 07:27:40","508-523","","","2615","","","Evaluating Sonified Rapid Serial Visual Presentation","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-36572-9_39","","","","","","Carbonell, Noëlle; Stephanidis, Constantine","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XY3CG4GL","bookSection","2012","Rowland, Duncan; Connor, Katy","PURE FLOW: Gallery Installation / Mobile Application","Advances in Computer Entertainment","978-3-642-34291-2 978-3-642-34292-9","","","http://link.springer.com/10.1007/978-3-642-34292-9_33","This paper describes the two phase development of the digital art piece PURE FLOW. The first deployment of this work was as a gallery based exhibit in which digital noise sampled from the Global Positioning System was exposed as dynamic sound and projected visual displays. The second piece extended these initial themes onto a handheld platform (iPhone) whereby the user could continually sample digital noise from positioning systems at their surrounding environment and generate an audio and visual experience specifically created for their immediate location. Aesthetic considerations are described along with implementation details leading to general reflections relating to collaborations between artists and technical specialists.","2012","2023-07-05 07:30:22","2023-07-19 11:11:13","2023-07-05 07:30:22","445-452","","","7624","","","PURE FLOW","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-34292-9_33","","","","","","Nijholt, Anton; Romão, Teresa; Reidsma, Dennis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AB2FU5R6","bookSection","2000","Harding, Chris; Kakadiaris, Ioannis; Loftin, R. Bowen","A Multimodal User Interface for Geoscientific Data Investigation","Advances in Multimodal Interfaces — ICMI 2000","978-3-540-41180-2 978-3-540-40063-9","","","http://link.springer.com/10.1007/3-540-40063-X_80","In this paper, we report on our ongoing research into multimodal investigation of geoscientific data. Our system integrates three-dimensional, interactive computer graphics, touch (haptics) and real-time sound synthesis into a multimodal interface. We present applications of multimodal investigations of geoscientific data that pertain to surface meshes on which several typical properties were mapped and to geophysical volume data. Finally, we report on the preliminary results of a psychological study, which is being conducted to increase our understanding of the recognition of audio value in an absolute sense.","2000","2023-07-05 07:30:22","2023-07-19 11:12:31","2023-07-05 07:30:22","615-623","","","1948","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-40063-X_80","","","","","","Tan, Tieniu; Shi, Yuanchun; Gao, Wen","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VKDSJYMU","bookSection","1994","Reed, Daniel A.","Experimental analysis of parallel systems: Techniques and open problems","Computer Performance Evaluation Modelling Techniques and Tools","978-3-540-58021-8 978-3-540-48416-5","","","http://link.springer.com/10.1007/3-540-58021-2_2","Massively parallel systems pose daunting performance instrumentation and data analysis problems. Balancing instrumentation detail, application perturbation, data reduction costs, and presentation complexity requires a mix of science, engineering, and art. This paper surveys current techniques for performance instrumentation and data presentation, illustrates one approach to tool extensibility, and discusses the implications of massive parallelism for performance analysis environments.","1994","2023-07-05 07:30:22","2023-07-19 23:50:15","2023-07-05 07:30:22","25-51","","","794","","","Experimental analysis of parallel systems","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-58021-2_2","","","","","","Haring, Günter; Kotsis, Gabriele","Goos, G.; Hartmanis, J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DFJSETKP","bookSection","2014","Bertacchini, Francesca; Bilotta, Eleonora; Carini, Manuela; Gabriele, Lorella; Pantano, Pietro; Tavernise, Assunta","Learning in the Smart City: A Virtual and Augmented Museum Devoted to Chaos Theory","New Horizons in Web Based Learning","978-3-662-43453-6 978-3-662-43454-3","","","http://link.springer.com/10.1007/978-3-662-43454-3_27","This paper presents a virtual museum introducing the interactive VR and MEMS applications related to the learning of chaos and complexity theory. In this museum, the user can learn the history of the dynamical systems and how to build Chua’s circuit, as well as realize artistic artifacts transforming attractors into sounds and music. This environment can be used in the city in order to create new ways of experiencing science, turning physical activities into virtual ones, an important step towards being able to have the museum in the smart city. Moreover, some applications have been developed to work on iPad and iPhone and can be used as a guide in the real exhibitions. A user-centred design strategy with 40 students has been carried out in order to implement the Virtual Museum of Chua’s Attractors, aiming at widening the experience in the smart city and allowing a considerable public participation.","2014","2023-07-05 07:30:22","2023-07-21 04:38:50","2023-07-05 07:30:22","261-270","","","7697","","","Learning in the Smart City","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-662-43454-3_27","","","","","","Chiu, Dickson K. W.; Wang, Minhong; Popescu, Elvira; Li, Qing; Lau, Rynson","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Kobsa, Alfred; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Doug; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J8N58A5W","bookSection","2021","Voisin, Frédéric; Bidotti, Arnaud; Mourey, France","Designing Soundscapes for Alzheimer’s Disease Care, with Preliminary Clinical Observations","Perception, Representations, Image, Sound, Music","978-3-030-70209-0 978-3-030-70210-6","","","https://link.springer.com/10.1007/978-3-030-70210-6_34","Acoustic environment is a prime source of conscious and unconscious information which allows listeners to place themselves, to communicate, to feel, to remember. Recently, there has been a growing interest to the acoustic environment and its perceptual counterparts of care facilities. In this contribution, the authors describe the process of designing a new audio interactive apparatus for Alzheimer’s Disease care in the context of an active multidisciplinary research project led by a sound designer since 2018, in collaboration with a residential longterm care (EHPAD) in France, a geriatrician, a gerontologist, psychologists and caregivers. The apparatus, named «Madeleines Sonores» in reference to Proust’s madeleine, has been providing virtual soundscapes for two years 24/7 to elderly people suffering from Alzheimer disease. The configuration and sound processes of the apparatus are presented in relation to Alzheimer Disease care. Preliminary psychological and clinical observations are discussed in relation to dementia and to the activity of caring to evaluate the benefits of such a disposal in Alzheimer’s disease therapy and in caring dementia.","2021","2023-07-05 07:30:22","2023-07-21 04:45:50","2023-07-05 07:30:22","533-553","","","12631","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-70210-6_34","","","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Aramaki, Mitsuko","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3L64YVCF","journalArticle","2010","Gygi, Brian; Shafiro, Valeriy","Development of the Database for Environmental Sound Research and Application (DESRA): Design, Functionality, and Retrieval Considerations","EURASIP Journal on Audio, Speech, and Music Processing","","1687-4714, 1687-4722","10.1155/2010/654914","http://asmp.eurasipjournals.com/content/2010/1/654914","","2010","2023-07-05 07:30:22","2023-07-05 07:30:22","2023-07-05 07:30:22","1-12","","","2010","","EURASIP Journal on Audio, Speech, and Music Processing","Development of the Database for Environmental Sound Research and Application (DESRA)","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZC4ACNH4","bookSection","2012","Aramaki, Mitsuko; Kronland-Martinet, Richard; Ystad, Sølvi","Perceptual Control of Environmental Sound Synthesis","Speech, Sound and Music Processing: Embracing Research in India","978-3-642-31979-2 978-3-642-31980-8","","","http://link.springer.com/10.1007/978-3-642-31980-8_13","In this article we explain how perceptual control of synthesis processes can be achieved through a multidisciplinary approach relating physical and signal properties of sound sources to evocations induced by sounds. This approach is applied to environmental and abstract sounds in 3 different experiments. In the first experiment a perceptual control of synthesized impact sounds evoking sound sources of different materials and shapes is presented. The second experiment describes an immersive environmental synthesizer simulating different kinds of environmental sounds evoking natural events such as rain, waves, wind and fire. In the last example motion evoked by abstract sounds is investigated. A tool for describing perceived motion through drawings is proposed in this case.","2012","2023-07-05 07:30:22","2023-07-21 05:02:37","2023-07-05 07:30:22","172-186","","","7172","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-31980-8_13","","/Users/minsik/Zotero/storage/F6P77WX4/Aramaki et al. - 2012 - Perceptual Control of Environmental Sound Synthesi.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer; Mohanty, Sanghamitra","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U5RUPYTT","journalArticle","2008","Zimmermann, Andreas; Lorenz, Andreas","LISTEN: a user-adaptive audio-augmented museum guide","User Modeling and User-Adapted Interaction","","0924-1868, 1573-1391","10.1007/s11257-008-9049-x","http://link.springer.com/10.1007/s11257-008-9049-x","Modern personalized information systems have been proven to support the user with information at the appropriate level and in the appropriate form. In specific environments like museums and exhibitions, focusing on the control of such a system is contradictory to establishing a relationship with the artifacts and exhibits. Preferably, the technology becomes invisible to the user and the physical reality becomes the interface to an additional virtual layer: by naturally moving in the space and/or manipulating physical objects in our surroundings the user will access information and operate the virtual layer. The LISTEN project is an attempt to make use of the inherent “everyday” integration of aural and visual perception, developing a tailored, immersive audio-augmented environment for the visitors of art exhibitions. The challenge of the LISTEN project is to provide a personalized immersive augmented environment, an aim which goes beyond the guiding purpose. The visitors of the museum implicitly interact with the system because the audio presentation is adapted to the users’ contexts (e.g. interests, preferences, motion, etc.), providing an intelligent audio-based environment. This article describes the realization and user evaluation of the LISTEN system focusing on the personalization component. As this system has been installed at the Kunstmuseum Bonn in the context of an exhibition comprising artworks of the painter August Macke, a detailed evaluation could be conducted.","2008-11","2023-07-05 07:30:22","2023-07-21 05:13:39","2023-07-05 07:30:22","389-416","","5","18","","User Model User-Adap Inter","LISTEN","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VFQ2SPJW","bookSection","2007","Oakley, Ian; Park, Jun-Seok","Designing Eyes-Free Interaction","Haptic and Audio Interaction Design","978-3-540-76701-5","","","http://link.springer.com/10.1007/978-3-540-76702-2_13","As the form factors of computational devices diversify, the concept of eyes-free interaction is becoming increasingly relevant: it is no longer hard to imagine use scenarios in which screens are inappropriate. However, there is currently little consensus about this term. It is regularly employed in different contexts and with different intents. One key consequence of this multiplicity of meanings is a lack of easily accessible insights into how to best build an eyes-free system. This paper seeks to address this issue by thoroughly reviewing the literature, proposing a concise definition and presenting a set of design principles. The application of these principles is then elaborated through a case study of the design of an eyes-free motion input system for a wearable device.","2007","2023-07-05 07:30:22","2023-07-20 00:16:03","2023-07-05 07:30:22","121-132","","","4813","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-76702-2_13","","","","","","Oakley, Ian; Brewster, Stephen","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XENVQ5N2","bookSection","2009","Grant, Jane; Matthias, John; Hodgson, Tim; Miranda, Eduardo","Hearing Thinking","Applications of Evolutionary Computing","978-3-642-01128-3 978-3-642-01129-0","","","http://link.springer.com/10.1007/978-3-642-01129-0_70","This paper describes early experiments, which attempt to reconfigure the sound of a breath using a network of artificial spiking cortical neurons. The connectivity of the network evolves according to a spike timing dependent plasticity algorithm and the instrument triggers grains of sound from recordings of the breath when any of the neurons fire.","2009","2023-07-05 07:30:22","2023-07-19 11:31:20","2023-07-05 07:30:22","609-614","","","5484","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-01129-0_70","","","","","","Giacobini, Mario; Brabazon, Anthony; Cagnoni, Stefano; Di Caro, Gianni A.; Ekárt, Anikó; Esparcia-Alcázar, Anna Isabel; Farooq, Muddassar; Fink, Andreas; Machado, Penousal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L6JF6TCM","journalArticle","2021","Rosso, Mattia; Maes, Pieter J.; Leman, Marc","Modality-specific attractor dynamics in dyadic entrainment","Scientific Reports","","2045-2322","10.1038/s41598-021-96054-8","https://www.nature.com/articles/s41598-021-96054-8","Abstract             Rhythmic joint coordination is ubiquitous in daily-life human activities. In order to coordinate their actions towards shared goals, individuals need to co-regulate their timing and move together at the collective level of behavior. Remarkably, basic forms of coordinated behavior tend to emerge spontaneously as long as two individuals are exposed to each other’s rhythmic movements. The present study investigated the dynamics of spontaneous dyadic entrainment, and more specifically how they depend on the sensory modalities mediating informational coupling. By means of a novel interactive paradigm, we showed that dyadic entrainment systematically takes place during a minimalistic rhythmic task despite explicit instructions to ignore the partner. Crucially, the interaction was organized by clear dynamics in a modality-dependent fashion. Our results showed highly consistent coordination patterns in visually-mediated entrainment, whereas we observed more chaotic and more variable profiles in the auditorily-mediated counterpart. The proposed experimental paradigm yields empirical evidence for the overwhelming tendency of dyads to behave as coupled rhythmic units. In the context of our experimental design, it showed that coordination dynamics differ according to availability and nature of perceptual information. Interventions aimed at rehabilitating, teaching or training sensorimotor functions can be ultimately informed and optimized by such fundamental knowledge.","2021-09-15","2023-07-05 07:30:22","2023-07-05 07:30:22","2023-07-05 07:30:22","18355","","1","11","","Sci Rep","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/5SSNDS97/Rosso et al. - 2021 - Modality-specific attractor dynamics in dyadic ent.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UFTWXXD5","journalArticle","2011","Hoffman, Guy; Weinberg, Gil","Interactive improvisation with a robotic marimba player","Autonomous Robots","","0929-5593, 1573-7527","10.1007/s10514-011-9237-0","http://link.springer.com/10.1007/s10514-011-9237-0","Shimon is a interactive robotic marimba player, developed as part of our ongoing research in Robotic Musicianship. The robot listens to a human musician and continuously adapts its improvisation and choreography, while playing simultaneously with the human. We discuss the robot’s mechanism and motion-control, which uses physics simulation and animation principles to achieve both expressivity and safety. We then present an interactive improvisation system based on the notion of physical gestures for both musical and visual expression. The system also uses anticipatory action to enable real-time improvised synchronization with the human player. We describe a study evaluating the effect of embodiment on one of our improvisation modules: antiphony, a call-and-response musical synchronization task. We conducted a 3×2 within-subject study manipulating the level of embodiment, and the accuracy of the robot’s response. Our findings indicate that synchronization is aided by visual contact when uncertainty is high, but that pianists can resort to internal rhythmic coordination in more predictable settings. We find that visual coordination is more effective for synchronization in slow sequences; and that occluded physical presence may be less effective than audio-only note generation. Finally, we test the effects of visual contact and embodiment on audience appreciation. We find that visual contact in joint Jazz improvisation makes for a performance in which audiences rate the robot as playing better, more like a human, as more responsive, and as more inspired by the human. They also rate the duo as better synchronized, more coherent, communicating, and coordinated; and the human as more inspired and more responsive.","2011-10","2023-07-05 07:30:22","2023-07-19 11:40:11","2023-07-05 07:30:22","133-153","","2-3","31","","Auton Robot","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WX84XM6G","bookSection","2008","Wilkie, Sonia; Stevens, Catherine; Dean, Roger","Psychoacoustic Manipulation of the Sound-Induced Illusory Flash","Computer Music Modeling and Retrieval. Sense of Sounds","978-3-540-85034-2 978-3-540-85035-9","","","http://link.springer.com/10.1007/978-3-540-85035-9_15","Psychological research on cross-modal perception has focused on the manipulation of sensory information predominantly by visual information. There is a lacuna in using auditory stimuli to manipulate other sensory information. The Sound Induced Illusory Flash is one illusory paradigm that uses the auditory system to bias other sensory information. However, more research is needed into the different conditions under which the Sound Induced Illusory Flash manifests and is enhanced or reduced. The experiment reported here investigates the effect of new auditory variables on the Sound Induced Illusory Flash. The variables to be discussed include the use of pitch intervals and harmonic relationships. The ultimate aim is to develop the illusory effect as a basis for new multi-media techniques and creative applications for the temporal manipulation and spatialisation of visual objects.","2008","2023-07-05 07:30:22","2023-07-19 23:49:37","2023-07-05 07:30:22","223-234","","","4969","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-85035-9_15","","/Users/minsik/Zotero/storage/HEQA4QTY/Wilkie et al. - 2008 - Psychoacoustic Manipulation of the Sound-Induced I.pdf","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IX6A26B5","bookSection","2016","Balvis, Luca; Boratto, Ludovico; Mulas, Fabrizio; Spano, Lucio Davide; Carta, Salvatore; Fenu, Gianni","Keep the Beat: Audio Guidance for Runner Training","Human-Centered and Error-Resilient Systems Development","978-3-319-44901-2 978-3-319-44902-9","","","http://link.springer.com/10.1007/978-3-319-44902-9_16","Understanding how to map the feedback by fitness apps into concrete actions during the exercise performance is crucial for their effectiveness, for both inexperienced and advanced users. In this paper we focus on audio feedback for running, describing a beat-rhythm representation of the target cadence for helping the user in keeping it. We designed the feedback system in order to balance two conflicting objectives: its effectiveness in helping the user in reaching the training goal and its intrusiveness with respect to concurrent activities (e.g., listening to the music). We detail how we track the user’s cadence through standard smartphone sensors, how and when we generate the audio messages. Finally, we discuss the results of a user-study, showing effectiveness with respect to the adherence to the exercise goal and the overall usability.","2016","2023-07-05 07:30:22","2023-07-20 05:54:14","2023-07-05 07:30:22","246-257","","","9856","","","Keep the Beat","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-44902-9_16","","/Users/minsik/Zotero/storage/6MEWSQFU/Balvis et al. - 2016 - Keep the Beat Audio Guidance for Runner Training.pdf","","","","Bogdan, Cristian; Gulliksen, Jan; Sauer, Stefan; Forbrig, Peter; Winckler, Marco; Johnson, Chris; Palanque, Philippe; Bernhaupt, Regina; Kis, Filip","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MQMJD8R3","journalArticle","2018","Ishizawa, Fumiko; Sakamoto, Mizuki; Nakajima, Tatsuo","Extracting intermediate-level design knowledge for speculating digital–physical hybrid alternate reality experiences","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-017-5595-8","http://link.springer.com/10.1007/s11042-017-5595-8","This paper reports a process to derive intermediate-level knowledge as a service design and analysis framework for designing digital services to offer alternate reality experiences, and analyzes the possible opportunities and pitfalls of the framework. The user experience felt by refining the meaning of real space through virtuality is defined as alternate reality experiences. Alternate reality experiences are typically achieved by modifying our eyesight or replacing our five senses to others, and they make our world interactive by implicitly influencing human attitudes and behaviors. First, the paper extracts observations for deriving the intermediate-level knowledge through the discussions raised in exploration workshops. In the workshops, the three digital services that utilize diverse strategies to offer alternate reality experiences are chosen. The workshops’ main focus is to examine how a person could have a sense of values in alternate reality experiences via the three digital services. Second, the paper shows how to derive the proposed service design and analysis framework from the extracted observations through expert analysis, then an overview of the framework is explained. Finally, the paper presents feasibility analysis of the proposed framework through a new digital service named Mindful Reminder as a case study for refining the service through focus group discussions. The approach described in the paper is to report a concrete process through which extracted observations can be converted into intermediate-level knowledge that can be used to design alternate reality experiences. Traditionally, the process for generating intermediate-level knowledge has not been well-documented; however, documenting the process is very important in theorizing the design of alternate reality experiences and helps effectively develop a variety of emerging advanced digital services that will offer alternate reality experiences in the future.","2018-08","2023-07-05 07:30:22","2023-07-21 04:32:47","2023-07-05 07:30:22","21329-21370","","16","77","","Multimed Tools Appl","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IKB7XBCX","bookSection","2006","Sánchez, Jaime; Baloian, Nelson","Issues in Implementing Awareness in Collaborative Software for Blind People","Computers Helping People with Special Needs","978-3-540-36020-9 978-3-540-36021-6","","","http://link.springer.com/10.1007/11788713_190","There is no doubt among the members of the CSCW community that awareness is a key issue in the design of successful collaborative software. In many systems awareness mechanisms have been implemented through displaying graphic information over the system’s interface. However, this strategy does not apply when the end users of the system are blind people. In this work we report the problems we encountered when implementing a collaborative game for supporting the learning of music and sound by blind people when trying to develop effective awareness mechanisms. The preliminary results have helped us to be ""aware"" about some characteristics awareness mechanisms should have for blind people which are not as prominent and problematic for sighted people.","2006","2023-07-05 07:30:22","2023-07-19 23:52:58","2023-07-05 07:30:22","1318-1325","","","4061","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11788713_190","","","","","","Miesenberger, Klaus; Klaus, Joachim; Zagler, Wolfgang L.; Karshmer, Arthur I.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4JFNPTYX","journalArticle","2014","Toharia, Pablo; Morales, Juan; De Juan, Octavio; Fernaud, Isabel; Rodríguez, Angel; DeFelipe, Javier","Musical Representation of Dendritic Spine Distribution: A New Exploratory Tool","Neuroinformatics","","1539-2791, 1559-0089","10.1007/s12021-013-9195-0","http://link.springer.com/10.1007/s12021-013-9195-0","Dendritic spines are small protrusions along the dendrites of many types of neurons in the central nervous system and represent the major target of excitatory synapses. For this reason, numerous anatomical, physiological and computational studies have focused on these structures. In the cerebral cortex the most abundant and characteristic neuronal type are pyramidal cells (about 85 % of all neurons) and their dendritic spines are the main postsynaptic target of excitatory glutamatergic synapses. Thus, our understanding of the synaptic organization of the cerebral cortex largely depends on the knowledge regarding synaptic inputs to dendritic spines of pyramidal cells. Much of the structural data on dendritic spines produced by modern neuroscience involves the quantitative analysis of image stacks from light and electron microscopy, using standard statistical and mathematical tools and software developed to this end. Here, we present a new method with musical feedback for exploring dendritic spine morphology and distribution patterns in pyramidal neurons. We demonstrate that audio analysis of spiny dendrites with apparently similar morphology may “sound” quite different, revealing anatomical substrates that are not apparent from simple visual inspection. These morphological/music translations may serve as a guide for further mathematical analysis of the design of the pyramidal neurons and of spiny dendrites in general.","2014-01-07","2023-07-05 07:32:03","2023-07-21 04:37:23","2023-07-05 07:32:03","","","","","","Neuroinform","Musical Representation of Dendritic Spine Distribution","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/QTSJ8C47/Toharia et al. - 2014 - Musical Representation of Dendritic Spine Distribu.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9D6BU49R","journalArticle","2017","Eckstein, Justin","Radiolab’s Sound Strategic Maneuvers","Argumentation","","0920-427X, 1572-8374","10.1007/s10503-016-9416-4","http://link.springer.com/10.1007/s10503-016-9416-4","How might argumentation scholars approach sound? Using the analytics afforded by strategic maneuvering, this essay identifies three unique features of sonic presentational devices: they are immersive, immediate and embodied. Although these features offer arguers presentational resource, they also pose new problems to the reasonable resolution of disagreement: immersion hazards overlap (mask), immediacy risks rate of delivery beyond reflection (velocity), and materiality can coerce listeners (force). To theorize strategic use of sound, I reconstruct and analyze a popular Radiolab segment “The Unconscious Toscanini of the Brain.” I find Radiolab uses three different sonic figures: (1) synchronicity, or the translation of data into sound to foreground temporal relations; (2) musical stings, an auditory invocation of embodied memory and (3) the wave, a sonic strategy to arouse and narrow attention. I conclude that Radiolab’s use of sound is reasonable because it extends the critical discussion.","2017-12","2023-07-05 07:32:03","2023-07-19 11:32:30","2023-07-05 07:32:03","663-680","","4","31","","Argumentation","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FPKG5566","bookSection","2010","Hug, Daniel","Investigating Narrative and Performative Sound Design Strategies for Interactive Commodities","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_2","Computing technologies turn everyday artefacts into narrative, procedural objects. This observation suggests that the narrative sound design strategies used in films and many video games could also be applied for the design of interactive commodities. However, it is unknown whether these strategies from immersive media can be applied in physical artefacts of everyday use. In this paper we describe methodological considerations and outline a structure of a revisable, design oriented, participatory research process, which allows to explore narrative sound designs and their possible application in interactive commodities in a systematic yet explorative way. The process, which focused on interpretational aspects, has been applied in two workshops and their results are reported and discussed. The experience of the prototyping and evaluation method, which made use of theatrical strategies, raised important questions about the role of performativity in the emergence of meaning and the possible limitations of a strictly hermeneutic aesthetics, when dealing with sonically enhanced interactive commodities.","2010","2023-07-05 07:32:03","2023-07-19 11:37:48","2023-07-05 07:32:03","12-40","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_2","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q2AZ3CAK","bookSection","2023","Rodrigues, Telma; Maçãs, Catarina; Rodrigues, Ana","Visual Representation of the Internet Consumption in the European Union","Artificial Intelligence in Music, Sound, Art and Design","978-3-031-29955-1 978-3-031-29956-8","","","https://link.springer.com/10.1007/978-3-031-29956-8_16","The impact of internet usage on the environment is a contradictory topic. While it can help reduce carbon emissions, with smart grids or the automation of services and resources, it can also increase e-waste that end up affecting the environment. To draw attention to the impact of energy consumption on the environment, we proposed and developed a computational artifact that unites the areas of Data Aesthetics and Interaction Design. The artifact, displayed in an interactive installation, was divided into three panels: (i) the left panel, which represents the countries—from the European Union (EU)—with the lowest energy consumption impact on the environment; (ii) the central panel, which use swarming boids to represent the internet usage at the installation site and its impact; and (iii) the right panel, which represents the EU countries with the highest energy impact on the environment. The arrangement of the three panels in a single interactive installation aims to establish a visual connection between the energy consumption in the EU and the energy consumption in the installation’s site and to promote awareness of its impact on the environment.","2023","2023-07-05 07:32:03","2023-07-19 11:34:20","2023-07-05 07:32:03","244-259","","","13988","","","","","","","","Springer Nature Switzerland","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-031-29956-8_16","","","","","","Johnson, Colin; Rodríguez-Fernández, Nereida; Rebelo, Sérgio M.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CFHB45EM","bookSection","2006","Murphy, Emma; Pirhonen, Antti; McAllister, Graham; Yu, Wai","A Semiotic Approach to the Design of Non-speech Sounds","Haptic and Audio Interaction Design","978-3-540-37595-1 978-3-540-37596-8","","","http://link.springer.com/10.1007/11821731_12","In the field of auditory display there is currently a lack of theoretical support for the design of non-speech sounds as elements of a user interface. Sound design methods are often based on ad hoc choices or the personal preferences of the designer. A method is proposed in this paper based on a semiotic approach to the design of non-speech sounds. In this approach, the design process is conceptualised by referring to structural semiotics, acknowledging the unique qualities of non-speech sounds, as a mode of conveying information. This method is based on a rich use scenario presented to a design panel. A case study where the design method has been applied is presented and evaluated. Finally recommendations for a practical design method are presented supported by this empirical investigation.","2006","2023-07-05 07:32:03","2023-07-20 00:15:41","2023-07-05 07:32:03","121-132","","","4129","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11821731_12","","","","","","McGookin, David; Brewster, Stephen","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZKMFKNVG","journalArticle","2012","Drioli, Carlo; Rocchesso, Davide","Acoustic rendering of particle-based simulation of liquids in motion","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0063-7","http://link.springer.com/10.1007/s12193-011-0063-7","In interaction and interface design, the representation of continuous processes often uses liquid metaphors, such as dripping or streaming. When an auditory display of such processes is required, an approach to sound-synthesis based on the physics of liquids in motion would be the most convincing, especially when real-time interaction is into play. In order to bridge the complexity of fluid-dynamic simulations with the needs of interactive sonification, we propose a multi-rate sound synthesis of liquid phenomena. Low-rate smoothed-particle hydrodynamics is used to model liquids in motion and to trigger sound-emitting events. Such events, such as solid-liquid collision, or bubble formation, are synthesized at audio rate. The proposed method is applied to the two important cases of liquid falling into a vessel, and of solid object falling into a liquid. Some example applications in interaction design are presented.","2012-05","2023-07-05 07:32:03","2023-07-20 06:53:33","2023-07-05 07:32:03","187-195","","3-4","5","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q3EHH9B4","journalArticle","2012","Cristofol, Jean","Elephant fish and GPS","AI & SOCIETY","","0951-5666, 1435-5655","10.1007/s00146-011-0336-4","http://link.springer.com/10.1007/s00146-011-0336-4","Elephant fish and GPS is an attempt to reflect on data flux, and artistic practice considered as a way to implement an experience specific to a flux. Sonification is particularly well suited to this type of implementation. As such, it leads us to question the nature of this type of experience, the position of the person who is faced with the artistic object, and the position and function of the artist. It allows us to query the status of devices produced by such an artistic practice and the status of what is, traditionally, called a “work of art”. The approach is derived from neither the study of specific artistic projects that could be considered as examples from which a general model could be extrapolated, nor, conversely, that of the enunciation of models illustrated by concrete references. The intention is not to categorize practice or behavior. It is rather an approach that seeks to shift concepts and ways of thinking through progressive analogies aimed at critically questioning the notion of experience in a transition from “object” based logic to “flux” or “field”, based logic, coming from an aesthetic perspective.","2012-05","2023-07-05 07:32:03","2023-07-19 11:18:58","2023-07-05 07:32:03","183-187","","2","27","","AI & Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3QGHTI46","bookSection","2011","Milne, Andrew J.; Carlé, Martin; Sethares, William A.; Noll, Thomas; Holland, Simon","Scratching the Scale Labyrinth","Mathematics and Computation in Music","978-3-642-21589-6 978-3-642-21590-2","","","http://link.springer.com/10.1007/978-3-642-21590-2_14","In this paper, we introduce a new approach to computeraided microtonal improvisation by combining methods for (1) interactive scale navigation, (2) real-time manipulation of musical patterns and (3) dynamical timbre adaption in solidarity with the respective scales. On the basis of the theory of well-formed scales we offer a visualization of the underlying combinatorial ramifications in terms of a scale labyrinth. This involves the selection of generic well-formed scales on a binary tree (based on the Stern-Brocot tree) as well as the choice of specific tunings through the specification of the sizes of a period (pseudo-octave) and a generator (pseudo-fifth), whose limits are constrained by the actual position on the tree. We also introduce a method to enable transformations among the modes of a chosen scale (generalized and refined “diatonic” and “chromatic” transpositions). To actually explore the scales and modes through the shaping and transformation of rhythmically and melodically interesting tone patterns, we propose a playing technique called Fourier Scratching. It is based on the manipulation of the “spectra” (DFT) of playing gestures on a sphere. The coordinates of these gestures affect score and performance parameters such as scale degree, loudness, and timbre. Finally, we discuss a technique to dynamically match the timbre to the selected scale tuning.","2011","2023-07-05 07:32:03","2023-07-21 04:28:47","2023-07-05 07:32:03","180-195","","","6726","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-21590-2_14","","/Users/minsik/Zotero/storage/N2EGAIU2/Milne et al. - 2011 - Scratching the Scale Labyrinth.pdf","","","","Agon, Carlos; Andreatta, Moreno; Assayag, Gérard; Amiot, Emmanuel; Bresson, Jean; Mandereau, John","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6QUWWTIS","bookSection","2008","Merer, Adrien; Ystad, Sølvi; Kronland-Martinet, Richard; Aramaki, Mitsuko","Semiotics of Sounds Evoking Motions: Categorization and Acoustic Features","Computer Music Modeling and Retrieval. Sense of Sounds","978-3-540-85034-2 978-3-540-85035-9","","","http://link.springer.com/10.1007/978-3-540-85035-9_9","The current study is part of a larger project aiming at offering intuitive mappings of control parameters piloting synthesis models by semantic descriptions of sounds, i.e. simple verbal labels related to various feelings, emotions, gestures or motions. Hence, this work is directly related to the general problem of semiotics of sounds. We here put a special interest in sounds evoking different perceived motions. In this paper, the experimental design of the listening tests is described and the results obtained from behavioural data are discussed. Then a set of signal descriptors is compared to categories using feature selection methods. A special interest is given to applications for sound synthesis.","2008","2023-07-05 07:32:03","2023-07-19 23:49:16","2023-07-05 07:32:03","139-158","","","4969","","","Semiotics of Sounds Evoking Motions","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-85035-9_9","","","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UIBP4XRT","journalArticle","2022","Cobos, Maximo; Ahrens, Jens; Kowalczyk, Konrad; Politis, Archontis","An overview of machine learning and other data-based methods for spatial audio capture, processing, and reproduction","EURASIP Journal on Audio, Speech, and Music Processing","","1687-4722","10.1186/s13636-022-00242-x","https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-022-00242-x","Abstract             The domain of spatial audio comprises methods for capturing, processing, and reproducing audio content that contains spatial information. Data-based methods are those that operate directly on the spatial information carried by audio signals. This is in contrast to model-based methods, which impose spatial information from, for example, metadata like the intended position of a source onto signals that are otherwise free of spatial information. Signal processing has traditionally been at the core of spatial audio systems, and it continues to play a very important role. The irruption of deep learning in many closely related fields has put the focus on the potential of learning-based approaches for the development of data-based spatial audio applications. This article reviews the most important application domains of data-based spatial audio including well-established methods that employ conventional signal processing while paying special attention to the most recent achievements that make use of machine learning. Our review is organized based on the topology of the spatial audio pipeline that consist in capture, processing/manipulation, and reproduction. The literature on the three stages of the pipeline is discussed, as well as on the spatial audio representations that are used to transmit the content between them, highlighting the key references and elaborating on the underlying concepts. We reflect on the literature based on a juxtaposition of the prerequisites that made machine learning successful in domains other than spatial audio with those that are found in the domain of spatial audio as of today. Based on this, we identify routes that may facilitate future advancement.","2022-05-16","2023-07-05 07:32:03","2023-07-05 07:32:03","2023-07-05 07:32:03","10","","1","2022","","J AUDIO SPEECH MUSIC PROC.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/RN7V7XKU/Cobos et al. - 2022 - An overview of machine learning and other data-bas.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9AICZU54","bookSection","2012","Chan, Shih-Han; Natkin, Stéphane; Tiger, Guillaume; Topol, Alexandre","Extensible Sound Description in COLLADA: A Unique File for a Rich Sound Design","Advances in Computer Entertainment","978-3-642-34291-2 978-3-642-34292-9","","","http://link.springer.com/10.1007/978-3-642-34292-9_11","Most standard scene description languages include a sound description and factorize common elements needed by the description of visual and auditory information. Both aspects are described with the same coordinate system for example. However, as soon as a dynamic description or external data are required, this benefit is lost and all the glue must be done by a programming solution that does not fit designers or authors usual skills. In this paper we address this problem and propose a solution to give back to designers the bigger role even when the scene is dynamic or based on procedural synthesizers. This solution is based on the COLLADA file format in which we have added sound support, scripting capabilities and external extensions. The use of this augmented COLLADA language is illustrated through the creation of a dynamic urban soundscape.","2012","2023-07-05 07:32:03","2023-07-19 11:10:43","2023-07-05 07:32:03","151-166","","","7624","","","Extensible Sound Description in COLLADA","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-34292-9_11","","","","","","Nijholt, Anton; Romão, Teresa; Reidsma, Dennis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TIYFDF3G","bookSection","2005","Nicol, Craig; Brewster, Stephen; Gray, Philip","A System for Manipulating Audio Interfaces Using Timbre Spaces","Computer-Aided Design of User Interfaces IV","978-1-4020-3145-8","","","http://link.springer.com/10.1007/1-4020-3304-4_29","The creation of audio interfaces is currently hampered by the difficulty of designing sounds for them. This paper presents a novel system for generating and manipulating non-speech sounds. The system is designed to generate Auditory Icons and Earcons through a common interface. It has been developed to make the design of audio interfaces easier. Using a Timbre Space representation of the sound, it generates output via an FM synthesiser. The Timbre Space has been compiled in both Fourier and Constant Q Transform versions using Principal Components Analysis (PCA). The design of the system and initial evaluations of these two versions are discussed, showing that the Fourier analysis appears to be better, contrary to initial expectations.","2005","2023-07-05 07:32:03","2023-07-19 23:50:47","2023-07-05 07:32:03","361-374","","","","","","","","","","","Springer-Verlag","Berlin/Heidelberg","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/1-4020-3304-4_29","","","","","","Jacob, Robert J.K.; Limbourg, Quentin; Vanderdonckt, Jean","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"75J4Q6BR","bookSection","2017","Ucar, Ezgi","Eclipse: A Wearable Instrument for Performance Based Storytelling","Bridging People and Sound","978-3-319-67737-8 978-3-319-67738-5","","","http://link.springer.com/10.1007/978-3-319-67738-5_7","","2017","2023-07-05 07:32:03","2023-07-05 07:32:03","2023-07-05 07:32:03","125-133","","","10525","","","Eclipse","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-67738-5_7","","","","","","Aramaki, Mitsuko; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"39DQR6VX","bookSection","2016","Solèr, Matthias; Bazin, Jean-Charles; Wang, Oliver; Krause, Andreas; Sorkine-Hornung, Alexander","Suggesting Sounds for Images from Video Collections","Computer Vision – ECCV 2016 Workshops","978-3-319-48880-6 978-3-319-48881-3","","","http://link.springer.com/10.1007/978-3-319-48881-3_59","Given a still image, humans can easily think of a sound associated with this image. For instance, people might associate the picture of a car with the sound of a car engine. In this paper we aim to retrieve sounds corresponding to a query image. To solve this challenging task, our approach exploits the correlation between the audio and visual modalities in video collections. A major difficulty is the high amount of uncorrelated audio in the videos, i.e., audio that does not correspond to the main image content, such as voice-over, background music, added sound effects, or sounds originating off-screen. We present an unsupervised, clustering-based solution that is able to automatically separate correlated sounds from uncorrelated ones. The core algorithm is based on a joint audio-visual feature space, in which we perform iterated mutual kNN clustering in order to effectively filter out uncorrelated sounds. To this end we also introduce a new dataset of correlated audio-visual data, on which we evaluate our approach and compare it to alternative solutions. Experiments show that our approach can successfully deal with a high amount of uncorrelated audio.","2016","2023-07-05 07:32:03","2023-07-19 23:50:23","2023-07-05 07:32:03","900-917","","","9914","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-48881-3_59","","","","","","Hua, Gang; Jégou, Hervé","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PK5JEEIG","bookSection","2019","Kalonaris, Stefano","Evolutionary Games for Audiovisual Works: Exploring the Demographic Prisoner’s Dilemma","Computational Intelligence in Music, Sound, Art and Design","978-3-030-16666-3 978-3-030-16667-0","","","https://link.springer.com/10.1007/978-3-030-16667-0_7","This paper presents a minimalist audiovisual display of an evolutionary game known as the Demographic Prisoner’s Dilemma, in which cooperation emerges as an evolutionary stable behaviour. Abiding by a dialogical approach foregrounding the dynamical negotiation of the author’s aesthetic aspirational levels, the cross-space mapping between the formal model and the audiovisual work is explored, and the system undergoes several variations and modifications. Questions regarding computational measures of beauty are raised and discussed.","2019","2023-07-05 07:32:03","2023-07-19 23:47:25","2023-07-05 07:32:03","98-109","","","11453","","","Evolutionary Games for Audiovisual Works","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-16667-0_7","","","","","","Ekárt, Anikó; Liapis, Antonios; Castro Pena, María Luz","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MVL9VS6J","bookSection","2011","Löchtefeld, Markus; Gehring, Sven; Jung, Ralf; Krüger, Antonio","Using Mobile Projection to Support Guitar Learning","Smart Graphics","978-3-642-22570-3 978-3-642-22571-0","","","http://link.springer.com/10.1007/978-3-642-22571-0_9","The guitar is one of the most widespread instruments amongst autodidacts, but even though a huge amount of learning material exists, it is still hard to learn especially without a guitar teacher. In this paper we propose an Augmented Reality concept that assists guitar students mastering their instrument using a mobile projector. With the projector mounted onto the headstock of the guitar, it is possible to project instructions directly onto the strings of the guitar. With that the user is easily able to realize where the fingers have to be placed on the fretboard (fingering) to play a certain chord or a tone sequence correctly.","2011","2023-07-05 07:32:03","2023-07-21 04:59:39","2023-07-05 07:32:03","103-114","","","6815","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-22571-0_9","","","","","","Dickmann, Lutz; Volkmann, Gerald; Malaka, Rainer; Boll, Susanne; Krüger, Antonio; Olivier, Patrick","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"62GRN6KI","bookSection","2012","Endo, Hideyuki; Yoshioka, Hideki","Enhancing Tactile Imagination through Sound and Light","Advances in Computer Entertainment","978-3-642-34291-2 978-3-642-34292-9","","","http://link.springer.com/10.1007/978-3-642-34292-9_40","Drive Mind is a unique electro-acoustic system, which offers an audience a new sonic experience produced by the refraction of light. The main feature of this system is to visualize abstract figures of sound using a ray of LED light and to manipulate the system using acrylic objects. By this manipulation, the system creates a refraction of light and attendant positional data. This positional data is used to produce sound. The complexity of refraction of the light and the frame rate of the camera cause subtle fluctuations and produce distinctive sounds. The object is to enhance an audience’s imagination by enabling them to identify with the performer’s action visually, and help understanding of complex digital expression, using not only physical material but also physical phenomena when operating the system. This system helps the audience to become familiar with complex digital expression and experience the new possibilities of sound art.","2012","2023-07-05 07:32:03","2023-07-19 11:10:54","2023-07-05 07:32:03","481-484","","","7624","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-34292-9_40","","","","","","Nijholt, Anton; Romão, Teresa; Reidsma, Dennis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DTLMU87D","bookSection","2009","Aramaki, Mitsuko; Brancheriau, Loïc; Kronland-Martinet, Richard; Ystad, Sølvi","Perception of Impacted Materials: Sound Retrieval and Synthesis Control Perspectives","Computer Music Modeling and Retrieval. Genesis of Meaning in Sound and Music","978-3-642-02517-4 978-3-642-02518-1","","","http://link.springer.com/10.1007/978-3-642-02518-1_9","In this study, we aimed at determining statistical models that allowed for the classification of impact sounds according to the perceived material (Wood, Metal and Glass). For that purpose, everyday life sounds were recorded, analyzed and resynthesized to insure the generation of realistic sounds. Listening tests were conducted to define sets of typical sounds of each material category by using a statistical approach. For the construction of statistical models, acoustic descriptors known to be relevant for timbre perception and for material identification were investigated. These models were calibrated and validated using a binary logistic regression method. A discussion about the applications of these results in the context of sound synthesis concludes the article.","2009","2023-07-05 07:32:03","2023-07-19 23:48:40","2023-07-05 07:32:03","134-146","","","5493","","","Perception of Impacted Materials","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-02518-1_9","","","","","","Ystad, Sølvi; Kronland-Martinet, Richard; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VD97JLIQ","journalArticle","2005","Srinivasan, Uma; Pfeiffer, Silvia; Nepal, Surya; Lee, Michael; Gu, Lifang; Barrass, Stephen","A Survey of MPEG-1 Audio, Video and Semantic Analysis Techniques","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-005-2716-6","http://link.springer.com/10.1007/s11042-005-2716-6","Digital audio & video data have become an integral part of multimedia information systems. To reduce storage and bandwidth requirements, they are commonly stored in a compressed format, such as MPEG-1. Increasing amounts of MPEG encoded audio and video documents are available online and in proprietary collections. In order to effectively utilise them, we need tools and techniques to automatically analyse, segment, and classify MPEG video content. Several techniques have been developed both in the audio and visual domain to analyse videos. This paper presents a survey of audio and visual analysis techniques on MPEG-1 encoded media that are useful in supporting a variety of video applications. Although audio and visual feature analyses have been carried out extensively, they become useful to applications only when they convey a semantic meaning of the video content. Therefore, we also present a survey of works that provide semantic analysis on MPEG-1 encoded videos.","2005-09","2023-07-05 07:34:57","2023-07-21 04:33:25","2023-07-05 07:34:57","105-141","","1","27","","Multimed Tools Appl","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KKFQDJ5C","bookSection","2010","Bevilacqua, Frédéric; Zamborlin, Bruno; Sypniewski, Anthony; Schnell, Norbert; Guédy, Fabrice; Rasamimanana, Nicolas","Continuous Realtime Gesture Following and Recognition","Gesture in Embodied Communication and Human-Computer Interaction","978-3-642-12552-2 978-3-642-12553-9","","","http://link.springer.com/10.1007/978-3-642-12553-9_7","We present a HMM based system for real-time gesture analysis. The system outputs continuously parameters relative to the gesture time progression and its likelihood. These parameters are computed by comparing the performed gesture with stored reference gestures. The method relies on a detailed modeling of multidimensional temporal curves. Compared to standard HMM systems, the learning procedure is simplified using prior knowledge allowing the system to use a single example for each class. Several applications have been developed using this system in the context of music education, music and dance performances and interactive installation. Typically, the estimation of the time progression allows for the synchronization of physical gestures to sound files by time stretching/compressing audio buffers or videos.","2010","2023-07-05 07:34:57","2023-07-20 00:08:19","2023-07-05 07:34:57","73-84","","","5934","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12553-9_7","","","","","","Kopp, Stefan; Wachsmuth, Ipke","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KG2I7XZ8","bookSection","2012","Necciari, Thibaud; Balazs, Peter; Kronland-Martinet, Richard; Ystad, Sølvi; Laback, Bernhard; Savel, Sophie; Meunier, Sabine","Auditory Time-Frequency Masking: Psychoacoustical Data and Application to Audio Representations","Speech, Sound and Music Processing: Embracing Research in India","978-3-642-31979-2 978-3-642-31980-8","","","http://link.springer.com/10.1007/978-3-642-31980-8_12","In this paper, the results of psychoacoustical experiments on auditory time-frequency (TF) masking using stimuli (masker and target) with maximal concentration in the TF plane are presented. The target was shifted either along the time axis, the frequency axis, or both relative to the masker. The results show that a simple superposition of spectral and temporal masking functions does not provide an accurate representation of the measured TF masking function. This confirms the inaccuracy of simple models of TF masking currently implemented in some perceptual audio codecs. In the context of audio signal processing, the present results constitute a crucial basis for the prediction of auditory masking in the TF representations of sounds. An algorithm that removes the inaudible components in the wavelet transform of a sound while causing no audible difference to the original sound after re-synthesis is proposed. Preliminary results are promising, although further development is required.","2012","2023-07-05 07:34:57","2023-07-21 05:02:48","2023-07-05 07:34:57","146-171","","","7172","","","Auditory Time-Frequency Masking","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-31980-8_12","","/Users/minsik/Zotero/storage/VEZRZMSL/Necciari et al. - 2012 - Auditory Time-Frequency Masking Psychoacoustical .pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer; Mohanty, Sanghamitra","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6EPSVILY","journalArticle","2023","El Raheb, Katerina; Buccoli, Michele; Zanoni, Massimiliano; Katifori, Akrivi; Kasomoulis, Aristotelis; Sarti, Augusto; Ioannidis, Yannis","Towards a general framework for the annotation of dance motion sequences: A framework and toolkit for collecting movement descriptions as ground-truth datasets","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-022-12602-y","https://link.springer.com/10.1007/s11042-022-12602-y","In this paper, we present a conceptual framework and toolkit for movement annotation. We explain how the design of the annotation systems, based on the framework, if combined with specific strategies for the process of annotation, can enhance the collection of ground-truth datasets for training algorithms. Computational algorithms, such as machine learning, show promising results for massive and scalable automatic movement annotation. Nevertheless, the need for reliable ground-truth datasets annotated by human experts, to train the machine learning algorithms and for bridging the gap between machine measurable and human perceived expressive aspects remains an open issue. This need constitutes a challenging task, due to the complexity of human movement and diversity of possible descriptors, as well as the high subjectivity that accompanies movement characterisation by both experts and non-expert users. We contribute to addressing this problem, by proposing a conceptual framework for dance movement manual annotation which we evaluate through the development and deployment of the toolkit. Finally, we discuss how the different design choices affect the process and the reliability of collecting data sets regarding qualitative aspects of movement.","2023-01","2023-07-05 07:34:57","2023-07-21 04:32:02","2023-07-05 07:34:57","3363-3395","","3","82","","Multimed Tools Appl","Towards a general framework for the annotation of dance motion sequences","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4HLT36JP","bookSection","2009","Grosshauser, Tobias; Hermann, Thomas","Augmented Haptics – An Interactive Feedback System for Musicians","Haptic and Audio Interaction Design","978-3-642-04075-7 978-3-642-04076-4","","","http://link.springer.com/10.1007/978-3-642-04076-4_11","This paper presents integrated vibrotactiles, a novel interface for movement and posture tuition that provides real-time feedback in a tactile form by means of interactive haptic feedback, thereby conveying information neither acoustically nor visually and it is a promising feedback means for movements in 3D-space. In this paper we demonstrate haptic augmentation for applications for musicians, since it (a) doesn’t affect the visual sense, occupied by reading music and communication, (b) doesn’t disturb in bang sensitive situations such as concerts, (c) allows to relate feedback information in the same tactile medium as the output of the musical instrument, so that an important feedback channel for musical instrument playing is extended and trained supportive. Even more, instructions from the teacher and the computer can be transmitted directly and unobtrusively in this channel. This paper presents a prototype system together with demonstrations of applications that support violinists during musical instrument learning.","2009","2023-07-05 07:34:57","2023-07-20 00:14:52","2023-07-05 07:34:57","100-108","","","5763","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-04076-4_11","","","","","","Altinsoy, M. Ercan; Jekosch, Ute; Brewster, Stephen","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DYJMQXNY","journalArticle","2021","Moumdjian, Lousin; Vervust, Thomas; Six, Joren; Schepers, Ivan; Lesaffre, Micheline; Feys, Peter; Leman, Marc","The Augmented Movement Platform For Embodied Learning (AMPEL): development and reliability","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-020-00354-8","http://link.springer.com/10.1007/s12193-020-00354-8","Background. Balance and gait impairments are highly prevalent in the neurological population. Although current rehabilitation strategies focus on motor learning principles, it is of interest to expand into embodied sensori-motor learning; that is learning through a continuous interaction between the cognition and the motor system, within an enriched sensory environment. Current developments in engineering allow for the development of enriched sensory environments through interactive feedback. Methodology. The Augmented Movement Platform for Embodied Learning (AMPEL) was developed, both in terms of hardware and software by an inter-disciplinary circular participatory design strategy. The developed device was then tested for in-between session reliability for the outcome measures inter-step interval and total onset time was investigated. Ten healthy participants walked in four experimental paths on the device in two different sessions, and between session correlations were calculated. Results. AMPEL was developed both in terms of software and hardware, with three Plug-In systems (auditory, visual, auditory + visual). The auditory Plug-In allows for flexible application of augmented feedback. The in-between session reliability of the outcomes measured by the system were between high and very high on all 4 walked paths, tested on ten healthy participants [mean age 41.8 ± 18.5; BMI 24.8 ± 6.1]. Conclusion. AMPEL shows full functionality, and has shown between session reliability for the measures of inter-step-intervals and total-onset-time in healthy controls during walking on different paths.","2021-03","2023-07-05 07:34:57","2023-07-20 07:03:10","2023-07-05 07:34:57","77-83","","1","15","","J Multimodal User Interfaces","The Augmented Movement Platform For Embodied Learning (AMPEL)","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/ZPHS49HK/Moumdjian et al. - 2021 - The Augmented Movement Platform For Embodied Learn.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GK7DW9RS","journalArticle","2018","Rachman, Laura; Liuni, Marco; Arias, Pablo; Lind, Andreas; Johansson, Petter; Hall, Lars; Richardson, Daniel; Watanabe, Katsumi; Dubal, Stéphanie; Aucouturier, Jean-Julien","DAVID: An open-source platform for real-time transformation of infra-segmental emotional cues in running speech","Behavior Research Methods","","1554-3528","10.3758/s13428-017-0873-y","http://link.springer.com/10.3758/s13428-017-0873-y","We present an open-source software platform that transforms emotional cues expressed by speech signals using audio effects like pitch shifting, inflection, vibrato, and filtering. The emotional transformations can be applied to any audio file, but can also run in real time, using live input from a microphone, with less than 20-ms latency. We anticipate that this tool will be useful for the study of emotions in psychology and neuroscience, because it enables a high level of control over the acoustical and emotional content of experimental stimuli in a variety of laboratory situations, including real-time social situations. We present here results of a series of validation experiments aiming to position the tool against several methodological requirements: that transformed emotions be recognized at above-chance levels, valid in several languages (French, English, Swedish, and Japanese) and with a naturalness comparable to natural speech.","2018-02","2023-07-05 07:34:57","2023-07-19 23:34:25","2023-07-05 07:34:57","323-343","","1","50","","Behav Res","DAVID","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/U742KSQ3/Rachman et al. - 2018 - DAVID An open-source platform for real-time trans.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2TDKMK55","journalArticle","2023","De Melo, Hyrandir Cabral","Plants detect and respond to sounds","Planta","","0032-0935, 1432-2048","10.1007/s00425-023-04088-1","https://link.springer.com/10.1007/s00425-023-04088-1","Plants are responsive to environmental stimuli such as sound. However, little is known about their sensory apparatus, mechanisms, and signaling pathways triggered by these stimuli. Thus, it is important to understand the effect of sounds on plants and their technological potential. This review addresses the effects of sounds on plants, the sensory elements inherent to sound detection by the cell, as well as the triggering of signaling pathways that culminate in plant responses. The importance of sound standardization for the study of phytoacoustics is demonstrated. Studies on the sounds emitted or reflected by plants, acoustic stress in plants, and recognition of some sound patterns by plants are also explored.","2023-03","2023-07-05 07:34:57","2023-07-21 04:54:56","2023-07-05 07:34:57","55","","3","257","","Planta","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RZPC4G4E","bookSection","2020","Smrčina, Michal","Approaching Urban Experience Through Rhythmanalysis","Design, User Experience, and Usability. Case Studies in Public and Personal Interactive Systems","978-3-030-49756-9 978-3-030-49757-6","","","http://link.springer.com/10.1007/978-3-030-49757-6_10","The paper revolves around the urban environment and its particular way of analysis. It aims to provide for a proper understanding of space and its meaningful design. It represents a theoretical basis of a more extensive research design that focuses on the dynamics of transit places, namely railway stations. There are three key parts. In the first one, I explain and contextualize rhythmanalysis. In the second one, I position it into the urban environment and discuss issues of spaces and places. The third part envisions the future case studies as it concerns the particular case of a railway station and shows possible approaches of the method from this distinct perspective. These research phases are illustrated by several hands-on examples.","2020","2023-07-05 07:34:57","2023-07-19 23:55:19","2023-07-05 07:34:57","142-161","","","12202","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-49757-6_10","","","","","","Marcus, Aaron; Rosenzweig, Elizabeth","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PEA7FLJ8","journalArticle","2012","McMullen, Shannon C.; Winkler, Fabian","Images of nature: collaboration at the intersection of nature, art and technology","The Environmentalist","","0251-1088, 1573-2991","10.1007/s10669-011-9355-4","http://link.springer.com/10.1007/s10669-011-9355-4","This paper argues that interdisciplinary collaboration between the sciences, the arts/humanities and engineering will provide innovative responses to important changes in our natural environment. Specifically, it will introduce “Images of Nature”, a case study on creative collaboration and a multi-level research project at Purdue University, headed by Prof. Shannon McMullen and Prof. Fabian Winkler. By bringing together scientists, engineers and artists, “Images of Nature” aims to convey the significance of new understandings of nature in images and tangible artifacts (e.g., data visualization, functional devices, generative and kinetic installations) for the public. It is our hope that this project will be the starting point for a flexible network connecting science, engineering and the arts on Purdue’s West Lafayette campus to enrich STEM education and provide a local model for STE(A)M (STEM disciplines plus Art), which emphasizes creativity and innovation; critical thinking and problem solving; flexibility and adaptability; cross-disciplinary communication and collaboration in the context of our changing natural environment.","2012-09","2023-07-05 07:34:57","2023-07-21 05:07:07","2023-07-05 07:34:57","311-317","","3","32","","Environmentalist","Images of nature","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KWMTBM2X","bookSection","2022","Kasibhatla, Raghav; Mahmud, Saifuddin; Sourave, Redwanul Haque; Arnett, Marcus; Kim, Jong-Hoon","Design of a Smart Puppet Theatre System for Computational Thinking Education","Intelligent Human Computer Interaction","978-3-030-98403-8 978-3-030-98404-5","","","https://link.springer.com/10.1007/978-3-030-98404-5_29","Many efforts have failed to achieve tangible models of a robotic theatre, as opposed to virtual or simulated theatres, despite many attempts to merge the progress of robotics with the growth of theatre and the performing arts. Many of the initiatives that have achieved significant progress in these domains are on a considerably larger scale, with the primary goal of entertaining rather than demonstrating the interdisciplinary nature of Robotics and Engineering. The purpose of this paper is to correctly unite the principles of Science, Technology, Engineering, Arts, and Mathematics in a small size robotic theatre that will allow for a more portable and changeable exhibition. The Tortoise and Hare play will be performed in the theatre, which is made up of both stage and puppet elements. A pan and tilt lighting system, audio integration via an external device, automated curtains with stepper motors, props, and a grid stage are among the stage’s components. A camera tracking module in the light system detects the location of a robot and communicates with the light fixtures to angle the spotlight. A transportable module that interacts wirelessly with its environment, as well as simple-moving, decorative puppet cutouts protruding from the module, make up the smart puppets. The mBlock IDE is used to edit the story in the theatre software, providing for a simple technique of programming the scene. The Smart Mini Theatre’s production of the Tortoise and Hare play intends to encourage performing arts students to experiment with robots and programming to create their own shows, in the hopes of inspiring them to pursue Robotics and Engineering as a potential career choice.","2022","2023-07-05 07:34:57","2023-07-20 06:34:59","2023-07-05 07:34:57","301-312","","","13184","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-98404-5_29","","","","","","Kim, Jong-Hoon; Singh, Madhusudan; Khan, Javed; Tiwary, Uma Shanker; Sur, Marigankar; Singh, Dhananjay","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X2QUFNUU","bookSection","2007","Foursa, Maxim; Wesche, Gerold","Movement-Based Interaction and Event Management in Virtual Environments with Optical Tracking Systems","Human-Computer Interaction. HCI Intelligent Multimodal Interaction Environments","978-3-540-73108-5 978-3-540-73110-8","","","http://link.springer.com/10.1007/978-3-540-73110-8_67","In this paper we present our experience in using optical tracking systems in Virtual Environment applications. First we briefly describe the tracking systems we used, and then we describe the application scenarios and present how we adapted the scenarios for the tracking systems. One of the tracking systems is markerless, that means that a user doesn’t have to wear any specific devices to be tracked and can interact with an application with free hand movements. With our application we compare the performance of different tracking systems and demonstrate that it is possible to perform complex actions in an intuitive way with just small special knowledge of the system and without any specific devices. This is a step forward to a more natural human-computer interface.","2007","2023-07-05 07:34:57","2023-07-20 06:31:25","2023-07-05 07:34:57","615-624","","","4552","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73110-8_67","","","","","","Jacko, Julie A.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P562EPXX","bookSection","2007","Mahmud, Murni; Sporka, Adam J.; Kurniawan, Sri H.; Slavík, Pavel","A Comparative Longitudinal Study of Non-verbal Mouse Pointer","Human-Computer Interaction – INTERACT 2007","978-3-540-74799-4 978-3-540-74800-7","","","http://link.springer.com/10.1007/978-3-540-74800-7_44","A longitudinal study of two non-speech continuous cursor control systems is presented in this paper: Whistling User Interface (U3I) and Vocal Joystick (VJ). This study combines the quantitative and qualitative methods to get a better understanding of novice users’ experience over time. Three hypotheses were tested in this study. The quantitative data show that U3I performed better in error rate and in simulating a mouse click; VJ was better on other measures. The qualitative data indicate that the participants’ opinions regarding both tools improved day-by-day. U3I was perceived as less fatiguing than VJ. U3I approached the performance of VJ at the end of the study period, indicating that these two systems can achieve similar performances as users get more experienced in using them. This study supports two hypotheses but does not provide enough evidence to support one hypothesis.","2007","2023-07-05 07:34:57","2023-07-20 06:28:22","2023-07-05 07:34:57","489-502","","","4663","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-74800-7_44","","/Users/minsik/Zotero/storage/E8QMIJRY/Mahmud et al. - 2007 - A Comparative Longitudinal Study of Non-verbal Mou.pdf","","","","Baranauskas, Cécilia; Palanque, Philippe; Abascal, Julio; Barbosa, Simone Diniz Junqueira","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y96RR4ZH","bookSection","2001","Walker, Ashley; Brewster, Stephen; McGookin, David; Ng, Adrian","Diary in the Sky: A Spatial Audio Display for a Mobile Calendar","People and Computers XV—Interaction without Frontiers","978-1-85233-515-1 978-1-4471-0353-0","","","http://link.springer.com/10.1007/978-1-4471-0353-0_33","We present a spatial audio display technique that overcomes the presentation rate bottleneck of traditional monauralaudio displays. Our compact speech display works by encoding message semantics into the acoustic spatialisation. In user testing, this display facilitated better recall of events than acon ventionalsmall screen visual display.Mor eover, results showed that this mapping aided in the recall of the absolute position of events — as opposed to merely their relativeorders — in atemporally ordered data set.","2001","2023-07-05 07:37:11","2023-07-21 04:42:43","2023-07-05 07:37:11","531-539","","","","","","Diary in the Sky","","","","","Springer London","London","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-1-4471-0353-0_33","","/Users/minsik/Zotero/storage/B8BSUPL3/Walker et al. - 2001 - Diary in the Sky A Spatial Audio Display for a Mo.pdf","","","","Blandford, Ann; Vanderdonckt, Jean; Gray, Phil","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BB9T8EE2","journalArticle","2012","Vesna, Victoria","Vibration matters: collective blue morph effect","AI & SOCIETY","","0951-5666, 1435-5655","10.1007/s00146-011-0359-x","http://link.springer.com/10.1007/s00146-011-0359-x","Once an artist takes on the challenge of making the invisible visible, or the inaudible audible, he/she is almost immediately thrown into the realm of energy at the edge of art and science. The established art world based on visual culture finds it difficult to place this kind of work. The scientific community, used to working in this realm in a reductionist way, finds it hard to comprehend. Yet, the public seems to be drawn to artwork residing “in between,” and there seems to be a universal need for a connection to the spiritual realm beyond what established religions offer. As many speculative ideas in the West circulate around ideas of energetic approach to matter in general, particularly the body and mind, alternative medicine and other Eastern philosophies are thriving. This essay will show how, in collaboration with nanoscientist James Gimzewski, we have investigated these ideas from the sounds of cells to the concept and realization of the Blue Morph installation at the Integratron [the Integratron is the creation of George Van Tassel and is based on the design of Moses’ Tabernacle, the writings of Nikola Tesla and telepathic directions from extraterrestrials. This one-of-a-kind building is a 38-foot-high, 55-foot-diameter, nonmetallic structure originally designed by Van Tassel as a rejuvenation and time machine (The Integratron 2009)].","2012-05","2023-07-05 07:37:11","2023-07-19 11:28:18","2023-07-05 07:37:11","319-323","","2","27","","AI & Soc","Vibration matters","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9U65N9AA","bookSection","2006","Hermann, Thomas; Paschalidou, Stella; Beckmann, Dirk; Ritter, Helge","Gestural Interactions for Multi-parameter Audio Control and Audification","Gesture in Human-Computer Interaction and Simulation","978-3-540-32624-3 978-3-540-32625-0","","","http://link.springer.com/10.1007/11678816_37","This paper presents an interactive multi-modal system for real-time multi-parametric gestural control of audio processing applications. We claim that this can ease the use / control of different tasks and for this we present the following as a demonstration: (1) A musical application, i.e. the multi-parametric control of digital audio effects, and (2) a scientific application, i.e. the interactive navigation of audifications. In the first application we discuss the use of PCA-based control axes and clustering to obtain dimensionality reduced control variables. In the second application we show how the tightly closed human-computer loop actively supports the detection and discovery of features in data under analysis.","2006","2023-07-05 07:37:11","2023-07-20 00:09:11","2023-07-05 07:37:11","335-338","","","3881","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11678816_37","","","","","","Gibet, Sylvie; Courty, Nicolas; Kamp, Jean-François","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KBYQZF8H","bookSection","2021","Pereira, Frederico; Marques, Rui; Vieria, Joana","Auditory Alarms Design Tool: Spectral Masking Estimation Based on a Psychoacoustic Model","Advances in Design, Music and Arts","978-3-030-55699-0 978-3-030-55700-3","","","http://link.springer.com/10.1007/978-3-030-55700-3_43","Human ability to detect auditory alarms in the presence of noise has been identified as an issue in various working environments, with potentially serious consequences. Spectral masking of alarms is recognized as a contributing factor to response failures. In this paper, a GNU OCTAVE code implementation for the estimation of spectral masking is detailed. The method is based on a psychoacoustic model of the peripheral human auditory system and is suggested as a tool to support the design of efficient auditory alarms. Three scenarios are investigated using standardized clinical auditory alarms as test stimuli: (1) pair of same priority alarms; (2) pair of different priority alarms and (3) alarm in the presence of environmental noise. The implemented method offers a visualization of estimated masked signal spectral components, enabling the sound designer to evaluate manipulations for masking avoidance.","2021","2023-07-05 07:37:11","2023-07-19 11:11:38","2023-07-05 07:37:11","621-639","","","9","","","Auditory Alarms Design Tool","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Springer Series in Design and Innovation DOI: 10.1007/978-3-030-55700-3_43","","","","","","Raposo, Daniel; Neves, João; Silva, José; Correia Castilho, Luísa; Dias, Rui","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NFNVPUEM","bookSection","2023","Mocchi, Martino; Sillano, Carlotta; Rocca, Lorena","Sound Beyond the Hedge. Towards an Acoustic Construction of Images","Proceedings of the 3rd International and Interdisciplinary Conference on Image and Imagination","978-3-031-25905-0 978-3-031-25906-7","","","https://link.springer.com/10.1007/978-3-031-25906-7_33","The use of images for representing the world should be considered as the product of a generative act of the subject more than the result of an objective process. It could be said that the construction of images is produced by a consonance, or even a resonance – a common vibration between the real and the subject, amplified in his mental processing: a physical as well as emotional, symbolic, cultural connection. Replacing the metaphorical sense of “resonance” with its proper meaning, there are many artistic, architectural, literary experiences that have attempted a translation of sound into visual images and vice versa – sometimes aiming at a synesthetic stimulation of our perception, other times configuring “silent media” able to inspire our imagination. More peculiarly, the role of sound in the process of unveiling and building images assumes a great importance in the experience of blind people. Sound strongly contributes to develop a “gaze” capable of perceiving and judging the outer world. This extends the scope of the image beyond the mere visual, placing it in a multisensory dimension. Through the method of interviewing and analysis, the paper focuses on the acoustic dynamics that affect the symbolic horizon at the basis of the construction of the image, favoring an inclusive perspective, with possible repercussions in the field of communication, art, society and environment.","2023","2023-07-05 07:37:11","2023-07-21 04:55:16","2023-07-05 07:37:11","307-314","","","631","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Networks and Systems DOI: 10.1007/978-3-031-25906-7_33","","","","","","Villa, Daniele; Zuccoli, Franca","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LUAY5U99","journalArticle","1994","McLellan, Hilary","Virtual reality and multiple intelligences: Potentials for higher education","Journal of Computing in Higher Education","","1042-1726","10.1007/BF02948570","http://link.springer.com/10.1007/BF02948570","IN THIS PAPER WE EXAMINE how virtual reality, an emerging computer-based technology, can promote learning that engages all seven of the multiple intelligences proposed by Harvard educational psychologist Howard Gardner. We provides an overview of virtual reality technologies and an overview of Gardner’s multiple intelligences. There is an extensive discussion of how virtual reality supports learning within and across seven intelligence domains. Finally, there is a review of technical and conceptual issues concerning the implementation of virtual reality in education. Educational experiences that promote the various multiple intelligences and interlinkages are needed in the emerging electronic age more than at any previous time.","1994-03","2023-07-05 07:37:11","2023-07-20 06:47:06","2023-07-05 07:37:11","33-66","","2","5","","J. Comput. High. Educ.","Virtual reality and multiple intelligences","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZHZ752V9","journalArticle","2017","Black, David; Hansen, Christian; Nabavi, Arya; Kikinis, Ron; Hahn, Horst","A Survey of auditory display in image-guided interventions","International Journal of Computer Assisted Radiology and Surgery","","1861-6410, 1861-6429","10.1007/s11548-017-1547-z","http://link.springer.com/10.1007/s11548-017-1547-z","Purpose—This article investigates the current state of the art of the use of auditory display in image-guided medical interventions. Auditory display is a means of conveying information using sound, and we review the use of this approach to support navigated interventions. We discuss the benefits and drawbacks of published systems and outline directions for future investigation. Methods—We undertook a review of scientific articles on the topic of auditory rendering in image-guided intervention. This includes methods for avoidance of risk structures and instrument placement and manipulation. The review did not include auditory display for status monitoring, for instance in anesthesia. Results—We identified 13 publications in the course of the search. Most of the literature (62%) investigates the use of auditory display to convey distance of a tracked instrument to an object using proximity or safety margins. The remainder discuss continuous guidance for navigated instrument placement. Four of the articles present clinical evaluations, 9 present laboratory evaluations, and 3 present informal evaluation (3 present both laboratory and clinical evaluations). Conclusion—Auditory display is a growing field that has been largely neglected in research in image-guided intervention. Despite benefits of auditory displays reported in both the reviewed literature and non-medical fields, adoption in medicine has been slow. Future challenges include increasing interdisciplinary cooperation with auditory display investigators to develop more meaningful auditory display designs and comprehensive evaluations which target the benefits and drawbacks of auditory display in image guidance.","2017-10","2023-07-05 07:37:11","2023-07-20 06:41:13","2023-07-05 07:37:11","1665-1676","","10","12","","Int J CARS","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/KE2FI8PQ/Black et al. - 2017 - A Survey of auditory display in image-guided inter.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RC9799LZ","journalArticle","2019","Indans, Reinis; Hauthal, Eva; Burghardt, Dirk","Towards an Audio-Locative Mobile Application for Immersive Storytelling","KN - Journal of Cartography and Geographic Information","","2524-4957, 2524-4965","10.1007/s42489-019-00007-1","http://link.springer.com/10.1007/s42489-019-00007-1","We live in an age in which digital media is omnipresent and augmented reality is beginning to find its way into our everyday lives, GPS allows us to determine our position with meter precision and the sensor capabilities of smartphones are increasing. All these technologies in combination enable us to explore one of the oldest human art forms in a new way: storytelling. The presented work aims at developing approaches that combine different elements of audio media playback, geolocation and other sensor capabilities of smartphones to allow the creation of immersive geolocated narratives within a mobile application. Thus, the narrative can be precisely adjusted to the specific spatial and temporal context of the user. One of the main goals of such a geolocated narrative would be to influence the cognitive processing of the users. As the visual perception of the location stays the same while using the app, it is only the auditory cognition that causes a subtle enhancement and thus alteration of the reality which could result in a very strong sense of immersion. There are many location-based audio applications on the market. Most of them are intended for tourism in the form of an audio tour guide. Some of those applications have been analyzed before developing our own ideas on how to intertwine sound, space and time into an immersive mobile application for storytelling with semi-linear narrative structures including a map. Possible features could be influenceability of the narrative by, e.g., speed or approach direction of the user, sound movemen","2019-05","2023-07-05 07:37:11","2023-07-21 04:28:03","2023-07-05 07:37:11","41-50","","1","69","","KN J. Cartogr. Geogr. Inf.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/V8JTA4QE/Indans et al. - 2019 - Towards an Audio-Locative Mobile Application for I.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QPGYV4I3","bookSection","2004","Velleman, Eric; Van Tol, Richard; Huiberts, Sander; Verwey, Hugo","3D Shooting Games, Multimodal Games, Sound Games and More Working Examples of the Future of Games for the Blind","Computers Helping People with Special Needs","978-3-540-22334-4 978-3-540-27817-7","","","http://link.springer.com/10.1007/978-3-540-27817-7_39","Blind people shooting virtual monsters on a real football field. Is that possible? The European Media Master of Arts-program of the Utrecht School of the Arts (Arts, Media & Technology) and the Bartiméus Accessibility Foundation in Zeist have developed a curriculum for accessible game and program development together. Within this curriculum already many spectacular games have been developed like Drive, The Curb Game, Hall of Sound, Powerchords, Wow, Demor and others. The games include the use of user panels and extensive user testing and present the future possibilities of gaming.","2004","2023-07-05 07:37:11","2023-07-19 23:53:30","2023-07-05 07:37:11","257-263","","","3118","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-27817-7_39","","","","","","Miesenberger, Klaus; Klaus, Joachim; Zagler, Wolfgang L.; Burger, Dominique","Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SZAB7UPX","bookSection","2014","Cordeiro, João; Barbosa, Álvaro","Privacy in Sound-Based Social Networks","Multidisciplinary Social Networks Research","978-3-662-45070-3 978-3-662-45071-0","","","http://link.springer.com/10.1007/978-3-662-45071-0_29","In this paper we address the issue of privacy in Online Social Networks (OSN), focusing on those that use environmental sound as a contextual cue for users activity. Through the use of a costume-made research tool consisting of an Online Sound-Based Social Network (OSBSN), we undertook scientific experiments aiming to assess how users deal with the use of sound. Results show that contextual sound is regarded as important and useful for OSN but raises important privacy concerns. In order to deal with this constraint, we propose a system based on the automatic classification of sound environment rather than capturing and sharing actual audio.","2014","2023-07-05 07:37:11","2023-07-21 04:30:41","2023-07-05 07:37:11","355-367","","","473","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Communications in Computer and Information Science DOI: 10.1007/978-3-662-45071-0_29","","","","","","Wang, Leon Shyue-Liang; June, Jason J.; Lee, Chung-Hong; Okuhara, Koji; Yang, Hsin-Chang","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HT9HDM25","journalArticle","2013","Yamabe, Tetsuo; Nakajima, Tatsuo","Playful training with augmented reality games: case studies towards reality-oriented system design","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-011-0979-7","http://link.springer.com/10.1007/s11042-011-0979-7","In this paper, we propose a reality-oriented augmentation approach to support training activities. The approach aims at adding new value and playful features to traditional training environments with keeping their original look-and-feel. For example, a game monitoring service enables to automatically record game events so that players can review a gaming process and strategy for soul-searching, or replay most impressive scenes to share the experience with others after the game finishes. Even several services are running on background, digital devices and services are seamlessly integrated to the game environment in unobtrusive way so that players can concentrate on training as usual. The concept can be applied to both traditional games (e.g., poker and the game of Go) and non-gaming activities (e.g., calligraphy and drumming). We developed four case studies on the concept: Augmented Reality Go, EmoPoker, Augmented Calligraphy and AR Drum Kit. We discuss design issues in the reality-oriented augmentation process based on user study results.","2013-01","2023-07-05 07:37:11","2023-07-21 04:33:34","2023-07-05 07:37:11","259-286","","1","62","","Multimed Tools Appl","Playful training with augmented reality games","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PIEHX9RY","bookSection","2021","Hoy, Rory; Van Nort, Doug","Augmentation of Sonic Meditation Practices: Resonance, Feedback and Interaction Through an Ecosystemic Approach","Perception, Representations, Image, Sound, Music","978-3-030-70209-0 978-3-030-70210-6","","","http://link.springer.com/10.1007/978-3-030-70210-6_38","This paper describes the design and creation of an interactive sound environment project, titled dispersion.eLabOrate. The system is defined by a ceiling array of microphones, audio input analysis, and synthesis directly driven by this analysis. Created to augment a Deep Listening performative environment, this project explores the role that interactive installations can fulfill within a structured listening context. Echoing, modulating, and extending what it hears, the system generates an environment in which its output is a product of ambient sound, feedback, and participant input. Relating to and building upon the ecosystemic model, we discuss the benefit of designing for participant incorporation within such a responsive listening environment.","2021","2023-07-05 07:37:11","2023-07-21 04:45:23","2023-07-05 07:37:11","591-599","","","12631","","","Augmentation of Sonic Meditation Practices","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-70210-6_38","","","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Aramaki, Mitsuko","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8ERHQKXF","bookSection","2022","Schöning, Julius; Diers, Julia; Lindner, Dennis; Paßfeld, Thorsten","Olfactory UIs: New Possibilities for Displaying System and Application States","Advances in Information and Communication","978-3-030-98011-5 978-3-030-98012-2","","","https://link.springer.com/10.1007/978-3-030-98012-2_50","Monitoring of critical system and application states is mainly done via graphical user interfaces (UI). Consequently, the user’s visual sense focuses on both their primary task and the monitoring task. Audiovisual in nature, the main task typically receives more attention from the user so that the monitoring task is neglected. As a solution that maps the monitoring of system and application states to the user’s sense of smell, this work systematically designs olfactory UIs by human-centered design (HCD). These olfactory UIs allow users to focus on an audiovisual task without neglecting the monitoring. Based on an online survey, a user study, and usability tests, this paper provides evidence that olfactory UIs are suitable to reliably establish an association between a specific scent and the severity of Syslog messages. The presented results were also transferred into a design solution for an olfactory UI indicating system and application states by scents.","2022","2023-07-05 07:39:56","2023-07-19 11:12:02","2023-07-05 07:39:56","704-721","","","438","","","Olfactory UIs","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Networks and Systems DOI: 10.1007/978-3-030-98012-2_50","","","","","","Arai, Kohei","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VDWGMMSC","journalArticle","2017","Geelhoed, Erik; Singh-Barmi, Kuldip; Biscoe, Ian; Cesar, Pablo; Jansen, Jack; Wang, Chen; Kaiser, Rene","Co-present and remote audience experiences: intensity and cohesion","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-016-3879-z","http://link.springer.com/10.1007/s11042-016-3879-z","This article presents the results of modelling audience response to new types of networked theatre plays. As the main contribution of the work we introduce two types of metrics: intensity, relating to how intensively co-present and remote aspects of a performance are rated, and cohesion, relating to how a performance as a whole, the combination of copresent and remote aspects, affects an audience. In particular, we model audience response based on two in the wild evaluations, staged by a low budget theatre company, a streamed and a distributed performance. The streamed performance is similar to NT Live, where a theatre play is delivered to other theatres with an audience. The distributed performance, on the other hand, connects actors in two different theatres (with audiences) creating one single play. The streamed performance was experienced as less intense as well as less cohesive by the remote audience, whilst the distributed performance integrated co-present and remote aspects tightly. Remote aspects of the distributed performance were still experienced as less intense, but the performance as a whole was highly cohesive. Apart from the identification of these two new metrics (intensity and cohesion), based on our experiences we argue that an innovative way of bundling relevant emerging technologies is needed to give a voice to the, as yet silent, remote audience.","2017-02","2023-07-05 07:39:56","2023-07-21 04:32:17","2023-07-05 07:39:56","5573-5606","","4","76","","Multimed Tools Appl","Co-present and remote audience experiences","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/74I5WBA4/Geelhoed et al. - 2017 - Co-present and remote audience experiences intens.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FYPHFMG8","bookSection","2009","Dorin, Alan","Habitat: Engineering in a Simulated Audible Ecosystem","Applications of Evolutionary Computing","978-3-642-01128-3 978-3-642-01129-0","","","http://link.springer.com/10.1007/978-3-642-01129-0_55","This paper introduces a novel appr oach to generating audio or visual heterogeneity by simulating multi -level habitat formation by ecosystem engineer organisms. Ecosystem engineers generate habitat by modulation of environmental factors, such as erosion or radiation exposure, and provision of substrate. We describe Habitat, a simulation that runs on a two -dimensional grid occupied by an evolving population of stationary agents. The bodies of these agents provide local, differentiated habitat for new agents. Agents evolve using a conventional evolutionary algorithm that acts on their habitat preferences, habitat provision and lifespan, to populate the space and one another. This generates heterogeneous, dynamic structures that have been used in a prototype sonic artwork and simple visualisatio n.","2009","2023-07-05 07:39:56","2023-07-19 11:31:04","2023-07-05 07:39:56","488-497","","","5484","","","Habitat","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-01129-0_55","","/Users/minsik/Zotero/storage/XY7X9SMB/Dorin - 2009 - Habitat Engineering in a Simulated Audible Ecosys.pdf","","","","Giacobini, Mario; Brabazon, Anthony; Cagnoni, Stefano; Di Caro, Gianni A.; Ekárt, Anikó; Esparcia-Alcázar, Anna Isabel; Farooq, Muddassar; Fink, Andreas; Machado, Penousal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U676VH53","journalArticle","2017","Wang, Qi; Markopoulos, Panos; Yu, Bin; Chen, Wei; Timmermans, Annick","Interactive wearable systems for upper body rehabilitation: a systematic review","Journal of NeuroEngineering and Rehabilitation","","1743-0003","10.1186/s12984-017-0229-y","http://jneuroengrehab.biomedcentral.com/articles/10.1186/s12984-017-0229-y","Background: The development of interactive rehabilitation technologies which rely on wearable-sensing for upper body rehabilitation is attracting increasing research interest. This paper reviews related research with the aim: 1) To inventory and classify interactive wearable systems for movement and posture monitoring during upper body rehabilitation, regarding the sensing technology, system measurements and feedback conditions; 2) To gauge the wearability of the wearable systems; 3) To inventory the availability of clinical evidence supporting the effectiveness of related technologies. Method: A systematic literature search was conducted in the following search engines: PubMed, ACM, Scopus and IEEE (January 2010–April 2016). Results: Forty-five papers were included and discussed in a new cuboid taxonomy which consists of 3 dimensions: sensing technology, feedback modalities and system measurements. Wearable sensor systems were developed for persons in: 1) Neuro-rehabilitation: stroke (n = 21), spinal cord injury (n = 1), cerebral palsy (n = 2), Alzheimer (n = 1); 2) Musculoskeletal impairment: ligament rehabilitation (n = 1), arthritis (n = 1), frozen shoulder (n = 1), bones trauma (n = 1); 3) Others: chronic pulmonary obstructive disease (n = 1), chronic pain rehabilitation (n = 1) and other general rehabilitation (n = 14). Accelerometers and inertial measurement units (IMU) are the most frequently used technologies (84% of the papers). They are mostly used in multiple sensor configurations to measure upper limb kinematics and/or trunk posture. Sensors are placed mostly on the trunk, upper arm, the forearm, the wrist, and the finger. Typically sensors are attachable rather than embedded in wearable devices and garments; although studies that embed and integrate sensors are increasing in the last 4 years. 16 studies applied knowledge of result (KR) feedback, 14 studies applied knowledge of performance (KP) feedback and 15 studies applied both in various modalities. 16 studies have conducted their evaluation with patients and reported usability tests, while only three of them conducted clinical trials including one randomized clinical trial. Conclusions: This review has shown that wearable systems are used mostly for the monitoring and provision of feedback on posture and upper extremity movements in stroke rehabilitation. The results indicated that accelerometers and IMUs are the most frequently used sensors, in most cases attached to the body through ad hoc contraptions for the purpose of improving range of motion and movement performance during upper body rehabilitation. Systems featuring sensors embedded in wearable appliances or garments are only beginning to emerge. Similarly, clinical evaluations are scarce and are further needed to provide evidence on effectiveness and pave the path towards implementation in clinical settings.","2017-12","2023-07-05 07:39:56","2023-07-21 07:42:26","2023-07-05 07:39:56","20","","1","14","","J NeuroEngineering Rehabil","Interactive wearable systems for upper body rehabilitation","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/8T2JKEX6/Wang et al. - 2017 - Interactive wearable systems for upper body rehabi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LIV92Q9D","bookSection","2008","Pirhonen, Antti; Tuuri, Kai","In Search for an Integrated Design Basis for Audio and Haptics","Haptic and Audio Interaction Design","978-3-540-87882-7 978-3-540-87883-4","","","http://link.springer.com/10.1007/978-3-540-87883-4_9","Audio and haptics as interaction modalities share properties, which make them highly appropriate to be handled within a single conceptual framework. This paper outlines such framework, gaining ingredients from the literature concerning cross-modal integration and embodied cognition. The resulting framework is bound up with a concept of physical embodiment, which has been introduced within several scientific disciplines to reveal the role of bodily experience and the corresponding mental imagery as the core of meaning-creation. In addition to theoretical discussion, the contribution of the proposed approach in design is outlined.","2008","2023-07-05 07:39:56","2023-07-20 00:16:23","2023-07-05 07:39:56","81-90","","","5270","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-87883-4_9","","","","","","Pirhonen, Antti; Brewster, Stephen","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MTWHW5CZ","bookSection","2009","McCormack, Jon; Bown, Oliver","Life’s What You Make: Niche Construction and Evolutionary Art","Applications of Evolutionary Computing","978-3-642-01128-3 978-3-642-01129-0","","","http://link.springer.com/10.1007/978-3-642-01129-0_59","This paper advances new methods for ecosystemic approaches to evolutionary music and art. We explore the biological concept of the niche and its role in evolutionary dynamics, applying it to creative computational systems. Using the process of niche construction organisms are able to change and adapt their environment, and potentially that of other species. Constructed niches may become heritable environments for offspring, paralleling the way genes are passed from parent to child. In a creative ecosystem, niche construction can be used by agents to increase the diversity and heterogeneity of their output. We illustrate the usefulness of this technique by applying niche construction to line drawing and music composition.","2009","2023-07-05 07:39:56","2023-07-19 11:31:35","2023-07-05 07:39:56","528-537","","","5484","","","Life’s What You Make","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-01129-0_59","","","","","","Giacobini, Mario; Brabazon, Anthony; Cagnoni, Stefano; Di Caro, Gianni A.; Ekárt, Anikó; Esparcia-Alcázar, Anna Isabel; Farooq, Muddassar; Fink, Andreas; Machado, Penousal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YITJGPMX","bookSection","2008","Jokiniemi, Maria; Raisamo, Roope; Lylykangas, Jani; Surakka, Veikko","Crossmodal Rhythm Perception","Haptic and Audio Interaction Design","978-3-540-87882-7 978-3-540-87883-4","","","http://link.springer.com/10.1007/978-3-540-87883-4_12","Research on rhythm perception has mostly been focused on the auditory and visual modalities. Previous studies have shown that the auditory modality dominates rhythm perception. Rhythms can also be perceived through the tactile senses, for example, as vibrations, but only few studies exist. We investigated unimodal and crossmodal rhythm perception with auditory, tactile, and visual modalities. Pairs of rhythm patterns were presented to the subject who made a same-different judgment. We used all possible combinations of the three modalities. The results showed that the unimodal auditory condition had the highest rate (79.2%) of correct responses. The unimodal tactile condition (75.0%) and the auditory-tactile condition (74.2%) were close. The average rate remained under 61.7% when the visual modality was involved. The results confirmed that auditory and tactile modalities are suitable for presenting rhythmic information, and they are also preferred by the users.","2008","2023-07-05 07:39:56","2023-07-20 00:15:01","2023-07-05 07:39:56","111-119","","","5270","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-87883-4_12","","","","","","Pirhonen, Antti; Brewster, Stephen","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WDT422YX","bookSection","2015","Sun, Yuanjing; Jeon, Myounghoon","Lyricon (Lyrics + Earcons) Improves Identification of Auditory Cues","Design, User Experience, and Usability: Users and Interactions","978-3-319-20897-8 978-3-319-20898-5","","","http://link.springer.com/10.1007/978-3-319-20898-5_37","Auditory researchers have developed various non-speech cues in designing auditory user interfaces. A preliminary study of “lyricons” (lyrics + earcons [1]) has provided a novel approach to devising auditory cues in electronic products, by combining the concurrent two layers of musical speech and earcons (short musical motives). An experiment on sound-function meaning mapping was conducted between earcons and lyricons. It demonstrated that lyricons significantly more enhanced the relevance between the sound and the meaning compared to earcons. Further analyses on error type and confusion matrix show that lyricons showed a higher identification rate and a shorter mapping time than earcons. Factors affecting auditory cue identification and application directions of lyricons are discussed.","2015","2023-07-05 07:39:56","2023-07-19 23:55:07","2023-07-05 07:39:56","382-389","","","9187","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-20898-5_37","","","","","","Marcus, Aaron","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LVFJR25F","journalArticle","2020","Schnass, Karin; Teixeira, Flavio","Compressed Dictionary Learning","Journal of Fourier Analysis and Applications","","1069-5869, 1531-5851","10.1007/s00041-020-09738-6","http://link.springer.com/10.1007/s00041-020-09738-6","In this paper we show that the computational complexity of the Iterative Thresholding and K-residual-Means (ITKrM) algorithm for dictionary learning can be significantly reduced by using dimensionality-reduction techniques based on the Johnson-Lindenstrauss lemma. The dimensionality reduction is efficiently carried out with the fast Fourier transform. We introduce the Iterative compressed-Thresholding and K-Means (IcTKM) algorithm for fast dictionary learning and study its convergence properties. We show that IcTKM can locally recover an incoherent, overcomplete generating dictionary of K atoms from training signals of sparsity level S with high probability. Fast dictionary learning is achieved by embedding the training data and the dictionary into m < d dimensions, and recovery is shown to be locally stable with an embedding dimension which scales as low as m = O(S log4 S log3 K). The compression effectively shatters the data dimension bottleneck in the computational cost of ITKrM, reducing it by a factor O(m/d). Our theoretical results are complemented with numerical simulations which demonstrate that IcTKM is a powerful, low-cost algorithm for learning dictionaries from high-dimensional data sets.","2020-04","2023-07-05 07:39:56","2023-07-20 06:47:14","2023-07-05 07:39:56","33","","2","26","","J Fourier Anal Appl","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/PJJTZ3WY/Schnass and Teixeira - 2020 - Compressed Dictionary Learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SQ7WHQTB","journalArticle","2008","Peticolas, L. M.; Craig, N.; Kucera, T.; Michels, D. J.; Gerulskis, J.; MacDowall, R. J.; Beisser, K.; Chrissotimos, C.; Luhmann, J. G.; Galvin, A. B.; Ratta, L.; Drobnes, E.; Méndez, B. J.; Hill, S.; Marren, K.; Howard, R.","The Solar Terrestrial Relations Observatory (STEREO) Education and Outreach (E/PO) Program","Space Science Reviews","","0038-6308, 1572-9672","10.1007/s11214-007-9287-y","http://link.springer.com/10.1007/s11214-007-9287-y","The STEREO mission’s Education and Outreach (E/PO) program began early enough its team benefited from many lessons learned as NASA’s E/PO profession matured. Originally made up of discrete programs, by launch the STEREO E/PO program had developed into a quality suite containing all the program elements now considered standard: education workshops, teacher/student guides, national and international collaboration, etc. The benefit of bringing so many unique programs together is the resulting diverse portfolio, with scientists, E/PO professionals, and their education partners all of whom can focus on excellent smaller programs. The drawback is a less cohesive program nearly impossible to evaluate in its entirety with the given funding. When individual components were evaluated, we found our programs mostly made positive impact. In this paper, we elaborate on the programs, hoping that others will effectively use or improve upon them. When possible, we indicate the programs’ effects on their target audiences.","2008-04","2023-07-05 07:39:56","2023-07-21 05:02:18","2023-07-05 07:39:56","627-646","","1-4","136","","Space Sci Rev","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MGUAN2HG","journalArticle","1997","Edwards, Lynne K.; Link, Stephen W.; Null, Cynthia H.","High-performance computer applications in the behavioral sciences","Behavior Research Methods, Instruments, &amp Computers","","0743-3808, 1532-5970","10.3758/BF03200580","http://link.springer.com/10.3758/BF03200580","This symposium revisited the 1985conference on Advanced Computing for Psychology. That meeting examined the application of new supercomputers in the behavioral sciences. The present symposium reviewed high-performance computing as applied to psychological models, human vision, neuralphysiological processes, and statistical analysis. The recent past and the projected future of high-performance computing in the behavioral sciences were evaluated.","1997-03","2023-07-05 07:39:56","2023-07-19 23:34:35","2023-07-05 07:39:56","122-125","","1","29","","Behavior Research Methods, Instruments, & Computers","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/WTKRVH9K/Edwards et al. - 1997 - High-performance computer applications in the beha.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CVQASUYR","bookSection","2008","Jones, Daniel","AtomSwarm: A Framework for Swarm Improvisation","Applications of Evolutionary Computing","978-3-540-78760-0 978-3-540-78761-7","","","http://link.springer.com/10.1007/978-3-540-78761-7_45","This paper introduces AtomSwarm, a framework for sound-based performance using swarm dynamics. The classical ruleset for flocking simulations is augmented with genetically-encoded behaviours, hormonal flows, and viral ‘memes’, creating a complex sonic ecosystem that is capable of temporal adaptation and self-regulation. The architecture and sound design methodologies are summarised here, with critical reference to its biomimetic design process, sonic spatialisation and self-organising capabilities. It is finally suggested that the system’s lifelikeness is a product of its relational complexity, creating empathic engagement purely through abstract formal structures.","2008","2023-07-05 07:39:56","2023-07-19 11:31:27","2023-07-05 07:39:56","423-432","","","4974","","","AtomSwarm","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-78761-7_45","","","","","","Giacobini, Mario; Brabazon, Anthony; Cagnoni, Stefano; Di Caro, Gianni A.; Drechsler, Rolf; Ekárt, Anikó; Esparcia-Alcázar, Anna Isabel; Farooq, Muddassar; Fink, Andreas; McCormack, Jon; O’Neill, Michael; Romero, Juan; Rothlauf, Franz; Squillero, Giovanni; Uyar, A. Şima; Yang, Shengxiang","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8K23ADKV","bookSection","2008","Murray-Smith, Roderick; Strachan, Steven","Rotational Dynamics for Design of Bidirectional Feedback during Manual Interaction","Fun and Games","978-3-540-88321-0 978-3-540-88322-7","","","http://link.springer.com/10.1007/978-3-540-88322-7_1","Rotational dynamic system models can be used to enrich tightlycoupled embodied control of movement-sensitive mobile devices, and support a more bidirectional, negotiated style of interaction. This can provide a constructive, as well as informative, approach to the design of engaging, playful elements in interaction mechanisms. A simulated rotational spring system is used for natural eyes-free feedback in both the audio and haptic channels, and in a Mobile Spatial Interaction application, using twisting and tilting motions to drag and drop content, where users perceived the effect of varying the parameters of the simulated dynamic system.","2008","2023-07-05 07:39:56","2023-07-20 00:07:17","2023-07-05 07:39:56","1-10","","","5294","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-88322-7_1","","/Users/minsik/Zotero/storage/2LBBLA34/Murray-Smith and Strachan - 2008 - Rotational Dynamics for Design of Bidirectional Fe.pdf","","","","Markopoulos, Panos; De Ruyter, Boris; IJsselsteijn, Wijnand; Rowland, Duncan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I66YN99X","journalArticle","2012","El-Shimy, Dalia; Grond, Florian; Olmos, Adriana; Cooperstock, Jeremy R.","Eyes-free environmental awareness for navigation","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0065-5","http://link.springer.com/10.1007/s12193-011-0065-5","We consider the challenge of delivering location-based information through rich audio representations of the environment, and the associated opportunities that such an approach offers to support navigation tasks. This challenge is addressed by In-Situ Audio Services, or ISAS, a system intended primarily for use by the blind and visually impaired communities. It employs spatialized audio rendering to convey the relevant content, which may include information about the immediate surroundings, such as restaurants, cultural sites, public transportation locations, and other points of interest. Information is aggregated mostly from online data resources, converted using text-to-speech technology, and “displayed”, either as speech or more abstract audio icons, through a location-aware mobile device or smartphone. This is suitable not only for the specific constraints of the target population, but is equally useful for general mobile users whose visual attention is otherwise occupied with navigation. We designed and conducted an experiment to evaluate two techniques for delivering spatialized audio content to users via interactive auditory maps: the shockwave mode and the radar mode. While neither mode proved to be significantly better than the other, subjects proved competent at navigating the maps using these rendering strategies, and reacted positively to the system, demonstrating that spatial audio can be an effective technique for conveying location-based information. The results of this experiment and its implications to our project are described here.","2012-05","2023-07-05 07:39:56","2023-07-20 06:54:13","2023-07-05 07:39:56","131-141","","3-4","5","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WCFCN4XY","journalArticle","2021","Gilat, Moran; Ginis, Pieter; Zoetewei, Demi; De Vleeschhauwer, Joni; Hulzinga, Femke; D’Cruz, Nicholas; Nieuwboer, Alice","A systematic review on exercise and training-based interventions for freezing of gait in Parkinson’s disease","npj Parkinson's Disease","","2373-8057","10.1038/s41531-021-00224-4","https://www.nature.com/articles/s41531-021-00224-4","Freezing of gait (FOG) in Parkinson’s disease (PD) causes severe patient burden despite pharmacological management. Exercise and training are therefore advocated as important adjunct therapies. In this meta-analysis, we assess the existing evidence for such interventions to reduce FOG, and further examine which type of training helps the restoration of gait function in particular. The primary meta-analysis across 41 studies and 1838 patients revealed a favorable moderate effect size (ES = −0.37) of various training modalities for reducing subjective FOG-severity (p < 0.00001), though several interventions were not directly aimed at FOG and some included non-freezers. However, exercise and training also proved beneficial in a secondary analysis on freezers only (ES = −0.32, p = 0.007). We further revealed that dedicated training aimed at reducing FOG episodes (ES = −0.24) or ameliorating the underlying correlates of FOG (ES = −0.40) was moderately effective (p < 0.01), while generic exercises were not (ES = −0.14, p = 0.12). Relevantly, no retention effects were seen after cessation of training (ES = −0.08, p = 0.36). This review thereby supports the implementation of targeted training as a treatment for FOG with the need for long-term engagement.","2021-09-10","2023-07-05 07:40:37","2023-07-05 07:40:37","2023-07-05 07:40:37","1-18","","1","7","","npj Parkinsons Dis.","","","","","","","","en","2021 The Author(s)","","","","www.nature.com","","Number: 1 Publisher: Nature Publishing Group","","/Users/minsik/Zotero/storage/L92NU4EQ/Gilat et al. - 2021 - A systematic review on exercise and training-based.pdf","","","Rehabilitation; Parkinson's disease","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CBEX9KQB","journalArticle","2023","Ahmedien, Diaa Ahmed Mohamed","Analysing bio-art’s epistemic landscape: from metaphoric to post-metaphoric structure","BioSocieties","","1745-8552, 1745-8560","10.1057/s41292-022-00270-y","https://link.springer.com/10.1057/s41292-022-00270-y","Abstract             Since its emergence, bio-art has developed numerous metaphors central to the transfer of concepts of modern biology, genetics, and genomics to the public domain that reveal several cultural, ethical, and social variations in their related themes. This article assumes that a general typology of metaphors developed by practices related to bio-art can be categorised into two categories: pictorial and operational metaphors. Through these, information regarding several biological issues is transferred to the public arena. Based on the analysis, this article attempts to answer the following questions: How does bio-art develop metaphors to advance epistemic and discursive agendas that constitute public understanding of a set of deeply problematic assumptions regarding how today’s biology operates? Under the influence of today’s synthetic biology, could bio-media operationally reframe these epistemic agendas by reframing complex and multi-layered metaphors towards post-metaphoric structures? Finally, what are the scientific, cultural, and social implications of reframing?","2023-06","2023-07-05 07:43:00","2023-07-05 07:43:00","2023-07-05 07:43:00","308-334","","2","18","","BioSocieties","Analysing bio-art’s epistemic landscape","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/D2H2PGU9/Ahmedien - 2023 - Analysing bio-art’s epistemic landscape from meta.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZKWFMAKR","journalArticle","1998","Loomis, Jack M.; Klatzky, Roberta L.; Philbeck, John W.; Golledge, Reginald G.","Assessing auditory distance perception using perceptually directed action","Perception & Psychophysics","","0031-5117, 1532-5962","10.3758/BF03211932","http://link.springer.com/10.3758/BF03211932","Three experiments investigated auditory distance perception under natural listening conditions in a large open field. Targets varied in egocentric distance from 3 to 16 m. By presenting visual targets at these same locations on other trials, we were able to compare visual and auditory distance perception under similar circumstances. In some experimental conditions, observers made verbal reports of target distance. In others, observers viewed or listened to the target and then, without further perceptual information about the target, attempted to face the target, walk directly to it, or walk along a two-segment indirect path to it. The primary results were these. First, the verbal and walking responses were largely concordant, with the walking responses exhibiting less between-observer variability. Second, different motoric responses provided consistent estimates of the perceived target locations and, therefore, of the initially perceived distances. Third, under circumstances for which visual targets were perceived more or less correctly in distance using the more precise walking response, auditory targets were generally perceived with considerable systematic error. In particular, the perceived locations of the auditory targets varied only about half as much in distance as did the physical targets; in addition, there was a tendency to underestimate target distance, except for the closest targets.","1998-09","2023-07-05 07:43:00","2023-07-21 04:43:49","2023-07-05 07:43:00","966-980","","6","60","","Perception & Psychophysics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/4SLXJCKL/Loomis et al. - 1998 - Assessing auditory distance perception using perce.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MZSDFSKT","journalArticle","2015","Larsson, Matz","Tool-use-associated sound in the evolution of language","Animal Cognition","","1435-9448, 1435-9456","10.1007/s10071-015-0885-x","http://link.springer.com/10.1007/s10071-015-0885-x","Proponents of the motor theory of language evolution have primarily focused on the visual domain and communication through observation of movements. In the present paper, it is hypothesized that the production and perception of sound, particularly of incidental sound of locomotion (ISOL) and tool-use sound (TUS), also contributed. Human bipedalism resulted in rhythmic and more predictable ISOL. It has been proposed that this stimulated the evolution of musical abilities, auditory working memory, and abilities to produce complex vocalizations and to mimic natural sounds. Since the human brain proficiently extracts information about objects and events from the sounds they produce, TUS, and mimicry of TUS, might have achieved an iconic function. The prevalence of sound symbolism in many extant languages supports this idea. Self-produced TUS activates multimodal brain processing (motor neurons, hearing, proprioception, touch, vision), and TUS stimulates primate audiovisual mirror neurons, which is likely to stimulate the development of association chains. Tool use and auditory gestures involve motor processing of the forelimbs, which is associated with the evolution of vertebrate vocal communication. The production, perception, and mimicry of TUS may have resulted in a limited number of vocalizations or protowords that were associated with tool use. A new way to communicate about tools, especially when out of sight, would have had selective advantage. A gradual change in acoustic properties and/or meaning could have resulted in arbitrariness and an expanded repertoire of words. Humans have been increasingly exposed to TUS over millions of years, coinciding with the period during which spoken language evolved. ISOL and tool-use-related sound are worth further exploration.","2015-09","2023-07-05 07:43:00","2023-07-19 11:28:49","2023-07-05 07:43:00","993-1005","","5","18","","Anim Cogn","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GRZGSGGB","journalArticle","2023","Rubab, Sadia; Yu, Lingyun; Tang, Junxiu; Wu, Yingcai","Exploring Effective Relationships Between Visual-Audio Channels in Data Visualization","Journal of Visualization","","1343-8875, 1875-8975","10.1007/s12650-023-00909-3","https://link.springer.com/10.1007/s12650-023-00909-3","In recent years, there has been a growing trend towards taking advantage of audio--visual representations. Previous research has aimed at improving users’ performance and engagement with these representations. The attainment of these benefits primarily depends on the effectiveness of audio--visual relationships used to represent the data. However, the visualization field yet lacks an empirical study that guides the effective relationships. Given the compatibility effect between visual and auditory channels, this research presents the effectiveness of four audio channels (timbre, pitch, loudness, and tempo) with six visual channels (spatial position, color, position, length, angle, and area). In six experiments, one per visual channel, we observed how each audio channel, when used with a visual channel, impacted users’ ability to perform the differentiation or similarity task accurately. Each experiment provided the ranking of audio channels along a visual channel. Central to our experiments was the evaluation at two stages, and accordingly, we identified the effectiveness. Our results showed that timbre, with spatial position and color, aided in more accurate target identification than the three other audio channels. With position and length, pitch allowed a more accurate judgment of the magnitude of data than loudness and tempo but was less accurate than the other two channels along angle and area. Overall, our experiments showed that the choice of representation methods and tasks had impacted the effectiveness of audio channels.","2023-08","2023-07-05 07:43:00","2023-07-20 06:50:02","2023-07-05 07:43:00","937-956","","4","26","","J Vis","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EHXIRA8S","journalArticle","2017","Buzzi, Maria Claudia; Buzzi, Marina; Leporini, Barbara; Trujillo, Amaury","Analyzing visually impaired people’s touch gestures on smartphones","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-016-3594-9","http://link.springer.com/10.1007/s11042-016-3594-9","We present an analysis of how visually impaired people perform gestures on touch-screen smartphones and report their preferences, explaining the procedure and technical implementation that we followed to collect gesture samples. To that end, we recruited 36 visually impaired participants and divided them into two main groups of low-vision and blind people respectively. We then examined their touch-based gesture preferences in terms of number of strokes, multi-touch, and shape angle, as well as their execution in geometric, kinematic and relative terms. For this purpose, we developed a wireless system to simultaneously record sample gestures from several participants, with the possibility of monitoring the capture process. Our results are consistent with previous research regarding the preference of visually impaired users for simple gestures: with one finger, a single stroke, and in one or two cardinal directions. Of the two groups of participants, blind people are less consistent with multi-stroke gestures. In addition, they are more likely than low-vision people to go outside the bounds of the display in the absence of its physical delimitation of, especially with multi-touch gestures. In the case of more complex gestures, rounded shapes are greatly preferred to angular ones, especially by blind people, who have difficulty performing straight gestures with steep or right angles. Based on these results and on previous related research, we offer suggestions to improve gesture accessibility of handheld touchscreen devices.","2017-02","2023-07-05 07:43:00","2023-07-21 04:31:47","2023-07-05 07:43:00","5141-5169","","4","76","","Multimed Tools Appl","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2DZVHFDT","bookSection","2014","Hülsmann, Adrian; Maicher, Julian","HOUDINI: Introducing Object Tracking and Pen Recognition for LLP Tabletops","Human-Computer Interaction. Advanced Interaction Modalities and Techniques","978-3-319-07229-6 978-3-319-07230-2","","","http://link.springer.com/10.1007/978-3-319-07230-2_23","Tangible objects on a \tabletop offer a lot of different opportunities to interact with an application. Most of the current tabletops are built using optical tracking principles and especially LLP tabletops provide very good tracking results for touch input. In this paper we introduce HOUDINI as a method for LLP object tracking and pen recognition, which is based on three different sizes of touch points that help us to identify touch points belonging to fingers, objects and pens. As a result, the whole recognition process is performed at the level of touch information rather than frame by frame image analysis. This leads to a very efficient and reliable tracking, thus allowing the objects to be moved very fast without being lost.","2014","2023-07-05 07:43:00","2023-07-20 06:30:31","2023-07-05 07:43:00","234-244","","","8511","","","HOUDINI","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-07230-2_23","","/Users/minsik/Zotero/storage/VKFKH68Z/Hülsmann and Maicher - 2014 - HOUDINI Introducing Object Tracking and Pen Recog.pdf","","","","Kurosu, Masaaki","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Kobsa, Alfred; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Doug; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4I84YDUF","journalArticle","2018","Marchetti, Emanuela; Valente, Andrea","Interactivity and multimodality in language learning: the untapped potential of audiobooks","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-017-0549-5","http://link.springer.com/10.1007/s10209-017-0549-5","In this work we present three case studies, involving classes in primary and secondary schools, in Denmark. The studies, conducted in the past 2 years, show how audio contents can be generated and shared among teachers and learners, how audio materials can be made more interactive to offer fruition similar to that of digital games, and how language learning can benefit from adding a social dimension to audiobooks. All case studies were conducted in a user centered fashion and build on social semiotics, in which interactive audiobooks are seen as providing new ways to receive, interpret and share literary texts. Local primary and secondary schools were involved in ethnographic user studies and qualitative evaluations with semi-functioning prototypes. In the main case study presented, social interaction was chosen as key feature to allow high-school students and teachers to annotate audiobooks, then share and comment on the annotations; the social context in this case is a digitally-augmented English teaching class. To better investigate the potential of sharable audiobook annotations we also created a mockup supporting the workflow of the main case study, using standard YouTube annotations and freely available audiobooks. The findings and technical solutions explored in the three studies are the basis for design guidelines aiming at making audiobooks interactive and better integrated in learning contexts.","2018-06","2023-07-05 07:43:00","2023-07-21 05:11:52","2023-07-05 07:43:00","257-274","","2","17","","Univ Access Inf Soc","Interactivity and multimodality in language learning","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/TF46TL2E/Marchetti and Valente - 2018 - Interactivity and multimodality in language learni.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5KZYIYKZ","journalArticle","2022","Buehler, Markus J.","DeepFlames: Neural network-driven self-assembly of flame particles into hierarchical structures","MRS Communications","","2159-6867","10.1557/s43579-022-00171-y","https://link.springer.com/10.1557/s43579-022-00171-y","The spontaneous assembly of materials from elementary building blocks is one of the most intriguing natural phenomena. Conventional modeling relies physical approaches to examine such processes. In this paper, a framework is proposed to offer an alternative paradigm, via the use of deep learning, and specifically the use of generative adversarial models as well as a combination of natural language processing and transformer neural nets to create hierarchical assemblies of building blocks. We study the assembly of elementary flame particles into hierarchical materials with features across scales, illustrating the Universality–Diversity Principle (UDP), and create novel material using additive manufacturing.","2022-04","2023-07-05 07:43:00","2023-07-21 04:30:32","2023-07-05 07:43:00","257-265","","2","12","","MRS Communications","DeepFlames","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CXV2UXKV","bookSection","2010","Tuuri, Kai","Gestural Attributions as Semantics in User Interface Sound Design","Gesture in Embodied Communication and Human-Computer Interaction","978-3-642-12552-2 978-3-642-12553-9","","","http://link.springer.com/10.1007/978-3-642-12553-9_23","This paper proposes a gesture-based approach to user interface sound design, which utilises projections of body movements in sounds as meaningful attributions. The approach is founded on embodied conceptualisation of human cognition and it is justified through a literature review on the subject of interpersonal action understanding. According to the resulting hypothesis, stereotypical gestural cues, which correlate with, e.g., a certain communicative intention, represent specific non-linguistic meanings. Based on this theoretical framework, a model of a process is also outlined where stereotypical gestural cues are implemented in sound design","2010","2023-07-05 07:43:00","2023-07-20 00:08:43","2023-07-05 07:43:00","257-268","","","5934","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12553-9_23","","","","","","Kopp, Stefan; Wachsmuth, Ipke","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5VL7T6GD","bookSection","2008","Devallez, Delphine; Rocchesso, Davide; Fontana, Federico","An Audio-Haptic Interface Concept Based on Depth Information","Haptic and Audio Interaction Design","978-3-540-87882-7 978-3-540-87883-4","","","http://link.springer.com/10.1007/978-3-540-87883-4_11","We present an interaction tool based on rendering distance cues for ordering sound sources in depth. The user interface consists of a linear position tactile sensor made by conductive material. The touch position is mapped onto the listening position on a rectangular virtual membrane, modeled by a bidimensional Digital Waveguide Mesh and providing distance cues. Spatialization of sound sources in depth allows a hierarchical display of multiple audio streams, as in auditory menus. Besides, the similar geometries of the haptic interface and the virtual auditory environment allow a direct mapping between the touch position and the listening position, providing an intuitive and continuous interaction tool for auditory navigation.","2008","2023-07-05 07:43:00","2023-07-20 00:14:39","2023-07-05 07:43:00","102-110","","","5270","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-87883-4_11","","","","","","Pirhonen, Antti; Brewster, Stephen","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RNMYFI44","bookSection","2005","Wieczorkowska, Alicja A.; Raś, Zbigniew W.","Do We Need Automatic Indexing of Musical Instruments?","Intelligent Media Technology for Communicative Intelligence","978-3-540-29035-3 978-3-540-31738-8","","","http://link.springer.com/10.1007/11558637_24","Increasing growth and popularity of multimedia resources available on the Web brought the need to provide new, more advanced tools needed for their search. However, searching through multimedia data is highly non-trivial task that requires content-based indexing of the data. Our research is focused on automatic extraction of information about the sound timbre, and indexing sound data with information about musical instrument(s) playing in a given segment. Our goal is to perform automatic classification of musical instrument sound from real recordings for broad range of sounds, independently on the fundamental frequency of the sound.","2005","2023-07-05 07:43:00","2023-07-20 06:35:40","2023-07-05 07:43:00","239-245","","","3490","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11558637_24","","","","","","Bolc, Leonard; Michalewicz, Zbigniew; Nishida, Toyoaki","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A3E4YU69","bookSection","2006","Röber, Niklas; Huber, Cornelius; Hartmann, Knut; Feustel, Matthias; Masuch, Maic","Interactive Audiobooks: Combining Narratives with Game Elements","Technologies for Interactive Digital Storytelling and Entertainment","978-3-540-49934-3 978-3-540-49935-0","","","http://link.springer.com/10.1007/11944577_36","The authoring and the design of immersive, non-linear plots remains one of the main challenges in interactive digital storytelling. This paper introduces the concept of interactive audiobooks, which combines the potential of complex (non-)linear narratives (e.g. books and radio plays) with interactive elements from computer games. The design concentrates on a flexible degree of interaction, in a way that the listener’s experience ranges between a passive listening to an interactive audio-only computer game. In this paper we discuss the story-engine used in interactive audiobooks, as well as present an authoring framework along several design guidelines to create them. Finally, we demonstrate the capabilities of our system with an adaptation of a short story from Edgar Allen Poe.","2006","2023-07-05 07:43:00","2023-07-21 05:04:00","2023-07-05 07:43:00","358-369","","","4326","","","Interactive Audiobooks","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11944577_36","","","","","","Göbel, Stefan; Malkewitz, Rainer; Iurgel, Ido","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EJ3HKRJV","journalArticle","2010","Shimomura, Yayoi; Hvannberg, Ebba Thora; Hafsteinsson, Hjalmtyr","Accessibility of audio and tactile interfaces for young blind people performing everyday tasks","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-009-0183-y","http://link.springer.com/10.1007/s10209-009-0183-y","Increasingly, computers are becoming tools of communication, information exploring and studying for young people, regardless of their abilities. Scientists have been building knowledge on how blind people can substitute hearing or touch for sight or how the combination of senses, i.e., multimodalities, can provide the user with an effective way of exploiting the power of computers. Evaluation of such multimodal user interfaces in the right context, i.e., appropriate users, tasks, tools and environment, is essential to give designers accurate feedback on blind users’ needs. This paper presents a study on how young blind people use computers for everyday tasks with the aids of assistive technologies, aiming to understand what hindrances they encounter when interacting with a computer using individual senses, and what supports them. A common assistive technology is a screen reader, producing output to a speech synthesizer or a Braille display. Those two modes are often used together, but the research studied how visually impaired students interact with computers using either form, i.e., a speech synthesizer or a Braille display. A usability test has been performed to assess blind grade-school students’ ability to carry out common tasks with the help of a computer, including solving mathematical problems, navigating the web, communicating with e-mail and using word processing. During the usability tests, students were allowed to use either auditory mode or tactile mode. Although blind users most commonly use a speech synthesizer (audio), the results indicate that this was not always the most suitable modality. While the effectiveness of the Braille display (tactile user interface) to accomplish certain tasks was similar to that of the audio user interface, the users’ satisfaction rate was higher. The contribution of this work lies in answering two research questions by analysing two modes of interaction (tactile and speech), while carrying out tasks of varying genre, i.e., web searching, collaboration through e-mail, word processing and mathematics. A second contribution of this work is the classification of observations into four categories: usability and accessibility, software fault, cognitive mechanism and learning method. Observations, practical recommendations and open research problems are then presented and discussed. This provides a framework for similar studies in the future. A third contribution of this work is the elaboration of practical recommendations for user interface designers and a research agenda for scientists.","2010-11","2023-07-05 07:43:00","2023-07-21 05:12:50","2023-07-05 07:43:00","297-310","","4","9","","Univ Access Inf Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M2BQ7HRM","journalArticle","1992","Uwajeh, P. N.","Orature in literature: Myths as structural elements in Achebe'sAnthills of the Savannah: Introduction: Orature and literature","Neohelicon","","0324-4652, 1588-2810","10.1007/BF02028624","http://link.springer.com/10.1007/BF02028624","","1992-03","2023-07-05 07:43:00","2023-07-05 07:43:00","2023-07-05 07:43:00","297-306","","1","19","","Neohelicon","Orature in literature","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9MJF229T","journalArticle","1980","Luyster, Robert","King ego and the double-sex dancer","Journal of Religion and Health","","0022-4197, 1573-6571","10.1007/BF01006424","http://link.springer.com/10.1007/BF01006424","The god representative of the energy of life in ancient Greece was Dionysos. As such, he was bisexual, for life in its wholeness contains all the individual forms it generates. The emphasis upon androgyny and sexual fusion in his cult results from man's effort to realize the wholeness of that life of which he is merely a fragment. The achievement of that fullness, life's experience of itself, is ecstasy; suffering, on the other hand, is fundamentally life's alienation from itself. In the myths and rituals of Dionysos we witness the dialectic of affirmation and denial of the life force.","1980","2023-07-05 07:44:26","2023-07-20 06:49:27","2023-07-05 07:44:26","121-129","","2","19","","J Relig Health","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"69K2REIA","journalArticle","2021","Ranganathan, Rajiv; Tomlinson, Aimee D.; Lokesh, Rakshith; Lin, Tzu-Hsiang; Patel, Priya","A tale of too many tasks: task fragmentation in motor learning and a call for model task paradigms","Experimental Brain Research","","0014-4819, 1432-1106","10.1007/s00221-020-05908-6","http://link.springer.com/10.1007/s00221-020-05908-6","Motor learning encompasses a broad set of phenomena that requires a diverse set of experimental paradigms. However, excessive variation in tasks across studies creates fragmentation that can adversely affect the collective advancement of knowledge. Here, we show that motor learning studies tend toward extreme fragmentation in the choice of tasks, with almost no overlap between task paradigms across studies. We argue that this extreme level of task fragmentation poses serious theoretical and methodological barriers to advancing the field. To address these barriers, we propose the need for developing common ‘model’ task paradigms which could be widely used across labs. Combined with the open sharing of methods and data, we suggest that these model task paradigms could be an important step in increasing the robustness of the motor learning literature and facilitate the cumulative process of science.","2021-01","2023-07-05 07:44:26","2023-07-20 00:05:20","2023-07-05 07:44:26","1-19","","1","239","","Exp Brain Res","A tale of too many tasks","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L4KPGDSS","journalArticle","2019","Issachar, Gil; Bar-Shalita, Tami; Baruch, Yair; Horing, Bar; Portnoy, Sigal","Design and Implementation of a Novel Subject-Specific Neurofeedback Evaluation and Treatment System","Annals of Biomedical Engineering","","0090-6964, 1573-9686","10.1007/s10439-019-02228-x","http://link.springer.com/10.1007/s10439-019-02228-x","Electroencephalography (EEG)-based neurofeedback (NF) is a safe, non-invasive, non-painful method for treating various conditions. Current NF systems enable the selection of only one NF parameter, so that two parameters cannot be feedback simultaneously. Consequently, the ability to individually-tailor the treatment to a patient is limited, and treatment efficiency may therefore be compromised. We aimed to design, implement and test an all-in-one, novel, computerized platform for closed-loop NF treatment, based on principles from learning theories. Our prototype performs numeric evaluation based on quantifying resting EEG and event-related EEG responses to various sensory stimuli. The NF treatment was designed according to principles of efficient learning, and implemented as a gradual, patient-adaptive 1D or 2D computer game, that utilizes automatic EEG feature extraction. Verification was performed as we compared the mean area under curve (AUC) of the theta band of a dozen subjects staring at a wall or performing the NF. Most of the subjects (75%) increased their theta band AUC during the NF session compared with the trial staring at the wall (p = 0.041). Our system enables multiple feature selection and its machine learning capabilities allow an accurate discovery of patient-specific biomarkers and treatment targets. Its novel characteristics may allow for improved evaluation of patients and treatment outcomes.","2019-05","2023-07-05 07:44:26","2023-07-19 11:30:22","2023-07-05 07:44:26","1203-1211","","5","47","","Ann Biomed Eng","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZGJ5WIYP","bookSection","2006","Marcus, Aaron","Cross-Cultural User-Experience Design","Diagrammatic Representation and Inference","978-3-540-35623-3 978-3-540-35624-0","","","http://link.springer.com/10.1007/11783183_4","Designers of information visualization and user interfaces must take account of culture in the design of metaphors, mental models, navigation, interaction, and appearance. Culture models define dimensions of difference and similarity among groups of people regarding signs, rituals, heroes/heroines, and values. Examples on the Web reveal these dimensions. Developers will increasingly need to take into account culture and other factors in development to better ensure usability, usefulness, and appeal.","2006","2023-07-05 07:44:26","2023-07-19 23:56:19","2023-07-05 07:44:26","16-24","","","4045","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11783183_4","","","","","","Barker-Plummer, Dave; Cox, Richard; Swoboda, Nik","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RGBR5QC8","bookSection","2010","Ortega-González, Vladimir; Garbaya, Samir; Merienne, Frédéric","Reducing Reversal Errors in Localizing the Source of Sound in Virtual Environment without Head Tracking","Haptic and Audio Interaction Design","978-3-642-15840-7 978-3-642-15841-4","","","http://link.springer.com/10.1007/978-3-642-15841-4_10","This paper presents a study about the effect of using additional audio cueing and Head-Related Transfer Function (HRTF) on human performance in sound source localization task without using head movement. The existing techniques of sound spatialization generate reversal errors. We intend to reduce these errors by introducing sensory cues based on sound effects. We conducted and experimental study to evaluate the impact of additional cues in sound source localization task. The results showed the benefit of combining the additional cues and HRTF in terms of the localization accuracy and the reduction of reversal errors. This technique allows significant reduction of reversal errors compared to the use of the HRTF separately. For instance, this technique could be used to improve audio spatial alerting, spatial tracking and target detection in simulation applications when head movement is not included.","2010","2023-07-05 07:44:26","2023-07-20 00:16:14","2023-07-05 07:44:26","85-96","","","6306","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-15841-4_10","","/Users/minsik/Zotero/storage/VW574XAS/Ortega-González et al. - 2010 - Reducing Reversal Errors in Localizing the Source .pdf","","","","Nordahl, Rolf; Serafin, Stefania; Fontana, Federico; Brewster, Stephen","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A22ECMQF","journalArticle","2018","Black, David; Hahn, Horst K.; Kikinis, Ron; Wårdell, Karin; Haj-Hosseini, Neda","Auditory display for fluorescence-guided open brain tumor surgery","International Journal of Computer Assisted Radiology and Surgery","","1861-6410, 1861-6429","10.1007/s11548-017-1667-5","http://link.springer.com/10.1007/s11548-017-1667-5","Purpose—Protoporphyrin (PpIX) fluorescence allows discrimination of tumor and normal brain tissue during neurosurgery. A handheld fluorescence (HHF) probe can be used for spectroscopic measurement of 5-ALA-induced PpIX to enable objective detection compared to visual evaluation of fluorescence. However, current technology requires that the surgeon either views the measured values on a screen or employs an assistant to verbally relay the values. An auditory feedback system was developed and evaluated for communicating measured fluorescence intensity values directly to the surgeon. Methods—The auditory display was programmed to map the values measured by the HHF probe to the playback of tones that represented three fluorescence intensity ranges and one error signal. Ten persons with no previous knowledge of the application took part in a laboratory evaluation. After a brief training period, participants performed measurements on a tray of 96 wells of liquid fluorescence phantom and verbally stated the perceived measurement values for each well. The latency and accuracy of the participants’ verbal responses were recorded. The long-term memorization of sound function was evaluated in a second set of 10 participants 2–3 and 712 days after training. Results—The participants identified the played tone accurately for 98% of measurements after training. The median response time to verbally identify the played tones was 2 pulses. No correlation was found between the latency and accuracy of the responses, and no significant correlation with the musical proficiency of the participants was observed on the function responses. Responses for the memory test were 100% accurate. Conclusion—The employed auditory display was shown to be intuitive, easy to learn and remember, fast to recognize, and accurate in providing users with measurements of fluorescence intensity or error signal. The results of this work establish a basis for implementing and further evaluating auditory displays in clinical scenarios involving fluorescence guidance and other areas for which categorized auditory display could be useful.","2018-01","2023-07-05 07:44:26","2023-07-20 06:40:43","2023-07-05 07:44:26","25-35","","1","13","","Int J CARS","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/UBG7BRH2/Black et al. - 2018 - Auditory display for fluorescence-guided open brai.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QYH7CJS8","journalArticle","2017","Dawid, Herbert; Decker, Reinhold; Hermann, Thomas; Jahnke, Hermann; Klat, Wilhelm; König, Rolf; Stummer, Christian","Management science in the era of smart consumer products: challenges and research perspectives","Central European Journal of Operations Research","","1435-246X, 1613-9178","10.1007/s10100-016-0436-9","http://link.springer.com/10.1007/s10100-016-0436-9","Smart products not only provide novel functionalities, but also may establish new business models, markets, or distribution channels, strengthen relationships with consumers, and/or add smart remote services. While many technical obstacles of such products have already been overcome, the broad market dissemination of smart products still poses some vital managerial challenges for decision makers. In this paper, we outline the technical potential and future trends of smart consumer products, discuss economic challenges in four scopes, namely, preference-based new product development, market analysis, supply chain design, and industry development, and, in particular, we highlight research perspectives for management science in this promising field.","2017-03","2023-07-05 07:44:26","2023-07-19 23:37:46","2023-07-05 07:44:26","203-230","","1","25","","Cent Eur J Oper Res","Management science in the era of smart consumer products","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/6X69VUPD/Dawid et al. - 2017 - Management science in the era of smart consumer pr.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"45ZDUZHP","journalArticle","2020","Simonis, Sarah; Canfyn, Michaël; Van Dijck, Anton; Van Havere, Tina; Deconinck, Eric; Blanckaert, Peter; Gremeaux, Lies","Awareness of users and motivational factors for using new psychoactive substances in Belgium","Harm Reduction Journal","","1477-7517","10.1186/s12954-020-00393-0","https://harmreductionjournal.biomedcentral.com/articles/10.1186/s12954-020-00393-0","Abstract                            Background               Few data on motivations for using new psychoactive substances (NPS) are available. However, the cost, the legal status, and their accessibility through channels like internet contributed to the popularity of NPS. The objective of this article are first to gain a deeper understanding of the culture surrounding NPS in Belgium and second to define the awareness of the users concerning the content of the NPS they are consuming.                                         Methods               Snowball sampling and partners in the drug demand reduction field were used as a gateway in order to reach a heterogeneous study population. In total, 45 users were recruited and in-depth interviews were conducted. The personal experiences of NPS users and their needs for support along the continuum of care were explored through an interview guideline, while subjects were given the opportunity to deposit a NPS sample for forensic analysis in a recognized laboratory.                                         Results               A diversity of profiles was found among NPS users but also a wide diversity in the motives to consume NPS: personal reasons such as pleasure, mind exploration, being connected to others, or out of curiosity, but also external reasons such as price, accessibility or the specific effects procured by certain NPS. The results showed as well that a majority of NPS users seem to be aware of the substances they are using.                                         Conclusion               Understanding the motivations of use is of importance to determine which type of NPS targeted interventions are adapted to different profiles of users.","2020-12","2023-07-05 07:44:26","2023-07-05 07:44:26","2023-07-05 07:44:26","52","","1","17","","Harm Reduct J","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/2MUBBDXG/Simonis et al. - 2020 - Awareness of users and motivational factors for us.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A2HQ52Y9","bookSection","2011","Mertens, Alexander; Przybysz, Philipp; Groß, Alexander; Koch-Koerfges, David; Nick, Claudia; Kaethner, Martin; Schlick, Christopher M.","Age-Adapted Psychoacoustics: Target Group Oriented Sound Schemes for the Interaction with Telemedical Systems","Universal Access in Human-Computer Interaction. Applications and Services","978-3-642-21656-5 978-3-642-21657-2","","","https://link.springer.com/10.1007/978-3-642-21657-2_44","For the interaction of elderly people with IT systems, an ergonomic and intuitive design as well as self-explanatory handling processes are particularly relevant. Herein adequate acoustic feedback, which accounts for the specific needs and experience of the target group, provides high efficacy and acceptance of technology with regard to Human-Computer Interaction. In this study, five different types of sound schemes are evaluated on their intuitive understanding and memorization by older users. The participants assign audible feedback to typical applications of telemedical monitoring and have to reminisce given classifications. This approach makes it possible to elicit the homogeneity of psychoacoustic models of elderly people and give recommendations for the design of acoustic feedback mechanisms for this audience. As a result, the use of familiar sounds from everyday situations has been found significantly better in terms of the consistency of the intuitive mapping and memorization for use cases in a telemedical context, in comparison to synthetic sounds that obtain their semantic denotation just by convention.","2011","2023-07-05 07:44:26","2023-07-21 05:09:19","2023-07-05 07:44:26","406-415","","","6768","","","Age-Adapted Psychoacoustics","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-21657-2_44","","","","","","Stephanidis, Constantine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WRFZS7MH","bookSection","2003","Djennane, Safia","3D-Audio News Presentation Modeling","Universal Access Theoretical Perspectives, Practice, and Experience","978-3-540-00855-2 978-3-540-36572-3","","","http://link.springer.com/10.1007/3-540-36572-9_22","With the emergence of the mobile Internet combined with wearable personal computing, we are now entering a new information era where PCs are self-adapting their resources to human bodies, minds and preferences, prevailing a more effective work environment. In this new world, working effectively is inextricably related to accessing reliable information sources when needed. Therefore, people eager to stay connected, consume daily information in a myriad of forms: news, weather, business, road/traffic reports, voicemails, emails, as well as information associated to their daily activities or interests. In this paper, we propose innovative UI information presentations based on three-dimensional (3D) audio modeling. In this framework, we illustrate how news, weather and business reports are extracted, spatialized and presented to end-users using 3D audio modality.","2003","2023-07-05 07:44:26","2023-07-21 05:13:15","2023-07-05 07:44:26","280-286","","","2615","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-36572-9_22","","","","","","Carbonell, Noëlle; Stephanidis, Constantine","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DRXXRNSK","journalArticle","2014","Mi, Na; Cavuoto, Lora A.; Benson, Kenneth; Smith-Jackson, Tonya; Nussbaum, Maury A.","A heuristic checklist for an accessible smartphone interface design","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-013-0321-4","http://link.springer.com/10.1007/s10209-013-0321-4","Smartphone technology has evolved into a multi-functional device with advanced capabilities, but this mobile technology remains inaccessible to many individuals with visual impairments or upper extremity disabilities. This paper provides a heuristic checklist for accessible smartphone interface design, developed through reviewing existing design standards and guidelines and validating these guidelines with user involvement. Specifically, a set of preliminary user requirements (59 items) was extracted from existing standards, guidelines, and user requirements regarding mobile handheld device accessibility. Subsequently, the requirement set was filtered using a participatory method and then integrated to create an operational version of design guidelines. These guidelines were then used in a heuristic evaluation and usability testing on high-fidelity prototypes produced by a commercial manufacturer. A heuristic checklist for designing accessible smartphones was formed, which may also be applicable to other touchscreen handheld devices (e.g., printer screen) in terms of accessibility features. The initial set of 59 user requirements was re-organized into 44 statements in six general categories: mechanical controls, display, speech and general operation controls, audio feedback controls, touch-operated controls, and others. Using results from both qualitative and quantitative methods provides support, though with some limitations, for this accessibility checklist. This checklist is intended as a practical design support tool for use in early design phases of handheld products. A number of challenges and limitations are discussed as well.","2014-11","2023-07-05 07:44:26","2023-07-21 05:12:00","2023-07-05 07:44:26","351-365","","4","13","","Univ Access Inf Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GAI6RGSF","bookSection","2000","Semwal, Sudhanshu Kumar; Evans-Kamp, Debra Lee","Virtual Environments for Visually Impaired","Virtual Worlds","978-3-540-67707-9 978-3-540-45016-0","","","http://link.springer.com/10.1007/3-540-45016-5_25","We provide a systematic study for generating interactive, virtual environments for the blind. We present our system as a tool for shape recognition and mobility training for the blind. In our system, head movement can be detected to indicate horizontal and vertical movements. Audio feedback is used for reinforcement. Our experiment for shape learning can guide the user in tracing the surface of a sphere by using the audio feedback. We also present a compelling case for using force feedback devices for visually impaired, and our experience with the PHANToM(TM) force feedback device is summarized. A detailed survey of present research is also presented.","2000","2023-07-05 07:44:26","2023-07-21 05:15:15","2023-07-05 07:44:26","270-285","","","1834","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-45016-5_25","","","","","","Heudin, Jean-Claude","Goos, G.; Hartmanis, J.; Van Leeuwen, J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GX4NTQLC","bookSection","2009","Bologna, Guido; Deville, Benoît; Pun, Thierry","Blind Navigation along a Sinuous Path by Means of the See ColOr Interface","Bioinspired Applications in Artificial and Natural Computation","978-3-642-02266-1 978-3-642-02267-8","","","http://link.springer.com/10.1007/978-3-642-02267-8_26","The See ColOr interface transforms a small portion of a coloured video image into sound sources represented by spatialized musical instruments. This interface aims at providing visually impaired people with a capability of perception of the environment. In this work, the purpose is to verify the hypothesis that it is possible to use sounds from musical instruments to replace colour. Compared to state of the art devices, a quality of the See ColOr interface is that it allows the user to receive a feed-back auditory signal from the environment and its colours, promptly. An experiment based on a head mounted camera has been performed. Specifically, this experiment is related to outdoor navigation for which the purpose is to follow a sinuous path. Our participants successfully went along a red serpentine path for more than 80 meters.","2009","2023-07-05 07:44:26","2023-07-19 23:34:53","2023-07-05 07:44:26","235-243","","","5602","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-02267-8_26","","","","","","Mira, José; Ferrández, José Manuel; Álvarez, José R.; De La Paz, Félix; Toledo, F. Javier","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IKPH6YSH","bookSection","2002","Baloian, Nelson; Luther, Wolfram","Visualization for the Mind’s Eye","Software Visualization","978-3-540-43323-1 978-3-540-45875-3","","","http://link.springer.com/10.1007/3-540-45875-1_28","Software visualization has been almost exclusively tackled from the visual point of view; this means visualization occurs exclusively through the visual channel. This approach has its limitations. Considering previous work for blind people we propose that complementing usual approaches with those techniques used to develop interfaces for non-sighted people can enhance user awareness of logical structures or data types using different perception channels. To achieve better comprehension, we deal with new or augmented interfaces built on top of standard systems for data visualization and algorithm animation. The notion of specific concept keyboards is introduced. As a consequence, modern information and learning systems can be designed in such a way that not only sighted but also blind users can navigate within these systems.","2002","2023-07-05 07:44:26","2023-07-21 05:00:59","2023-07-05 07:44:26","354-367","","","2269","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-45875-1_28","","/Users/minsik/Zotero/storage/S6U8PI9T/Baloian and Luther - 2002 - Visualization for the Mind’s Eye.pdf","","","","Diehl, Stephan","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C6H7D4EC","bookSection","2014","Ford, Rebecca; Penn, Joe; Liu, Yu-Chieh; Nixon, Ken; Cronje, Willie; McCulloch, Malcolm","User-Centred Design of an Audio Feedback System for Power Demand Management","Design, User Experience, and Usability. User Experience Design for Everyday Life Applications and Services","978-3-319-07634-8 978-3-319-07635-5","","","http://link.springer.com/10.1007/978-3-319-07635-5_51","Low-income houses in South Africa are supplied with a pre-payment meter and a circuit breaker that trips at a low power level (about 20A, 4.5kW), resulting in many nuisance trips. Four categories of audio cues, each being able to represent five levels of power consumption, are assessed. A survey of 62 people was conducted. The numerical analysis of the results and the perceptions of the respondents both indicate that the use of changing tempo and texture is the most effective at conveying feedback information on the power consumption in the home.","2014","2023-07-05 07:44:26","2023-07-19 23:55:50","2023-07-05 07:44:26","530-541","","","8519","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-07635-5_51","","/Users/minsik/Zotero/storage/EECRG6X7/Ford et al. - 2014 - User-Centred Design of an Audio Feedback System fo.pdf","","","","Marcus, Aaron","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z79KI63Q","bookSection","2000","Nesbitt, Keith","Designing Multi-sensory Models for Finding Patterns in Stock Market Data","Advances in Multimodal Interfaces — ICMI 2000","978-3-540-41180-2 978-3-540-40063-9","","","http://link.springer.com/10.1007/3-540-40063-X_4","The rapid increase in available information has lead to many attempts to automatically locate patterns in large, abstract, multi-attributed information spaces. These techniques are often called ‘Data Mining’ and have met with varying degrees of success. An alternative approach to automatic pattern detection is to keep the user in the ‘exploration loop’. A domain expert is often better able to search data for relationships. Furthermore, it is now possible to construct user interfaces that provide multi-sensory interactions. For example, interfaces can be designed which utilise 3D visual spaces and also provide auditory and haptic feedback. These multi-sensory interfaces may assist in the interpretation of abstract information spaces by providing models that map different attributes of data to different senses. While this approach has the potential to assist in exploring these large information spaces what is unclear is how to choose the best models to define mappings between the abstract information and the human sensory channels. This paper describes some simple guidelines based on real world analogies for designing these models. These principles are applied to the problem of finding new patterns in stock market data.","2000","2023-07-05 07:44:26","2023-07-19 11:12:39","2023-07-05 07:44:26","24-31","","","1948","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-40063-X_4","","","","","","Tan, Tieniu; Shi, Yuanchun; Gao, Wen","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZJGIPK68","journalArticle","2016","Halim, Zahid; Kalsoom, Rizwana; Bashir, Shariq; Abbas, Ghulam","Artificial intelligence techniques for driving safety and vehicle crash prediction","Artificial Intelligence Review","","0269-2821, 1573-7462","10.1007/s10462-016-9467-9","http://link.springer.com/10.1007/s10462-016-9467-9","Accident prediction is one of the most critical aspects of road safety, whereby an accident can be predicted before it actually occurs and precautionary measures taken to avoid it. For this purpose, accident prediction models are popular in road safety analysis. Artificial intelligence (AI) is used in many real world applications, especially where outcomes and data are not same all the time and are influenced by occurrence of random changes. This paper presents a study on the existing approaches for the detection of unsafe driving patterns of a vehicle used to predict accidents. The literature covered in this paper is from the past 10 years, from 2004 to 2014. AI techniques are surveyed for the detection of unsafe driving style and crash prediction. A number of statistical methods which are used to predict the accidents by using different vehicle and driving features are also covered in this paper. The approaches studied in this paper are compared in terms of datasets and prediction performance. We also provide a list of datasets and simulators available for the scientific community to conduct research in the subject domain. The paper also identifies some of the critical open questions that need to be addressed for road safety using AI techniques.","2016-10","2023-07-05 07:44:26","2023-07-19 11:34:27","2023-07-05 07:44:26","351-387","","3","46","","Artif Intell Rev","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8RLKIUWI","journalArticle","1997","Hollier, M. P.; Rimmell, A. N.; Burraston, D.","Spatial audio technology for telepresence","BT Technology Journal","","1573-1995","10.1023/A:1018675327815","https://doi.org/10.1023/A:1018675327815","As people start to exploit new telepresence technologies to meet and work, they will be able to exploit all of their senses as they transmit, receive, and monitor information. An essential part of such three-dimensional spaces is the audio landscape. People are able to detect a wide variety of sounds and separate them in space. Spatial separation improves the detection and intelligibility of speech from multiple talkers, and enables simultaneous monitoring of multiple information streams through the use of multiple alert sounds. On-going research at BT Laboratories into spatial audio has resulted in a number of leading edge demonstrations and patent applications. This paper introduces the technologies employed to create spatial audio for real-time synthetic worlds including single and multiple users, and non-ideal acoustic environments.","1997-10-01","2023-07-05 07:44:35","2023-07-05 07:44:35","2023-07-05 07:44:35","33-41","","4","15","","BT Technology Journal","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/PVS5AEPY/Hollier et al. - 1997 - Spatial audio technology for telepresence.pdf","","","Human Computer Interaction; Multimedia Information; Patent Application; Spatial Separation; User Interface","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IC6EG6ND","bookSection","2014","Kim, Jee-Eun; Bessho, Masahiro; Koshizuka, Noboru; Sakamura, Ken","SaSYS: A Swipe Gesture-Based System for Exploring Urban Environments for the Visually Impaired","Mobile Computing, Applications, and Services","978-3-319-05451-3 978-3-319-05452-0","","","http://link.springer.com/10.1007/978-3-319-05452-0_5","Exploring and learning an environment is a particularly challenging issue faced by visually impaired people. Existing interaction techniques for allowing users to learn an environment may not be useful while traveling because they often use dedicated hardware or require users to focus on tactile or auditory feedback. In this paper, we introduce an intuitive interaction technique for selecting areas of interests in urban environments by performing simple swipe gestures on touchscreen. Based on the swipe-based interaction, we developed SaSYS, a location-aware system that enables users to discover points of interest (POI) around them using off-the-shelf smartphones. Our approach can be easily implemented on handheld devices without requiring any dedicated hardware and having users to constantly focus on tactile or auditory feedback. SaSYS also provides a fine-grained control over Text-to-Speech (TTS). Our user study shows that 9 of 11 users preferred swipe-based interaction to existing pointing-based interaction.","2014","2023-07-05 07:46:04","2023-07-21 04:29:50","2023-07-05 07:46:04","54-71","","","130","","","SaSYS","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering DOI: 10.1007/978-3-319-05452-0_5","","","","","","Memmi, Gérard; Blanke, Ulf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MFLPVVYL","bookSection","2020","Reynal, Maxime; Aricò, Pietro; Imbert, Jean-Paul; Hurter, Christophe; Borghini, Gianluca; Di Flumeri, Gianluca; Sciaraffa, Nicolina; Di Florio, Antonio; Terenzi, Michela; Ferreira, Ana; Pozzi, Simone; Betti, Viviana; Marucci, Matteo; Babiloni, Fabio","Involving Hearing, Haptics and Kinesthetics into Non-visual Interaction Concepts for an Augmented Remote Tower Environment","Computer Vision, Imaging and Computer Graphics Theory and Applications","978-3-030-41589-1 978-3-030-41590-7","","","http://link.springer.com/10.1007/978-3-030-41590-7_4","We investigated the contribution of specific HCI concepts to provide multimodal information to Air Traffic Controlers in the context of Remote Control Towers (i.e. when an airport is controlled from a distant location). We considered interactive spatial sound, tactile stimulation and body movements to design four different interaction and feedback modalities. Each of these modalities have been designed to provide specific solutions to typical Air Traffic Control identified use cases. Sixteen professional Air Traffic Controllers (ATCos) participated in the experiment, which was structured in four distinct scenarios. ATCos were immersed in an ecological setup, in which they were asked to control (i) one airport without augmentations modalities, (ii) two airports without augmentations, (iii) one airport with augmentations and (iv) two airports with augmentations. These experimental conditions constituted the four distinct experimental scenarios. Behavioral results shown a significant increase in overall participants’ performance when augmentation modalities were activated in remote control tower operations for one airport.","2020","2023-07-05 07:46:04","2023-07-19 23:50:34","2023-07-05 07:46:04","73-100","","","1182","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Communications in Computer and Information Science DOI: 10.1007/978-3-030-41590-7_4","","","","","","Cláudio, Ana Paula; Bouatouch, Kadi; Chessa, Manuela; Paljic, Alexis; Kerren, Andreas; Hurter, Christophe; Tremeau, Alain; Farinella, Giovanni Maria","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZAEVHB2W","bookSection","2001","Hoch, Michael; Jää-Aro, Kai-Mikael; Bowers, John","Round Table: A Physical Interface for Virtual Camera Deployment in Electronic Arenas","Immersive Projection Technology and Virtual Environments 2001","978-3-211-83671-2 978-3-7091-6221-7","","","http://link.springer.com/10.1007/978-3-7091-6221-7_27","In this paper, we describe a physical input device for the control of virtual cameras. The so called RoundTable has a round projection area where physical icons are used to stipulate the position of virtual cameras. With this scenario we propose a hybrid mixed reality environment for use by production personnel for real-time camera control during a live-broadcast. We present first results of using the RoundTable to support the managing of events in electronic arenas and compare them with traditional interfaces for camera control. We also comment on findings from a scenario in the field of sound mixing and sound composition.","2001","2023-07-05 07:46:04","2023-07-20 06:33:30","2023-07-05 07:46:04","267-276","","","","","","Round Table","","","","","Springer Vienna","Vienna","","","","","","DOI.org (Crossref)","","Series Title: Eurographics DOI: 10.1007/978-3-7091-6221-7_27","","/Users/minsik/Zotero/storage/MURQVVCY/Hoch et al. - 2001 - Round Table A Physical Interface for Virtual Came.pdf","","","","Fröhlich, Bernd; Deisinger, Joachim; Bullinger, Hans-Jörg","Hansmann, W.; Purgathofer, W.; Sillion, F.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FQMKRX3M","journalArticle","2022","Bach, Patric; Frank, Cornelia; Kunde, Wilfried","Why motor imagery is not really motoric: towards a re-conceptualization in terms of effect-based action control","Psychological Research","","0340-0727, 1430-2772","10.1007/s00426-022-01773-w","https://link.springer.com/10.1007/s00426-022-01773-w","Abstract             Overt and imagined action seem inextricably linked. Both have similar timing, activate shared brain circuits, and motor imagery influences overt action and vice versa. Motor imagery is, therefore, often assumed to recruit the same motor processes that govern action execution, and which allow one to play through or simulate actions offline. Here, we advance a very different conceptualization. Accordingly, the links between imagery and overt action do not arise because action imagery is intrinsically motoric, but because action planning is intrinsically imaginistic and occurs in terms of the perceptual effects one want to achieve. Seen like this, the term ‘motor imagery’ is a misnomer of what is more appropriately portrayed as ‘effect imagery’. In this article, we review the long-standing arguments for effect-based accounts of action, which are often ignored in motor imagery research. We show that such views provide a straightforward account of motor imagery. We review the evidence for imagery-execution overlaps through this new lens and argue that they indeed emerge because every action we execute is planned, initiated and controlled through an imagery-like process. We highlight findings that this new view can now explain and point out open questions.","2022-12-14","2023-07-05 07:46:04","2023-07-05 07:46:04","2023-07-05 07:46:04","","","","","","Psychological Research","Why motor imagery is not really motoric","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/T3DZHSD4/Bach et al. - 2022 - Why motor imagery is not really motoric towards a.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4Z8E9DT7","bookSection","2023","Chopda, Rishabh; Khan, Aayan; Goenka, Anuj; Dhere, Dakshal; Gupta, Shiwani","An Intelligent Voice Assistant Engineered to Assist the Visually Impaired","Intelligent Computing and Networking","978-981-9900-70-1 978-981-9900-71-8","","","https://link.springer.com/10.1007/978-981-99-0071-8_12","Visually handicapped people’s lives are subject to a multitude of unrelenting challenges because they’ve been made bereft of the gift of sight. The proposed solution is a wearable Smart Voice Assistant that is developed to accommodate the needs of the visually impaired to aid them in every aspect of their everyday lives. It takes advantage of recent breakthroughs in the fields of language processing and computer vision to provide a broad spectrum of applications, including emergency response functionality, object recognition, and optical character recognition. It comprises hardware components that provide feedback in the form of sound, haptics, and speech to help with obstacle avoidance. The voice assistant also interacts with a smartphone application to enhance the user’s experience by enabling them to read the messages from their phone, send an SOS message to their closest connections in an emergency, customize the device settings through the mobile application, and find the device with the press of a button if it is misplaced. The proposed solution will enable the user to live a life in relative safety and comfort, which is essential for people suffering from varying levels of visual impairment.","2023","2023-07-05 07:46:04","2023-07-20 06:34:51","2023-07-05 07:46:04","143-155","","","632","","","","","","","","Springer Nature Singapore","Singapore","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Networks and Systems DOI: 10.1007/978-981-99-0071-8_12","","","","","","Balas, Valentina Emilia; Semwal, Vijay Bhaskar; Khandare, Anand","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MNAEFA48","journalArticle","2017","The CSRA Team; Wrede, Sebastian; Leichsenring, Christian; Holthaus, Patrick; Hermann, Thomas; Wachsmuth, Sven","The Cognitive Service Robotics Apartment: A Versatile Environment for Human–Machine Interaction Research","KI - Künstliche Intelligenz","","0933-1875, 1610-1987","10.1007/s13218-017-0492-x","http://link.springer.com/10.1007/s13218-017-0492-x","The emergence of cognitive interaction technology offering intuitive and personalized support for humans in daily routines is essential for the success of future smart environments. Social robotics and ambient assisted living are well-established, active research fields but in the real world the number of smart environments that support humans efficiently on a daily basis is still rather low. We argue that research on ambient intelligence and human–robot interaction needs to be conducted in a strongly interdisciplinary process to facilitate seamless integration of assistance technologies into the users daily lives. With the cognitive service robotics apartment (CSRA), we are developing a novel kind of laboratory following this interdisciplinary approach. It combines a smart home with ambient intelligence functionalities with a cognitive social robot with advanced manipulation capabilities to explore the all day use of cognitive interaction technology for human assistance. This lab in conjunction with our development approach opens up new lines of inquiry and allows us to address new research questions in human–machine, human–agent and human–robot interaction","2017-08","2023-07-05 07:46:04","2023-07-21 04:27:40","2023-07-05 07:46:04","299-304","","3","31","","Künstl Intell","The Cognitive Service Robotics Apartment","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4FSNAF6Q","bookSection","2017","Rouse, Rebecca; Barba, Evan","Design for Emerging Media: How MR Designers Think About Storytelling, Process, and Defining the Field","Interactive Storytelling","978-3-319-71026-6 978-3-319-71027-3","","","http://link.springer.com/10.1007/978-3-319-71027-3_20","Given mixed reality’s (MR) unique status as an emerging medium that incorporates both the physical and the virtual in hybrid space, it is a particularly interesting field in which to study the design process as a whole, and interactive narrative design in particular. How prominently does story figure in MR design? What kinds of stories are being told? As MR tools become more accessible, the field is opening up to a wider variety of practitioners. However, the full breadth of methods and techniques being brought to bear in design for MR has not yet been studied. This paper presents findings from an interview study with fifteen leading MR designers, and describes the multiplicity of approaches they use. These approaches are presented as a matrix, composed of a opportunistic—deterministic spectrum (based on designs planned in advance vs. improvisation), and a storytelling—sensationalizing spectrum (based on designs aimed at narrative creation vs. development of a sensory experience).","2017","2023-07-05 07:46:04","2023-07-20 06:36:52","2023-07-05 07:46:04","245-258","","","10690","","","Design for Emerging Media","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-71027-3_20","","","","","","Nunes, Nuno; Oakley, Ian; Nisi, Valentina","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5IB2AW9P","bookSection","2005","Slavík, Pavel; Němec, Vladislav; Sporka, Adam J.","Speech Based User Interface for Users with Special Needs","Text, Speech and Dialogue","978-3-540-28789-6 978-3-540-31817-0","","","http://link.springer.com/10.1007/11551874_6","The number of people using computers is permanently increasing in last years. Not all potential users have all capabilities that allow them to use computers without obstacles. This is especially true for handicapped and elderly users. For this class of users a special approach for design and implementation of user interfaces is needed. The missing capabilities of these users must be substituted by capabilities that these users have. In most of cases the use of sounds and speech offers a natural solution to this problem. In the paper the outline of problems related to special user interfaces will be discussed. In further the examples of application of user interfaces using special forms of speech and related acoustic communication will be given.","2005","2023-07-05 07:46:04","2023-07-21 05:04:17","2023-07-05 07:46:04","45-55","","","3658","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11551874_6","","","","","","Matoušek, Václav; Mautner, Pavel; Pavelka, Tomáš","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4METMI9W","bookSection","2007","McGookin, David K.; Brewster, Stephen A.","Graph Builder: Constructing Non-visual Visualizations","People and Computers XX — Engage","978-1-84628-588-2","","","http://link.springer.com/10.1007/978-1-84628-664-3_20","This paper introduces a novel application called Graph Builder, which allows visually impaired people to interactively construct bar graphs using a force feedback device. We discuss the limitations of current technology to allow such interactive construction and explain why, in educational environments, such interactive construction is important. Evaluations of Graph Builder showed that users could construct graphs accurately. However results showed that a large number of ‘off-by-one’ errors occurred, where the bar was set either one unit too high or too low. Revisions to the mechanism to manipulate bars were made, and further non-speech audio feedback was added. A further evaluation showing that the proportion of ‘off-by-one’ errors had been reduced.","2007","2023-07-05 07:46:04","2023-07-21 04:42:53","2023-07-05 07:46:04","263-278","","","","","","Graph Builder","","","","","Springer London","London","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-1-84628-664-3_20","","","","","","Bryan-Kinns, Nick; Blanford, Ann; Curzon, Paul; Nigay, Laurence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SMTNQPVT","bookSection","2007","Bologna, Guido; Deville, Benoît; Pun, Thierry; Vinckenbosch, Michel","Identifying Major Components of Pictures by Audio Encoding of Colours","Nature Inspired Problem-Solving Methods in Knowledge Engineering","978-3-540-73054-5 978-3-540-73055-2","","","http://link.springer.com/10.1007/978-3-540-73055-2_10","The goal of the See ColOr project is to achieve a non-invasive mobility aid for blind users that will use the auditory pathway to represent in real-time frontal image scenes. More particularly, we have developed a prototype which transforms HSL coloured pixels into spatialized classical instrument sounds lasting for 300 ms. Hue is sonified by the timbre of a musical instrument, saturation is one of four possible notes, and luminosity is represented by bass when luminosity is rather dark and singing voice when it is relatively bright. Our first experiments are devoted to static images on the computer screen. Six participants with their eyes covered by a dark tissue were trained to associate colours with musical instruments and then asked to determine on several pictures, objects with specific shapes and colours. In order to simplify the protocol of experiments, we used a tactile tablet, which took the place of the camera. Overall, experiment participants found that colour was helpful for the interpretation of image scenes.","2007","2023-07-05 07:46:04","2023-07-21 04:35:21","2023-07-05 07:46:04","81-89","","","4528","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73055-2_10","","","","","","Mira, José; Álvarez, José R.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9T82YPVE","bookSection","2013","Singh, Surya P. N.; Pounds, Paul E. I.; Kurniawati, Hanna","I-Ball: A Programmable Sporting Aid for Children with a Visual Impairment to Play Soccer","Universal Access in Human-Computer Interaction. Design Methods, Tools, and Interaction Techniques for eInclusion","978-3-642-39187-3 978-3-642-39188-0","","","http://link.springer.com/10.1007/978-3-642-39188-0_63","The Interactive Ball (“I-Ball”) is a programmable tonal soccer ball that varies its output based on measurements from an inertial sensor. As a sporting aid for children with blindness and low-vision it makes participation in team sports more accessible without a conspicuous constant tone and in a manner the provides information when stationary. The paper presents the design rationale of the system. Exploitative evaluation with visually impaired users indicates that the encoded information provides utility, but also that noise and wind are complicating external factors that can limit perceptual range.","2013","2023-07-05 07:46:04","2023-07-21 05:10:16","2023-07-05 07:46:04","584-591","","","8009","","","I-Ball","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-39188-0_63","","","","","","Stephanidis, Constantine; Antona, Margherita","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A2EG9QKE","journalArticle","2012","Metatla, Oussama; Bryan-Kinns, Nick; Stockman, Tony","Interactive hierarchy-based auditory displays for accessing and manipulating relational diagrams","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0067-3","http://link.springer.com/10.1007/s12193-011-0067-3","An approach to designing hierarchy-based auditory displays that supports non-visual interaction with relational diagrams is presented. The approach is motivated by an analysis of the functional and structural properties of relational diagrams in terms of their role as external representations. This analysis informs the design of a multiple perspective hierarchy-based model that captures modality independent features of a diagram when translating it into an audio accessible form. The paper outlines design lessons learnt from two user studies that were conducted to evaluate the proposed approach.","2012-05","2023-07-05 07:46:04","2023-07-20 07:02:31","2023-07-05 07:46:04","111-122","","3-4","5","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BUSF9V9C","journalArticle","2014","Omodei, Elisa; Bazzani, Armando; Rambaldi, Sandro; Michieletto, Paolo; Giorgini, Bruno","The physics of the city: pedestrians dynamics and crowding panic equation in Venezia","Quality & Quantity","","0033-5177, 1573-7845","10.1007/s11135-012-9773-5","http://link.springer.com/10.1007/s11135-012-9773-5","In this paper we present the physics of the city, a new approach in order to investigate the urban dynamics. In particular we focus on the citizens’ mobility observation and modeling. Being in principle the social dynamics not directly observable, our main idea is that observing the human mobility processes we can deduce some features and characteristics of social dynamics. We define the automata gas paradigm and we write a crowding equation able to predict, in a statistical sense, the threshold between a selforganized crowd and a chaotic one, which we interpret as the emergence of a possible panic scenario. We show also some specific results obtained on the Venezia pedestrian network. Firstly, analyzing the network we estimate the Venice complexity, secondly measuring the pedestrian flow on some bridges we find significant statistical correlations, and by the experimental data we design two different bridges flow profiles depending from the pedestrian populations. Furthermore considering a reduced portion of the city, i.e. Punta della Dogana, we build up a theoretical model via a Markov approach, with a stationary state solution. Finally implementing some individual characteristics of pedestrians, we simulate the flows finding a good agreement with the empirical distributions. We underline that these results can be the basis to construct an E-governance mobility system.","2014-01","2023-07-05 07:46:04","2023-07-21 04:56:27","2023-07-05 07:46:04","347-373","","1","48","","Qual Quant","The physics of the city","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"78L3QU96","bookSection","2009","Rath, Matthias; Bienert, Sascha","A Tangible Game as Distributable Platform for Psychophysical Examination","Haptic and Audio Interaction Design","978-3-642-04075-7 978-3-642-04076-4","","","http://link.springer.com/10.1007/978-3-642-04076-4_17","Through the use of built-in accelerometers a game-software for recent generation MacBooks allows control of a scenario of virtual moving objects by tilting the computer. Together with integrated visual and continuous auditory feedback from models based on physical principles the software forms a possible platform for online collection of psychophysical data.","2009","2023-07-05 07:46:04","2023-07-20 00:16:30","2023-07-05 07:46:04","155-164","","","5763","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-04076-4_17","","","","","","Altinsoy, M. Ercan; Jekosch, Ute; Brewster, Stephen","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z8ETQF7U","bookSection","2014","Bornschein, Jens; Prescher, Denise; Weber, Gerhard","SVGPlott – Generating Adaptive and Accessible Audio-Tactile Function Graphs","Computers Helping People with Special Needs","978-3-319-08595-1 978-3-319-08596-8","","","http://link.springer.com/10.1007/978-3-319-08596-8_91","Curve sketching is a hard task for blind and visually impaired pupils and students, but it is an essential part in education. To help those students as well as their colleges, teachers and other people to prepare good tactile function plots the platform independent console program SVGPlott was developed. It enables users without any special knowledge about creating graphics for blind or visually impaired people to prepare highly adaptable mathematical function plots in the SVG format, which can also be used for audio-tactile exploration. SVGPlott was developed in a user-centered design process, including teachers and users. We show that blind and sighted users can prepare function plots including key as well as an automatically generated textual description not only for tactile, audio-tactile and print output, but also for usage on a dynamic tactile pin device and as a high contrast visualization for low vision people.","2014","2023-07-05 07:46:04","2023-07-19 23:51:07","2023-07-05 07:46:04","588-595","","","8547","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-08596-8_91","","","","","","Miesenberger, Klaus; Fels, Deborah; Archambault, Dominique; Peňáz, Petr; Zagler, Wolfgang","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VUM8YGDS","bookSection","2010","Brock, Derek; McClimens, Brian; Wasylyshyn, Christina; Trafton, J. Gregory; McCurry, Malcolm","Evaluating the Utility of Auditory Perspective-Taking in Robot Speech Presentations","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_14","In speech interactions, people routinely reason about each other’s auditory perspective and adjust their manner of speaking accordingly by raising their voice to overcome noise or distance, and sometimes by pausing and resuming when conditions are more favorable for their listener. In this paper we report the findings of a listening study motivated by both this observation and a prototype auditory interface for a mobile robot that monitors the aural parameters of its environment to infer its user’s listening requirements. The results provide significant empirical evidence of the utility of simulated auditory perspective taking and the inferred use of loudness and/or pauses to overcome the potential of ambient noise to mask synthetic speech.","2010","2023-07-05 07:46:04","2023-07-19 11:37:05","2023-07-05 07:46:04","266-286","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_14","","/Users/minsik/Zotero/storage/RVJQB8MR/Brock et al. - 2010 - Evaluating the Utility of Auditory Perspective-Tak.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6NX2Q7VV","journalArticle","2018","Ahn, Jonggil; Kim, Gerard Jounghyun","SPRinT: A Mixed Approach to a Hand-Held Robot Interface for Telepresence","International Journal of Social Robotics","","1875-4791, 1875-4805","10.1007/s12369-017-0463-2","http://link.springer.com/10.1007/s12369-017-0463-2","In this paper, we present SPRinT, a control interface design for a telepresence robot that uses only a smart phone without any external sensors. In addition to basic controllability, we focus on providing a reasonable level of spatial understanding as well as a feeling of telepresence through the interaction. The central idea of SPRinT is to have the operator control and interact with the system by “posing” as the robot in the remote area in order to elicit a sense of telepresence, promote spatial task performance, and improve the overall interactive experience. We have applied the proposed interaction design principle to remote robot control, and compared it to a nominal touch-button based interface in terms of the controllability, the level of user-felt telepresence, and spatial understanding. Our experiments showed that the proprioceptive and spatial nature of the motion-based rotational control was critical in eliciting the sense of telepresence and spatial understanding, and this in turn was also important to ensure effective exploration and awareness of remote spaces. On the other hand, the traditional touch-button interface was more appropriate for translation for which a proper proprioceptive metaphoric command could not be designed. Overall, the mixed approach (body/motion based for rotation and touch based for translation) proved to offer a good middle ground since the interaction method was familiar and easy to use with a reasonable level of telepresence.","2018-09","2023-07-05 07:46:04","2023-07-20 06:43:43","2023-07-05 07:46:04","537-552","","4","10","","Int J of Soc Robotics","SPRinT","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PN7GAY2N","bookSection","2014","Másilko, Lukáš; Pecl, Jiří","Making Graph Theory Algorithms Accessible to Blind Students","Computers Helping People with Special Needs","978-3-319-08595-1 978-3-319-08596-8","","","http://link.springer.com/10.1007/978-3-319-08596-8_86","The authors of the proposal are teachers of mathematics for students with visual impairment at Masaryk University (Brno, Czech Republic). When giving instruction, they face the following problem: how can blind people use a given mathematical algorithm in view of the fact that they follow all the information in linear way. Often the instructors have to decide whether to adapt such an algorithm or let blind students work with it in the same manner as their sighted peers do. Their goal is to find an optimal set of methods which would respect blind people’s linear manner of working with information and at the same time be sufficiently effective. In their paper, the authors will present several adaptations of two algorithms of Graph Theory. They will assess the pros and cons of all the proposed modifications.","2014","2023-07-05 07:46:04","2023-07-19 23:52:24","2023-07-05 07:46:04","549-556","","","8547","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-08596-8_86","","","","","","Miesenberger, Klaus; Fels, Deborah; Archambault, Dominique; Peňáz, Petr; Zagler, Wolfgang","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8W22QGW5","bookSection","2013","Antunes, Rui Filipe; Leymarie, Frederic Fol","Real-Time Behavioral Animation of Humanoid Non-Player Characters with a Computational Ecosystem","Intelligent Virtual Agents","978-3-642-40414-6 978-3-642-40415-3","","","http://link.springer.com/10.1007/978-3-642-40415-3_34","A novel approach to a decentralized autonomous model of agency for general purpose Non-Player Characters (NPCs) is presented: Computational Ecosystems as a model of AI. We describe the technology used to animate a population of gregarious humanoid characters in the virtual world Where is Lourenco Marques? an ethnographic artistic work characterized as a virtual world inhabited by a population of NPCs interacting autonomously among themselves as well as with an audience of outsiders (human observers). First, we present the background and motivations for the project. Then, we describe the technical details about the algorithm that was developed to generate the movements and behaviors of a population of NPC ‘storytellers’. Finally, we layout some of the critical aspects of this particular implementation and contextualize the work with regards to a wider usage in virtual worlds.","2013","2023-07-05 07:46:46","2023-07-20 06:36:42","2023-07-05 07:46:46","382-395","","","8108","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-40415-3_34","","","","","","Aylett, Ruth; Krenn, Brigitte; Pelachaud, Catherine; Shimodaira, Hiroshi","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3LBWPYQZ","journalArticle","2007","Shajahan, Peer; Irani, Pourang","One family, many voices: Can multiple synthetic voices be used as navigational cues in hierarchical interfaces?","International Journal of Speech Technology","","1381-2416, 1572-8110","10.1007/s10772-006-9000-7","http://link.springer.com/10.1007/s10772-006-9000-7","Many commercial applications use synthetic speech for conveying information. In many cases the structure of the information is hierarchical (e.g. menus). In this article, we describe the results of two experiments that examine the possibility of conveying hierarchies (family of trees) using multiple synthetic voices. We postulate that if hierarchical structures can be conveyed using synthetic speech, then navigation in these hierarchies can be improved. In the first experiment, hierarchies containing 10 nodes, with a depth of 3 levels, were created. We used synthetic voices to represent nodes in these hierarchies. A within-subjects study (N = 12) was conducted to compare multiple synthetic voices against single synthetic voices for locating the positions of nodes in a hierarchy. Multiple synthetic voices were created by manipulating synthetic voice parameters according to a set of design principles. Results of the first experiment showed that the subjects performed the tasks significantly better with multiple synthetic voices than with single synthetic voices. To investigate the effect of multiple synthetic voices on complex hierarchies a second experiment was conducted. A hierarchy of 27 nodes was created and a between-subjects study (N = 16) was carried out. The results of this experiment showed that the participants recalled 84.38% of the nodes accurately. Results from these studies imply that multiple synthetic voices can be effectively used to represent and provide navigation cues in interfaces structured as hierarchies.","2007-03-14","2023-07-05 07:46:46","2023-07-20 06:46:05","2023-07-05 07:46:46","1-15","","1-2","9","","Int J Speech Technol","One family, many voices","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HY979T55","bookSection","2013","Antunes, Rui Filipe; Leymarie, Frederic Fol","An Ecosystem Based Model for Real-Time Generative Animation of Humanoid Non-Player Characters","Progress in Artificial Intelligence","978-3-642-40668-3 978-3-642-40669-0","","","http://link.springer.com/10.1007/978-3-642-40669-0_7","In this paper a novel approach to a decentralized autonomous model of agency for general purpose Non-Player Characters (NPCs) is presented: the AI model of Computational Ecosystems. We describe the technology used to animate a population of gregarious humanoid avatars in a virtual world. This artistic work is an ethnographic project where a population of NPCs inhabit the virtual world and interact autonomously among themselves as well as with an audience of outsiders (human observers). First, we present the background, motivation and summary for the project. Then, we describe the algorithm that was developed to generate the movements and behaviors of the population of NPC “story-tellers”. Finally, we discuss some of the critical aspects of this implementation and contextualize the work with regards to a wider usage in computer games and virtual worlds.","2013","2023-07-05 07:46:46","2023-07-21 04:55:33","2023-07-05 07:46:46","66-77","","","8154","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-40669-0_7","","","","","","Correia, Luís; Reis, Luís Paulo; Cascalho, José","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G94PZB2L","journalArticle","2023","Bowman, Catherine D. D.; Elkins-Tanton, Linda T.; Talamante, Adriana; Bell, James F.; Cisneros, Ernest; Cook, Alexandra; Frieman, Jason D.; Gainor, Danya; Hunziker, Jamie; Khan, Shaheer; Lawler, Christopher R.; Maschino, Jessica; McCoy, Timothy J.; Nessi, Kaxandra; Oran, Rona; Seal, David; Simon, Amber; Singh, Rohit; Tolbert, Carol M.; Valentine, Karin; Weiss, Benjamin; Wenkert, Daniel D.; Williams, David A.","Mission to Psyche: Including Undergraduates and the Public on the Journey to a Metal World","Space Science Reviews","","0038-6308, 1572-9672","10.1007/s11214-023-00967-x","https://link.springer.com/10.1007/s11214-023-00967-x","Abstract             The NASA Psyche mission’s program to engage university undergraduates and the public in the mission is inspired by and built upon the extensive foundation of public engagement, educational outreach activities, and expertise of NASA and mission partner institutions. The program leverages the enthusiasm and contributions of undergraduates nationwide to the benefit of the mission, the students and their institutions and communities, and the broader public. Psyche Student Collaborations consists of four main programs, two (Psyche Capstone and Psyche Inspired) are available solely to undergraduates enrolled at universities or community colleges in the United States and its territories and two (Innovation Toolkit free online courses and Science Outreach Interns and Docents) invite broader participation by engaging the talents and creativity of undergraduate interns to help create content and events to reach the public and lifelong learners. Together, these offerings provide multiple entry points and a spectrum of intensity of experiences, numbers of participants, disciplinary diversity, and mode of delivery. Involving undergraduates in all phases of the program supports the development of the next generation of explorers, contributes to the nation’s workforce preparation, and complements NASA’s existing undergraduate offerings by providing long-term opportunities for students to participate with the mission through established postsecondary education structures like capstone courses.","2023-04","2023-07-05 07:46:46","2023-07-05 07:46:46","2023-07-05 07:46:46","25","","3","219","","Space Sci Rev","Mission to Psyche","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/ZDACBFAU/Bowman et al. - 2023 - Mission to Psyche Including Undergraduates and th.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CKUXV93A","journalArticle","2022","Gold, Nicolas E.; Krinke, Jens","Ethics in the mining of software repositories","Empirical Software Engineering","","1382-3256, 1573-7616","10.1007/s10664-021-10057-7","https://link.springer.com/10.1007/s10664-021-10057-7","Abstract             Research in Mining Software Repositories (MSR) is research involving human subjects, as the repositories usually contain data about developers’ and users’ interactions with the repositories and with each other. The ethics issues raised by such research therefore need to be considered before beginning. This paper presents a discussion of ethics issues that can arise in MSR research, using the mining challenges from the years 2006 to 2021 as a case study to identify the kinds of data used. On the basis of contemporary research ethics frameworks we discuss ethics challenges that may be encountered in creating and using repositories and associated datasets. We also report some results from a small community survey of approaches to ethics in MSR research. In addition, we present four case studies illustrating typical ethics issues one encounters in projects and how ethics considerations can shape projects before they commence. Based on our experience, we present some guidelines and practices that can help in considering potential ethics issues and reducing risks.","2022-01","2023-07-05 07:46:46","2023-07-05 07:46:46","2023-07-05 07:46:46","17","","1","27","","Empir Software Eng","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/J4T3VBPC/Gold and Krinke - 2022 - Ethics in the mining of software repositories.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UKZ8B66Y","bookSection","2006","Coleman, Graeme W.; Macaulay, Catriona; Newell, Alan F.","Listen to This – Using Ethnography to Inform the Design of Auditory Interfaces","Haptic and Audio Interaction Design","978-3-540-37595-1 978-3-540-37596-8","","","http://link.springer.com/10.1007/11821731_13","Within the wider Human-Computer Interaction community, many researchers have turned to ethnography to inform systems design. However, such approaches have yet to be fully utilized within auditory interface research, a field hitherto driven by technology-inspired design work and the addressing of specific cognitive issues. It is proposed that the time has come to investigate the role ethnographic methods have to play within auditory interface design. We begin by discussing “traditional” ethnographic methods by presenting our experiences conducting a field study with a major UK-based computer games developer, highlighting issues pertinent to the design of auditory interfaces, before suggesting ways in which such techniques could be expanded to consider the role sound plays in people’s lived experiences and thus merit further research.","2006","2023-07-05 07:53:21","2023-07-20 00:14:30","2023-07-05 07:53:21","133-144","","","4129","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11821731_13","","","","","","McGookin, David; Brewster, Stephen","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M3HXZUJM","journalArticle","2014","Kiriella, Dawpadee B.; Kumari, Shyama C.; Ranasinghe, Kavindu C.; Jayaratne, Lakshman","Music Training Interface for Visually Impaired through a Novel Approach to Optical Music Recognition","GSTF Journal on Computing (JoC)","","2010-2283","10.7603/s40601-013-0045-6","http://www.globalsciencejournals.com/article/10.7603/s40601-013-0045-6","Abstract                            Some inherited barriers which limits the human abilities can be surprisingly win through technology. This research focuses on defining a more reliable and a controllable interface for visually impaired people to read and study eastern music notations which are widely available in printed format. One of another concept behind was that differently-abled people should be assisted in a way which they can proceed interested tasks in an independent way. The research provide means to continue on researching the validity of using a controllable auditory interface instead using Braille music scripts converted with the help of 3               rd               parties. The research further summarizes the requirements aroused by the relevant users, design considerations, evaluation results on user feedbacks of proposed interface.","2014-05","2023-07-05 07:53:21","2023-07-05 07:53:21","2023-07-05 07:53:21","45","","4","3","","GSTF J Comput","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LUHESLME","bookSection","2004","Ku, Kyong-I; Won, Jae-Yong; Park, Jaehyun; Kim, Yoo-Sung","A Content-Based Music Retrieval System Using Multidimensional Index of Time-Sequenced Representative Melodies from Music Database","Advances in Databases and Information Systems","978-3-540-23243-8 978-3-540-30204-9","","","http://link.springer.com/10.1007/978-3-540-30204-9_17","In content-based music retrieval systems, since both the correctness and the performance of retrievals are important, a few content-based music retrieval systems have the melody index which contains the representative melodies of music to be likely used as users’ queries. In this paper, we describe the development of a content-based music retrieval system in which the multidimensional index of time-sequenced representative melodies extracted appropriately based on musical composition forms is used to support quick and appropriate retrievals to users’ melody queries. From the experimental results, we can see that the developed system can retrieve more relevant results than previous systems with smaller storage overhead than whole melody index.","2004","2023-07-05 07:53:21","2023-07-19 11:11:31","2023-07-05 07:53:21","246-258","","","3255","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-30204-9_17","","","","","","Benczúr, András; Demetrovics, János; Gottlob, Georg","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S6ILUD2H","bookSection","2005","Won, Jae-Yong; Lee, Jae-Heon; Ku, KyongI; Park, Jaehyun; Kim, Yoo-Sung","A Content-Based Music Retrieval System Using Representative Melody Index from Music Databases","Computer Music Modeling and Retrieval","978-3-540-24458-5 978-3-540-31807-1","","","http://link.springer.com/10.1007/978-3-540-31807-1_21","For content-based music retrieval, since not only the correctness of retrieval results but also the performance of retrievals is important, there are great needs for efficient content-based music retrieval systems that can quickly retrieve the relevant music on demand from large music database with low storage overhead. In this paper, we describe the design and implementation of a content-based music retrieval system in which the representative melody index is systemically constructed and used to support quick and appropriate retrievals to users’ melody queries. By using the proposed system for digital music libraries, it could save up to 65% of index space than that for the whole motifs index while the appropriateness of retrieval results is maintained.","2005","2023-07-05 07:53:21","2023-07-19 23:48:32","2023-07-05 07:53:21","280-294","","","3310","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-31807-1_21","","","","","","Wiil, Uffe Kock","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DUWIKXFR","bookSection","2020","Huang, Yu Ting; Chu, Chi Nung","Multi-sensations Mechanism of Users on the Learning Platform Design of Music Aural Skills","Frontier Computing","9789811532498 9789811532504","","","http://link.springer.com/10.1007/978-981-15-3250-4_240","This paper discusses the nature of aural skills learning with technology implementation in multi-sensations mechanism. By providing the learning platform design of aural skills with different interface design guidelines, the use of technology with multi-sensations to assist teaching and learning could facilitate more novices to music world.","2020","2023-07-05 07:53:21","2023-07-20 00:06:42","2023-07-05 07:53:21","1803-1809","","","551","","","","","","","","Springer Singapore","Singapore","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Electrical Engineering DOI: 10.1007/978-981-15-3250-4_240","","","","","","Hung, Jason C.; Yen, Neil Y.; Chang, Jia-Wei","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H2D86RXQ","journalArticle","2018","Collares, Leandro; Tavares, Tiago F.; Gooch, Amy; Tzanetakis, George","Personalizing self-organizing music spaces with anchors: design and evaluation","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-017-4465-8","http://link.springer.com/10.1007/s11042-017-4465-8","We propose and evaluate a system for content-based visualization and exploration of music collections. The system is based on a modification of Kohonen’s Self-Organizing Map algorithm and allows users to choose the locations of clusters containing acoustically similar tracks on the music space. A user study conducted to evaluate the system shows that the possibility of personalizing the music space was perceived as difficult. Conversely, the user study and objective metrics derived from users’ interactions with the interface demonstrate that the proposed system helped individuals create playlists faster and, under some circumstances, more effectively. We believe that personalized browsing interfaces are an important area of research in Multimedia Information Retrieval, and both the system and user study contribute to the growing work in this field.","2018-03","2023-07-05 07:53:21","2023-07-21 04:31:53","2023-07-05 07:53:21","5525-5545","","5","77","","Multimed Tools Appl","Personalizing self-organizing music spaces with anchors","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EP78HLJB","bookSection","1992","Holland, Simon","Interface Design for Empowerment: a Case Study from Music","Multimedia Interface Design in Education","978-3-540-55046-4 978-3-642-58126-7","","","http://link.springer.com/10.1007/978-3-642-58126-7_12","It is very seldom that psychological theory is applied to human - computer interface design — because very few theories have yet been formulated which are applicable. For the most part designers have to be content to use guidelines and models, which have less applicability. So, the work described in this chapter is unusual, because it describes an interface to a program which teaches about musical harmony, based on psychological theories. The success of that approach is borne out by the fact that the theories suggest the use of a specific style of interface, based on a two-dimensional spatial representation of harmony relationships. This in turn has been shown to be very successful in teaching novice users about harmony.","1992","2023-07-05 07:53:21","2023-07-21 04:30:50","2023-07-05 07:53:21","177-194","","","","","","Interface Design for Empowerment","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-3-642-58126-7_12","","","","","","Edwards, Alistair D. N.; Holland, Simon","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4VFXQYKZ","bookSection","2017","Chung, Szu-Ming; Wu, Chun-Tsai","Mobile Device Applications for Head Start Experience in Music","Interactivity, Game Creation, Design, Learning, and Innovation","978-3-319-55833-2 978-3-319-55834-9","","","http://link.springer.com/10.1007/978-3-319-55834-9_22","This research intends to develop music games as mobile device applications on android system for head start experience in music. The study of design content includes the perception, knowledge formation, musical knowledge and ability, and children’s play and learning motivation. 8 mobile device applications across two levels have been created and 4 of the first game level are tested by 21 preoperational children. In the latter part of this qualitative research, researchers collect data from participants’ observation, video recording, and tablet input data records. The credibility and validation study consisted of two steps: analyzing and comparing 3 dimensions of attitude, interaction, and problem solving of collected data.","2017","2023-07-05 07:53:21","2023-07-20 06:37:21","2023-07-05 07:53:21","189-196","","","196","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering DOI: 10.1007/978-3-319-55834-9_22","","","","","","Brooks, Anthony L.; Brooks, Eva","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EQHW794T","journalArticle","2022","Costa, Daniel; Duarte, Carlos","Audio rendering smart TV apps through mobile devices","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-021-00796-1","https://link.springer.com/10.1007/s10209-021-00796-1","TV service providers now offer a variety of features, including broadcast-related ones like electronic programme guides, catch-up and recording features, but also Internet access and a variety of TV applications. These new features turn TV devices into more versatile and interesting platforms, but also clog the screen with more content than ever. Since this content is mainly visual, this means that TVs have even more inaccessible content for visually impaired people. This paper presents the design of a solution that audio renders the TV application’s user interface through a mobile device. Resorting to a mix of accessibility experts and user studies, we compared multiple feedback versions containing different contextual information. Participants reported that the use of repetitive sentences should be avoided; concise feedback feels smoother and quicker but, for some, lacks information while extended feedback can be annoying and take too much time though is appropriate for learning phases. For menus, most participants suggested to include the position of an element and the number of elements. The results helped to identify the critical information to convey to the user and to tailor two modes differing in the amount of contextual information provided, suitable for differently skilled users. Additionally, we condensed the findings into a list of design guidelines which can be generalized to other auditory interfaces meant to be operated by a VI user.","2022-08","2023-07-05 07:53:21","2023-07-21 05:11:10","2023-07-05 07:53:21","675-689","","3","21","","Univ Access Inf Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G6TH8FE9","bookSection","2021","Men, Delai; Wu, Lingfang","The Investigation into Design Elements of Auditory Pleasure Experience for the Elderly Based on a Testing Tools Development","HCI International 2021 - Late Breaking Papers: Cognition, Inclusion, Learning, and Culture","978-3-030-90327-5 978-3-030-90328-2","","","https://link.springer.com/10.1007/978-3-030-90328-2_16","The vision system of the elderly will undergo age-related changes. Hearing, as a sensory channel for processing external information second only to vision, will impair the use and perception of products or services. User experience is a hot topic in design research related to the elderly in recent years. Designers also pay more attention to thinking about the experience needs of the elderly from the perspective of multi-sensory channels. However, the research on auditory aging design has not been fully developed yet. The purpose of this study was to investigate the design elements and characteristics of the elderly’s auditory pleasure experience, so as to provide a reference basis for the auditory dimension of the aging design. The research methods include literature study, questionnaire, test, and interview. This study defined and extracted the elements that affect the pleasure auditory experience. A set of materials and tools was innovatively developed for the elderly auditory audio test. 40 elderly people took the hearing test. Through qualitative and quantitative analysis, the study had drawn the conclusion of the key features of the design elements of the elderly’s auditory pleasure experience and proposed a suitable aging design strategy based on the elderly’s auditory pleasure experience. In sum, the current study has guiding significance for aging design in different fields. The elderly’s pleasant hearing experience test system developed and applied in this research is universal. The mapping relationship between the auditory design elements and the elderly’s pleasant hearing experience revealed by it makes the sound more in line with the elderly’s hearing experience preferences, which is helpful to enhance the interactive experience of products or services for the elderly.","2021","2023-07-05 07:53:21","2023-07-20 05:50:15","2023-07-05 07:53:21","258-276","","","13096","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-90328-2_16","","","","","","Stephanidis, Constantine; Harris, Don; Li, Wen-Chin; Schmorrow, Dylan D.; Fidopiastis, Cali M.; Antona, Margherita; Gao, Qin; Zhou, Jia; Zaphiris, Panayiotis; Ioannou, Andri; Sottilare, Robert A.; Schwarz, Jessica; Rauterberg, Matthias","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4DSRBIIP","journalArticle","2014","McNaull, James; Augusto, Juan Carlos; Mulvenna, Maurice; McCullagh, Paul","Flexible context aware interface for ambient assisted living","Human-centric Computing and Information Sciences","","2192-1962","10.1186/2192-1962-4-1","https://link.springer.com/10.1186/2192-1962-4-1","Abstract             A Multi Agent System that provides a (cared for) person, the subject, with assistance and support through an Ambient Assisted Living Flexible Interface (AALFI) during the day while complementing the night time assistance offered by NOCTURNAL with feedback assistance, is presented. It has been tailored to the subject’s requirements profile and takes into account factors associated with the time of day; hence it attempts to overcome shortcomings of current Ambient Assisted Living Systems. The subject is provided with feedback that highlights important criteria such as quality of sleep during the night and possible breeches of safety during the day. This may help the subject carry out corrective measures and/or seek further assistance. AALFI provides tailored interaction that is either visual or auditory so that the subject is able to understand the interactions and this process is driven by a Multi-Agent System. User feedback gathered from a relevant user group through a workshop validated the ideas underpinning the research, the Multi-agent system and the adaptable interface.","2014-12","2023-07-05 07:53:21","2023-07-05 07:53:21","2023-07-05 07:53:21","1","","1","4","","Hum. Cent. Comput. Inf. Sci.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/74JQG5B4/McNaull et al. - 2014 - Flexible context aware interface for ambient assis.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z5GZJ7ZL","bookSection","2023","Bhattacharya, Arpita; Backonja, Uba; Le, Anh; Antony, Ria; Si, Yujia; Lee, Jin Ha","Understanding the Influence of Music on People’s Mental Health Through Dynamic Music Engagement Model","Information for a Better World: Normality, Virtuality, Physicality, Inclusivity","978-3-031-28034-4 978-3-031-28035-1","","","https://link.springer.com/10.1007/978-3-031-28035-1_8","Research shows that music helps people regulate and process emotions to positively impact their mental health, but there is limited research on how to build music systems or services to support this. We investigated how engagement with music can help the listener support their mental health through a case study of the BTS ARMY fandom. We conducted a survey with 1,190 BTS fans asking about the impact BTS’ music has on their mental health and wellbeing. Participants reported that certain songs are appropriate for specific types of mood regulations, attributed largely to lyrics. Reflection, connection, and comfort were the top three experiences listeners shared during and after listening to BTS’ music. External factors like knowledge about the context of a song’s creation or other fans’ reactions to a song also influenced people’s feelings toward the music. Our research suggests an expanded view of music’s impact on mental health beyond a single-modal experience to a dynamic, multi-factored experience that evolves over time within the interconnected ecosystem of the fandom. We present the Dynamic Music Engagement Model which represents the complex, multifaceted, context-dependent nature of how music influences people’s mental health, followed by design suggestions for music information systems and services.","2023","2023-07-05 07:53:21","2023-07-20 06:34:06","2023-07-05 07:53:21","91-108","","","13971","","","","","","","","Springer Nature Switzerland","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-031-28035-1_8","","","","","","Sserwanga, Isaac; Goulding, Anne; Moulaison-Sandy, Heather; Du, Jia Tina; Soares, António Lucas; Hessami, Viviane; Frank, Rebecca D.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7YHCTW2A","bookSection","2013","Wei, Ming-Hsuan; Hwang, Sheue-Ling; Chang, Hsin-Chang; Hung, Jian-Yung; Kuo, Chih-Chung","Ergonomics Design with Novice Elicitation on an Auditory-Only In-Vehicle Speech System","Human-Computer Interaction. Applications and Services","978-3-642-39261-0 978-3-642-39262-7","","","http://link.springer.com/10.1007/978-3-642-39262-7_74","This research is aimed to design an auditory-only in-vehicle speech system, named as Talking Car Novice Mode, and provide with elicitation that even a novice can easily handle. In this study, 19 participants were asked to use radio and music functions in two kinds of in-vehicle speech systems, the original Talking Car and Talking Car Novice Mode, while driving through a virtual world. Data of secondary task performance, the amount of time spent on tasks and the times of calling help function were recorded by a camera. The annoyed score of sentences, NASA-TLX questionnaire and subjective questionnaire were completed after the test. The result indicated that there was no significant difference between driving with and without tasks on either the reaction time of slamming the brake or the times user call for help. Besides, the learning curve of Talking Car Novice Mode is steep and ensures that Talking Car Novice Mode provides enough elicitation to novices. Hence, the Talking Car Novice Mode is expected to be friendlier and safer than original Talking Car in-vehicle speech system for a novice user.","2013","2023-07-05 07:53:21","2023-07-20 06:31:09","2023-07-05 07:53:21","654-660","","","8005","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-39262-7_74","","","","","","Kurosu, Masaaki","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EYYQJLUZ","bookSection","2006","O’Sullivan, Conor; Chang, Angela","An Activity Classification for Vibrotactile Phenomena","Haptic and Audio Interaction Design","978-3-540-37595-1 978-3-540-37596-8","","","http://link.springer.com/10.1007/11821731_14","We observe that the recent availability of audio-haptic actuators allow richer vibration content to be available in commercial devices. However, we note that consumers are unable to take advantage of these rich experiences, mainly due to the lack of a descriptive language for vibration. We analyze the current methods for classifying vibrations. We propose a new framework for describing vibrotactile haptic phenomena, based on an organizing the media based on content activity. We describe this naming system, based on Russolo’s families of noise, and address other pertinent issues to introducing vibration content into commercial devices.","2006","2023-07-05 07:55:08","2023-07-20 00:15:49","2023-07-05 07:55:08","145-156","","","4129","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11821731_14","","/Users/minsik/Zotero/storage/8GR5M766/O’Sullivan and Chang - 2006 - An Activity Classification for Vibrotactile Phenom.pdf","","","","McGookin, David; Brewster, Stephen","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RTPJY36L","bookSection","2013","Rajan, Rahul; Hsiao, Joey; Lahoti, Deven; Selker, Ted","“Roger that!” — The Value of Adding Social Feedback in Audio-Mediated Communications","Human-Computer Interaction – INTERACT 2013","978-3-642-40497-9 978-3-642-40498-6","","","http://link.springer.com/10.1007/978-3-642-40498-6_37","Losing track of who is in a conversation, and what is being said, is always a problem especially on audio-only conference calls. This paper investigates how domain-independent social feedback can support such interactions, and improve communication, through the use of audio cues. In particular, we show how an agent can improve people’s ability to accurately identify and distinguish between speakers, reassure users about the presence of other collaborators on the line, and announce events like entry & exit with minimum impact on users cognitive ability.","2013","2023-07-05 07:55:08","2023-07-20 06:29:08","2023-07-05 07:55:08","471-488","","","8120","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-40498-6_37","","/Users/minsik/Zotero/storage/2DNNPXNL/Rajan et al. - 2013 - “Roger that!” — The Value of Adding Social Feedbac.pdf","","","","Kotzé, Paula; Marsden, Gary; Lindgaard, Gitte; Wesson, Janet; Winckler, Marco","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YF9CASXK","journalArticle","2004","Brown, Silas S.; Robinson, Peter","Transformation frameworks and their relevance in universal design","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-004-0107-9","http://link.springer.com/10.1007/s10209-004-0107-9","Music, engineering, mathematics, and many other disciplines have established notations for writing their documents. Adjusting these notations can contribute to universal access by helping to address access difficulties, such as disabilities, cultural backgrounds, or restrictive hardware. Tools that support the programming of such transformations can also assist by allowing the creation of new notations on demand, which is an under-explored option in the relief of educational difficulties. This paper reviews some programming tools that can be used to effect such transformations. It also introduces a tool, called “4DML,” which allows the programmer to create a “model” of the desired result, from which the transformation is derived.","2004-10","2023-07-05 07:55:08","2023-07-21 05:10:51","2023-07-05 07:55:08","209-223","","3-4","3","","Univ Access Inf Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KES46YF4","bookSection","2011","Ichiyanagi, Yuki; Cooper, Eric W.; Kryssanov, Victor V.; Ogawa, Hitoshi","A Haptic Emotional Model for Audio System Interface","Human-Computer Interaction. Towards Mobile and Intelligent Interaction Environments","978-3-642-21615-2 978-3-642-21616-9","","","http://link.springer.com/10.1007/978-3-642-21616-9_60","The presented study deals with the problem of selecting music content in digital media, such as mp3 file collections. Usually, to select a specific music file (e.g. a song), one has to directly use some a priori data about the file content, e.g. the artist’s name, genre, year of release, or the like. In many situations, however, this data is not visible, does not offer enough information, or otherwise does not provide for any immediately accessible mode for selecting the audio content. With the appropriate models of interaction, haptic output devices have a number of advantages for such selection tasks. First, as haptically enabled systems are becoming common, users are becoming more and more familiar with this modality of user-system interaction. Results of recent studies also suggest that the sense of touch may be more closely associated with moods and emotions than other modalities of interaction. Finally, the sense of touch is available without interference with visual or auditory channels. In the presented study, a model is proposed that links emotional states apparently evoked by music content to specific haptic stimuli. An experiment is conducted to verify tactile-emotive associations assumed by the model, and also to explore whether music specific characteristics, such as genre, would directly be related to haptic sensations. Experimental results obtained are discussed and used to design a novel user interface for an audio system. The envisaged interface would allow for selecting music through tactile interactions. The study’s conclusions are drawn, and future work is outlined.","2011","2023-07-05 07:55:08","2023-07-20 06:33:14","2023-07-05 07:55:08","535-542","","","6763","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-21616-9_60","","","","","","Jacko, Julie A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KN5M7H34","journalArticle","2022","Fedotchev, A. I.","Correction of Stress-Induced States Using Sensory Stimulation Automatically Modulated by Endogenous Human Rhythms","Neuroscience and Behavioral Physiology","","0097-0549, 1573-899X","10.1007/s11055-022-01322-3","https://link.springer.com/10.1007/s11055-022-01322-3","This article considers the dynamics of the development of a potential approach to correcting stress-induced states in humans, i.e., adaptive neurostimulation. The approach consists of presenting sensory stimulation automatically modulated by intrinsic rhythmic human processes such as the respiratory rhythm, the heartbeat rhythm, and electroencephalograph (EEG) rhythms. Many examples have shown that real-time self-adjustment of the stimulation parameters by these rhythms leads to a high level personalization of therapeutic stimulation and increases in its efficacy in suppressing stress-induced states. The publications reviewed here point to the advantages of this approach for developing innovatory technologies using complex feedback from endogenous human rhythms to correct a wide spectrum of functional disorders.","2022-07","2023-07-05 07:55:08","2023-07-21 04:37:55","2023-07-05 07:55:08","947-952","","6","52","","Neurosci Behav Physi","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/8XBHNBI4/Fedotchev - 2022 - Correction of Stress-Induced States Using Sensory .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KAIRA6R8","bookSection","2012","Kandemir, Melih; Klami, Arto; Vetek, Akos; Kaski, Samuel","Unsupervised Inference of Auditory Attention from Biosensors","Machine Learning and Knowledge Discovery in Databases","978-3-642-33485-6 978-3-642-33486-3","","","http://link.springer.com/10.1007/978-3-642-33486-3_26","We study ways of automatically inferring the level of attention a user is paying to auditory content, with applications for example in automatic podcast highlighting and auto-pause, as well as in a selection mechanism in auditory interfaces. In particular, we demonstrate how the level of attention can be inferred in an unsupervised fashion, without requiring any labeled training data. The approach is based on measuring the (generalized) correlation or synchrony between the auditory content and physiological signals reflecting the state of the user. We hypothesize that the synchrony is higher when the user is paying attention to the content, and show empirically that the level of attention can indeed be inferred based on the correlation. In particular, we demonstrate that the novel method of time-varying Bayesian canonical correlation analysis gives unsupervised prediction accuracy comparable to having trained a supervised Gaussian process regression with labeled training data recorded from other users.","2012","2023-07-05 07:55:08","2023-07-21 04:28:16","2023-07-05 07:55:08","403-418","","","7524","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-33486-3_26","","/Users/minsik/Zotero/storage/UQ32AD4N/Kandemir et al. - 2012 - Unsupervised Inference of Auditory Attention from .pdf","","","","Flach, Peter A.; De Bie, Tijl; Cristianini, Nello","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X9JQFEXG","bookSection","1998","Rigas, Dimitrios I.; Alty, James L.","How Can Multimedia Designers Utilize Timbre?","People and Computers XIII","978-3-540-76261-4 978-1-4471-3605-7","","","http://link.springer.com/10.1007/978-1-4471-3605-7_17","When musical sound is required during development of auditory or multimedia interfaces, designers often need to utilize different musical voices or timbre (usually produced via a multiple timbre synthesizer or a sound card) in order to communicate information. Currently, there is a limited set of guidelines assisting multimedia designers to select appropriate timbre. This paper reports a set of recall and recognition experiments on timbres produced by a multiple timbre synthesizer. Results indicate that a number of instruments were successfully recalled and recognized. A set of empirically derived guidelines are suggested to assist multimedia designers in selecting timbre.","1998","2023-07-05 07:55:08","2023-07-21 04:42:03","2023-07-05 07:55:08","273-286","","","","","","","","","","","Springer London","London","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-1-4471-3605-7_17","","","","","","Johnson, Hilary; Nigay, Lawrence; Roast, Christopher","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RFV96P5Z","bookSection","2014","Qin, Xiangang","The Measurement of Perceived Quality of Various Audio: Sampling Rate and Frame Loss Rate","Engineering Psychology and Cognitive Ergonomics","978-3-319-07514-3 978-3-319-07515-0","","","http://link.springer.com/10.1007/978-3-319-07515-0_27","In this paper, the influence of Audio Sampling Rate (ASR) and Frame Loss Rate (FLR) on perceived Quality of Experience (QoE) was studied. The result indicated that users are very sensitive to the damaged auditory quality caused by frame loss at 8 kHz and 12 kHz no matter how much it losses. The perceived damage of auditory quality caused by frame loss at 16 kHz and 24 kHz is also much lower that at 8 kHz and 12 kHz. Users even failed to perceive the negative impact of frame loss on auditory quality at 32 kHz whatever the frame loss rate is. The interaction effect indicates that users are not so sensitive to the negative impact of frame loss when the sampling rates increase to 16 kHz or higher.","2014","2023-07-05 07:55:08","2023-07-19 23:58:58","2023-07-05 07:55:08","265-271","","","8532","","","The Measurement of Perceived Quality of Various Audio","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-07515-0_27","","","","","","Harris, Don","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Kobsa, Alfred; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Doug; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MMNPJJYR","bookSection","1993","Karshmer, Arthur I.; Oliver, Richard L.","Special computer interfaces for the visually handicapped: F.O.B. The manufacturer","Human-Computer Interaction","978-3-540-57433-0 978-3-540-48152-2","","","http://link.springer.com/10.1007/3-540-57433-6_56","Many techniques have been suggested, and some even brought to market, to allow the visually handicapped person to more easily interact with modern computing equipment Most of the work to date has focused on providing special purpose hardware and software to accomplish this task. In the current work, we describe an approach that would allow all computer manufacturers to ship systems based on today's popular graphical user interfaces (GUIs) that will also serve the needs of the visually handicapped user. By building the user interface into the GUI normally supplied by the manufacturer, the cost of such interfaces should go down, while the availability should go up.","1993","2023-07-05 07:55:08","2023-07-20 05:54:44","2023-07-05 07:55:08","272-280","","","753","","","Special computer interfaces for the visually handicapped","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-57433-6_56","","","","","","Bass, Leonard J.; Gornostaev, Juri; Unger, Claus","Goos, G.; Hartmanis, J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NLD8XZDF","bookSection","2009","Kern, Dagmar; Marshall, Paul; Hornecker, Eva; Rogers, Yvonne; Schmidt, Albrecht","Enhancing Navigation Information with Tactile Output Embedded into the Steering Wheel","Pervasive Computing","978-3-642-01515-1 978-3-642-01516-8","","","http://link.springer.com/10.1007/978-3-642-01516-8_5","Navigation systems are in common use by drivers and typically present information using either audio or visual representations. However, there are many pressures on the driver's cognitive systems in a car and navigational systems can add to this complexity. In this paper, we present two studies which investigated how vibro-tactile representations of navigational information, might be presented to the driver via the steering wheel to ameliorate this problem. Our results show that adding tactile information to existing audio, or particularly visual representations, can improve both driving performance and experience.","2009","2023-07-05 07:56:42","2023-07-21 04:54:47","2023-07-05 07:56:42","42-58","","","5538","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-01516-8_5","","/Users/minsik/Zotero/storage/VYUI5TP9/Kern et al. - 2009 - Enhancing Navigation Information with Tactile Outp.pdf","","","","Tokuda, Hideyuki; Beigl, Michael; Friday, Adrian; Brush, A. J. Bernheim; Tobe, Yoshito","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CSRB9BU2","bookSection","1998","Pittarello, Fabio; Pittarello, Mauro; Italiano, Giuseppe F.","Architecture and Digital Exhibitions the Einstein Tower World","Virtual Environments ’98","978-3-211-83233-2 978-3-7091-7519-4","","","http://link.springer.com/10.1007/978-3-7091-7519-4_16","This work is part of a general research about three-dimensional worlds usability issues, aimed at analysing the current points of strength and weakness of immersive navigation in virtual worlds on the net and at developing new cognitive artefacts to improve the quality of these experiences. The Einstein Tower World, a system conceived in occasion of the German Expressionism exhibition held in 1997 at Palazzo Grassi in Venice, can be seen as a first implementation of the results achieved so far by our research. The Einstein Tower, a sun observatory built in Potsdam from 1919 to 1923 by Erich Mendelsohn and chosen as a symbol of the real exhibition in Venice, becomes the focus of a virtual exhibition where architecture, paintings, manifestos, cinema fragments and music melt into a unique composition, a small account of gesamtkunstwerk (an integrated esthetical experience achieved by eliminating the divisions between architecture, music and visual arts) proposed by expressionist artists.","1998","2023-07-05 07:56:42","2023-07-21 05:13:55","2023-07-05 07:56:42","162-171","","","","","","","","","","","Springer Vienna","Vienna","","","","","","DOI.org (Crossref)","","Series Title: Eurographics DOI: 10.1007/978-3-7091-7519-4_16","","","","","","Göbel, Martin; Landauer, Jürgen; Lang, Ulrich; Wapler, Matthias","Hansmann, W.; Hewitt, W. T.; Purgathofer, W.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A3BCII5R","bookSection","2021","Chen, Weiwen; Lu, Xiaobo; Tang, Xuelin","Toward a Theory-Driven Model of Emotional Interaction Design in Mobile Games Research","HCI in Games: Experience Design and Game Mechanics","978-3-030-77276-5 978-3-030-77277-2","","","https://link.springer.com/10.1007/978-3-030-77277-2_1","The rapid development of mobile information technology provides good opportunities for the maturity and growth of mobile games. In busy modern life, mobile games are playing an increasingly important role in people’s social connection. In addition to entertainment functions, mobile games in the near future have the potential to become a new method of teaching, by which users can learn a variety of knowledge in a lively and interesting way. What factors contributed to the birth of an excellent mobile game? Obviously, the emotional factors in interaction design play a vital role. Emotion-based interaction design can evoke various positive emotions of players, such as the sense of accomplishment, satisfaction, flow experience, awe, and other psychological experiences. Thus, it is of importance to analyze the emotional factors in mobile game interaction design and discuss emotional interaction design principles and methods. The purpose of this paper to establish an emotional interaction design model, which could provide the theoretical basis for improving mobile game design theory, expanding mobile game functions and value, meeting users’ more advanced emotional needs and guiding mobile game development and design practice. This paper first discusses the emotional design theory proposed by Professor Don Norman. And then it analyzes the intersection and integration of interaction design and psychology, human factors engineering, aesthetics, and other disciplines. Corresponding to the three levels of emotional design: visceral, behavioral, and reflective, it puts forward an emotional interaction design model consisting of user interface design, interaction operation design, and interaction experience design. Finally, it improves and applies the emotional interaction design model in mobile games through case studies.","2021","2023-07-05 07:56:42","2023-07-20 05:43:21","2023-07-05 07:56:42","3-19","","","12789","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-77277-2_1","","","","","","Fang, Xiaowen","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"37W7NFY4","journalArticle","2015","Hussain, Ibrar; Chen, Ling; Mirza, Hamid Turab; Chen, Gencai; Hassan, Saeed-Ul","Right mix of speech and non-speech: hybrid auditory feedback in mobility assistance of the visually impaired","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-014-0350-7","http://link.springer.com/10.1007/s10209-014-0350-7","Despite the growing awareness about mobility issues surrounding auditory interfaces used by visually impaired people, designers still face challenges while creating sound for auditory interfaces. This paper presents a new approach of hybrid auditory feedback, which converts frequently used speech instructions to non-speech (i.e., spearcons), based on users’ travelled frequency and sound repetition. Using a within-subject design, twelve participants (i.e., blind people) carried out a task, using a mobility assistant application in an indoor environment. As surfaced from the study results, the hybrid auditory feedback approach is more effective than non-speech and it is pleasant compared with repetitive speech-only. In addition, it can substantially improve user experience. Finally, these findings may help researchers and practitioners use hybrid auditory feedback, rather than using speech- or non-speech-only, when designing or creating accessibility/assistive products and systems.","2015-11","2023-07-05 07:56:42","2023-07-21 05:11:32","2023-07-05 07:56:42","527-536","","4","14","","Univ Access Inf Soc","Right mix of speech and non-speech","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SWPB65BZ","bookSection","2012","Murphy, Emma; Moussette, Camille; Verron, Charles; Guastavino, Catherine","Supporting Sounds: Design and Evaluation of an Audio-Haptic Interface","Haptic and Audio Interaction Design","978-3-642-32795-7 978-3-642-32796-4","","","http://link.springer.com/10.1007/978-3-642-32796-4_2","The design and evaluation of a multimodal interface is presented in order to investigate how spatial audio and haptic feedback can be used to convey the navigational structure of a virtual environment. The non-visual 3D virtual environment is composed of a number of parallel planes with either horizontal or vertical orientations. The interface was evaluated using a target-finding task to explore how auditory feedback can be used in isolation or combined with haptic feedback for navigation. Twenty-three users were asked to locate targets using auditory feedback in the virtual structure across both horizontal and vertical orientations of the planes, with and without haptic feedback. Findings from the evaluation experiment reveal that users performed the task faster in the bi-modal conditions (with combined auditory and haptic feedback) with a horizontal orientation of the virtual planes.","2012","2023-07-05 07:56:42","2023-07-20 00:15:32","2023-07-05 07:56:42","11-20","","","7468","","","Supporting Sounds","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-32796-4_2","","","","","","Magnusson, Charlotte; Szymczak, Delphine; Brewster, Stephen","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N25S2A5F","journalArticle","2020","Shoaib, Muhammad; Hussain, Ibrar; Mirza, Hamid Turab","Automatic switching between speech and non-speech: adaptive auditory feedback in desktop assistance for the visually impaired","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-019-00696-5","http://link.springer.com/10.1007/s10209-019-00696-5","Continual enrichments in auditory interfaces of desktop applications allow visually impaired people to successfully use computers in education, employment, and social interaction. Designers face multiple challenges while producing sound for auditory interfaces. This paper presents a new method of adaptive auditory feedback, which converts speech-only instructions to non-speech (i.e., spearcons), based on users’ interaction with the application in the desktop environment. Using within-subject design, fifteen participants (i.e., visually impaired) were involved in the study. Results from the study demonstrate that the adaptive auditory feedback method is more efficient than non-speech and more pleasurable with respect to repetitive speech-only instructions. Furthermore, adaptive auditory feedback improves task completion time and action awareness as compared to speech-only. Lastly, these findings may benefit researchers and developers to use adaptive auditory feedback, instead of using speech-only or non-speech feedback while designing auditory feedback for interfaces in desktop environment for the people with visual impairment.","2020-11","2023-07-05 07:56:42","2023-07-21 05:12:59","2023-07-05 07:56:42","813-823","","4","19","","Univ Access Inf Soc","Automatic switching between speech and non-speech","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XQUKQBTQ","bookSection","2012","Alharbi, Saad","Empirically Derived Guidelines for Audio-Visual E-mail Browsing","Advances in New Technologies, Interactive Interfaces and Communicability","978-3-642-34009-3 978-3-642-34010-9","","","http://link.springer.com/10.1007/978-3-642-34010-9_10","This paper presents a set of design guidelines for the use of information visualization techniques and non-speech sounds such as auditory icons and earcons in email browsing. These guidelines were derived based on a previous experimental work consisted of three experimental phases, each phase aimed at investigating different aspects of email browsing. Several key points were covered in these guidelines such as the presentation of email information, finding email information and using audio metaphors for communicating email information.","2012","2023-07-05 07:56:42","2023-07-19 11:12:46","2023-07-05 07:56:42","104-113","","","7547","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-34010-9_10","","","","","","Cipolla-Ficarra, Francisco; Veltman, Kim; Verber, Domen; Cipolla-Ficarra, Miguel; Kammüller, Florian","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"934RH594","bookSection","2013","Garcia, Franco Eusébio; De Almeida Neris, Vânia Paula","Design Guidelines for Audio Games","Human-Computer Interaction. Applications and Services","978-3-642-39261-0 978-3-642-39262-7","","","http://link.springer.com/10.1007/978-3-642-39262-7_26","This paper presents guidelines to aid on the design of audio games. Audio games are games on which the user interface and game events use primarily sounds instead of graphics to convey information to the player. Those games can provide an accessible gaming experience to visually impaired players, usually handicapped by conventional games. The presented guidelines resulted of existing literature research on audio games design and implementation, of a case study and of a user observation performed by the authors. The case study analyzed how audio is used to create an accessible game on nine audio games recommended for new players. The user observation consisted of a playtest on which visually impaired users played an audio game, on which some interaction problems were identified. The results of those three studies were analyzed and compiled in 50 design guidelines.","2013","2023-07-05 07:56:42","2023-07-20 06:30:57","2023-07-05 07:56:42","229-238","","","8005","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-39262-7_26","","/Users/minsik/Zotero/storage/8H6R92ZX/Garcia and De Almeida Neris - 2013 - Design Guidelines for Audio Games.pdf","","","","Kurosu, Masaaki","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MY8U375Z","bookSection","2013","Gross, Richard; Bockholt, Ulrich; Biersack, Ernst W.; Kuijper, Arjan","Multimodal Kinect-Supported Interaction for Visually Impaired Users","Universal Access in Human-Computer Interaction. Design Methods, Tools, and Interaction Techniques for eInclusion","978-3-642-39187-3 978-3-642-39188-0","","","http://link.springer.com/10.1007/978-3-642-39188-0_54","This paper discusses Kreader, a proof-of-concept for a new interface for blind or visually impaired users to have text read to them. We use the Kinect device to track the users body. All feedback is presented with auditory cues, while a minimal visual interface can be turned on optionally. Interface elements are organized in a list manner and placed ego-centric, in relation to the user’s body. Moving around in the room does not change the element’s location. Hence visually impaired users can utilize their ”body-sense” to find elements. Two test sessions were used to evaluate Kreader. We think the results are encouraging and provide a solid foundation for future research into such an interface, that can be navigated by sighted and visually impaired users.","2013","2023-07-05 07:56:42","2023-07-21 05:10:07","2023-07-05 07:56:42","500-509","","","8009","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-39188-0_54","","","","","","Stephanidis, Constantine; Antona, Margherita","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V96GMUSI","webpage","","","A multimodal psychological, physiological and behavioural dataset for human emotions in driving tasks | Scientific Data","","","","","https://www.nature.com/articles/s41597-022-01557-2","Human emotions are integral to daily tasks, and driving is now a typical daily task. Creating a multi-modal human emotion dataset in driving tasks is an essential step in human emotion studies. we conducted three experiments to collect multimodal psychological, physiological and behavioural dataset for human emotions (PPB-Emo). In Experiment I, 27 participants were recruited, the in-depth interview method was employed to explore the driver’s viewpoints on driving scenarios that induce different emotions. For Experiment II, 409 participants were recruited, a questionnaire survey was conducted to obtain driving scenarios information that induces human drivers to produce specific emotions, and the results were used as the basis for selecting video-audio stimulus materials. In Experiment III, 40 participants were recruited, and the psychological data and physiological data, as well as their behavioural data were collected of all participants in 280 times driving tasks. The PPB-Emo dataset will largely support the analysis of human emotion in driving tasks. Moreover, The PPB-Emo dataset will also benefit human emotion research in other daily tasks.","","2023-07-05 07:56:59","2023-07-21 07:42:47","2023-07-05 07:56:59","","","","","","","","","","","","","","","","","","","","","","","/Users/minsik/Zotero/storage/TYYIKAFV/s41597-022-01557-2.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7HF2VA3P","journalArticle","2016","Tsonos, Dimitrios; Kouroupetroglou, Georgios","Prosodic mapping of text font based on the dimensional theory of emotions: a case study on style and size","EURASIP Journal on Audio, Speech, and Music Processing","","1687-4722","10.1186/s13636-016-0087-8","https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-016-0087-8","Current text-to-speech systems do not support the effective provision of the semantics and the cognitive aspects of the documents’ typographic cues (e.g., font type, style, and size). A novel approach is introduced for the acoustic rendition of text font based on the emotional analogy between the visual (text font cues) and the acoustic (speech prosody) modalities. The methodology is based on: a) modeling reader’s emotional state response (“Pleasure”, “Arousal” and “Dominance”) induced by the document’s font cues and b) the acoustic mapping of the emotional state using expressive speech synthesis. A case study was conducted for the proposed methodology by calculating the prosodic values on specific font cues (several font styles and font sizes) and by examining listeners’ preferences on the acoustic rendition of bold, italics, bold-italics, and various font sizes. The experimental results after the user evaluation indicate that the acoustic rendition of font size variations as well as bold and italics is recognized successfully, but bold-italics are confused with bold, due to the similarities of their prosodic variations.","2016-12","2023-07-05 07:58:12","2023-07-20 00:02:09","2023-07-05 07:58:12","8","","1","2016","","J AUDIO SPEECH MUSIC PROC.","Prosodic mapping of text font based on the dimensional theory of emotions","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/FUKFEVFF/Tsonos and Kouroupetroglou - 2016 - Prosodic mapping of text font based on the dimensi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P26TNWC7","journalArticle","2022","Fedotchev, A. I.; Parin, S. B.; Polevaya, S. A.","Neural Interfaces Based on Endogenous Body Rhythms for Optimization of the Functional State of Humans and Cognitive Rehabilitation","Neuroscience and Behavioral Physiology","","0097-0549, 1573-899X","10.1007/s11055-022-01278-4","https://link.springer.com/10.1007/s11055-022-01278-4","We report here an analysis of studies in the last five years on a construction and management challenge in technogenic systems, i.e., neural interfaces and neurobiocontrol systems. Current approaches to the use of neural interfaces in medicine, engineering psychology, and cognitive rehabilitation of humans are addressed. The main focus of attention is on neural interfaces based on use of system-forming endogenous body rhythms – electroencephalogram (EEG) rhythms, heart rate, and the respiratory rhythm. The advantages, state of the art, and challenges in this line of research are discussed and potential pathways for answering its key questions are outlined. The results of the authors’ own developments in this direction are presented.","2022-05","2023-07-05 07:58:12","2023-07-21 04:38:20","2023-07-05 07:58:12","591-597","","4","52","","Neurosci Behav Physi","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CPYMZBR8","bookSection","2007","Spath, Dieter; Peissner, Matthias; Hagenmeyer, Lorenz; Ringbauer, Brigitte","New Approaches to Intuitive Auditory User Interfaces","Human Interface and the Management of Information. Methods, Techniques and Tools in Information Design","978-3-540-73344-7 978-3-540-73345-4","","","http://link.springer.com/10.1007/978-3-540-73345-4_110","This paper gives an overview of recent projects done at Fraunhofer IAO which take innovative approaches to the use of non-speech audio in human-computer interfaces. The examples illustrate the benefits that nonspeech sound - alone and in combination with other modalities - can provide for supporting an effective interaction.","2007","2023-07-05 07:58:12","2023-07-20 05:53:43","2023-07-05 07:58:12","975-984","","","4557","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73345-4_110","","/Users/minsik/Zotero/storage/VJUIG9ZY/Spath et al. - 2007 - New Approaches to Intuitive Auditory User Interfac.pdf","","","","Smith, Michael J.; Salvendy, Gavriel","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BLIPVN2T","bookSection","2019","Siqueira Da Silva, Isabel Cristina","The Promotion of Empathy for the Experience of Users with Visual Impairment in the Game Design Education","Universal Access in Human-Computer Interaction. Theory, Methods and Tools","978-3-030-23559-8 978-3-030-23560-4","","","http://link.springer.com/10.1007/978-3-030-23560-4_24","People are constantly seeking new experiences through perceptions that involve practical and subjective aspects such as usability, efficiency, satisfaction, and accessibility. The advent of digital inclusion has encouraged the software development with support for accessibility, which is proposed to promote the improvement of the quality of life of people with some type of disability, although also reflects positively with people without disabilities. In this sense, accessible digital games constitute a growing demand and, among the different solutions that have been proposed, the audiogames propose to offer a differentiated user experience, with the exploration of different sound stimuli that complement the visual stimuli. Thus, audio features are explored with the aim of guiding the player about the game universe based on different sounds and such features are significant for the experience of visually impaired players. However, the development of audiogames that are really accessible and provide a satisfying interaction for visually impaired users is still a challenge for game designers, who need to develop empathy for their target audience by understanding their needs and expectations. This article presents an experience report involving the development of audiogames that, though considering the accessibility to visually impaired, focus on the experience provided to game designers without visually impaired so as to awaken in these the empathy for games accessible through the understanding of how sound stimuli can compensate for lack of vision.","2019","2023-07-05 07:58:12","2023-07-21 05:10:32","2023-07-05 07:58:12","326-341","","","11572","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-23560-4_24","","","","","","Antona, Margherita; Stephanidis, Constantine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C4ZPDDUD","bookSection","1994","Rauterberg, Matthias; Styger, Erich","Positive effects of sound feedback during the operation of a plant simulator","Human-Computer Interaction","978-3-540-58648-7 978-3-540-49036-4","","","http://link.springer.com/10.1007/3-540-58648-2_24","An experiment was carried out to estimate the effect of sound feedback on the work of a plant operator. Eight students of computer science operated a process simulation program of an assembly line with computer numeric controlled (CNC) robots. Relevant information of disturbances and machine breakdowns was given only in a visual (test condition 1), and in visual and audible form (test condition 2). The results indicate, that the additional sound feedback improves significantly the operator performance and increases positively some mood aspects.","1994","2023-07-05 07:58:12","2023-07-20 05:55:06","2023-07-05 07:58:12","35-44","","","876","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-58648-2_24","","","","","","Blumenthal, Brad; Gornostaev, Juri; Unger, Claus","Goos, Gerhard.; Hartmanis, Juris; Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MDU3YM55","bookSection","2010","Dicke, Christina; Aaltonen, Viljakaisa; Billinghurst, Mark","Simulator Sickness in Mobile Spatial Sound Spaces","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_15","In this paper we summarize, evaluate, and discuss the effect of movement patterns in a spatial sound space on the perceived amount of simulator sickness, the pleasantness of the experience, and the perceived workload. During our user study nearly 48 percent of all participants showed mild to moderate symptoms of simulator sickness, with a trend towards stronger symptoms for those experiencing left to right movements. We found evidence for predictable left to right movements leading to a perceived unpleasantness that is significantly higher than for unpredictable or no movement at all. However none of the movement patterns had a noticable effect on the perceived cognitive load for simple tasks. We also found some differences in the perception of the sound space between men and women. Women tended to have a stronger dislike for the sound space and found the task to be more difficult.","2010","2023-07-05 07:58:12","2023-07-19 11:37:12","2023-07-05 07:58:12","287-305","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_15","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PBJ44XDM","journalArticle","2021","Cohen, Michael; Satō, Rintarō; Noji, Ryota; Iida, Takato; Tokumitsu, Yoshiki","Directional selectivity in panoramic and pantophonic interfaces: Flashdark, Narrowcasting for Stereoscopic Photospherical Cinemagraphy, Akabeko Ensemble","The Visual Computer","","0178-2789, 1432-2315","10.1007/s00371-021-02293-1","https://link.springer.com/10.1007/s00371-021-02293-1","We investigate the potential of interactive user interfaces with omnidirectional horizontal selection. Three novel multimodal interfaces have been developed, exploring different ways of displaying and controlling spaces that encourage panoramic and pantophonic experience by featuring selection of objects distributed around the subjective equator. Besides being explicitly omnidirectional, at least regarding azimuthal orientation, they are all stereoscopic “omni-stereo.” “Flashdark” allows the experience of actively darkening a physical area, inverting the usual polarity of lighting control by using a smartphone affordance with intuitive operation as a virtual “anti-light.” “Narrowcasting for Stereoscopic Photospherical Cinemagraphy” uses a visibility atlas to selectively animate articulated sectors of a photographically captured binocular scene. “Akabeko Ensemble” is visual music, using a keyboard controller to trigger pantophonic display of a helical chorus by an annular speaker array.","2021-12","2023-07-05 07:58:12","2023-07-21 05:07:28","2023-07-05 07:58:12","3125-3137","","12","37","","Vis Comput","Directional selectivity in panoramic and pantophonic interfaces","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V95N6NH9","journalArticle","2017","Beun, Robbert Jan; Fitrianie, Siska; Griffioen-Both, Fiemke; Spruit, Sandor; Horsch, Corine; Lancee, Jaap; Brinkman, Willem-Paul","Talk and Tools: the best of both worlds in mobile user interfaces for E-coaching","Personal and Ubiquitous Computing","","1617-4909, 1617-4917","10.1007/s00779-017-1021-5","http://link.springer.com/10.1007/s00779-017-1021-5","In this paper, a user interface paradigm, called Talk-and-Tools, is presented for automated e-coaching. The paradigm is based on the idea that people interact in two ways with their environment: symbolically and physically. The main goal is to show how the paradigm can be applied in the design of interactive systems that offer an acceptable coaching process. As a proof of concept, an ecoaching system is implemented that supports an insomnia therapy on a smartphone. A human coach was replaced by a cooperative virtual coach that is able to interact with a human coachee. In the interface of the system, we distinguish between a set of personalized conversations (BTalk^) and specialized modules that form a coherent structure of input and output facilities (BTools^). Conversations contained a minimum of variation to exclude unpredictable behavior but included the necessary mechanisms for variation to offer personalized consults and support. A variety of system and user tests was conducted to validate the use of the system. After a 6-week therapy, some users spontaneously reported the experience of building a relationship with the e-coach. It is concluded that the addition of a conversational component fills an important gap in the design of current mobile systems.","2017-08","2023-07-05 07:59:23","2023-07-21 04:46:26","2023-07-05 07:59:23","661-674","","4","21","","Pers Ubiquit Comput","Talk and Tools","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/9FYLG954/Beun et al. - 2017 - Talk and Tools the best of both worlds in mobile .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BREADWIC","bookSection","1997","Brewster, Stephen","Navigating Telephone-Based Interfaces with Earcons","People and Computers XII","978-3-540-76172-3 978-1-4471-3601-9","","","http://link.springer.com/10.1007/978-1-4471-3601-9_3","Non-speech audio messages called earcons can provide powerful navigation cues in menu hierarchies. However, previous research on earcons has not addressed the particular problems of menus in telephone-based interfaces (TBI’s) such as: Does the lower quality of sound in TBI’s lower recall rates, can users remember earcons over a period of time and what effect does training type have on recall. An experiment was conducted and results showed that sound quality did lower the recall of earcons. However, redesign of the earcons overcame this problem with 73% recalled correctly. Participants could still recall earcons at this level after a week had passed. Training type also affected recall. With ‘personal training’ participants recalled 73% of the earcons but with purely textual training results were significantly lower. These results show that earcons can provide excellent navigation cues for telephonebased interfaces.","1997","2023-07-05 07:59:23","2023-07-21 04:39:27","2023-07-05 07:59:23","39-56","","","","","","","","","","","Springer London","London","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-1-4471-3601-9_3","","/Users/minsik/Zotero/storage/3MPYII8P/Brewster - 1997 - Navigating Telephone-Based Interfaces with Earcons.pdf","","","","Thimbleby, Harold; O’Conaill, Brid; Thomas, Peter J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TEZRQTXX","journalArticle","2016","Strayer, David L.; Cooper, Joel M.; Turrill, Jonna; Coleman, James R.; Hopman, Rachel J.","Talking to your car can drive you to distraction","Cognitive Research: Principles and Implications","","2365-7464","10.1186/s41235-016-0018-3","http://cognitiveresearchjournal.springeropen.com/articles/10.1186/s41235-016-0018-3","This research examined the impact of in-vehicle information system (IVIS) interactions on the driver’s cognitive workload; 257 subjects participated in a weeklong evaluation of the IVIS interaction in one of ten different model-year 2015 automobiles. After an initial assessment of the cognitive workload associated with using the IVIS, participants took the vehicle home for 5 days and practiced using the system. At the end of the 5 days of practice, participants returned and the workload of these IVIS interactions was reassessed. The cognitive workload was found to be moderate to high, averaging 3.34 on a 5-point scale and ranged from 2.37 to 4.57. The workload was associated with the intuitiveness and complexity of the system and the time it took participants to complete the interaction. The workload experienced by older drivers was significantly greater than that experienced by younger drivers performing the same operations. Practice did not eliminate the interference from IVIS interactions. In fact, IVIS interactions that were difficult on the first day were still relatively difficult to perform after a week of practice. Finally, there were long-lasting residual costs after the IVIS interactions had terminated. The higher levels of workload should serve as a caution that these voice-based interactions can be cognitively demanding and ought not to be used indiscriminately while operating a motor vehicle.","2016-12","2023-07-05 07:59:23","2023-07-19 23:47:01","2023-07-05 07:59:23","16","","1","1","","Cogn. Research","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/QTSSSIGJ/Strayer et al. - 2016 - Talking to your car can drive you to distraction.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WM9MHGQG","journalArticle","1995","Maybury, Mark T.","Research in multimedia and multimodal parsing and generation","Artificial Intelligence Review","","0269-2821, 1573-7462","10.1007/BF00849175","http://link.springer.com/10.1007/BF00849175","This overview introduces the emerging set of techniques for parsing and generating multiple media (e.g., text, graphics, maps, gestures) using multiple sensory modalities (e.g., auditory, visual, tactile). We first briefly introduce and motivate the value of such techniques. Next we describe various computational methods for parsing input from heterogeneous media and modalities (e.g., natural language, gesture, gaze). We subsequently overview complementary techniques for generating coordinated multimedia and multimodal output. Finally, we discuss systems that have integrated both parsing and generation to enable multimedia dialogue in the context of intelligent interfaces. The article concludes by outlining fundamental problems which require further research.","1995-06","2023-07-05 07:59:23","2023-07-19 11:34:36","2023-07-05 07:59:23","103-127","","2-3","9","","Artif Intell Rev","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DPCCD6AC","journalArticle","2007","Porta, Marco","Human–Computer input and output techniques: an analysis of current research and promising applications","Artificial Intelligence Review","","0269-2821, 1573-7462","10.1007/s10462-009-9098-5","http://link.springer.com/10.1007/s10462-009-9098-5","Personal computing applications are constantly increasing their potential power, thanks to steadily growing hardware capabilities and large diffusion of high-quality multimedia output devices. At the same time, mobile communication tools are becoming an integral part of our everyday life, with new advanced functionalities offered at an unrestrainable pace. Although the way we interact with information machines is substantially the same since 20 years—based on keyboard, mouse and window metaphor—other communication modalities are possible, and shortly may become popular as additional interaction methods. Given the paramount importance of the “interface” in present computer applications, no alternative should be ignored, as it could greatly improve the quality of both interaction processes and user cognitive performance. Without pretending to foresee the future, in this paper we provide an overview of the main current technologies which can enable potential novel interfaces, discussing their features, strengths, weaknesses and promising applications.","2007-10","2023-07-05 07:59:23","2023-07-19 11:34:44","2023-07-05 07:59:23","197-226","","3","28","","Artif Intell Rev","Human–Computer input and output techniques","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R8ZCPS3H","bookSection","2011","Wolf, Katrin; Naumann, Anja; Rohs, Michael; Müller, Jörg","A Taxonomy of Microinteractions: Defining Microgestures Based on Ergonomic and Scenario-Dependent Requirements","Human-Computer Interaction – INTERACT 2011","978-3-642-23773-7 978-3-642-23774-4","","","http://link.springer.com/10.1007/978-3-642-23774-4_45","This paper explores how microgestures can allow us to execute a secondary task, for example controlling mobile applications, without interrupting the manual primary task, for instance, driving a car. In order to design microgestures iteratively, we interviewed sports- and physiotherapists while asking them to use task related props, such as a steering wheel, a cash card , and a pen for simulating driving a car, an ATM scenario, and a drawing task. The primary objective here is to define microgestures that are easily performable without interrupting or interfering the primary task. Using expert interviews, we developed a taxonomy that classifies these gestures according to their task context. We also assessed the ergonomic and attentional attributes that influence the feasibility and task suitability of microinteractions, and evaluated their level of resources required. Accordingly, we defined 21 microgestures that allow performing microinteractions within a manual, dual task context. Our taxonomy poses a basis for designing microinteraction techniques.","2011","2023-07-05 07:59:23","2023-07-20 06:28:44","2023-07-05 07:59:23","559-575","","","6946","","","A Taxonomy of Microinteractions","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-23774-4_45","","/Users/minsik/Zotero/storage/NNDZNRFY/Wolf et al. - 2011 - A Taxonomy of Microinteractions Defining Microges.pdf","","","","Campos, Pedro; Graham, Nicholas; Jorge, Joaquim; Nunes, Nuno; Palanque, Philippe; Winckler, Marco","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EFWFPACI","journalArticle","2010","Power, Christopher; Jürgensen, Helmut","Accessible presentation of information for people with visual disabilities","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-009-0164-1","http://link.springer.com/10.1007/s10209-009-0164-1","Personal computers, palm top computers, media players and cell phones provide instant access to information from around the world. There are a wide variety of options available to make that information available to people with visual disabilities, so many that choosing one for use in any given context can often feel daunting to someone new to the field of accessibility. This paper reviews tools and techniques for the presentation of textual, graphic, mathematic and web documents through audio and haptic modalities to people with visual disabilities.","2010-06","2023-07-05 07:59:23","2023-07-21 05:12:41","2023-07-05 07:59:23","97-119","","2","9","","Univ Access Inf Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I5BK9DX4","bookSection","2011","Rutkowski, Tomasz M.","Auditory Brain-Computer/Machine-Interface Paradigms Design","Haptic and Audio Interaction Design","978-3-642-22949-7 978-3-642-22950-3","","","http://link.springer.com/10.1007/978-3-642-22950-3_12","The paper discusses novel and interesting, from users’ point of view, design of auditory brain-computer/machine interfaces (BCI/ BMI) utilizing human auditory responses. Two concepts of auditory stimuli BCI/BMI are presented. The first paradigm is based on steady-state tonal or musical stimuli yielding satisfactory EEG response classification for several seconds long stimuli. The second discussed paradigm is based on spatial sound localization and the brain evoked responses estimation, requiring shorter than a second stimuli presentation. In conclusion the preliminary results are discussed and suggestions for further applications are drawn.","2011","2023-07-05 07:59:23","2023-07-20 00:16:39","2023-07-05 07:59:23","110-119","","","6851","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-22950-3_12","","","","","","Cooper, Eric W.; Kryssanov, Victor V.; Ogawa, Hitoshi; Brewster, Stephen","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TMKQQLS7","bookSection","2012","Hachisu, Taku; Kajimoto, Hiroyuki","Augmentation of Toothbrush by Modulating Sounds Resulting from Brushing","Advances in Computer Entertainment","978-3-642-34291-2 978-3-642-34292-9","","","http://link.springer.com/10.1007/978-3-642-34292-9_3","Brushing teeth is a daily habit to maintain oral hygiene, including the maintenance of oral cleanliness and prevention of caries and periodontal disease. However, tooth brushing is often not carried out correctly or forgotten because the task is boring. Although several works have contributed to improving brushing performance and motivation, the feedback seems to be very remote from the brushing itself, i.e., not intuitive. In this study, we establish two objectives to deal with these issues. The first is not to present information on a visual display, but to augment the ordinary tooth brushing experience consisting of haptic and auditory sensations, while the other is to design the modulation so that users feel as if their teeth are gradually becoming cleaner, thereby providing the necessary motivation. To achieve these aims, we propose a novel approach to augment the tooth brushing experience by modulating the brushing sounds to make tooth brushing entertaining in an intuitive manner. A microphone embedded in the toothbrush records the brushing sounds, which are presented to users after being modified by a PC. In the experiment, we demonstrate that increasing the sound gain and manipulating the frequency can control the overall impression of brushing by giving a sense of comfort and accomplishment.","2012","2023-07-05 08:00:53","2023-07-19 11:11:05","2023-07-05 08:00:53","31-43","","","7624","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-34292-9_3","","","","","","Nijholt, Anton; Romão, Teresa; Reidsma, Dennis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PYBDVY54","journalArticle","2012","Rosati, Giulio; Oscari, Fabio; Spagnol, Simone; Avanzini, Federico; Masiero, Stefano","Effect of task-related continuous auditory feedback during learning of tracking motion exercises","Journal of NeuroEngineering and Rehabilitation","","1743-0003","10.1186/1743-0003-9-79","http://jneuroengrehab.biomedcentral.com/articles/10.1186/1743-0003-9-79","Background: This paper presents the results of a set of experiments in which we used continuous auditory feedback to augment motor training exercises. This feedback modality is mostly underexploited in current robotic rehabilitation systems, which usually implement only very basic auditory interfaces. Our hypothesis is that properly designed continuous auditory feedback could be used to represent temporal and spatial information that could in turn, improve performance and motor learning. Methods: We implemented three different experiments on healthy subjects, who were asked to track a target on a screen by moving an input device (controller) with their hand. Different visual and auditory feedback modalities were envisaged. The first experiment investigated whether continuous task-related auditory feedback can help improve performance to a greater extent than error-related audio feedback, or visual feedback alone. In the second experiment we used sensory substitution to compare different types of auditory feedback with equivalent visual feedback, in order to find out whether mapping the same information on a different sensory channel (the visual channel) yielded comparable effects with those gained in the first experiment. The final experiment applied a continuously changing visuomotor transformation between the controller and the screen and mapped kinematic information, computed in either coordinate system (controller or video), to the audio channel, in order to investigate which information was more relevant to the user. Results: Task-related audio feedback significantly improved performance with respect to visual feedback alone, whilst error-related feedback did not. Secondly, performance in audio tasks was significantly better with respect to the equivalent sensory-substituted visual tasks. Finally, with respect to visual feedback alone, video-task-related sound feedback decreased the tracking error during the learning of a novel visuomotor perturbation, whereas controller-task-related sound feedback did not. This result was particularly interesting, as the subjects relied more on auditory augmentation of the visualized target motion (which was altered with respect to arm motion by the visuomotor perturbation), rather than on sound feedback provided in the controller space, i.e., information directly related to the effective target motion of their arm. Conclusions: Our results indicate that auditory augmentation of visual feedback can be beneficial during the execution of upper limb movement exercises. In particular, we found that continuous task-related information provided through sound, in addition to visual feedback can improve not only performance but also the learning of a novel visuomotor perturbation. However, error-related information provided through sound did not improve performance and negatively affected learning in the presence of the visuomotor perturbation.","2012","2023-07-05 08:00:53","2023-07-21 07:43:04","2023-07-05 08:00:53","79","","1","9","","J NeuroEngineering Rehabil","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/5NCZJYSE/Rosati et al. - 2012 - Effect of task-related continuous auditory feedbac.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DTCNDF9B","bookSection","2009","Duarte, Carlos; Carriço, Luís","When You Can’t Read It, Listen to It! An Audio-Visual Interface for Book Reading","Universal Access in Human-Computer Interaction. Applications and Services","978-3-642-02712-3 978-3-642-02713-0","","","http://link.springer.com/10.1007/978-3-642-02713-0_3","This paper presents a prototype of a mobile Digital Talking Book player, which, by combining visual and non-visual means of interaction, strives to achieve universal accessibility. Details on the non-visual aspects of the interaction, both input and output, are provided. To assess the validity of the proposed solutions, an experiment evaluates the non-visual operation of the prototype. Results show users can complete the same tasks with visual and non-visual interaction. However, some limitations are identified, and the observations prompt a discussion on how the use of multimodal interfaces can improve their accessibility and usability.","2009","2023-07-05 08:00:53","2023-07-21 05:08:55","2023-07-05 08:00:53","24-33","","","5616","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-02713-0_3","","","","","","Stephanidis, Constantine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TZKRWAHR","bookSection","2006","Crombie, David","People with Disabilities: Accessible Content Processing","Computers Helping People with Special Needs","978-3-540-36020-9 978-3-540-36021-6","","","http://link.springer.com/10.1007/11788713_1","The Special Thematic Session (STS) on Accessible Content Processing is intended to provide a focus for several different activities which can be grouped under this area. The European Accessible Information Network (EUAIN) was established in order to bring together the different stakeholders in the accessible content processing chain and to build on common concerns. EUAIN has now completed a systemic overview of this area and will use this information to provide guidelines, training materials and input into standardisation activities. The papers in this STS address many of these issues.","2006","2023-07-05 08:00:53","2023-07-19 23:51:37","2023-07-05 08:00:53","1-5","","","4061","","","People with Disabilities","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11788713_1","","","","","","Miesenberger, Klaus; Klaus, Joachim; Zagler, Wolfgang L.; Karshmer, Arthur I.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MAPUU43R","bookSection","2010","Schaar, Anne Kathrin; Ziefle, Martina","Potential of e-Travel Assistants to Increase Older Adults’ Mobility","HCI in Work and Learning, Life and Leisure","978-3-642-16606-8 978-3-642-16607-5","","","http://link.springer.com/10.1007/978-3-642-16607-5_9","In this empirical study we examine the willingness of travelers to use small screen devices providing electronic travel (“e-travel”) services. As in the near future increasingly more and older adults are travelling around, it is a basic question how we can support this wish for mobility. However, electronic travel services on mobile device are only accepted if it is understood in how far these devices meet the actual travel behavior on the one hand and user requirements respecting the usability of devices on the other. Yet, only little knowledge is prevalent regarding the individual reasons for the choice of means of transportation as well as the perceived needs when being supported by a device providing travel services. In order to get a broad insight into age-related mobility patterns, users of a wide age range (N = 151; 18-75 years of age) were questioned in a survey, in which the travel experience (frequency of using different means of transportation and their evaluation) as well as technical experience (Internet usage and handling of small screen devices) were explored. The findings show that age (but not gender) is a crucial factor regarding the acceptance of electronic travel assistants, and services. The crucial factor underlying age effects is the technical experience and travel expertise: The higher the familiarity with electronic services in general (Internet usage) and specifically (handling of mobile devices) and domain knowledge (travel experience), the higher is the perceived usefulness of future e-travel services. Outcomes might be helpful for the development of e-travel applications especially with for the intention to keep the elderly mobile and fit for travelling.","2010","2023-07-05 08:00:53","2023-07-20 05:43:54","2023-07-05 08:00:53","138-155","","","6389","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-16607-5_9","","","","","","Leitner, Gerhard; Hitz, Martin; Holzinger, Andreas","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QACRTNPX","journalArticle","2015","Csapó, Ádám; Wersényi, György; Nagy, Hunor; Stockman, Tony","A survey of assistive technologies and applications for blind users on mobile platforms: a review and foundation for research","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-015-0182-7","http://link.springer.com/10.1007/s12193-015-0182-7","This paper summarizes recent developments in audio and tactile feedback based assistive technologies targeting the blind community. Current technology allows applications to be efficiently distributed and run on mobile and handheld devices, even in cases where computational requirements are significant. As a result, electronic travel aids, navigational assistance modules, text-to-speech applications, as well as virtual audio displays which combine audio with haptic channels are becoming integrated into standard mobile devices. This trend, combined with the appearance of increasingly user-friendly interfaces and modes of interaction has opened a variety of new perspectives for the rehabilitation and training of users with visual impairments. The goal of this paper is to provide an overview of these developments based on recent advances in basic research and application development. Using this overview as a foundation, an agenda is outlined for future research in mobile interaction design with respect to users with special needs, as well as ultimately in relation to sensor-bridging applications in general.","2015-12","2023-07-05 08:00:53","2023-07-20 06:53:22","2023-07-05 08:00:53","275-286","","4","9","","J Multimodal User Interfaces","A survey of assistive technologies and applications for blind users on mobile platforms","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/8CU5GFJL/Csapó et al. - 2015 - A survey of assistive technologies and application.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5CWMP4B8","bookSection","2010","Bonner, Matthew N.; Brudvik, Jeremy T.; Abowd, Gregory D.; Edwards, W. Keith","No-Look Notes: Accessible Eyes-Free Multi-touch Text Entry","Pervasive Computing","978-3-642-12653-6 978-3-642-12654-3","","","http://link.springer.com/10.1007/978-3-642-12654-3_24","Mobile devices with multi-touch capabilities are becoming increasingly common, largely due to the success of the Apple iPhone and iPod Touch. While there have been some advances in touchscreen accessibility for blind people, touchscreens remain inaccessible in many ways. Recent research has demonstrated that there is great potential in leveraging multi-touch capabilities to increase the accessibility of touchscreen applications for blind people. We have created No-Look Notes, an eyes-free text entry system that uses multi-touch input and audio output. No-Look Notes was implemented on Apple’s iPhone platform. We have performed a within-subjects (n = 10) user study of both No-Look Notes and the text entry component of Apple’s VoiceOver, the recently released official accessibility component on the iPhone. No-Look Notes significantly outperformed VoiceOver in terms of speed, accuracy and user preference.","2010","2023-07-05 08:00:53","2023-07-21 04:54:37","2023-07-05 08:00:53","409-426","","","6030","","","No-Look Notes","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12654-3_24","","","","","","Floréen, Patrik; Krüger, Antonio; Spasojevic, Mirjana","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FB4Q8MYM","journalArticle","2021","Kim, Hubert; Asbeck, Alan T.","Just noticeable differences for elbow joint torque feedback","Scientific Reports","","2045-2322","10.1038/s41598-021-02630-3","https://www.nature.com/articles/s41598-021-02630-3","Abstract             Joint torque feedback is a new and promising means of kinesthetic feedback imposed by a wearable device. The torque feedback provides the wearer temporal and spatial information during a motion task. Nevertheless, little research has been conducted on quantifying the psychophysical parameters of how well humans can perceive external torques under various joint conditions. This study aims to investigate the just noticeable difference (JND) perceptual ability of the elbow joint to joint torques. The paper focuses on the ability of two primary joint proprioceptors, the Golgi-tendon organ (GTO) and muscle spindle (MS), to detect elbow torques, since touch and pressure sensors were masked. We studied 14 subjects while the arm was isometrically contracted (static condition) and was moving at a constant speed (dynamic condition). In total there were 10 joint conditions investigated, which varied the direction of the arm’s movement and the preload direction as well as torque direction. The JND torques under static conditions ranged from 0.097 Nm with no preload to 0.197 Nm with a preload of 1.28 Nm. The maximum dynamic JND torques were 0.799 Nm and 0.428 Nm, when the arm was flexing and extending at 213 degrees per second, respectively.","2021-12-07","2023-07-05 08:00:53","2023-07-05 08:00:53","2023-07-05 08:00:53","23553","","1","11","","Sci Rep","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/PBNTX5JP/Kim and Asbeck - 2021 - Just noticeable differences for elbow joint torque.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XF8E4WLF","journalArticle","2006","Tahboub, Karim A.","Intelligent Human-Machine Interaction Based on Dynamic Bayesian Networks Probabilistic Intention Recognition","Journal of Intelligent and Robotic Systems","","0921-0296, 1573-0409","10.1007/s10846-005-9018-0","http://link.springer.com/10.1007/s10846-005-9018-0","In this article, a novel human–machine interaction based on the machine intention recognition of the human is presented. This work is motivated by the desire that intelligent machines as robots imitate human–human interaction, that is to minimize the need for classical direct human–machine interface and communication. A philosophical and technical background for intention recognition is discussed. Here, the intention–action–state scenario is modified and modeled by Dynamic Bayesian Networks to facilitate for probabilistic intention inference. The recognized intention, then, drives the interactive behavior of the machine such that it complies with the human intention in light of the real state of the world. An illustrative example of a human commanding a mobile robot remotely is given and discussed in details.","2006-01","2023-07-05 08:00:53","2023-07-20 06:47:25","2023-07-05 08:00:53","31-52","","1","45","","J Intell Robot Syst","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"47YIHHJZ","bookSection","2007","Duarte, Carlos; Carriço, Luís","Conveying Browsing Context Through Audio on Digital Talking Books","Universal Access in Human-Computer Interaction. Applications and Services","978-3-540-73282-2 978-3-540-73283-9","","","http://link.springer.com/10.1007/978-3-540-73283-9_30","This paper presents the results of a study comparing the use of auditory icons, earcons and speech in an audio only interface for a digital talking book player. The different techniques were evaluated according to the identification errors made, and subjective measures of understandability, intrusiveness and pleasurability. Results suggest the use of auditory icons combined with speech whenever necessary, in detriment to the use of earcons, for applications sharing the characteristics of digital talking book players.","2007","2023-07-05 08:00:53","2023-07-21 05:08:48","2023-07-05 08:00:53","259-268","","","4556","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73283-9_30","","","","","","Stephanidis, Constantine","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V49BH9QU","conferencePaper","2011","Absar, Rafa; Guastavino, Catherine","Nonspeech Sound Design for a Hierarchical Information System","Human Centered Design","978-3-642-21753-1","","10.1007/978-3-642-21753-1_52","","This research describes a human-centered design methodology for creating nonspeech sounds to enhance navigation in a visual user interface. This paper describes how the sound design methodology proposed in [10][11] was extended to sonify a novel 3D-visualized information system for sighted users navigating a hierarchical structure. The method ensures that the sounds designed are not based on personal or ad hoc choices, and instead exploits the creativity of a user group as an application of participatory design in sound. Recommendations are derived from this case study on how to design auditory cues for familiar or novel user interfaces to convey structural information in an informative and intuitive way.","2011","2023-07-05 08:01:54","2023-07-05 08:01:54","","461-470","","","","","","","Lecture Notes in Computer Science","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/UXIQWD9P/Absar and Guastavino - 2011 - Nonspeech Sound Design for a Hierarchical Informat.pdf","","","Auditory Feedback; Depth Level; Panel Session; Sound Design; Task Description","Kurosu, Masaaki","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MEE7J2CR","bookSection","2007","Vilimek, Roman; Zimmer, Alf","Development and Evaluation of a Multimodal Touchpad for Advanced In-Vehicle Systems","Engineering Psychology and Cognitive Ergonomics","978-3-540-73330-0 978-3-540-73331-7","","","http://link.springer.com/10.1007/978-3-540-73331-7_92","Multimodal interaction can substantially improve human-computer interaction by employing multiple perceptual channels. We report on the development and evaluation of a touchpad with auditory, tactile and visual feedback for in-vehicle applications. In a simulator study, we assessed its suitability for interacting with a menu-based on-board system and investigated the effects of uni-, bi- and trimodal feedback on task and driving performance, workload and visual distraction in comparison to a conventional rotary push-button. In summary our results show that users clearly benefit from additional non-visual feedback while driving. When using the touchpad with multimodal feedback, our subjects also reached a higher level of performance compared to the rotary push-button.","2007","2023-07-05 08:02:36","2023-07-19 23:59:17","2023-07-05 08:02:36","842-851","","","4562","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73331-7_92","","/Users/minsik/Zotero/storage/C2HUI4UK/Vilimek and Zimmer - 2007 - Development and Evaluation of a Multimodal Touchpa.pdf","","","","Harris, Don","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3K3CG2UU","bookSection","2007","Sánchez, Jaime; Galáz, Iván","AudioStoryTeller: Enforcing Blind Children Reading Skills","Universal Access in Human-Computer Interaction. Applications and Services","978-3-540-73282-2 978-3-540-73283-9","","","http://link.springer.com/10.1007/978-3-540-73283-9_85","Children tend to learn language conventions through processing environment stimuli. Thus, strategies for reading comprehension are commonly used for this purpose. This paper introduces AudioStoryTeller, a tool for pocketPC to support the development of reading and writing skills in learners with visual disabilities (LWVD) through storytelling, providing diverse evaluation tools to measure those skills. We implemented usability and cognitive evaluation to the AudioStoryTeller software. In the usability evaluation, the easiness of use of the proposed hardware by LWVD was established. The goal of the cognitive evaluation was to measure the development of reading skills through interactive audio narrations using a pocketPC device. Results indicate that users were able to utilize effortless the pocketPC device. AudioStoryTeller software together with cognitive tasks, can contribute to the development of cognitive skills in LWVD. This application allows LVD to have access to unlimited scope of books not available in printed Braille.","2007","2023-07-05 08:02:36","2023-07-21 05:09:40","2023-07-05 08:02:36","786-795","","","4556","","","AudioStoryTeller","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73283-9_85","","/Users/minsik/Zotero/storage/WHHGJTVS/Sánchez and Galáz - 2007 - AudioStoryTeller Enforcing Blind Children Reading.pdf","","","","Stephanidis, Constantine","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QVCGMR8E","bookSection","2006","Fitzpatrick, D.","Mathematics: How and What to Speak","Computers Helping People with Special Needs","978-3-540-36020-9 978-3-540-36021-6","","","http://link.springer.com/10.1007/11788713_173","Access to mathematical content for blind and vision impaired people continues to be a problem. The inherently visual nature of this form of presentation is neither easily or readily accessible using the linear representations in common usage by this community. This paper proposes methodology for depicting mathematics in a non-visual manner. It will be shown how, through the prosodic component found in spoken language, the structure of mathematical formulae may be disambiguated. We will also discuss lexical cues which can be added to the utterance to further reduce the ambiguity which can be very evident in this form of material.","2006","2023-07-05 08:02:36","2023-07-19 23:51:47","2023-07-05 08:02:36","1199-1206","","","4061","","","Mathematics","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11788713_173","","","","","","Miesenberger, Klaus; Klaus, Joachim; Zagler, Wolfgang L.; Karshmer, Arthur I.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XWJH6XJ6","bookSection","2014","Kacorri, Hernisa; Riga, Paraskevi; Kouroupetroglou, Georgios","Performance Metrics and Their Extraction Methods for Audio Rendered Mathematics","Computers Helping People with Special Needs","978-3-319-08595-1 978-3-319-08596-8","","","http://link.springer.com/10.1007/978-3-319-08596-8_95","We introduce and compare three approaches to calculate structure- and content-based performance metrics for user-based evaluation of math audio rendering systems: Syntax Tree alignment, Baseline Structure Tree alignment, and MathML Tree Edit Distance. While the first two require “manual” tree transformation and alignment of the mathematical expressions, the third obtains the metrics without human intervention using the minimum edit distance algorithm on the corresponding MathML representations. Our metrics are demonstrated in a pilot user study evaluating the Greek audio rendering rules of MathPlayer with 7 participants and 39 stimuli. We observed that the obtained results for the metrics are significantly correlated between all three approaches.","2014","2023-07-05 08:02:36","2023-07-19 23:52:05","2023-07-05 08:02:36","614-621","","","8547","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-08596-8_95","","","","","","Miesenberger, Klaus; Fels, Deborah; Archambault, Dominique; Peňáz, Petr; Zagler, Wolfgang","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HZQPQZFX","conferencePaper","1994","Karshmer, Arthur I.; Ogden, Bill; Brawner, Pres; Kaugars, Karlis; Reiswig, George","Adapting graphical user interfaces for use by visually handicapped computer users: Current results and continuing research","Computers for Handicapped Persons","978-3-540-48989-4","","10.1007/3-540-58476-5_100","","The use of modern computers and software by the visually handicapped has become more difficult over the past few years. In earlier systems the user interface was a simple character based environment. In those systems, simple devices like screen readers, braille output and speech synthesizers were effective. Current systems now run Graphical User Interfaces (GUIs) which have rendered these simple aids almost useless. In no area has this problem become more important than in technologies for the handicapped. What has become enabling technology for the sighted has become disabling technology for the visually impaired. In the current work we discuss new and innovative approaches to permit non-sighted users to interface with GUIs, having the salutary effect of gaining needed access to the most modern computing equipment for a subset of our population that is otherwise excluded from such access.","1994","2023-07-05 08:12:36","2023-07-05 08:12:36","","16-24","","","","","","Adapting graphical user interfaces for use by visually handicapped computer users","Lecture Notes in Computer Science","","","","Springer","Berlin, Heidelberg","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/VBAICP6Y/Karshmer et al. - 1994 - Adapting graphical user interfaces for use by visu.pdf","","","Impaired User; Menu Structure; Mouse Click; Speech Synthesizer; Visually Handicap","Zagler, Wolfgang L.; Busby, Geoffrey; Wagner, Roland R.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VDFT69N4","journalArticle","2016","Katz, Brian F. G.; Marentakis, Georgios","Advances in auditory display research","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-016-0226-7","http://link.springer.com/10.1007/s12193-016-0226-7","The research community on Auditory Design has been active and organized now for almost 25 years. Work in Auditory Display is pluri-disciplinary/inter-disciplinary in nature, and involves disciplines such as perception, acoustics, digital signal processing, multimodality, ergonomics, aesthetics, cognition, etc. Published works have spanned fundamental, phenomenological, theoretical, applicative, and artistic studies. Over the years, various journals have had special issues related to such work. The range of the associated journals reflects the variety of implicated domains. This current special issue presents a collection of extended works that were selected from papers presented at the 2015 International Conference on Auditory Display. We present here an overview of the selection process and the accepted papers.","2016-09","2023-07-06 00:46:54","2023-07-20 06:59:42","2023-07-06 00:46:54","191-193","","3","10","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/CCR5HKUR/Katz and Marentakis - 2016 - Advances in auditory display research.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KAG8HBKA","journalArticle","2012","Vazquez-Alvarez, Yolanda; Oakley, Ian; Brewster, Stephen A.","Auditory display design for exploration in mobile audio-augmented reality","Personal and Ubiquitous Computing","","1617-4909, 1617-4917","10.1007/s00779-011-0459-0","http://link.springer.com/10.1007/s00779-011-0459-0","In this paper, we compare four different auditory displays in a mobile audio-augmented reality environment (a sound garden). The auditory displays varied in the use of non-speech audio, Earcons, as auditory landmarks and 3D audio spatialization, and the goal was to test the user experience of discovery in a purely exploratory environment that included multiple simultaneous sound sources. We present quantitative and qualitative results from an initial user study conducted in the Municipal Gardens of Funchal, Madeira. Results show that spatial audio together with Earcons allowed users to explore multiple simultaneous sources and had the added benefit of increasing the level of immersion in the experience. In addition, spatial audio encouraged a more exploratory and playful response to the environment. An analysis of the participants’ logged data suggested that the level of immersion can be related to increased instances of stopping and scanning the environment, which can be quantified in terms of walking speed and head movement.","2012-12","2023-07-06 00:46:54","2023-07-21 04:54:27","2023-07-06 00:46:54","987-999","","8","16","","Pers Ubiquit Comput","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KNSW8EBI","journalArticle","2019","Plazak, Joseph; DiGiovanni, Daniel A.; Collins, D. Louis; Kersten-Oertel, Marta","Cognitive load associations when utilizing auditory display within image-guided neurosurgery","International Journal of Computer Assisted Radiology and Surgery","","1861-6410, 1861-6429","10.1007/s11548-019-01970-w","http://link.springer.com/10.1007/s11548-019-01970-w","Purpose The combination of data visualization and auditory display (e.g., sonification) has been shown to increase accuracy, and reduce perceived difficulty, within 3D navigation tasks. While accuracy within such tasks can be measured in real time, subjective impressions about the difficulty of a task are more elusive to obtain. Prior work utilizing electrophysiology (EEG) has found robust support that cognitive load and working memory can be monitored in real time using EEG data. Methods In this study, we replicated a 3D navigation task (within the context of image-guided surgery) while recording data pertaining to participants’ cognitive load through the use of EEG relative alpha-band weighting data. Specifically, 13 subjects navigated a tracked surgical tool to randomly placed 3D virtual locations on a CT cerebral angiography volume while being aided by visual, aural, or both visual and aural feedback. During the study EEG data were captured from the participants, and after the study a NASA TLX questionnaire was filled out by the subjects. In addition to replicating an existing experimental design on auditory display within image-guided neurosurgery, our primary aim sought to determine whether EEG-based markers of cognitive load mirrored subjective ratings of task difficulty Results Similar to existing literature, our study found evidence consistent with the hypothesis that auditory display can increase the accuracy of navigating to a specified target. We also found significant differences in cognitive working load across different feedback modalities, but none of which supported the experiments hypotheses. Finally, we found mixed results regarding the relationship between real-time measurements of cognitive workload and a posteriori subjective impressions of task difficulty. Conclusions Although we did not find a significant correlation between the subjective and physiological measurements, differences in cognitive working load were found. As well, our study further supports the use of auditory display in image-guided surgery.","2019-08","2023-07-06 00:46:54","2023-07-20 06:41:27","2023-07-06 00:46:54","1431-1438","","8","14","","Int J CARS","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YLJT4V9R","bookSection","2014","Sirkka, Anna; Fagerlönn, Johan; Lindberg, Stefan; Frimalm, Ronja","An Auditory Display to Convey Urgency Information in Industrial Control Rooms","Engineering Psychology and Cognitive Ergonomics","978-3-319-07514-3 978-3-319-07515-0","","","http://link.springer.com/10.1007/978-3-319-07515-0_53","Auditory warning signals are common features in industrial control rooms. Finding sound signals that convey higher degrees of urgency while keeping the potential for annoyance low is challenging. In the present study, evaluations were performed on four different types of auditory displays. The displays were all designed to convey three levels of urgency. The examination focused on the following questions: (1) “How reliably can the operators identify the three levels of urgency?” and (2) “How annoying do the operators find the sound signals?”. Fourteen operators participated in the study. For every signal within each auditory display, the participants were asked to rate the level of urgency and annoyance. The results show that one can design auditory displays that employ appropriate urgency mapping while the perceived annoyance is kept at a low level. The work also suggests that involving the end users in the design process could be advantageous.","2014","2023-07-06 00:46:54","2023-07-19 23:59:08","2023-07-06 00:46:54","533-544","","","8532","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-07515-0_53","","/Users/minsik/Zotero/storage/AW2IRCV4/Sirkka et al. - 2014 - An Auditory Display to Convey Urgency Information .pdf","","","","Harris, Don","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Kobsa, Alfred; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Doug; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HU3SZWIC","journalArticle","1999","Barrass, Stephen; Kramer, Gregory","Using sonification","Multimedia Systems","","0942-4962, 1432-1882","10.1007/s005300050108","http://link.springer.com/10.1007/s005300050108","The idea behind sonification is that synthetic non-verbal sounds can represent numerical data and provide support for information processing activities of many different kinds. This article describes some of the ways that sonification has been used in assistive technologies, remote collaboration, engineering analyses, scientific visualisations, emergency services and aircraft cockpits. Approaches for designing sonifications are surveyed, and issues raised by the existing approaches and applications are outlined. Relations are drawn to other areas of knowledge where similar issues have also arisen, such as human-computer interaction, scientific visualisation, and computer music. At the end is a list of resources that will help you delve further into the topic.","1999-01-01","2023-07-06 00:46:54","2023-07-21 04:31:20","2023-07-06 00:46:54","23-31","","1","7","","Multimedia Systems","","","","","","","","","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"93E5YCGS","journalArticle","2020","Jeon, Myounghoon; Andreopoulou, Areti; Katz, Brian F. G.","Auditory displays and auditory user interfaces: art, design, science, and research","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-020-00324-0","https://link.springer.com/10.1007/s12193-020-00324-0","For almost 3 decades, research on auditory displays and sonification has been well advanced. Now, the auditory display community has arrived at the stage of sonic information design with a more systematic, refined necessity, going beyond random mappings between the referents and sounds. Due to its innate transdisciplinary nature of auditory display, it would be difficult to unify the methods to study it. This special issue covers a diverse collection of approaches to auditory displays, involving art, design, science, and research. Accordingly, the works in the present special issue included new theories, frameworks, methods, and applications about auditory displays and auditory user interfaces. We hope that this special issue can provide the state of art of auditory display research and auditory user interface design, offering fresh inspiration and motivation to researchers and designers for their future works.","2020-06","2023-07-06 00:46:54","2023-07-20 06:59:30","2023-07-06 00:46:54","139-141","","2","14","","J Multimodal User Interfaces","Auditory displays and auditory user interfaces","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/4NH4RXWD/Jeon et al. - 2020 - Auditory displays and auditory user interfaces ar.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z5A2ELU2","journalArticle","2022","Rocchesso, Davide; Andolina, Salvatore; Ilardo, Giacomo; Palumbo, Salvatore Danilo; Galluzzo, Ylenia; Randazzo, Mario","A perceptual sound space for auditory displays based on sung-vowel synthesis","Scientific Reports","","2045-2322","10.1038/s41598-022-23736-2","https://www.nature.com/articles/s41598-022-23736-2","Abstract             When designing displays for the human senses, perceptual spaces are of great importance to give intuitive access to physical attributes. Similar to how perceptual spaces based on hue, saturation, and lightness were constructed for visual color, research has explored perceptual spaces for sounds of a given timbral family based on timbre, brightness, and pitch. To promote an embodied approach to the design of auditory displays, we introduce the Vowel–Type–Pitch (VTP) space, a cylindrical sound space based on human sung vowels, whose timbres can be synthesized by the composition of acoustic formants and can be categorically labeled. Vowels are arranged along the circular dimension, while voice type and pitch of the vowel correspond to the remaining two axes of the cylindrical VTP space. The decoupling and perceptual effectiveness of the three dimensions of the VTP space are tested through a vowel labeling experiment, whose results are visualized as maps on circular slices of the VTP cylinder. We discuss implications for the design of auditory and multi-sensory displays that account for human perceptual capabilities.","2022-11-12","2023-07-06 00:46:54","2023-07-06 00:46:54","2023-07-06 00:46:54","19370","","1","12","","Sci Rep","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/SUKCHVYJ/Rocchesso et al. - 2022 - A perceptual sound space for auditory displays bas.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XIRHWXGX","journalArticle","2014","McGregor, Iain","Comparing designers’ and listeners’ experiences","AI & SOCIETY","","0951-5666, 1435-5655","10.1007/s00146-013-0489-4","http://link.springer.com/10.1007/s00146-013-0489-4","This paper compares the listening experiences of non-experts and the designers of two sound designs. To date, no such comparisons have been examined empirically, and so for ease of comparison, repertory grids were chosen to explore these experiences, which preclude the need for listener training. The results suggest that (a) it is meaningful to compare designers’ and non-experts’ listening experiences, (b) points of agreement and disagreement are readily identified and (c) the use of repertory grids is a practical means of conducting such studies. The findings further suggest that a taxonomy of sound attributes based on these experiences rather than designers’ intuition or predilection is also possible.","2014-11","2023-07-06 00:46:54","2023-07-19 11:22:38","2023-07-06 00:46:54","473-483","","4","29","","AI & Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JJ22H7U7","bookSection","2010","Allali, Julien; Ferraro, Pascal; Hanna, Pierre; Robine, Matthias","Polyphonic Alignment Algorithms for Symbolic Music Retrieval","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_24","Melody is an important property for the perceptual description of Western musical pieces. A lot of applications rely on the evaluation of similarity between two melodies. While several existing techniques assume a monophonic context or extract a monophonic melody from polyphonic pieces, in this paper, we propose to consider the whole polyphonic context to evaluate the similarity without reducing to a monophonic melody. We thus propose a new model and a corresponding methodology that takes into account all the notes, even if they sound at the same time or if they overlap. Our model relies on a quotiented sequence representation of music. A quotiented sequence is a sequence graph defined with an additional equivalent relation on its vertices and such that the quotient graph is also a sequence graph. The core of the comparison method is based on an adaptation of edit-distance metrics, regularly applied in bio-informatic context. This algorithm is currently being used to evaluate the similarity between a monophonic or polyphonic query and a database of polyphonic musical pieces. First experiments show that the adaptation to polyphony does not degrade the quality of the algorithm with monophonic musical pieces. Furthermore, the results of experiments with polyphonic pieces are promising, even if they show some limitations.","2010","2023-07-06 00:48:35","2023-07-19 11:36:05","2023-07-06 00:48:35","466-482","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_24","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8HR4HYNG","bookSection","2010","Baldan, Stefano; Ludovico, Luca A.; Mauro, Davide A.","Algorithms for an Automatic Transcription of Live Music Performances into Symbolic Format","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_22","This paper addresses the problem of the real-time automatic transcription of a live music performance into a symbolic format. The source data are given by any music instrument or other device able to communicate through a performance protocol. During a performance, music events are parsed and their parameters are evaluated thanks to rhythm and pitch detection algorithms. The final step is the creation of a well-formed XML document, validated against the new international standard known as IEEE 1599. This work will shortly describe both the software environment and the XML format, but the main analysis will involve the real-time recognition of music events. Finally, a case study will be presented: PureMX, a set of Pure Data externals, able to perform the automatic transcription of MIDI events.","2010","2023-07-06 00:48:35","2023-07-19 11:36:44","2023-07-06 00:48:35","422-437","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_22","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"732HWCE7","journalArticle","2018","Matinfar, Sasan; Nasseri, M. Ali; Eck, Ulrich; Kowalsky, Michael; Roodaki, Hessam; Navab, Navid; Lohmann, Chris P.; Maier, Mathias; Navab, Nassir","Surgical soundtracks: automatic acoustic augmentation of surgical procedures","International Journal of Computer Assisted Radiology and Surgery","","1861-6410, 1861-6429","10.1007/s11548-018-1827-2","http://link.springer.com/10.1007/s11548-018-1827-2","Purpose Advances in sensing and digitalization enable us to acquire and present various heterogeneous datasets to enhance clinical decisions. Visual feedback is the dominant way of conveying such information. However, environments rich with many sources of information all presented through the same channel pose the risk of over stimulation and missing crucial information. The augmentation of the cognitive field by additional perceptual modalities such as sound is a workaround to this problem. A major challenge in auditory augmentation is the automatic generation of pleasant and ergonomic audio in complex routines, as opposed to overly simplistic feedback, to avoid alarm fatigue. Methods In this work, without loss of generality to other procedures, we propose a method for aural augmentation of medical procedures via automatic modification of musical pieces. Results Evaluations of this concept regarding recognizability of the conveyed information along with qualitative aesthetics show the potential of our method. Conclusion In this paper, we proposed a novel sonification method for automatic musical augmentation of tasks within surgical procedures. Our experimental results suggest that these augmentations are aesthetically pleasing and have the potential to successfully convey useful information. This work opens a path for advanced sonification techniques in the operating room, in order to complement traditional visual displays and convey information more efficiently.","2018-09","2023-07-06 00:48:35","2023-07-21 07:43:23","2023-07-06 00:48:35","1345-1355","","9","13","","Int J CARS","Surgical soundtracks","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9Y8NLNWS","bookSection","1995","Levkowitz, Haim; Pickett, Ronald M.; Smith, Stuart; Torpey, Mark","An Environment and Studies for Exploring Auditory Representations of Multidimensional Data","Perceptual Issues in Visualization","978-3-642-79059-1 978-3-642-79057-7","","","http://link.springer.com/10.1007/978-3-642-79057-7_5","The field of auditory data representation has produced several intriguing proof-of-concept systems, but up until now there has been little formal research to measure the effectiveness of auditory data displays or to increase our understanding of how they work and how to improve them. Formal assessment is necessary throughout the process of developing new auditory display technologies in order to learn how to restrict the universe of possible sound attributes to those that are most effective for data representation. The capability to run quick psychometric tests to obtain quantitative figures of merit for alternative auditory representations is a requirement for auditory-display researchers engaged in the development of new technologies. For the first time, this capability is realized with a special-purpose workstation designed to generate and administer psychometric tests automatically using test patterns generated from statistically well-specified synthetic data. We describe the characteristics of one such workstation we have developed. We also describe a testing methodology we propose for the development of new auditory data displays of a type that we have been working with for the last few years. Finally, we describe a specific set of studies we are now beginning to conduct.","1995","2023-07-06 00:48:35","2023-07-21 04:45:58","2023-07-06 00:48:35","47-58","","","","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-3-642-79057-7_5","","","","","","Grinstein, Georges; Levkowitz, Haim","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IAQXU3JM","bookSection","2013","Schubert, Emery; Ferguson, Sam; Farrar, Natasha; Taylor, David; McPherson, Gary E.","The Six Emotion-Face Clock as a Tool for Continuously Rating Discrete Emotional Responses to Music","From Sounds to Music and Emotions","978-3-642-41247-9 978-3-642-41248-6","","","http://link.springer.com/10.1007/978-3-642-41248-6_1","Recent instruments measuring continuous self-reported emotion responses to music have tended to use dimensional rating scale models of emotion such as valence (happy to sad). However, numerous retrospective studies of emotion in music use checklist style responses, usually in the form of emotion words, (such as happy, angry, sad…) or facial expressions. A response interface based on six simple sketch style emotion faces aligned into a clock-like distribution was developed with the aim of allowing participants to quickly and easily rate emotions in music continuously as the music unfolded. We tested the interface using six extracts of music, one targeting each of the six faces: ‘Excited’ (at 1 o’clock), ‘Happy’ (3), ‘Calm’ (5), ‘Sad’ (7), ‘Scared’ (9) and ‘Angry’ (11). 30 participants rated the emotion expressed by these excerpts on our ‘emotion-face-clock’. By demonstrating how continuous category selections (votes) changed over time, we were able to show that (1) more than one emotion-face could be expressed by music at the same time and (2) the emotion face that best portrayed the emotion the music conveyed could change over time, and (3) the change could be attributed to changes in musical structure. Implications for research on orientation time and mixed emotions are discussed.","2013","2023-07-06 00:48:35","2023-07-20 00:06:10","2023-07-06 00:48:35","1-18","","","7900","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-41248-6_1","","","","","","Aramaki, Mitsuko; Barthet, Mathieu; Kronland-Martinet, Richard; Ystad, Sølvi","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VBNWLU84","bookSection","2010","Frissen, Ilja; Katz, Brian F. G.; Guastavino, Catherine","Effect of Sound Source Stimuli on the Perception of Reverberation in Large Volumes","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_18","The aim of the presented research is to determine whether the perception of reverberation is dependent on the type of sound stimuli used. We quantified the discrimination thresholds for reverberations that are representative for large rooms such as concert halls (reverberation times around 1.8 s). For exponential decays, simulating an ideal simple room, thresholds are around 6% (Experiment 1). We found no difference in thresholds between a short noise burst and a male voice spoken word, suggesting that discrimination is not dependent on the type, or spectral content, of the sound source (Experiment 2). In two further experiments using a magnitude estimation paradigm we assessed the perceived amount of reverberation as a function of various types of stimuli. Whereas the discrimination of reverberant stimuli does not seem to be affected by the sound stimulus, the perceived amount of reverberation is affected. Vocal stimuli are perceived as being more reverberant than nonvocal stimuli. The results are discussed in light of current neuroscientific models of auditory processing of complex stimuli but also with respect to their consequences for the use of reverberation in auditory display.","2010","2023-07-06 00:48:35","2023-07-19 11:37:32","2023-07-06 00:48:35","358-376","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_18","","/Users/minsik/Zotero/storage/IR2GFI2L/Frissen et al. - 2010 - Effect of Sound Source Stimuli on the Perception o.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2M8UAXS8","bookSection","2018","Seiça, Mariana; Lopes, Rui; Martins, Pedro; Cardoso, F. Amílcar","Sonifying Twitter’s Emotions Through Music","Music Technology with Swing","978-3-030-01691-3 978-3-030-01692-0","","","http://link.springer.com/10.1007/978-3-030-01692-0_39","Sonification is a scientific field that seeks to explore the potential of sound as an instrument to convey and interpret data. Its techniques have been developing significantly with the growth of technology and supporting hardware and software, which have spread in our daily environment. This allowed the establishment of new communication tools to share information, opinion and feelings as part of our daily routine. The aim of this project was to unite the social media phenomena with sonification, using Twitter data to extract user’s emotions and translate them into musical compositions. The focus was to explore the potential of music in translating data as personal and subjective as human emotions, developing a musically complex and captivating mapping based on the rules of Western Music. The music is accompanied by a simple visualization, which results in emotions being heard and seen with the corresponding tweets, in a multimodal experience that represents Twitter’s emotional reality. The mapping was tested through an online survey, and despite a few misunderstandings, the results were generally positive, expressing the efficiency and impact of the developed system.","2018","2023-07-06 00:48:35","2023-07-21 04:34:49","2023-07-06 00:48:35","586-608","","","11265","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-01692-0_39","","","","","","Aramaki, Mitsuko; Davies, Matthew E. P.; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZUWSY564","bookSection","2006","Hermann, Thomas; Höner, Oliver; Ritter, Helge","AcouMotion – An Interactive Sonification System for Acoustic Motion Control","Gesture in Human-Computer Interaction and Simulation","978-3-540-32624-3 978-3-540-32625-0","","","http://link.springer.com/10.1007/11678816_35","This paper introduces AcouMotion as a new hard-/software system for combining human body motion, tangible interfaces and sonification to a closed-loop human computer interface that allows non-visual motor control by using sonification (non-speech auditory displays) as major feedback channel. AcouMotion’s main components are (i) a sensor device for measuring motion parameters (ii) a computer simulation to represent the dynamical evolution of a model world, and (iii) a sonification engine which generates an auditory representation of objects and any interactions in the model world. The intended applications of AcouMotion range from new kinds of sport games that can be played without visual displays and therefore may be particularly interesting for people with visual impairment to further applications in data mining, physiotherapy and cognitive research. The first application of AcouMotion presented in this paper is Blindminton, a sport game similar to Badminton which is particularly adapted to the abilities of people with visual impairment. We describe our current system and its state of development, and we present first sound examples for interactive sonification using an early prototype. Finally, we discuss some interesting research directions based on the fact that AcouMotion binds auditory stimuli and body motion, and thus can represent a counterpart to the Eye-tracker device that exploits the binding of visual stimuli and eye-movement in cognitive research.","2006","2023-07-06 00:54:30","2023-07-20 00:09:03","2023-07-06 00:54:29","312-323","","","3881","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11678816_35","","","","","","Gibet, Sylvie; Courty, Nicolas; Kamp, Jean-François","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SIA9QLWZ","bookSection","2010","Garavaglia, Javier Alejandro","Raising Awareness about Complete Automation of Live-Electronics: A Historical Perspective","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_23","The article raises attention to the advantages and disadvantages of complete automation in DSP processes (including the triggering of events) during the performance of interactive music. By proposing a historic summary divided in three main periods according to the technologies, methods and processes available during each of them, (including examples of key works and composers), it shows how the usage of automation in live-electronics was dependent on the development of new technologies, specially digital. Further, it explains how full automation works in two works of mine, describing the features and techniques involved. Considering those examples, the advantages and disadvantages resulting from the introduction of complete automation of DSP in live performance are finally discussed. Even though automation is not a new technique in the field, I am keen to dedicate special attention to completely automated events -including their triggering- given the impact that automation can have on performances.","2010","2023-07-06 00:54:30","2023-07-19 11:37:40","2023-07-06 00:54:29","438-465","","","5954","","","Raising Awareness about Complete Automation of Live-Electronics","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_23","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X7S2LUAM","bookSection","2007","Young-hyun, Park; Kwang-hee, Han","Information Display of Wearable Devices Through Sound Feedback of Wearable Computing","Human-Computer Interaction. Interaction Platforms and Techniques","978-3-540-73106-1","","","http://link.springer.com/10.1007/978-3-540-73107-8_132","Functions in wearable devices came to be various and specialized during the developmental process of wearable computing. However, such surfacing of new functions made it difficult and time-consuming for the devices to display the condition of their status, such as whether they are turned on or off. Moreover, mere dependency on visual display could lead to an overload of users’ visual cognition. In this research, sounds were used for relaying information feedback of the status of wearable devices. We first verified the usefulness of adding sound feedback, and next, the effect of each device-specific sound was confirmed as a sound feedback.","2007","2023-07-06 00:54:30","2023-07-20 06:32:31","2023-07-06 00:54:29","1200-1209","","","4551","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73107-8_132","","","","","","Jacko, Julie A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UTYPYE45","bookSection","2023","Martín-Gómez, Lucía; Pérez-Marcos, Javier; Rivero, Alfonso José López; Bermúdez, Giovanny Mauricio Tarazona","Drawing Music: Using Neural Networks to Compose Descriptive Music from Illustrations","New Trends in Disruptive Technologies, Tech Ethics and Artificial Intelligence","978-3-031-14858-3 978-3-031-14859-0","","","https://link.springer.com/10.1007/978-3-031-14859-0_3","The creative capacity of machines is still questioned by researchers and users alike. For this reason, computational creativity does not only focus on the development of machines for the creation of artistic content but also on the evaluation of the generated content. This works presents a system that composes polyphonic music from the drawings of a user in real time. Our proposal provides an analysis of the Fantasia film, produced in 1940 by Walt Disney and deduces the relationship between its audio and images. As part of system development, an LSTM-based Recurrent Neural Network was trained with MIDI music files and a model was obtained. As a result, the proposed system generates polyphonic music with expressive timing and dynamics by inferring chords from the user’s drawings. To assess the creative ability of the machine a Turing test was conducted and the quality of the interconnection between drawings and music was measured by another user test. Additionally, the performance of the considered classifiers is discussed.","2023","2023-07-06 00:54:30","2023-07-21 04:38:58","2023-07-06 00:54:29","30-42","","","1430","","","Drawing Music","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Advances in Intelligent Systems and Computing DOI: 10.1007/978-3-031-14859-0_3","","","","","","De La Iglesia, Daniel H.; De Paz Santana, Juan F.; López Rivero, Alfonso J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BWNPG65K","bookSection","2021","Yihang, Du; Ke, Niu; Yan, Pei; Petrovna, Gnativ Marina; Lijun, Wang","Multi-scale Evaluation of HCI Acoustic Expression in Digital Performance Space","Advances in Usability, User Experience, Wearable and Assistive Technology","978-3-030-80090-1 978-3-030-80091-8","","","https://link.springer.com/10.1007/978-3-030-80091-8_103","With the development of digitization and interaction technology, the digital performance space (DPS) based on multi-sensory channels create more abundant aesthetic experience for audience, and rationality of soundscape is an important part of DPS design. An evaluation system was established by critical incidence technique (CIT) for human-computer interactive (HCI) acoustic expression in DPS, including nine indicators under the two dimensions of sound environment and sound sequence. Three art installation works were selected from Chinese Theatre Art Interactive Area in Beijing Design Week to verify the effectiveness of evaluation system. Fifteen adults with normal hearing were recruited to the experiments. The result shows quietness should be highlighted in the work with auditory dominant information, and the work which with auxiliary information in auditory, should to focus on content foresight and consistency of auditory-visual of acoustic information. The results will provide optimization strategy in an active and passive measures for acoustic design in DPS.","2021","2023-07-06 00:54:30","2023-07-19 11:13:05","2023-07-06 00:54:29","869-879","","","275","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Networks and Systems DOI: 10.1007/978-3-030-80091-8_103","","","","","","Ahram, Tareq Z.; Falcão, Christianne S.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"32VFFRU2","journalArticle","2018","Baur, Kilian; Speth, Florina; Nagle, Aniket; Riener, Robert; Klamroth-Marganska, Verena","Music meets robotics: a prospective randomized study on motivation during robot aided therapy","Journal of NeuroEngineering and Rehabilitation","","1743-0003","10.1186/s12984-018-0413-8","https://jneuroengrehab.biomedcentral.com/articles/10.1186/s12984-018-0413-8","Background: Robots have been successfully applied in motor training during neurorehabilitation. As music is known to improve motor function and motivation in neurorehabilitation training, we aimed at integrating music creation into robotic-assisted motor therapy. We developed a virtual game-like environment with music for the arm therapy robot ARMin, containing four different motion training conditions: a condition promoting creativity (C+) and one not promoting creativity (C–), each in a condition with (V+) and without (V–) a visual display (i.e., a monitor). The visual display was presenting the game workspace but not contributing to the creative process itself. In all four conditions the therapy robot haptically displayed the game workspace. Our aim was to asses the effects of creativity and visual display on motivation. Methods: In a prospective randomized single-center study, healthy participants were randomly assigned to play two of the four training conditions, either with (V+) or without visual display (V–). In the third round, the participants played a repetition of the preferred condition of the two first rounds, this time with a new V condition (i.e., with or without visual display). For each of the three rounds, motivation was measured with the Intrinsic Motivation Inventory (IMI) in the subscales interest/enjoyment, perceived choice, value/usefulness, and man-machine-relation. We recorded the actual training time, the time of free movement, and the velocity profile and administered a questionnaire to measure perceived training time and perceived effort. All measures were analysed using linear mixed models. Furthermore, we asked if the participants would like to receive the created music piece. Results: Sixteen healthy subjects (ten males, six females, mean age: 27.2 years, standard deviation: 4.1 years) with no known motor or cognitive deficit participated. Promotion of creativity (i.e., C+ instead of C–) significantly increased the IMI-item interest/enjoyment (p = 0.001) and the IMI-item perceived choice (p = 0.010). We found no significant effects in the IMI-items man-machine relation and value/usefulness. Conditions promoting creativity (with or without visual display) were preferred compared to the ones not promoting creativity. An interaction effect of promotion of creativity and omission of visual display was present for training time (p = 0.013) and training intensity (p < 0.001). No differences in relative perceived training time, perceived effort, and perceived value among the four training conditions were found.","2018-12","2023-07-06 00:54:30","2023-07-21 07:43:39","2023-07-06 00:54:29","79","","1","15","","J NeuroEngineering Rehabil","Music meets robotics","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/Q6GTJCKH/Baur et al. - 2018 - Music meets robotics a prospective randomized stu.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9BACPKPI","bookSection","2010","Verron, Charles; Aramaki, Mitsuko; Kronland-Martinet, Richard; Pallone, Grégory","Spatialized Synthesis of Noisy Environmental Sounds","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_20","In this paper, an overview of the stochastic modeling for analysis/synthesis of noisy sounds is presented. In particular, we focused on the time-frequency domain synthesis based on the inverse fast Fourier transform (IFFT) algorithm from which we proposed the design of a spatialized synthesizer. The originality of this synthesizer remains in its one-stage architecture that efficiently combines the synthesis with 3D audio techniques at the same level of sound generation. This architecture also allowed including a control of the source width rendering to reproduce naturally diffused environments. The proposed approach led to perceptually realistic 3D immersive auditory scenes. Applications of this synthesizer are here presented in the case of noisy environmental sounds such as air swishing, sea wave or wind sound. We finally discuss the limitations but also the possibilities offered by the synthesizer to achieve sound transformations based on the analysis of recorded sounds.","2010","2023-07-06 00:54:30","2023-07-19 11:38:59","2023-07-06 00:54:29","392-407","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_20","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B4A2D9VH","journalArticle","2020","Roddy, Stephen; Bridges, Brian","Mapping for meaning: the embodied sonification listening model and its implications for the mapping problem in sonic information design","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-020-00318-y","http://link.springer.com/10.1007/s12193-020-00318-y","This is a theoretical paper that considers the mapping problem, a foundational issue which arises when designing a sonification, as it applies to sonic information design. We argue that this problem can be addressed by using models from the field of embodied cognitive science, including embodied image schema theory, conceptual metaphor theory and conceptual blends, and from research which treats sound and musical structures using these models, when mapping data to sound. However, there are currently very few theoretical frameworks for applying embodied cognition principles in a sonic information design context. This article describes one such framework, the Embodied Sonification Listening Model, which provides a theoretical description of sonification listening in terms of Conceptual Metaphor Theory.","2020-06","2023-07-06 00:54:30","2023-07-20 06:56:45","2023-07-06 00:54:29","143-151","","2","14","","J Multimodal User Interfaces","Mapping for meaning","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/29NLEMJR/Roddy and Bridges - 2020 - Mapping for meaning the embodied sonification lis.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NWDS48SW","bookSection","2017","Matinfar, Sasan; Nasseri, M. Ali; Eck, Ulrich; Roodaki, Hessam; Navab, Navid; Lohmann, Chris P.; Maier, Mathias; Navab, Nassir","Surgical Soundtracks: Towards Automatic Musical Augmentation of Surgical Procedures","Medical Image Computing and Computer-Assisted Intervention − MICCAI 2017","978-3-319-66184-1 978-3-319-66185-8","","","https://link.springer.com/10.1007/978-3-319-66185-8_76","Advances in sensing and digitalization enable us to acquire and present various heterogeneous datasets to enhance clinical decisions. Visual feedback is the dominant way of conveying such information. However, environments rich with many sources of information all presented through the same channel pose the risk of over stimulation and missing crucial information. The augmentation of the cognitive field by additional perceptual modalities such as sound is a workaround to this problem. A major challenge in auditory augmentation is the automatic generation of pleasant and ergonomic audio in complex routines, as opposed to overly simplistic feedback, to avoid fatigue. In this work, without loss of generality to other procedures, we propose a method for aural augmentation of ophthalmic procedures via automatic modification of musical pieces. Evaluations of this first proof of concept regarding recognizability of the conveyed information along with qualitative aesthetics show the potential of our method.","2017","2023-07-06 00:54:30","2023-07-21 04:29:05","2023-07-06 00:54:29","673-681","","","10434","","","Surgical Soundtracks","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-66185-8_76","","","","","","Descoteaux, Maxime; Maier-Hein, Lena; Franz, Alfred; Jannin, Pierre; Collins, D. Louis; Duchesne, Simon","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SL3TX393","bookSection","2010","Fagerlönn, Johan; Liljedahl, Mats","Designing a Web-Based Tool That Informs the Audio Design Process","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_4","Research on auditory displays has shown how various properties of sounds can influence perception and performance. However, a challenge for system developers is how to find signals that correspond to specific user situations and make sense within a user context. This paper presents a web-based tool called AWESOME Sound design tool. The fundamental idea with the tool is to give end users control over some aspects of the auditory stimuli and encourage them to manipulate the sound with a specific user scenario in mind. A first evaluation of the tool has been conducted in which aspects of both usefulness and usability were addressed.","2010","2023-07-06 00:54:30","2023-07-19 11:37:19","2023-07-06 00:54:29","68-79","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_4","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LTKE6NDQ","journalArticle","2022","Hagen, Edward H.","The Biological Roots of Music and Dance: Extending the Credible Signaling Hypothesis to Predator Deterrence","Human Nature","","1045-6767, 1936-4776","10.1007/s12110-022-09429-9","https://link.springer.com/10.1007/s12110-022-09429-9","After they diverged from panins, hominins evolved an increasingly committed terrestrial lifestyle in open habitats that exposed them to increased predation pressure from Africa’s formidable predator guild. In the Pleistocene, Homo transitioned to a more carnivorous lifestyle that would have further increased predation pressure. An effective defense against predators would have required a high degree of cooperation by the smaller and slower hominins. It is in the interest of predator and potential prey to avoid encounters that will be costly for both. A wide variety of species, including carnivores and apes and other primates, have therefore evolved visual and auditory signals that deter predators by credibly signaling detection and/or the ability to effectively defend themselves. In some cooperative species, these predator deterrent signals involve highly synchronized visual and auditory displays among group members. Hagen and Bryant (Human Nature, 14(1), 21–51, 2003) proposed that synchronized visual and auditory displays credibly signal coalition quality. Here, this hypothesis is extended to include credible signals to predators that they have been detected and would be met with a highly coordinated defensive response, thereby deterring an attack. Within-group signaling functions are also proposed. The evolved cognitive abilities underlying these behaviors were foundations for the evolution of fully human music and dance.","2022-09","2023-07-06 00:54:30","2023-07-20 05:53:59","2023-07-06 00:54:29","261-279","","3","33","","Hum Nat","The Biological Roots of Music and Dance","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VTRC2WU7","bookSection","2005","Eldridge, Alice C.","Extra-Music(ologic)al Models for Algorithmic Composition","Applications of Evolutionary Computing","978-3-540-25396-9 978-3-540-32003-6","","","http://link.springer.com/10.1007/978-3-540-32003-6_58","This paper addresses design approaches to algorithmic composition and suggests that music-theoretic tenets alone are unsuitable as prescriptive principles and could be profitably complemented by attempts to represent and recreate dynamical structures of music. Examples of ongoing work using adaptive dynamical processes for generating dynamic structures are presented.","2005","2023-07-06 00:54:30","2023-07-19 11:31:12","2023-07-06 00:54:29","557-562","","","3449","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-32003-6_58","","","","","","Rothlauf, Franz; Branke, Jürgen; Cagnoni, Stefano; Corne, David Wolfe; Drechsler, Rolf; Jin, Yaochu; Machado, Penousal; Marchiori, Elena; Romero, Juan; Smith, George D.; Squillero, Giovanni","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LEWB5JMW","journalArticle","2001","Johannsen, Gunnar","Auditory Displays in Human–Machine Interfaces of Mobile Robots for Non-Speech Communication with Humans","Journal of Intelligent and Robotic Systems","","1573-0409","10.1023/A:1013953213049","https://doi.org/10.1023/A:1013953213049","Auditory displays are developed and investigated for mobile service robots in a human–machine environment. The service robot domain was chosen as an example for future use of auditory displays within multimedia process supervision and control applications in industrial, transportation, and medical systems. The design of directional sounds and of additional sounds for robot states as well as the design of more complicated robot sound tracks are explained. Basic musical elements and robot-movement sounds are combined. Experimental studies on the auditory perception of directional sounds as well as of sound tracks for the predictive display of intended robot trajectories in a simulated supermarket scenario are described.","2001-10-01","2023-07-06 00:54:44","2023-07-06 00:54:44","2023-07-06 00:54:44","161-169","","2","32","","Journal of Intelligent and Robotic Systems","","","","","","","","en","","","","","Springer Link","","","","/Users/minsik/Zotero/storage/LG856EUC/Johannsen - 2001 - Auditory Displays in Human–Machine Interfaces of M.pdf","","","auditory displays; auditory perception; directional sounds; human–machine interfaces; mobile service robots; multimedia process control; robot sound tracks; robot-movement sounds; simulated robot environment; sound communication of states and intentions","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RVS6JQWV","bookSection","2019","Paul, Tirthankar; Vainio, Seppo; Roning, Juha","Towards Personalised, DNA Signature Derived Music via the Short Tandem Repeats (STR)","Intelligent Computing","978-3-030-01176-5 978-3-030-01177-2","","","http://link.springer.com/10.1007/978-3-030-01177-2_69","Our inheritance is defined by the genetic code, which is based on the exact order of the nucleotides A, T, C and G of the deoxyribonucleic acid (DNA) polymer in the nucleus. The personalised genetic makeup is set in meiosis in the germline cells, while the particular genetic construction is established upon fertilisation. Given the capacity to define the exact genome structure by deep sequencing in a high throughput manner in populations, this enabled establishment of gene banks for not only human, but also for a wealth of other species. Computer added programming offers capacity to transform or translate the genetic code to other modalities such as sound. Variations in the nucleotides and the repeat regions of a short sequence, the Short Tandem Repeats (STRs), serve to define the biological identity of a person, especially in 13 genomic, the combined DNA index system (CODIS) locations. The number of STR and the STR sequences were selected as the units of musical elements and programmed to a musical instrument digital interference (MIDI) file to frequencies that can be heard by the ear. The novelty of the work is that an algorithm can be expected to provide DNA-based musical communication platform, data based bio-authentication via the personal music, and to identify for example, the musical similarity score for the family members.","2019","2023-07-06 00:55:52","2023-07-20 06:34:43","2023-07-06 00:55:52","951-964","","","857","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Advances in Intelligent Systems and Computing DOI: 10.1007/978-3-030-01177-2_69","","","","","","Arai, Kohei; Kapoor, Supriya; Bhatia, Rahul","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N2IRMMJU","bookSection","2011","Klapuri, Anssi","Pattern Induction and Matching in Music Signals","Exploring Music Contents","978-3-642-23125-4 978-3-642-23126-1","","","http://link.springer.com/10.1007/978-3-642-23126-1_13","This paper discusses techniques for pattern induction and matching in musical audio. At all levels of music - harmony, melody, rhythm, and instrumentation - the temporal sequence of events can be subdivided into shorter patterns that are sometimes repeated and transformed. Methods are described for extracting such patterns from musical audio signals (pattern induction) and computationally feasible methods for retrieving similar patterns from a large database of songs (pattern matching).","2011","2023-07-06 00:55:52","2023-07-20 00:05:29","2023-07-06 00:55:52","188-204","","","6684","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-23126-1_13","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3CQP8TR9","bookSection","2013","Marchegiani, Letizia; Fafoutis, Xenofon","A Behavioral Study on the Effects of Rock Music on Auditory Attention","Human Behavior Understanding","978-3-319-02713-5 978-3-319-02714-2","","","http://link.springer.com/10.1007/978-3-319-02714-2_2","We are interested in the distribution of top-down attention in noisy environments, in which the listening capability is challenged by rock music playing in the background. We conducted behavioral experiments in which the subjects were asked to focus their attention on a narrative and detect a specific word, while the voice of the narrator was masked by rock songs that were alternating in the background. Our study considers several types of songs and investigates how their distinct features affect the ability to segregate sounds. Additionally, we examine the effect of the subjects’ familiarity to the music.","2013","2023-07-06 00:55:52","2023-07-20 05:50:43","2023-07-06 00:55:52","15-26","","","8212","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-02714-2_2","","","","","","Salah, Albert Ali; Hung, Hayley; Aran, Oya; Gunes, Hatice","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FT2Z38IT","journalArticle","2016","Albrecht, Robert; Väänänen, Riitta; Lokki, Tapio","Guided by music: pedestrian and cyclist navigation with route and beacon guidance","Personal and Ubiquitous Computing","","1617-4909, 1617-4917","10.1007/s00779-016-0906-z","http://link.springer.com/10.1007/s00779-016-0906-z","Music listening and navigation are both common tasks for mobile device users. In this study, we integrated music listening with a navigation service, allowing users to follow the perceived direction of the music to reach their destination. This navigation interface provided users with two different guidance methods: route guidance and beacon guidance. The user experience of the navigation service was evaluated with pedestrians in a city center and with cyclists in a suburban area. The results show that spatialized music can be used to guide pedestrians and cyclists toward a destination without any prior training, offering a pleasant navigation experience. Both route and beacon guidance were deemed good alternatives, but the preference between them varied from person to person and depended on the situation. Beacon guidance was generally considered to be suitable for familiar surroundings, while route guidance was seen as a better alternative for areas that are unfamiliar or more difficult to navigate.","2016-02","2023-07-06 00:55:52","2023-07-21 04:46:05","2023-07-06 00:55:52","121-145","","1","20","","Pers Ubiquit Comput","Guided by music","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AUKTBA4S","bookSection","2014","Yamaguchi, Daiki; Kiyozaki, Mutsumi; Magatani, Kazushige","A Study of the Influence Which MP3 Formatted Sound Gives EEG of the Human","The 15th International Conference on Biomedical Engineering","978-3-319-02912-2 978-3-319-02913-9","","","https://link.springer.com/10.1007/978-3-319-02913-9_238","currently, many types of no-reversible compressed sound source, represented by MP3 (MPEG Audio Layer-3) are popular in the world and they are widely used to make the music file size smaller. The sound data created in this way has less information as compared to pre-compressed data. The objective of this study is by analyzing EEG to determine if people can recognize such difference as differences in sound. Measurement systems that can measure and analyze EEG when a subject listens to music were experimentally developed. And ten subjects were studied with this system. In this experiment, a WAVE formatted music data and a MP3 compressed music data that is made from the WAVE formatted data were prepared. Each subject was made to hear these music sources at the same volume. From the results of this experiment, clear differences were confirmed between two sound sources.","2014","2023-07-06 00:55:52","2023-07-21 05:04:26","2023-07-06 00:55:52","924-927","","","43","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: IFMBE Proceedings DOI: 10.1007/978-3-319-02913-9_238","","","","","","Goh, James","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ISDMFL4F","bookSection","2020","Inoue, Naoki; Fujimoto, Yuichiro; Plopski, Alexander; Okahashi, Sayaka; Kanbara, Masayuki; Hsu, Hsiu-Yun; Kuo, Li-Chieh; Su, Fong-Chin; Kato, Hirokazu","Effect of Display Location on Finger Motor Skill Training with Music-Based Gamification","Human Aspects of IT for the Aged Population. Healthy and Active Aging","978-3-030-50248-5 978-3-030-50249-2","","","http://link.springer.com/10.1007/978-3-030-50249-2_6","The motor control of individual fingers is an important part of daily life, but there are many people who have difficulty with it, such as elderly people and stroke patients. While continuous rehabilitation is necessary for functional recovery of finger mobility and suppression of functional deterioration, it usually requires the assistance of occupational therapists. Furthermore, the rehabilitation process can be monotonous, which makes it difficult for patients to maintain their motivation. Over a series of studies, we have developed a finger movement training system that incorporates gamification and is based on playing music using a Pressing Evaluation Training System that can measure the force exerted by each finger. One remaining problem was that patients had difficulty recognizing the fingering information, and it took some time for them to get used to locating this information quickly. In this study, we applied augmented reality (AR) technology to display each sound element as close as possible to the position of the corresponding finger so that the user could directly perceive the information for each finger while wearing the head mounted display. We conducted a user study with 10 university students to determine if the distance between the sound element display position and the location of each finger had an effect on performance. The results indicated that incorporating AR allowed the users to recognize the correct finger positions more quickly.","2020","2023-07-06 00:55:52","2023-07-20 05:50:25","2023-07-06 00:55:52","78-90","","","12208","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-50249-2_6","","","","","","Gao, Qin; Zhou, Jia","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E88EGPD3","bookSection","2014","Argo, Jessica; Ma, Minhua; Kayser, Christoph","Immersive Composition for Sensory Rehabilitation: 3D Visualisation, Surround Sound, and Synthesised Music to Provoke Catharsis and Healing","Serious Games Development and Applications","978-3-319-11622-8 978-3-319-11623-5","","","http://link.springer.com/10.1007/978-3-319-11623-5_12","There is a wide range of sensory therapies using sound, music and visual stimuli. Some focus on soothing or distracting stimuli such as natural sounds or classical music as analgesic, while other approaches emphasize the active performance of producing music as therapy. This paper proposes an immersive multi-sensory Exposure Therapy for people suffering from anxiety disorders, based on a rich, detailed surround-soundscape. This soundscape is composed to include the users’ own idiosyncratic anxiety triggers as a form of habituation, and to provoke psychological catharsis, as a non-verbal, visceral and enveloping exposure. To accurately pinpoint the most effective sounds and to optimally compose the soundscape we will monitor the participants’ physiological responses such as electroencephalography, respiration, electromyography, and heart rate during exposure. We hypothesize that such physiologically optimized sensory landscapes will aid the development of future immersive therapies for various psychological conditions, Sound is a major trigger of anxiety, and auditory hypersensitivity is an extremely problematic symptom. Exposure to stress-inducing sounds can free anxiety sufferers from entrenched avoidance behaviors, teaching physiological coping strategies and encouraging resolution of the psychological issues agitated by the sound.","2014","2023-07-06 00:55:52","2023-07-21 04:59:29","2023-07-06 00:55:52","134-149","","","8778","","","Immersive Composition for Sensory Rehabilitation","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-11623-5_12","","/Users/minsik/Zotero/storage/DG5YYQ83/Argo et al. - 2014 - Immersive Composition for Sensory Rehabilitation .pdf","","","","Ma, Minhua; Oliveira, Manuel Fradinho; Baalsrud Hauge, Jannicke","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SAY2S2ES","bookSection","2006","Camurri, Antonio; Castellano, Ginevra; Ricchetti, Matteo; Volpe, Gualtiero","Subject Interfaces: Measuring Bodily Activation During an Emotional Experience of Music","Gesture in Human-Computer Interaction and Simulation","978-3-540-32624-3 978-3-540-32625-0","","","http://link.springer.com/10.1007/11678816_30","This paper focuses on the relationship between emotions induced by musical stimuli and movement. A pilot experiment has been realized with the aim to verify whether there are correlations between the emotional characterization of music excerpts and human movement. Subjects were asked to move a laser pointer on a white wall in front of them while listening to musical excerpts classified with respect to the type of emotions they can induce. Trajectories obtained moving the laser pointer have been recorded with a video camera and have been analyzed in a static and global way by using the EyesWeb platform. Results highlight a difference between trajectories associated to music stimuli classified as “fast” and “slow”, in term of smoothness/angularity, suggesting the existence of a strong link between the emotional characterization of the musical excerpts listened to and the movement performed.","2006","2023-07-06 00:55:52","2023-07-20 00:08:53","2023-07-06 00:55:52","268-279","","","3881","","","Subject Interfaces","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11678816_30","","","","","","Gibet, Sylvie; Courty, Nicolas; Kamp, Jean-François","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NSS8GCFW","journalArticle","2007","Nagel, Frederik; Kopiez, Reinhard; Grewe, Oliver; Altenmüller, Eckart","EMuJoy: Software for continuous measurement of perceived emotions in music","Behavior Research Methods","","1554-351X, 1554-3528","10.3758/BF03193159","http://link.springer.com/10.3758/BF03193159","An adequate study of emotions in music and film should be based on the real-time measurement of selfreported data using a continuous-response method. The recording system discussed in this article reflects two important aspects of such research: First, for a better comparison of results, experimental and technical standards for continuous measurement should be taken into account, and second, the recording system should be open to the inclusion of multimodal stimuli. In light of these two considerations, our article addresses four basic principles of the continuous measurement of emotions: (1) the dimensionality of the emotion space, (2) data acquisition (e.g., the synchronization of media and the self-reported data), (3) interface construction for emotional responses, and (4) the use of multiple stimulus modalities. Researcher-developed software (EMuJoy) is presented as a freeware solution for the continuous measurement of responses to different media, along with empirical data from the self-reports of 38 subjects listening to emotional music and viewing affective pictures.","2007-05","2023-07-06 00:55:52","2023-07-19 23:33:55","2023-07-06 00:55:52","283-290","","2","39","","Behavior Research Methods","EMuJoy","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/XQ3XEAUI/Nagel et al. - 2007 - EMuJoy Software for continuous measurement of per.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HFDFQRKI","journalArticle","2012","Fabiani, Marco; Bresin, Roberto; Dubus, Gaël","Interactive sonification of expressive hand gestures on a handheld device","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0076-2","http://link.springer.com/10.1007/s12193-011-0076-2","We present here a mobile phone application called MoodifierLive which aims at using expressive music performances for the sonification of expressive gestures through the mapping of the phone’s accelerometer data to the performance parameters (i.e. tempo, sound level, and articulation). The application, and in particular the sonification principle, is described in detail. An experiment was carried out to evaluate the perceived matching between the gesture and the music performance that it produced, using two distinct mappings between gestures and performance. The results show that the application produces consistent performances, and that the mapping based on data collected from real gestures works better than one defined a priori by the authors.","2012-07","2023-07-06 00:55:52","2023-07-20 06:54:23","2023-07-06 00:55:52","49-57","","1-2","6","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UV7WRQNY","bookSection","1997","Fernström, M.; Bannon, L.","Explorations in Sonic Browsing","People and Computers XII","978-3-540-76172-3 978-1-4471-3601-9","","","http://link.springer.com/10.1007/978-1-4471-3601-9_8","This paper describes a novel browser prototype that has been designed and implemented on PC’s and soundcards. Our focus has been on the development of a usable and engaging interface which exploits both visual and aural features of the data space. The project involves state-of-the-art work in human-computer interaction and multimedia development. We are working on a data set of musical compositions, and are designing and testing the prototype with a group of musicians. This paper provides some detail on the development process, the current architecture of the system, and describes some of the problems encountered.","1997","2023-07-06 00:55:52","2023-07-21 04:41:39","2023-07-06 00:55:52","117-131","","","","","","","","","","","Springer London","London","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-1-4471-3601-9_8","","","","","","Thimbleby, Harold; O’Conaill, Brid; Thomas, Peter J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SPCCNLU4","journalArticle","2022","Paté, Arthur; Farge, Gaspard; Holtzman, Benjamin K.; Barth, Anna C.; Poli, Piero; Boschi, Lapo; Karlstrom, Leif","Combining audio and visual displays to highlight temporal and spatial seismic patterns","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-021-00378-8","https://link.springer.com/10.1007/s12193-021-00378-8","Data visualization, and to a lesser extent data sonification, are classic tools to the scientific community. However, these two approaches are very rarely combined, although they are highly complementary: our visual system is good at recognizing spatial patterns, whereas our auditory system is better tuned for temporal patterns. In this article, data representation methods are proposed that combine visualization, sonification, and spatial audio techniques, in order to optimize the user’s perception of spatial and temporal patterns in a single display, to increase the feeling of immersion, and to take advantage of multimodal integration mechanisms. Three seismic data sets are used to illustrate the methods, covering different physical phenomena, time scales, spatial distributions, and spatio-temporal dynamics. The methods are adapted to the specificities of each data set, and to the amount of information that the designer wants to display. This leads to further developments, namely the use of audification with two time scales, the switch from pure audification to time-modulated noise, and the switch from pure audification to sonic icons. First user feedback from live demonstrations indicates that the methods presented in this article seem to enhance the perception of spatio-temporal patterns, which is a key parameter to the understanding of seismically active systems, and a step towards apprehending the processes that drive this activity.","2022-03","2023-07-06 00:57:05","2023-07-20 07:03:47","2023-07-06 00:57:05","125-142","","1","16","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NZJRYABC","journalArticle","1971","Pollack, Irwin","Discrimination of restrictions in sequentially-encoded auditory displays: Block designs","Perception & Psychophysics","","0031-5117, 1532-5962","10.3758/BF03213028","http://link.springer.com/10.3758/BF03213028","Sequential constraints, in the form represented by equal frequencies within blocks, were imposed upon finite-state statistical generators. The sequences were encoded in the form of interval-coded pulse trains and were transduced to sound by earphones. In discrimination tests, interval thresholds between the finite states provided a measure of the relative discriminability between different sequential constraints. These thresholds are shown to be quantitatively related to the difference in uncertainty in specification of the sequences. To a first approximation: Equal interstate interval thresholds are associated with equal differences in uncertainty.","1971-01","2023-07-06 00:57:05","2023-07-21 04:44:29","2023-07-06 00:57:05","57-60","","1","9","","Perception & Psychophysics","Discrimination of restrictions in sequentially-encoded auditory displays","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/ZN8TIT2P/Pollack - 1971 - Discrimination of restrictions in sequentially-enc.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"78MA7479","bookSection","2008","Kendall, Gary S.; Ardila, Mauricio","The Artistic Play of Spatial Organization: Spatial Attributes, Scene Analysis and Auditory Spatial Schemata","Computer Music Modeling and Retrieval. Sense of Sounds","978-3-540-85034-2 978-3-540-85035-9","","","http://link.springer.com/10.1007/978-3-540-85035-9_8","Electroacoustic music lacks a definitive vocabulary for describing its spatiality. Not only does it lack a vocabulary for describing the spatial attributes of individual sound sources, it lacks a vocabulary for describing how these attributes participate in artistic design and expression. Following work by Rumsey [15] the definition of spatial attributes is examined in the broader context of auditory scene analysis. A limited number of spatial attributes are found to be adequate to characterize the individual levels of organization nested within the auditory scene (levels that for acoustic music Rumsey labels as source, ensemble, room and scene). These levels are then viewed as products of both tangible spatial relationships and auditory spatial schemata, the recurrent patterns by which listeners understand the behavior of sound in space. In electroacoustic music the interrelationship of spatial attributes and spatial schemata is often engaged in a play of perceptual grouping that blurs and confounds distinctions like source and ensemble. Our ability to describe and categorize these complex interactions depends on having clear concepts and terminology so that we can recognize the crisscrossing of boundaries and the violation of conventions in this artistic interplay.","2008","2023-07-06 00:57:05","2023-07-19 23:49:08","2023-07-06 00:57:05","125-138","","","4969","","","The Artistic Play of Spatial Organization","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","ISSN: 0302-9743, 1611-3349 Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-85035-9_8","","/Users/minsik/Zotero/storage/874C2RG4/Kendall and Ardila - 2008 - The Artistic Play of Spatial Organization Spatial.pdf","","","","Kronland-Martinet, Richard; Ystad, Sølvi; Jensen, Kristoffer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QAEU6DYS","bookSection","2022","Asonitis, Tasos; Allmendinger, Richard; Benatan, Matt; Climent, Ricardo","SonOpt: Sonifying Bi-objective Population-Based Optimization Algorithms","Artificial Intelligence in Music, Sound, Art and Design","978-3-031-03788-7 978-3-031-03789-4","","","https://link.springer.com/10.1007/978-3-031-03789-4_1","We propose SonOpt, the first (open source) data sonification application for monitoring the progress of bi-objective populationbased optimization algorithms during search, to facilitate algorithm understanding. SonOpt provides insights into convergence/stagnation of search, the evolution of the approximation set shape, location of recurring points in the approximation set, and population diversity. The benefits of data sonification have been shown for various non-optimization related monitoring tasks. However, very few attempts have been made in the context of optimization and their focus has been exclusively on single-objective problems. In comparison, SonOpt is designed for biobjective optimization problems, relies on objective function values of non-dominated solutions only, and is designed with the user (listener) in mind; avoiding convolution of multiple sounds and prioritising ease of familiarizing with the system. This is achieved using two sonification paths relying on the concepts of wavetable and additive synthesis. This paper motivates and describes the architecture of SonOpt, and then validates SonOpt for two popular multi-objective optimization algorithms (NSGA-II and MOEA/D). Experience SonOpt yourself via https://github.com/tasos- a/SonOpt- 1.0 .","2022","2023-07-06 00:57:05","2023-07-19 11:33:28","2023-07-06 00:57:05","3-18","","","13221","","","SonOpt","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-031-03789-4_1","","/Users/minsik/Zotero/storage/IFXX5MMK/Asonitis et al. - 2022 - SonOpt Sonifying Bi-objective Population-Based Op.pdf","","","","Martins, Tiago; Rodríguez-Fernández, Nereida; Rebelo, Sérgio M.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IJ6RLFGU","bookSection","2010","Rubisch, Julian; Husinsky, Matthias; Raffaseder, Hannes","AllThatSounds: Associative Semantic Categorization of Audio Data","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_25","","2010","2023-07-06 00:57:05","2023-07-06 00:57:05","2023-07-06 00:57:05","483-491","","","5954","","","AllThatSounds","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_25","","/Users/minsik/Zotero/storage/Q2JYUDJG/Rubisch et al. - 2010 - AllThatSounds Associative Semantic Categorization.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4VMJAZJM","bookSection","2019","Arai, Kohei","Method for Audible Representation of Meteorological Data Derived from Remote Sensing Satellites","Intelligent Systems and Applications","978-3-030-01056-0 978-3-030-01057-7","","","http://link.springer.com/10.1007/978-3-030-01057-7_83","Method for audible representation of meteorological data derived from remote sensing satellites is proposed. Audible representation of multi-dimensional meteorological data layered wind direction and speed data derived from Earth observation satellite data is developed. Experimental results show rapidly changing meteorological data can be listened with the different musical scale (melody), harmony, rhythm and music instrument types. More importantly, weather analysts may much easily be aware of the inversely proportional relation between altitude and air temperature by hearing the multi-dimensional meteorological data.","2019","2023-07-06 00:57:05","2023-07-20 06:35:48","2023-07-06 00:57:05","1139-1149","","","869","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Advances in Intelligent Systems and Computing DOI: 10.1007/978-3-030-01057-7_83","","","","","","Arai, Kohei; Kapoor, Supriya; Bhatia, Rahul","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2GWR8TI3","journalArticle","2009","Valente, Luis; De Souza, Clarisse Sieckenius; Feijó, Bruno","Turn off the graphics: designing non-visual interfaces for mobile phone games","Journal of the Brazilian Computer Society","","0104-6500, 1678-4804","10.1007/BF03192576","https://journal-bcs.springeropen.com/articles/10.1007/BF03192576","Abstract             Mobile phones are a widespread platform for ICT applications because they are highly pervasive in contemporary society. Hence, we can think of mobile gaming as a serious candidate to being a prominent form of entertainment in the near future. However, most games (for computers, console and mobile devices) make extensive use of the visual medium, which tends to exclude visually-impaired users from the play. While mobile gaming could potentially reach many visually-impaired users, who are very familiar with this technology, currently there seems to be only very few alternatives for this community. In an attempt to explore new interactive possibilities for such users, this work presents an initial study on non-visual interfaces for mobile phone games. It is based on Semiotic Engineering principles, emphasizing communication through aural, tactile and gestural signs, and deliberately excluding visual information. Results include a number of issues that can be incorporated to a wider research agenda on mobile gaming accessibility, both for the visually-impaired and sighted.","2009-03","2023-07-06 00:57:05","2023-07-06 00:57:05","2023-07-06 00:57:05","45-58","","1","15","","J Braz Comp Soc","Turn off the graphics","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/7P5L73AN/Valente et al. - 2009 - Turn off the graphics designing non-visual interf.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UND37FYN","bookSection","2010","Mariette, Nicholas","Navigation Performance Effects of Render Method and Head-Turn Latency in Mobile Audio Augmented Reality","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_13","This study assessed participant performance of an outdoor navigation task using a mobile audio augmented reality system. Several quantitative performance measures and one subjective measure were used to compare the perceptual efficacy of Ambisonic and VBAP binaural rendering techniques, and a range of head-turn latencies. The study extends existing indoors research on the effects of head-turn latency for seated listeners. The pilot experiment found that a source capture radius of 2 meters significantly affected the sole participant’s navigation distance efficiency compared to other radii. The main experiment, using 8 participants, found that render method significantly affected all performance measures except subjective stability rating, while head-turn latency only affected mean track curvature and subjective stability. Results also showed an interaction in which the choice of rendering method mitigated or potentiated the effects of head-turn latency on perceived source stability.","2010","2023-07-06 00:57:05","2023-07-19 11:38:17","2023-07-06 00:57:05","239-265","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_13","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QQ9I4DFN","bookSection","2012","Nymoen, Kristian; Torresen, Jim; Godøy, Rolf Inge; Jensenius, Alexander Refsum","A Statistical Approach to Analyzing Sound Tracings","Speech, Sound and Music Processing: Embracing Research in India","978-3-642-31979-2 978-3-642-31980-8","","","http://link.springer.com/10.1007/978-3-642-31980-8_11","This paper presents an experiment on sound tracing, meaning an experiment on how people relate motion to sound. 38 participants were presented with 18 short sounds, and instructed to move their hands in the air while acting as though the sound was created by their hand motion. The hand motion of the participants was recorded, and has been analyzed using statistical tests, comparing results between different sounds, between different subjects, and between different sound classes. We have identified several relationships between sound and motion which are present in the majority of the subjects. A clear distinction was found in onset acceleration for motion to sounds with an impulsive dynamic envelope compared to non-impulsive sounds. Furthermore, vertical movement has been shown to be related to sound frequency, both in terms of spectral centroid and pitch. Moreover, a significantly higher amount of overall acceleration was observed for non-pitched sounds as compared to pitched sounds.","2012","2023-07-06 00:57:05","2023-07-21 05:02:57","2023-07-06 00:57:05","120-145","","","7172","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-31980-8_11","","/Users/minsik/Zotero/storage/X2IL8W9N/Nymoen et al. - 2012 - A Statistical Approach to Analyzing Sound Tracings.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer; Mohanty, Sanghamitra","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T78539B7","bookSection","2002","Lorho, Gaëtan; Hiipakka, Jarmo; Marila, Juha","Structured Menu Presentation Using Spatial Sound Separation","Human Computer Interaction with Mobile Devices","978-3-540-44189-2 978-3-540-45756-5","","","http://link.springer.com/10.1007/3-540-45756-9_51","This paper describes a technique to support user interaction in a hierarchical menu, based on spatial sound separation. A complex menu structure is represented in space using a limited number of sound positions obtained by stereo panning or 3-D audio processing techniques. Spatial organisation of menu items can be designed in a logical way to provide navigation cues to the user, independent of the menu item nature. Two different strategies for menu presentation and interaction are described and compared in this paper. Finally, an application of this technique to the navigation in a large music collection is considered. This case study is an interesting example of usage situation for which eyes-free interaction would be useful, for instance on a portable audio player using headphones and a small remote control.","2002","2023-07-06 00:57:05","2023-07-20 05:51:07","2023-07-06 00:57:05","419-424","","","2411","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-45756-9_51","","","","","","Paternò, Fabio","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HSPZQ8JW","journalArticle","1974","Cutting, James E.; Rosner, Burton S.","Categories and boundaries in speech and music*","Perception & Psychophysics","","0031-5117, 1532-5962","10.3758/BF03198588","http://link.springer.com/10.3758/BF03198588","Perceptual categories and boundaries arise when Ss respond to continuous variation on a physical dimension in a discontinuous fashion. It is more difficult to discriminate between members of the same category than to discriminate between members of different categories, even though the amount of physical difference between both pairs is the same. Speech stimuli have been the sole class of auditory signals to yield such perception; for example, each different consonant phoneme serves as a category label. Experiment I demonstrates that categories and boundaries occur for both speech and nonspeech stimuli differing in rise time. Experiment II shows that rise time cues categorical differences in both complex and simple nonspeech waveforms. Taken together, these results suggest that certain aspects of speech perception are intimately related to processes and mechanisms exploited in other domains. The many categories in speech may be based on categories that occur elsewhere in auditory perception.","1974-05","2023-07-06 00:58:15","2023-07-21 04:43:33","2023-07-06 00:58:15","564-570","","3","16","","Perception & Psychophysics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/JD2ZRUPN/Cutting and Rosner - 1974 - Categories and boundaries in speech and music.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W7D5UHU3","bookSection","2021","Paterson, Estrella; Sanderson, Penelope; Mohamed, Ismail; Salisbury, Isaac; Loeb, Robert G.; Paterson, Neil","Testing Interventions in a Medical Simulator: Challenges and Solutions","Proceedings of the 21st Congress of the International Ergonomics Association (IEA 2021)","978-3-030-74610-0 978-3-030-74611-7","","","https://link.springer.com/10.1007/978-3-030-74611-7_57","When testing medical interventions in a simulator, establishing an environment that allows findings to generalize to clinical settings; designing scenarios that are representative of clinical situations and that can be delivered consistently; and ensuring correct operation of simulator systems to ensure efficient data collection, can be challenging. We address these factors using the example of a study that tested auditory displays for the pulse oximeter conducted in a medical simulator with anesthesiologist participants. To establish fidelity of the clinical setting, the simulator was arranged as an operating room (OR) and actors trained to conduct surgery based on real cases. To ensure that scenarios were presented uniformly to each participant, we devised a novel approach: visual displays of vital signs were video-recorded and auditory displays then dubbed into the recordings that were displayed to participants via a monitor. Timing of actors’ actions was controlled by the simulator coordinator via verbal cues through earpieces according to a script that corresponded to the recorded displays. To allow for any technical malfunctions and to ensure effective data collection, we established redundant systems for recording scenarios and carried out piloting and training prior to conducting experimental sessions. Taking into account these factors, we were able to show that a novel auditory display was more effective for identifying oxygen saturation values than a standard display. Participants appeared to accept the simulator setting as natural and reacted in ways similar to behavior displayed in the OR. We conducted 20 sessions without any loss of data.","2021","2023-07-06 00:58:15","2023-07-21 04:55:25","2023-07-06 00:58:15","417-423","","","222","","","Testing Interventions in a Medical Simulator","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Networks and Systems DOI: 10.1007/978-3-030-74611-7_57","","","","","","Black, Nancy L.; Neumann, W. Patrick; Noy, Ian","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5EHGIRI7","bookSection","2006","Vialatte, François B.; Cichocki, Andrzej","Sparse Bump Sonification: A New Tool for Multichannel EEG Diagnosis of Mental Disorders; Application to the Detection of the Early Stage of Alzheimer’s Disease","Neural Information Processing","978-3-540-46484-6 978-3-540-46485-3","","","http://link.springer.com/10.1007/11893295_11","This paper investigates the use of sound and music as a means of representing and analyzing multichannel EEG recordings. Specific focus is given to applications in early detection and diagnosis of early stage of Alzheimer’s disease. We propose here a novel approach based on multi channel sonification, with a time-frequency representation and sparsification process using bump modeling. The fundamental question explored in this paper is whether clinically valuable information, not available from the conventional graphical EEG representation, might become apparent through an audio representation. Preliminary evaluation of the obtained music score – by sample entropy, number of notes, and synchronous activity – incurs promising results.","2006","2023-07-06 00:58:15","2023-07-21 04:36:58","2023-07-06 00:58:15","92-101","","","4234","","","Sparse Bump Sonification","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11893295_11","","","","","","King, Irwin; Wang, Jun; Chan, Lai-Wan; Wang, DeLiang","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TBGJTIZG","bookSection","2010","Neff, Flaithri; Mehigan, Tracey J.; Pitt, Ian","Accelerometer & Spatial Audio Technology: Making Touch-Screen Mobile Devices Accessible","Computers Helping People with Special Needs","978-3-642-14096-9 978-3-642-14097-6","","","http://link.springer.com/10.1007/978-3-642-14097-6_28","As mobile-phone design moves toward a touch-screen form factor, the visually disabled are faced with new accessibility challenges. The mainstream interaction model for touch-screen devices relies on the user having the ability to see spatially arranged visual icons, and to interface with these icons via a smooth glass screen. An inherent challenge for blind users with this type of interface is its lack of tactile feedback. In this paper we explore the concept of using a combination of spatial audio and accelerometer technology to enable blind users to effectively operate a touch-screen device. We discuss the challenges involved in representing icons using sound and we introduce a design framework that is helping us tease out some of these issues. We also outline a set of proposed user-studies that will test the effectiveness of our design using a Nokia N97. The results of these studies will be presented at ICCHP 2010.","2010","2023-07-06 00:58:15","2023-07-19 23:52:34","2023-07-06 00:58:15","170-177","","","6179","","","Accelerometer & Spatial Audio Technology","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-14097-6_28","","","","","","Miesenberger, Klaus; Klaus, Joachim; Zagler, Wolfgang; Karshmer, Arthur","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z95RLDJH","journalArticle","2020","Burdick, Kendall J.; Jorgensen, Seiver K.; Combs, Taylor N.; Holmberg, Megan O.; Kultgen, Samantha P.; Schlesinger, Joseph J.","SAVIOR ICU: sonification and vibrotactile interface for the operating room and intensive care unit","Journal of Clinical Monitoring and Computing","","1387-1307, 1573-2614","10.1007/s10877-019-00381-1","http://link.springer.com/10.1007/s10877-019-00381-1","Alarm fatigue is an issue for healthcare providers in the intensive care unit, and may result from desensitization of overbearing and under-informing alarms. To directly increase the overall identification of medical alarms and potentially contribute to a downstream decrease in the prevalence of alarm fatigue, we propose advancing alarm sonification by combining auditory and tactile stimuli to create a multisensory alarm. Participants completed four trials—two multisensory (auditory and tactile) and two unisensory (auditory). Analysis compared the unisensory trials to the multisensory trials based on the percentage of correctly identified point of change, direction of change and identity of three physiological parameters (indicated by different instruments): heart rate (drums), blood pressure (piano), blood oxygenation (guitar). A repeated-measures of ANOVA yielded a significant improvement in performance for the multisensory group compared to the unisensory group (p < 0.05). Specifically, the multisensory group had better performance in correctly identifying parameter (p < 0.05) and point of change (p < 0.05) compared to the unisensory group. Participants demonstrated a higher accuracy of identification with the use of multisensory alarms. Therefore, multisensory alarms may relieve the auditory burden of the medical environment and increase the overall quality of care and patient safety.","2020-08","2023-07-06 00:58:15","2023-07-20 06:46:47","2023-07-06 00:58:15","787-796","","4","34","","J Clin Monit Comput","SAVIOR ICU","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PZBV6PN8","bookSection","2010","Aramaki, Mitsuko; Gondre, Charles; Kronland-Martinet, Richard; Voinier, Thierry; Ystad, Sølvi","Imagine the Sounds: An Intuitive Control of an Impact Sound Synthesizer","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_21","In this paper we present a synthesizer developed for musical and Virtual Reality purposes that offers an intuitive control of impact sounds. A three layer control strategy is proposed for this purpose, where the top layer gives access to a control of the sound source through verbal descriptions, the middle layer to a control of perceptually relevant sound descriptors, while the bottom layer is directly linked to the parameters of the additive synthesis model. The mapping strategies between the parameters of the different layers are described. The synthesizer has been implemented using Max/MSP, offering the possibility to manipulate intrinsic characteristics of sounds in real-time through the control of few parameters.","2010","2023-07-06 00:58:15","2023-07-19 11:36:16","2023-07-06 00:58:15","408-421","","","5954","","","Imagine the Sounds","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_21","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3HA3BM84","bookSection","2013","Betsworth, Liam; Rajput, Nitendra; Srivastava, Saurabh; Jones, Matt","Audvert: Using Spatial Audio to Gain a Sense of Place","Human-Computer Interaction – INTERACT 2013","978-3-642-40497-9 978-3-642-40498-6","","","http://link.springer.com/10.1007/978-3-642-40498-6_35","We introduce Audvert – a system that facilitates serendipitous discovery and navigation through spatial audio; used to navigate and discover points of interest in large, unfamiliar indoor environments. Our main aim was to create a lightweight spatial audio display that can convey a sense of a place without complex point and select interactions. We conducted a preliminary study comparing two audio types to see which best suited sound localization and a study of Audvert used in a real world scenario. Our findings suggest that long continuous audio performs better than short intermittent audio for sound localisation. We also discover a change in behaviour when using the system, with a large percentage of users wanting to visit newly discovered shops after using the system. We discuss the findings and draw research conclusions.","2013","2023-07-06 00:58:15","2023-07-20 06:28:52","2023-07-06 00:58:15","455-462","","","8120","","","Audvert","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-40498-6_35","","/Users/minsik/Zotero/storage/YLWWBCSG/Betsworth et al. - 2013 - Audvert Using Spatial Audio to Gain a Sense of Pl.pdf","","","","Kotzé, Paula; Marsden, Gary; Lindgaard, Gitte; Wesson, Janet; Winckler, Marco","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JB55U9VL","bookSection","2009","Godøy, Rolf Inge","Geometry and Effort in Gestural Renderings of Musical Sound","Gesture-Based Human-Computer Interaction and Simulation","978-3-540-92864-5 978-3-540-92865-2","","","http://link.springer.com/10.1007/978-3-540-92865-2_23","As may be seen at concerts and in various everyday listening situations, people often make spontaneous gestures when listening to music. We believe these gestures are interesting to study because they may reveal important features of musical experience. In particular, hand movements may give us information on what features are perceived as salient by listeners. Based on various current ideas on embodied cognition, the aim of this paper is to argue that gestures are integral to music perception, and to present research in support of this. A conceptual model of separating geometry and effort is presented in order to better understand the variety of music-related gestures we may observe, leading up to some ideas on how to apply this conceptual model in present and future research.","2009","2023-07-06 00:58:15","2023-07-20 00:09:21","2023-07-06 00:58:15","205-215","","","5085","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-92865-2_23","","/Users/minsik/Zotero/storage/KJ6CQJNG/Godøy - 2009 - Geometry and Effort in Gestural Renderings of Musi.pdf","","","","Sales Dias, Miguel; Gibet, Sylvie; Wanderley, Marcelo M.; Bastos, Rafael","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WIVWW8SF","bookSection","2007","Jung, Ralf; Schwartz, Tim","A Location-Adaptive Human-Centered Audio Email Notification Service for Multi-user Environments","Human-Computer Interaction. HCI Intelligent Multimodal Interaction Environments","978-3-540-73108-5 978-3-540-73110-8","","","http://link.springer.com/10.1007/978-3-540-73110-8_36","In this paper, we introduce an application for a discreet notification of mobile persons in a multi-user environment. In particular we use the current user position to provide a personalized email notification with non-speech audio cues embedded in aesthetic background music. The notification is done in a peripheral way to avoid distration of other people in the surrounding.","2007","2023-07-06 00:58:15","2023-07-20 06:31:39","2023-07-06 00:58:15","340-348","","","4552","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73110-8_36","","/Users/minsik/Zotero/storage/QF6PF3DL/Jung and Schwartz - 2007 - A Location-Adaptive Human-Centered Audio Email Not.pdf","","","","Jacko, Julie A.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BVM9A9YC","journalArticle","2012","Ness, Steven; Reimer, Paul; Love, Justin; Schloss, W. Andrew; Tzanetakis, George","Sonophenology: A multimodal tangible interface for the sonification of phenological data at multiple time-scales","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-011-0066-4","http://link.springer.com/10.1007/s12193-011-0066-4","The study of periodic biological processes, such as when plants flower and birds arrive in the spring is known as Phenology. In recent years this field has gained interest from the scientific community because of the applicability of this data to the study of climate change and other ecological processes. In this paper we propose the use of tangible interfaces for interactive sonification with a specific example of a multimodal tangible interface consisting of a physical paper map and tracking of fiducial markers combined with a novel drawing interface. The designed interface enables one or more users to specify point queries with the map interface and to specify time queries with the drawing interface. This allows the user to explore both time and space while receiving immediate sonic feedback of their actions. This system can be used to study and explore the effects of climate change, both as tool to be used by scientists, and as a way to educate and involve members of the general public in a dynamic way in this research.","2012-05","2023-07-06 00:58:15","2023-07-20 07:03:17","2023-07-06 00:58:15","123-129","","3-4","5","","J Multimodal User Interfaces","Sonophenology","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SPNZRKB6","bookSection","2013","Li, Xin; Kong, Dehui; Zhang, Yong","OsgAL-Based Sound Source Management in Simulation Games","Intelligence Computation and Evolutionary Computation","978-3-642-31655-5 978-3-642-31656-2","","","https://link.springer.com/10.1007/978-3-642-31656-2_97","Sound rendering is a technique which uses auditory display to communicate information to a user and provides an alternative way of visualization. Traditional 3D sound rendering techniques are lower effective when the number and types of sound sources increase. In this paper, we propose a method of 3D sound source management based on osgAL toolkit, by using sound classification and XML document structure for sound properties. We allocate buffers for necessary sound sources and cull inaudible sound sources, which reduce system’s consumption of time and space. Experiments on train-driving simulation system show that our method provides a realistic and effective simulation.","2013","2023-07-06 00:58:15","2023-07-20 06:34:32","2023-07-06 00:58:15","723-732","","","180","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Advances in Intelligent Systems and Computing DOI: 10.1007/978-3-642-31656-2_97","","","","","","Du, Zhenyu","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EEY8DGEH","journalArticle","2023","Asonitis, Tasos; Allmendinger, Richard; Benatan, Matt; Climent, Ricardo","SonOpt: understanding the behaviour of bi-objective population-based optimisation algorithms through sound","Genetic Programming and Evolvable Machines","","1389-2576, 1573-7632","10.1007/s10710-023-09451-5","https://link.springer.com/10.1007/s10710-023-09451-5","Abstract                            We present an extension of SonOpt, the first ever openly available tool for the sonification of bi-objective population-based optimisation algorithms. SonOpt has already introduced benefits on the understanding of algorithmic behaviour by proposing the use of sound as a medium for the process monitoring of bi-objective optimisation algorithms. The first edition of SonOpt utilised two different sonification paths to provide information on convergence, population diversity, recurrence of objective values across consecutive generations and the shape of the approximation set. The present extension provides further insight through the introduction of a third sonification path, which involves hypervolume contributions to facilitate the understanding of the relative importance of non-dominated solutions. Using a different sound generation approach than the existing ones, this newly proposed sonification path utilizes pitch deviations to highlight the distribution of hypervolume contributions across the approximation set. To demonstrate the benefits of SonOpt we compare the sonic results obtained from two popular population-based multi-objective optimisation algorithms, Non-Dominated Sorting Genetic Algorithm (NSGA-II) and Multi-Objective Evolutionary Algorithm based on Decomposition (MOEA/D), and use a Multi-objective Random Search (MRS) approach as a baseline. The three algorithms are applied to numerous test problems and showcase how sonification can reveal various aspects of the optimisation process that may not be obvious from visualisation alone. SonOpt is available for download at               https://github.com/tasos-a/SonOpt-2.0               .","2023-06","2023-07-06 00:58:15","2023-07-06 00:58:15","2023-07-06 00:58:15","3","","1","24","","Genet Program Evolvable Mach","SonOpt","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/CKBAR2K3/Asonitis et al. - 2023 - SonOpt understanding the behaviour of bi-objectiv.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N9QNJSG4","bookSection","2015","Ng, Patrick; Nesbitt, Keith; Blackmore, Karen","Sound Improves Player Performance in a Multiplayer Online Battle Arena Game","Artificial Life and Computational Intelligence","978-3-319-14802-1 978-3-319-14803-8","","","http://link.springer.com/10.1007/978-3-319-14803-8_13","Sound in video games is often used by developers to enhance the visual experience on screen. Despite its importance in creating presence and improving visual screen elements, sound also plays an important role in providing additional information to a player when completing various game tasks. This preliminary study focuses on the use of informative sound in the popular multiplayer online battle arena game, Dota 2. Our initial results indicate that team performance improves with the use of sound. However, mixed results with individual performances were measured, with some individual performances better with sound and some better without sound.","2015","2023-07-06 00:58:15","2023-07-19 11:34:53","2023-07-06 00:58:15","166-174","","","8955","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-14803-8_13","","","","","","Chalup, Stephan K.; Blair, Alan D.; Randall, Marcus","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7VPZNNVJ","journalArticle","2014","Chima, Ranjit S.; Ortega, Rafael; Connor, Christopher W.","Melodic algorithms for pulse oximetry to allow audible discrimination of abnormal systolic blood pressures","Journal of Clinical Monitoring and Computing","","1387-1307, 1573-2614","10.1007/s10877-014-9558-6","http://link.springer.com/10.1007/s10877-014-9558-6","An anesthesiologist must remain vigilant of the patient’s clinical status, incorporating many independent physiological measurements. Oxygen saturation and heart rate are represented by continuous audible tones generated by the pulse oximeter, a mandated monitoring device. Other important clinical parameters—notably blood pressure—lack any audible representation beyond arbitrarily-configured threshold alarms. Attempts to introduce further continuous audible tones have apparently foundered; the complexity and interaction of these tones have exceeded the ability of clinicians to interpret them. Instead, we manipulate the tonal and rhythmic structure of the accepted pulse oximeter tone pattern melodically. Three melodic algorithms were developed to apply tonal and rhythmic variations to the continuous pulse oximeter tone, dependent on the systolic blood pressure. The algorithms distort the original audible pattern minimally, to facilitate comprehension of both the underlying pattern and the applied variations. A panel of anesthesia practitioners (attending anesthesiologists, residents and nurse anesthetists) assessed these algorithms in characterizing perturbations in cardiopulmonary status. Twelve scenarios, incorporating combinations of oxygen desaturation, bradycardia, tachycardia, hypotension and hypertension, were tested. A rhythmic variation in which additional auditory information was conveyed only at halftime intervals, with every other “beat” of the pulse oximeter, was strongly favored. The respondents also strongly favored the use of musical chords over single tones. Given three algorithms of tones embedded in the pulse oximeter signal, anesthesiologists preferred a melodic tone to signal a significant change in blood pressure.","2014-12","2023-07-06 00:58:15","2023-07-20 06:46:57","2023-07-06 00:58:15","597-603","","6","28","","J Clin Monit Comput","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QSKLV77D","bookSection","1997","Duce, David A.","Theory and practice in interactionally rich distributed systems","SOFSEM'97: Theory and Practice of Informatics","978-3-540-63774-5 978-3-540-69645-2","","","http://link.springer.com/10.1007/3-540-63774-5_105","This paper explores the notion of richness in interactive systems, at the device level, the application level and the theoretical level. The paper attempts to show something of the breadth of recent research and practice, with a somewhat unintentional leaning towards the domain of scientific visualization in a broad sense. Theoretical work which may lay the foundations for a systematic understanding of interactionally rich systems is also discussed, drawing on the literature of psychology and system modelling. The paper concludes with some thoughts on future directions for both theory and practice.","1997","2023-07-06 01:00:05","2023-07-21 05:00:15","2023-07-06 01:00:05","163-182","","","1338","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-63774-5_105","","","","","","Plášil, František; Jeffery, Keith G.","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M5RWJ99B","bookSection","2004","Holland, Simon; Day, Robert; Leplâtre, Grégory; Edwards, Alistair","Mobile HCI and Sound","Mobile Human-Computer Interaction - MobileHCI 2004","978-3-540-23086-1 978-3-540-28637-0","","","http://link.springer.com/10.1007/978-3-540-28637-0_73","Sound plays an increasingly varied and vital role in mobile and ubiquitous user interaction. One reason is the limited screen real-estate available in typical mobile devices. Another reason is that many mobile devices are used in minimal attention situations These are situations in which the user has only limited attention available for the interface: the user’s eyes may be busy elsewhere; and the user may be busy avoiding the normal hazards of moving around, and engaging with real-world tasks. In many circumstances, such interactions will involve non-speech audio and gesture to afford natural means of access to information, to other people, and to services and situations in the environment.","2004","2023-07-06 01:00:05","2023-07-21 04:30:05","2023-07-06 01:00:05","527-528","","","3160","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-28637-0_73","","","","","","Brewster, Stephen; Dunlop, Mark","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XKWIP2KY","bookSection","2018","Parseihian, Gaëtan; Aramaki, Mitsuko; Ystad, Sølvi; Kronland-Martinet, Richard","Exploration of Sonification Strategies for Guidance in a Blind Driving Game","Music Technology with Swing","978-3-030-01691-3 978-3-030-01692-0","","","http://link.springer.com/10.1007/978-3-030-01692-0_27","This paper explores the use of continuous auditory display for a dynamic guidance task. Through a driving game with blindfolded players, the success and the efficiency of a lane-keeping task in which no visual feedback is provided is observed. The results highlight the importance of the display information and reveal that a task-related rather than an error-related feedback should be used to enable the driver to finish the circuit. In terms of sound strategies, a first experiment explores the effect of two complex strategies (pitch and modulations) combined with a basic stereo strategy that informs the user about the distance and the direction to the target. The second experiment examines the influence of morphological sound attributes on the performance compared to the use of the spatial sound attributes alone. The results reveal the advantage of using morphological sound attributes for such kinds of applications.","2018","2023-07-06 01:00:05","2023-07-21 04:34:18","2023-07-06 01:00:05","413-428","","","11265","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-01692-0_27","","","","","","Aramaki, Mitsuko; Davies, Matthew E. P.; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K5U4XS5N","bookSection","2010","Tünnermann, René; Kolbe, Lukas; Bovermann, Till; Hermann, Thomas","Surface Interactions for Interactive Sonification","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_9","This paper presents novel interaction modes for Model-Based Sonification (MBS) via interactive surfaces. We first discuss possible interactions for MBS on a multi-touch surface. This is followed by a description of the Data Sonogram Sonification and the Growing Neural Gas Sonification Model and their implementation for the multi-touch interface. Modifications from the original sonification models such as the limited space scans are described and discussed with sonification examples. Videos showing interaction examples are provided. Furthermore, the presented system provides a basis for the implementation of known and novel sonification models. We discuss the available interaction modes with multi-touch surfaces and how these interactions can be profitably used to control spatial and non-spatial sonification models.","2010","2023-07-06 01:00:05","2023-07-19 11:38:43","2023-07-06 01:00:05","166-183","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_9","","/Users/minsik/Zotero/storage/U362RXUN/Tünnermann et al. - 2010 - Surface Interactions for Interactive Sonification.pdf","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AMSVZBWC","bookSection","2021","Sanghavi, Harsh; Jeon, Myounghoon; Nadri, Chihab; Ko, Sangjin; Sodnik, Jaka; Stojmenova, Kristina","Multimodal Takeover Request Displays for Semi-automated Vehicles: Focused on Spatiality and Lead Time","HCI in Mobility, Transport, and Automotive Systems","978-3-030-78357-0 978-3-030-78358-7","","","https://link.springer.com/10.1007/978-3-030-78358-7_22","To investigate the full potential of non-speech sounds, this study explored the effects of different multimodal takeover request displays in semi-automated vehicles. It used a mixed design - the visual and auditory notification lead time was within-subjects, whereas the auditory notification spatiality was between-subjects. The study was conducted in a motion-based driving simulator with 24 participants. All participants were engaged in four 9-min driving tasks in level-3 automated vehicle and simultaneously performed a non-driving related task (NDRT, online game). Each driving session contained three hazardous events with takeover request (in total 12 requests per user). The results showed that 3-s lead time evoked the fastest reaction time but caused high perceived workload and resulted in unsafe and non-comfortable maneuver. In terms of workload and maneuver, 7-s lead time showed better results than others. Auditory displays with directional information provided significantly better reaction times and reaction types. Subjective evaluation, on the other hand, did not show any significant differences between non-directional and directional displays. Additionally, the results showed that braking is a more common first reaction than steering, and that the NDRT did not influence the takeover request.","2021","2023-07-06 01:00:05","2023-07-20 05:43:44","2023-07-06 01:00:05","315-334","","","12791","","","Multimodal Takeover Request Displays for Semi-automated Vehicles","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-78358-7_22","","","","","","Krömker, Heidi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"539N4YQ8","journalArticle","2008","Amatriain, Xavier; Arumi, Pau; Garcia, David","A framework for efficient and rapid development of cross-platform audio applications","Multimedia Systems","","0942-4962, 1432-1882","10.1007/s00530-007-0109-6","http://link.springer.com/10.1007/s00530-007-0109-6","In this article, we present CLAM, a C++ software framework, that offers a complete development and research platform for the audio and music domain. It offers an abstract model for audio systems and includes a repository of processing algorithms and data types as well as all the necessary tools for audio and control input/output. The framework offers tools that enable the exploitation of all these features to easily build cross-platform applications or rapid prototypes for media processing algorithms and systems. Furthermore, included ready-to-use applications can be used for tasks such as audio analysis/synthesis, plug-in development, feature extraction or metadata annotation. CLAM represents a step forward over other similar existing environments in the multimedia domain. Nevertheless, it also shares models and constructs with many of those. These commonalities are expressed in the form of a metamodel for multimedia processing systems and a design pattern language.","2008-06","2023-07-06 01:00:05","2023-07-21 04:31:12","2023-07-06 01:00:05","15-32","","1","14","","Multimedia Systems","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YPFGQQEP","journalArticle","2005","Russo, Frank A.; Thompson, William Forde","The subjective size of melodic intervals over a two-octave range","Psychonomic Bulletin & Review","","1069-9384, 1531-5320","10.3758/BF03206445","http://link.springer.com/10.3758/BF03206445","Musically trained and untrained participants provided magnitude estimates of the size of melodic intervals. Each interval was formed by a sequence of two pitches that differed by between 50 cents (one half of a semitone) and 2,400 cents (two octaves) and was presented in a high or a low pitch register and in an ascending or a descending direction. Estimates were larger for intervals in the high pitch register than for those in the low pitch register and for descending intervals than for ascending intervals. Ascending intervals were perceived as larger than descending intervals when presented in a high pitch register, but descending intervals were perceived as larger than ascending intervals when presented in a low pitch register. For intervals up to an octave in size, differentiation of intervals was greater for trained listeners than for untrained listeners. We discuss the implications for psychophysical pitch scales and models of music perception.","2005-12","2023-07-06 01:00:05","2023-07-21 04:56:07","2023-07-06 01:00:05","1068-1075","","6","12","","Psychonomic Bulletin & Review","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/TBWHPQUY/Russo and Thompson - 2005 - The subjective size of melodic intervals over a tw.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8VHMZ9XJ","bookSection","2004","Tsukada, Koji; Yasumura, Michiaki","ActiveBelt: Belt-Type Wearable Tactile Display for Directional Navigation","UbiComp 2004: Ubiquitous Computing","978-3-540-22955-1 978-3-540-30119-6","","","http://link.springer.com/10.1007/978-3-540-30119-6_23","In this paper we propose a novel wearable interface called “ActiveBelt” that enables users to obtain multiple directional information with the tactile sense. Since the information provided by the tactile sense is relatively unobtrusive, it is suited for daily use in mobile environments. However, many existing systems don’t transmit complex information via the tactile sense. Most of them send only simple signals, such as vibration in cellular phones. ActiveBelt is a novel belt-type wearable tactile display that can transmit directional information. We have developed prototype systems and applications, evaluated system performance and usability, and demonstrated the possibility of practical use.","2004","2023-07-06 01:00:05","2023-07-21 05:08:08","2023-07-06 01:00:05","384-399","","","3205","","","ActiveBelt","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-30119-6_23","","/Users/minsik/Zotero/storage/LZFR2BWC/Tsukada and Yasumura - 2004 - ActiveBelt Belt-Type Wearable Tactile Display for.pdf","","","","Davies, Nigel; Mynatt, Elizabeth D.; Siio, Itiro","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WBKTGM69","bookSection","2003","Bornträger, Christian; Cheverst, Keith; Davies, Nigel; Dix, Alan; Friday, Adrian; Seitz, Jochen","Experiments with Multi-modal Interfaces in a Context-Aware City Guide","Human-Computer Interaction with Mobile Devices and Services","978-3-540-40821-5 978-3-540-45233-1","","","http://link.springer.com/10.1007/978-3-540-45233-1_10","In recent years there has been considerable research into the development of mobile context-aware applications. The canonical example of such an application is the context-aware tour-guide that offers city visitors information tailored to their preferences and environment. The nature of the user interface for these applications is critical to their success. Moreover, the user interface and the nature and modality of information presented to the user impacts on many aspects of the system’s overall requirements, such as screen size and network provision. Current prototypes have used a range of different interfaces developed in a largely ad-hoc fashion and there has been no systematic exploration of user preferences for information modality in mobile context-aware applications. In this paper we describe a series of experiments with multi-modal interfaces for context-aware city guides. The experiments build on our earlier research into the GUIDE system and include a series of field trials involving members of the general public. We report on the results of these experiments and extract design guid","2003","2023-07-06 01:00:05","2023-07-20 06:29:47","2023-07-06 01:00:05","116-130","","","2795","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-45233-1_10","","/Users/minsik/Zotero/storage/945X3CQ6/Bornträger et al. - 2003 - Experiments with Multi-modal Interfaces in a Conte.pdf","","","","Chittaro, Luca","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FA6KXPI9","bookSection","2009","Johannsen, Gunnar","Design of Visual and Auditory Human-Machine Interfaces with User Participation and Knowledge Support","Industrial Engineering and Ergonomics","978-3-642-01292-1 978-3-642-01293-8","","","https://link.springer.com/10.1007/978-3-642-01293-8_37","Human-machine interfaces for dynamic technical systems (such as industrial processes, vehicles, etc.) are a particularly important and complex subset of user interfaces. The design of these interfaces has become so sophisticated that it can, already for quite some time, no longer be handled in an intuitive fashion. The designer needs to possess a huge amount of multidisciplinary knowledge and experience with respect to the application domain of the respective technical process, the available automation and information technologies, the capabilities and limitations of the human users (human operators, maintenance personnel, and others), work psychological and organizational matters as well as ergonomic and cognitive engineering principles of good human-machine interface design.","2009","2023-07-06 01:00:05","2023-07-20 06:33:42","2023-07-06 01:00:05","499-509","","","","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-3-642-01293-8_37","","","","","","Schlick, Christopher M.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ELTPZ6A8","bookSection","2016","Eriksson, Martin Ljungdahl; Pareto, Lena","Sound Bubbles for Productive Office Work","Nordic Contributions in IS Research","978-3-319-43596-1 978-3-319-43597-8","","","http://link.springer.com/10.1007/978-3-319-43597-8_3","A growing number of organizations are moving towards more open and collaborative workplaces. In these offices workers share a common open space, often with flexible seating based on activities, so called activity-based offices. Most problems in these workplaces are related to sound. Thus, the question of how to design suitable acoustic environments, supporting both collaborative and individual work, has emerged. Noise-reduction approaches do not suffice. In this study we explored the possibility of adding context-sensitive, activity-based sound environments to enhance the office workplace. For this purpose, we developed the “sound bubble,” a prototype for individual work, sonically immersing the listener and generating a sensation of an encapsulating sonic environment. A total of 43 test subjects participated in an experience-based test using the sound bubble prototype while conducting self-selected, ordinary work tasks in their office landscape. Their behaviors during the test were observed and documented. All participants took a post-experience questionnaire about experiences working in the sound bubble, and two subjects were interviewed. The responses show that the sound bubble can enhance auditory work conditions for individual work that demands concentration.","2016","2023-07-06 01:02:06","2023-07-21 04:39:14","2023-07-06 01:02:06","29-42","","","259","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Business Information Processing DOI: 10.1007/978-3-319-43597-8_3","","","","","","Lundh Snis, Ulrika","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FBEY7BV7","bookSection","2014","Carvalho, Luiz Roberto; Cybis Pereira, Alice T.","Interface Design and Dynamic Audio","Human-Computer Interaction. Advanced Interaction Modalities and Techniques","978-3-319-07229-6 978-3-319-07230-2","","","http://link.springer.com/10.1007/978-3-319-07230-2_50","In the age of digital devices, text, image, sound, interactivity, blend themselves into a symbiotic and unique media, presenting a multifaceted specie of language called hypermedia. However, since many years ago, we have seen a notable emphasis on visual communication´s interfaces, and due to its limitations, products and services in design can often present inconsistencies when other sensory properties are relevant, as in the case of sound information. This over-emphasis on visual displays has constrained the development of interactive systems that are capable of making better use of the auditory modality. Recognizing the HCI as an integrating element of media and visual, sound and tactile metaphors, this study will demonstrate investigations that contextualize the role of sound into interactive environments by proposing an overview for the term interactive sound, suggesting its classification into direct-interactive and indirect-adaptative sounds, and pointing out its meanings and applications.","2014","2023-07-06 01:02:06","2023-07-20 06:30:18","2023-07-06 01:02:06","523-531","","","8511","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-07230-2_50","","","","","","Kurosu, Masaaki","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Kobsa, Alfred; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Doug; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4YAJCBYT","journalArticle","2023","Weaver, Alexis; Firmer, Genevieve; Motion, Alice; O’Regan, Jadey; O’Reilly, Chiara; Yeadon, Daniel","Sounding Out Science: the Sonaphor and Electronic Sound Design as a Learning Tool in Secondary Science","Postdigital Science and Education","","2524-485X, 2524-4868","10.1007/s42438-022-00321-4","https://link.springer.com/10.1007/s42438-022-00321-4","Abstract                            The divergent use of digital technologies provides an important opportunity for students to develop critical and postdigital approaches to learning. Despite the rising accessibility of music technology, creatively composed sound is a relatively underexplored educational tool compared to the musical elements of melody, rhythm, and lyrics. Sound’s ability to transfer spatial and temporal information renders it a transformative tool for teaching and learning. Embracing an interdisciplinary approach, our research explores the possibility of supplementing secondary science education with a sound-based learning tool which creatively interprets scientific concepts to increase comprehension and engagement. Building on the existing ways in which science is communicated through music and sound, we have developed the               Sonaphor               (abbreviated from ‘sonic metaphor’). This article will outline the capacity for experimental electronic sound design to increase engagement in contexts ranging from classrooms through to informal learning environments. We see potential for the               Sonaphor               as a learning tool that reignites wonder and curiosity in science; it combines learning and creativity in sound design and science, allowing learners to interact with, and create their own               Sonaphors               . Through exemplar               Sonaphors               , we highlight a proposed structure and discuss the importance of harmonious script, dialogue, and sound design. The flexibility of the digital medium and increasing ubiquity of sound recording and editing software presents an opportunity for               Sonaphors               to become ‘living’ digital objects that could be adapted by different narrators, sound designers, and artists for different cultures, languages, syllabi, and purposes that build inclusivity in science education and communication.","2023-04","2023-07-06 01:02:06","2023-07-06 01:02:06","2023-07-06 01:02:06","408-439","","2","5","","Postdigit Sci Educ","Sounding Out Science","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/IAFLZQMW/Weaver et al. - 2023 - Sounding Out Science the Sonaphor and Electronic .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XG9DPNZ7","bookSection","2013","Jeon, Myounghoon; Lee, Ju-Hwan","The Ecological AUI (Auditory User Interface) Design and Evaluation of User Acceptance for Various Tasks on Smartphones","Human-Computer Interaction. Interaction Modalities and Techniques","978-3-642-39329-7 978-3-642-39330-3","","","http://link.springer.com/10.1007/978-3-642-39330-3_6","With the rapid development of the touch screen technology, some usability issues of smartphones have been reported [1]. To tackle those user experience issues, there has been research on the use of non-speech sounds on the mobile devices [e.g., 2, 3-7]. However, most of them have focused on a single specific task of the device. Given the varying functions of the smartphone, the present study designed plausibly integrated auditory cues for diverse functions and evaluated user acceptance levels from the ecological interface design perspective. Results showed that sophisticated auditory design could change users’ preference and acceptance of the interface and the extent depended on usage contexts. Overall, participants gave significantly higher scores on the functional satisfaction and the fun scales in the sonically-enhanced smartphones than in the no-sound condition. The balanced sound design may free users from auditory pollution and allow them to use their devices more pleasantly.","2013","2023-07-06 01:02:06","2023-07-20 06:32:22","2023-07-06 01:02:06","49-58","","","8007","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-39330-3_6","","","","","","Kurosu, Masaaki","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S2TGDHJC","journalArticle","2022","Bruder, Alexandra L.; Rothwell, Clayton D.; Fuhr, Laura I.; Shotwell, Matthew S.; Edworthy, Judy Reed; Schlesinger, Joseph J.","The Influence of Audible Alarm Loudness and Type on Clinical Multitasking","Journal of Medical Systems","","0148-5598, 1573-689X","10.1007/s10916-021-01794-9","https://link.springer.com/10.1007/s10916-021-01794-9","In high-consequence industries such as health care, auditory alarms are an important aspect of an informatics system that monitors patients and alerts providers attending to multiple concurrent tasks. Alarms levels are unnecessarily high and alarm signals are uninformative. In a laboratory-based task setting, we studied 25 anesthesiology residents’ responses to auditory alarms in a multitasking paradigm comprised of three tasks: patient monitoring, speech perception/intelligibility, and visual vigilance. These tasks were in the presence of background noise plus/minus music, which served as an attention-diverting stimulus. Alarms signified clinical decompensation and were either conventional alarms or a novel informative auditory icon alarm. Both alarms were presented at four different levels. Task performance (accuracy and response times) were analyzed using logistic and linear mixed-effects regression. Salient findings were 1), the icon alarm had similar performance to the conventional alarm at a +2 dB signal-to-noise-ratio (SNR) (accuracy: OR 1.21 (95% CI 0.88, 1.67), response time: 0.04 s at 2 dB (95% CI: –0.16, 0.24), which is a much lower level than current clinical environments; 2) the icon alarm was associated with 27% greater odds (95% CI: 18%, 37%) of correctly addressing the vigilance task, regardless of alarm SNR, suggesting crossmodal/multisensory multitasking benefits; and 3) compared to the conventional alarm, the icon alarm was associated with an absolute improvement in speech perception of 4% in the presence of an attention-diverting auditory stimulus (p = 0.031). These findings suggest that auditory icons can provide multitasking benefits in cognitively demanding clinical environments.","2022-01","2023-07-06 01:02:06","2023-07-20 06:47:55","2023-07-06 01:02:06","5","","1","46","","J Med Syst","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CEYQZWFG","journalArticle","2010","Nordahl, Rolf","Evaluating Environmental Sounds from a Presence Perspective for Virtual Reality Applications","EURASIP Journal on Audio, Speech, and Music Processing","","1687-4714, 1687-4722","10.1155/2010/426937","http://asmp.eurasipjournals.com/content/2010/1/426937","We propose a methodology to design and evaluate environmental sounds for virtual environments. We propose to combine physically modeled sound events with recorded soundscapes. Physical models are used to provide feedback to users’ actions, while soundscapes reproduce the characteristic soundmarks of an environment. In this particular case, physical models are used to simulate the act of walking in the botanical garden of the city of Prague, while soundscapes are used to reproduce the particular sound of the garden. The auditory feedback designed was combined with a photorealistic reproduction of the same garden. A between-subject experiment was conducted, where 126 subjects participated, involving six different experimental conditions, including both uni- and bimodal stimuli (auditory and visual). The auditory stimuli consisted of several combinations of auditory feedback, including static sound sources as well as self-induced interactive sounds simulated using physical models. Results show that subjects’ motion in the environment is significantly enhanced when dynamic sound sources and sound of egomotion are rendered in the environment.","2010","2023-07-06 01:02:06","2023-07-21 07:43:57","2023-07-06 01:02:06","1-10","","","2010","","EURASIP Journal on Audio, Speech, and Music Processing","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/QH6YUYQZ/Nordahl - 2010 - Evaluating Environmental Sounds from a Presence Pe.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ATKGGCAW","bookSection","2010","Larsson, Pontus","Tools for Designing Emotional Auditory Driver-Vehicle Interfaces","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_1","Auditory interfaces are often used in vehicles to inform and alert the driver of various events and hazards. When designed properly, such interfaces can e.g. reduce reaction times and increase the impression of quality of the vehicle. In this paper it is argued that emotional response is an important aspect to consider when designing auditory driver-vehicle interfaces. This paper discusses two applications developed to investigate the emotional dimensions of auditory interfaces. EarconSampler is a tool for designing and modifying earcons. It allows for creating melodic patterns of wav-snippets and adjustment of parameters such as tempo and pitch. It also contains an analysis section where sound quality parameters, urgency and emotional response to the sound is calculated / predicted. SoundMoulder is another tool which offers extended temporal and frequency modifications of earcons. The primary idea with this application is to study how users design sounds given a desired emotional response.","2010","2023-07-06 01:02:06","2023-07-19 11:38:03","2023-07-06 01:02:06","1-11","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_1","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BMI8HSFS","bookSection","2012","Shahrul Azmi, Mohd Yusof; Nor Idayu, M.; Roshidi, D.; Yaakob, A. R.; Yaacob, Sazali","Noise Robustness of Spectrum Delta (SpD) Features in Malay Vowel Recognition","Computer Applications for Communication, Networking, and Digital Contents","978-3-642-35593-6 978-3-642-35594-3","","","http://link.springer.com/10.1007/978-3-642-35594-3_38","In Malaysia, there is increasing number of speech recognition researchers focusing on developing independent speaker speech recognition systems that uses Malay Language which are noise robust and accurate. The performance of speech recognition application under adverse noisy condition often becomes the topic of interest among speech recognition researchers regardless of the languages in use. This paper present a study of noise robust capability of an improved vowel feature extraction method called Spectrum Delta (SpD). The features are extracted from both original data and noise-added data and classified using three classifiers; (i) Multinomial Logistic Regression (MLR), (ii) K-Nearest Neighbors (k-NN) and (iii) Linear Discriminant Analysis (LDA). Results show that the proposed SpD is robust towards noise and LDA performs the best in overall vowel classification compared to MLR and k-NN in terms of robustness capability especially with signal-to-noise (SNR) above 20dB.","2012","2023-07-06 01:02:06","2023-07-19 23:47:52","2023-07-06 01:02:06","270-277","","","350","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Communications in Computer and Information Science DOI: 10.1007/978-3-642-35594-3_38","","","","","","Kim, Tai-hoon; Ko, Dae-sik; Vasilakos, Thanos; Stoica, Adrian; Abawajy, Jemal","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L7RPM8TN","bookSection","2009","McGookin, David; Brewster, Stephen; Priego, Pablo","Audio Bubbles: Employing Non-speech Audio to Support Tourist Wayfinding","Haptic and Audio Interaction Design","978-3-642-04075-7 978-3-642-04076-4","","","http://link.springer.com/10.1007/978-3-642-04076-4_5","We introduce the concept of Audio Bubbles - virtual spheres filled with audio that are geocentered on physical landmarks, providing navigational homing information for a user to more easily locate the landmark. We argue that the way in which tourists navigate is not well supported by traditional visual maps, and that Audio Bubbles better support the serendipitous discovery and homing behaviours exhibited in such tourist activities. We present a study comparing Audio Bubbles to a visual map in a real world navigation task. Navigation with Audio Bubbles appeared to be faster and was preferred by most of the participants. We discuss the findings and outline our future development plans.","2009","2023-07-06 01:02:06","2023-07-20 00:15:20","2023-07-06 01:02:06","41-50","","","5763","","","Audio Bubbles","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-04076-4_5","","/Users/minsik/Zotero/storage/KI5UAKPT/McGookin et al. - 2009 - Audio Bubbles Employing Non-speech Audio to Suppo.pdf","","","","Altinsoy, M. Ercan; Jekosch, Ute; Brewster, Stephen","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W5PHDBHW","bookSection","2012","Sade, Jack; Naz, Komal; Plaza, Malgorzata","Enhancing Audio Description: A Value Added Approach","Computers Helping People with Special Needs","978-3-642-31521-3 978-3-642-31522-0","","","http://link.springer.com/10.1007/978-3-642-31522-0_40","Audio Description makes films, shows and TV programs accessible to visually impaired audience. It is expensive so wide adoption of this technology is not practical. Canadian Radio-television and Telecommunications Commission requires that broadcaster describes a minimum of four hours of primetime programming a week. Production companies do not see any incentives to move beyond the required minimum. This paper investigates the possibility of making AD profitable by making a described movie, show or program attractive to all kind of audiences including visually impaired. We argue that AD can become a revenue generation product widely adopted by production companies.","2012","2023-07-06 01:02:06","2023-07-19 23:52:43","2023-07-06 01:02:06","270-277","","","7382","","","Enhancing Audio Description","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-31522-0_40","","","","","","Miesenberger, Klaus; Karshmer, Arthur; Penaz, Petr; Zagler, Wolfgang","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X24NFCC3","bookSection","2018","Pedersen, Jon Ram Bruun; Serafin, Stefania; Grani, Francesco","Investigating the Role of Auditory Feedback in a Multimodal Biking Experience","Music Technology with Swing","978-3-030-01691-3 978-3-030-01692-0","","","http://link.springer.com/10.1007/978-3-030-01692-0_22","In this paper, we investigate the role of auditory feedback in a↵ecting perception of e↵ort while biking in a virtual environment. Subjects were biking on a stationary chair bike, while exposed to 3D renditions of a recumbent bike inside a virtual environment (VE). The VE simulated a park and was created in the Unity5 engine. While biking, subjects were exposed to 9 kinds of auditory feedback (3 amplitude levels with three di↵erent filters) which were continuously triggered corresponding to pedal speed, representing the sound of the wheels and bike/chain mechanics. Subjects were asked to rate the perception of exertion using the Borg RPE scale. Results of the experiment showed that most subjects perceived a di↵erence in mechanical resistance from the bike between conditions, but did not consciously notice the variations of the auditory feedback, although these were significantly varied. This points towards interesting perspectives for subliminal perception potential for auditory feedback for VR exercise purposes.","2018","2023-07-06 01:02:06","2023-07-21 04:34:31","2023-07-06 01:02:06","327-337","","","11265","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-01692-0_22","","/Users/minsik/Zotero/storage/CQESQEXV/Pedersen et al. - 2018 - Investigating the Role of Auditory Feedback in a M.pdf","","","","Aramaki, Mitsuko; Davies, Matthew E. P.; Kronland-Martinet, Richard; Ystad, Sølvi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K8QII6TR","journalArticle","2008","Bormann, Karsten","Visuals are not what they look","Virtual Reality","","1359-4338, 1434-9957","10.1007/s10055-007-0068-4","http://link.springer.com/10.1007/s10055-007-0068-4","When developing virtual environments (VEs), most effort goes into developing the visuals. For many, the ideal is to create virtual worlds of photo-realistic quality or otherwise being of high fidelity. The purpose is to make the VE seem real to the user. This paper takes a closer look at subjects’ ratings of the visuals, and of the extent to which the VE feels real to the subjects, in the context of an experiment on audio in which subjects were to perform two search tasks: the first in an ordinary, textured house; the second in a bare structure consisting almost exclusively of the barren, white walls. Audio was never relevant to the search task in the first experiment, while in the second experiment it was relevant to the search task for half of the subjects. Subjects for whom audio was irrelevant to both their search tasks rated their visual involvement as large in the barren VE as in the higher quality one. However, subjects for which audio was relevant to their search task in the second experiment saw their visual involvement plummet, while their auditory involvement surged. Finally, the extent to which the VE felt real to the subjects did not correlate with their visual involvement but instead showed a strong correlation with the extent to which the interaction felt natural.","2008-06","2023-07-06 01:02:06","2023-07-21 05:14:11","2023-07-06 01:02:06","115-123","","2","12","","Virtual Reality","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5UVEPKC4","bookSection","2013","Frisson, Christian; Keyaerts, Gauthier; Grisard, Fabien; Dupont, Stéphane; Ravet, Thierry; Zajéga, François; Colmenares Guerra, Laura; Todoroff, Todor; Dutoit, Thierry","MashtaCycle: On-Stage Improvised Audio Collage by Content-Based Similarity and Gesture Recognition","Intelligent Technologies for Interactive Entertainment","978-3-319-03891-9 978-3-319-03892-6","","","http://link.springer.com/10.1007/978-3-319-03892-6_14","In this paper we present the outline of a performance in-progress. It brings together the skilled musical practices from Belgian audio collagist Gauthier Keyaerts aka Very Mash’ta; and the realtime, content-based audio browsing capabilities of the AudioCycle and LoopJam applications developed by the remaining authors. The tool derived from AudioCycle named MashtaCycle aids the preparation of collections of stem audio loops before performances by extracting content-based features (for instance timbre) used for the positioning of these sounds on a 2D visual map. The tool becomes an embodied on-stage instrument, based on a user interface which uses a depth-sensing camera, and augmented with the public projection of the 2D map. The camera tracks the position of the artist within the sensing area to trigger sounds similarly to the LoopJam installation. It also senses gestures from the performer interpreted with the Full Body Interaction (FUBI) framework, allowing to apply sound effects based on bodily movements. MashtaCycle blurs the boundary between performance and preparation, navigation and improvisation, installations and concerts.","2013","2023-07-06 01:02:06","2023-07-20 06:35:57","2023-07-06 01:02:06","114-123","","","124","","","MashtaCycle","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering DOI: 10.1007/978-3-319-03892-6_14","","","","","","Mancas, Matei; D’ Alessandro, Nicolas; Siebert, Xavier; Gosselin, Bernard; Valderrama, Carlos; Dutoit, Thierry","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XG8N6N58","bookSection","2014","Stoll, Thomas M.","Genomic: Evolving Sound Treatments Using Genetic Algorithms","Evolutionary and Biologically Inspired Music, Sound, Art and Design","978-3-662-44334-7 978-3-662-44335-4","","","http://link.springer.com/10.1007/978-3-662-44335-4_10","There are many systems for the evolution of creative musical material, that create and/or manipulate musical score data or synthesis parameters with a variety of techniques. This paper aims to add the technique of corpus-based sound sampling and processing to the list of applications used in conjunction with genetic algorithms. Genomic, a simple system for evolving sound treatment parameters, is presented, along with two simple use cases. Finally, a more complex process is outlined where sound treatment parameters are evolved and stored in a database with associated metadata for further organization and compositional use.","2014","2023-07-06 01:03:18","2023-07-20 00:04:37","2023-07-06 01:03:18","107-118","","","8601","","","Genomic","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-662-44335-4_10","","","","","","Romero, Juan; McDermott, James; Correia, João","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R4X6HWAY","bookSection","2010","Kobayashi, Yosuke; Kondo, Kazuhiro; Nakagawa, Kiyoshi","Intelligibility of HE-AAC Coded Japanese Words with Various Stereo Coding Modes in Virtual 3D Audio Space","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_12","In this paper, we investigated the influence of stereo coding on Japanese speech localized in virtual 3-D space. We encoded localized speech using joint stereo and parametric stereo modes within the HE-AAC encoder. First, we tested subjective quality of localized speech at various azimuths on the horizontal plane relative to the listener using the standard MUSHRA tests. We compared the encoded localized speech quality with various stereo encoding modes. The joint stereo mode showed significantly higher MUSHRA scores than the parametric stereo mode at azimuths of ±45 degrees. Next, the Japanese word intelligibility tests were conducted using the Japanese Diagnostic Rhyme Tests. Test speech was first localized at 0 and ±45 degrees and compared with localized speech with no coding. Parametric stereo-coded speech showed lower scores when localized at -45 degrees, but all other speech showed no difference between speech samples with no coding. Next, test speech was localized in front, while competing noise was localized at various angles. The two stereo coding modes with bit rates of 56, 32, and 24 kbps were tested. In most cases, these conditions show just as good intelligibility as speech with no encoding at all noise azimuths. This shows that stereo coding has almost no effect on the intelligibility in the bit rate range tested.","2010","2023-07-06 01:03:18","2023-07-19 11:37:56","2023-07-06 01:03:18","219-238","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_12","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7D837YJM","journalArticle","1977","Siegel, Jane A.; Siegel, William","Absolute identification of notes and intervals by musicians","Perception & Psychophysics","","0031-5117, 1532-5962","10.3758/BF03198717","http://link.springer.com/10.3758/BF03198717","Speech sounds are judged reliably and absolutely, while the judgment of nonspeech stimuli, such as tones, is thought to be unreliable and dependent on contextual cues. Here we demonstrated that the judgment of tonal stimuli may also be reliable and absolute, provided that the subjects are trained musicians, In Experiment 1, musicians with relative pitch identified 21 tonal intervals ranging from unison to major third, and the resulting identification functions were similar to those that have been previously obtained for speech. In Experiment 3, the judgment of intervals by musicians was shown to be free of context effects, since the best subjects gave virtually identical judgments to the same intervals in two stimulus contexts, Similar results were obtained in Experiments 2 and 4 for the judgment of single tones by possessors of absolute pitch. Performance with both notes and intervals by nonmusicians, however, was unreliable and greatly influenced by context. These findings suggest that musicians acquire categories for pitch that are functionally similar to phonemic categories for speech.","1977-03","2023-07-06 01:03:18","2023-07-21 04:44:37","2023-07-06 01:03:18","143-152","","2","21","","Perception & Psychophysics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/922I9UGV/Siegel and Siegel - 1977 - Absolute identification of notes and intervals by .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"REW3NU2H","bookSection","2017","Jeon, Myounghoon","Aesthetic Computing for Representation of the Computing Process and Expansion of Perceptual Dimensions: Cases for Art, Education, and Interfaces","Interactivity, Game Creation, Design, Learning, and Innovation","978-3-319-55833-2 978-3-319-55834-9","","","http://link.springer.com/10.1007/978-3-319-55834-9_36","With the advances of technologies, the application of computing to aesthetics has rapidly increased. However, little effort has been put to apply aesthetics to computing. Aesthetic Computing is an attempt to fill that research gap. The present paper revisits and elaborates its concept, and highlights “embodiment” as a new format of representation of Aesthetic Computing. The present paper also describes how embodiment can provide more opportunities for accessibility and personalization of computing across different projects – art, STEAM education, and in-vehicle interfaces. Finally, more aesthetic components in Aesthetic Computing are discussed for future research.","2017","2023-07-06 01:03:18","2023-07-20 06:37:30","2023-07-06 01:03:18","305-313","","","196","","","Aesthetic Computing for Representation of the Computing Process and Expansion of Perceptual Dimensions","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering DOI: 10.1007/978-3-319-55834-9_36","","","","","","Brooks, Anthony L.; Brooks, Eva","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EDULT8TW","bookSection","2010","Sanderson, Penelope M.","The Power and the Puzzles of Auditory Interfaces","Human-Computer Interaction","978-3-642-15230-6 978-3-642-15231-3","","","http://link.springer.com/10.1007/978-3-642-15231-3_1","Auditory interfaces are increasingly prevalent in work and everyday environments. I survey recent uses of non-speech auditory interfaces and advances in knowledge about them, highlighting research from The University of Queensland.","2010","2023-07-06 01:03:18","2023-07-20 05:55:14","2023-07-06 01:03:18","1-2","","","332","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: IFIP Advances in Information and Communication Technology DOI: 10.1007/978-3-642-15231-3_1","","/Users/minsik/Zotero/storage/KUZUEQMY/Sanderson - 2010 - The Power and the Puzzles of Auditory Interfaces.pdf","","","","Forbrig, Peter; Paternó, Fabio; Mark Pejtersen, Annelise","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YXP4MDI3","bookSection","2013","Morrell, Martin J.; Reiss, Joshua D.","Two-Dimensional Hybrid Spatial Audio Systems with User Variable Controls of Sound Source Attributes","From Sounds to Music and Emotions","978-3-642-41247-9 978-3-642-41248-6","","","http://link.springer.com/10.1007/978-3-642-41248-6_4","This paper presents two novel hybrid spatial audio systems demonstrated for use in two-dimensional applications with their scalability to three-dimensions. The emphasis of these hybrid systems is to give further creative freedom to a composer, sound engineer or sound designer. The systems are principally based on the end result of Ambisonics spatial audio reproduction systems. Since Ambisonics systems are used primarily for temporary sound installations and exhibits, the use of B-Format can be unnecessary. Therefore these systems revert to producing channel based content rather than sound field content that is later separately decoded. The presented systems use the decoder as a real-time sound manipulation feature on a per sound source basis. A comparison is drawn between the two systems and each method is described as to how it can be used as part of a standard music production workflow.","2013","2023-07-06 01:03:18","2023-07-20 00:06:01","2023-07-06 01:03:18","58-81","","","7900","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-41248-6_4","","","","","","Aramaki, Mitsuko; Barthet, Mathieu; Kronland-Martinet, Richard; Ystad, Sølvi","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MP3AE3WK","bookSection","2012","Wu, Victor K. Y.; Campbell, Roy H.","3D Audio Interface for Rich Mobile Web Experiences","Mobile Computing, Applications, and Services","978-3-642-29335-1 978-3-642-29336-8","","","http://link.springer.com/10.1007/978-3-642-29336-8_1","We propose a novel paradigm of consuming rich web content in a mobile setting (which are often eyes-free), through a predominantly 3D audio interface. Web content, formatted in audio, is streamed via the mobile device’s network connection, and placed virtually in a 3D audio space. The user moves in the virtual space, using a variety of human computer interaction (HCI) means, such as voice input, touching, rotating, and shaking the device, as well as hand and head gesturing. We provide applications benefitting from this paradigm of rich mobile 3D audio web consumption. We provide system designs, and a system architecture for mobile 3D audio. Finally, we implement our ideas in a system prototype using the Apple iOS mobile platform.","2012","2023-07-06 01:03:18","2023-07-21 04:29:57","2023-07-06 01:03:18","1-16","","","76","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering DOI: 10.1007/978-3-642-29336-8_1","","","","","","Gris, Martin; Yang, Guang","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GJQG4G2V","bookSection","1998","Cohen, Michael; Herder, Jens","Symbolic representations of exclude and include for audio sources and sinks: Figurative suggestions of & and &","Virtual Environments ’98","978-3-211-83233-2 978-3-7091-7519-4","","","http://link.springer.com/10.1007/978-3-7091-7519-4_23","Shared virtual environments require generalized control of user-dependent media streams. Traditional audio mixing idioms for enabling and disabling various sources employ and functions, which, along with selectively disable or focus on respective channels. Exocentric interfaces which explicitly model not only spatial audio sources, but also location, orientation, directivity, and multiplicity of sinks, motivate the generalization of & to exclude and include, manifested for sinks as & a narrowing of stimuli by explicitly blocking out and/or concentrating on selected entities. This paper introduces figurative representations of these functions, virtual hands to be clasped over avatars’ ears and mouths, with orientation suggesting the nature of the blocking. Applications include groupware for collaboration and teaching, teleconferencing and chat spaces, and authoring and manipulation of distributed virtual environments.","1998","2023-07-06 01:03:18","2023-07-21 05:13:47","2023-07-06 01:03:18","235-242","","","","","","Symbolic representations of exclude and include for audio sources and sinks","","","","","Springer Vienna","Vienna","","","","","","DOI.org (Crossref)","","Series Title: Eurographics DOI: 10.1007/978-3-7091-7519-4_23","","","","","","Göbel, Martin; Landauer, Jürgen; Lang, Ulrich; Wapler, Matthias","Hansmann, W.; Hewitt, W. T.; Purgathofer, W.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X9L8BF7J","journalArticle","2015","Bakker, Saskia; Van Den Hoven, Elise; Eggen, Berry","Peripheral interaction: characteristics and considerations","Personal and Ubiquitous Computing","","1617-4909, 1617-4917","10.1007/s00779-014-0775-2","http://link.springer.com/10.1007/s00779-014-0775-2","In everyday life, we are able to perceive information and perform physical actions in the background or periphery of attention. Inspired by this observation, several researchers have studied interactive systems that display digital information in the periphery of attention. To broaden the scope of this research direction, a few recent studies have focused on interactive systems that can not only be perceived in the background but also enable users to physically interact with digital information in their periphery. Such peripheral interaction designs can support computing technology to fluently embed in and become a meaningful part of people’s everyday routines. With the increasing ubiquity of technology in our everyday environment, we believe that this direction is highly relevant nowadays. This paper presents an in-depth analysis of three case studies on peripheral interaction. These case studies involved the design and development of peripheral interactive systems and deployment of these systems in the real context of use for a number of weeks. Based on the insights gained through these case studies, we discuss generalized characteristics and considerations for peripheral interaction design and evaluation. The aim of the work presented in this paper is to support interaction design researchers and practitioners in anticipating and facilitating peripheral interaction with the designs they are evaluating or developing.","2015-01","2023-07-06 01:03:18","2023-07-21 04:46:17","2023-07-06 01:03:18","239-254","","1","19","","Pers Ubiquit Comput","Peripheral interaction","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/MV4DGFE9/Bakker et al. - 2015 - Peripheral interaction characteristics and consid.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LY38TXA9","journalArticle","2004","Karjalainen, Matti; Erkut, Cumhur","Digital Waveguides versus Finite Difference Structures: Equivalence and Mixed Modeling","EURASIP Journal on Advances in Signal Processing","","1687-6180","10.1155/S1110865704401176","https://asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704401176","Digital waveguides and finite difference time domain schemes have been used in physical modeling of spatially distributed systems. Both of them are known to provide exact modeling of ideal one-dimensional (1D) band-limited wave propagation, and both of them can be composed to approximate two-dimensional (2D) and three-dimensional (3D) mesh structures. Their equal capabilities in physical modeling have been shown for special cases and have been assumed to cover generalized cases as well. The ability to form mixed models by joining substructures of both classes through converter elements has been proposed recently. In this paper, we formulate a general digital signal processing (DSP)-oriented framework where the functional equivalence of these two approaches is systematically elaborated and the conditions of building mixed models are studied. An example of mixed modeling of a 2D waveguide is presented.","2004-12","2023-07-06 01:03:18","2023-07-20 00:00:15","2023-07-06 01:03:18","561060","","7","2004","","EURASIP J. Adv. Signal Process.","Digital Waveguides versus Finite Difference Structures","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/RXBKQXJN/Karjalainen and Erkut - 2004 - Digital Waveguides versus Finite Difference Struct.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"259MMLSK","journalArticle","2022","Su, Isabelle; Hattwick, Ian; Southworth, Christine; Ziporyn, Evan; Bisshop, Ally; Mühlethaler, Roland; Saraceno, Tomás; Buehler, Markus J.","Interactive exploration of a hierarchical spider web structure with sound","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-021-00375-x","https://link.springer.com/10.1007/s12193-021-00375-x","3D spider webs exhibit highly intricate fiber architectures and owe their outstanding performance to a hierarchical organization that spans orders of magnitude in length scale from the molecular silk protein, to micrometer-sized fibers, and up to cm-scale web. Similarly, but in a completely different physical manifestation, music has a hierarchical structure composed of elementary sine wave building blocks that can be combined with other waveforms to create complex timbres, which are then arranged within larger-scale musical compositions. Although apparently different, spider webs and music have many similarities, as we point out in this work. Here, we propose an intuitive and interactive way to explore and visualize a 3D Cyrtophora citricola spider web geometry that has been digitally modeled with micron-scale details from full-scale laboratory experiments. We use model-based sonification to translate the web architecture into sound, allowing for aural perception and interpretation of its essential topological features. We implement this sonification using Unity3D and Max/MSP to create an interactive spider web environment in which a user travels through a virtual spider web. Each silk fiber in their field of view is sonified using different sine waves. Together, the sonified fibers create new and more complex timbres that reflects the architecture of 3D spider webs. These concepts are implemented into a spider web-based instrument for live performances, art installations and data exploration. It provides an unprecedented and creative way to immerse the composer, audience and user in an immersive multimedia experience generated by the complexity of a 3D spider web.","2022-03","2023-07-06 01:03:18","2023-07-20 07:04:41","2023-07-06 01:03:18","71-85","","1","16","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/WZ9B7IRU/Su et al. - 2022 - Interactive exploration of a hierarchical spider w.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DK7N4VED","journalArticle","2022","Misdariis, N.; Özcan, E.; Grassi, M.; Pauletto, S.; Barrass, S.; Bresin, R.; Susini, P.","Sound experts’ perspectives on astronomy sonification projects","Nature Astronomy","","2397-3366","10.1038/s41550-022-01821-w","https://www.nature.com/articles/s41550-022-01821-w","The Audible Universe project aims to create dialogue between two scientific domains investigating two distinct research objects: stars and sound. It has been instantiated within a collaborative workshop that began to mutually acculturate the two communities, by sharing and transmitting respective knowledge, skills and practices. One main outcome of this exchange was a global view on the astronomical data sonification paradigm for observing the diversity of tools, uses and users (including visually impaired people), but also the current limitations and potential methods of improvement. From this viewpoint, here we present basic elements gathered and contextualized by sound experts in their respective fields (sound perception/cognition, sound design, psychoacoustics, experimental psychology), to anchor sonification for astronomy in a more well informed, methodological and creative process.","2022-11","2023-07-06 01:03:36","2023-07-06 01:03:36","2023-07-06 01:03:36","1249-1255","","11","6","","Nat Astron","","","","","","","","en","2022 Springer Nature Limited","","","","www.nature.com","","Number: 11 Publisher: Nature Publishing Group","","/Users/minsik/Zotero/storage/KBVDW4WK/Misdariis et al. - 2022 - Sound experts’ perspectives on astronomy sonificat.pdf","","","Astronomy and astrophysics; Information theory and computation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FTKNKN2H","bookSection","2005","Nguyen, Cong Phuong; Pham, Thi Ngoc Yen; Eric, Castelli","Toward a Sound Analysis System for Telemedicine","Fuzzy Systems and Knowledge Discovery","978-3-540-28331-7 978-3-540-31828-6","","","http://link.springer.com/10.1007/11540007_44","Our work is within the framework of studying a sound analysis system in a telemedicine project. The task of this system is to detect situations of distress in a patient’s room basing on sound analysis. In this paper we present our studies on the constructions of a speech/non-speech discriminator and of a speech/scream-groan discriminator. The first discriminator’s task is to distinguish speech signal from non speech signal in a room such as sounds of broken glass, door shutting, chair falling, water in toilette, etc. The second one’s task is to detect sounds of scream-groan from speech signal. Results show that these discriminators are applicable to our sound analysis system.","2005","2023-07-06 01:06:12","2023-07-20 00:07:27","2023-07-06 01:06:12","352-361","","","3614","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11540007_44","","","","","","Wang, Lipo; Jin, Yaochu","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"36QBX6ND","bookSection","2013","Thoret, Etienne; Aramaki, Mitsuko; Kronland-Martinet, Richard; Velay, Jean-Luc; Ystad, Sølvi","Reenacting Sensorimotor Features of Drawing Movements from Friction Sounds","From Sounds to Music and Emotions","978-3-642-41247-9 978-3-642-41248-6","","","http://link.springer.com/10.1007/978-3-642-41248-6_8","Even though we generally don’t pay attention to the friction sounds produced when we are writing or drawing, these sounds are recordable, and can even evoke the underlying gesture. In this paper, auditory perception of such sounds, and the internal representations they evoke when we listen to them, is considered from the sensorimotor learning point of view. The use of synthesis processes of friction sounds makes it possible to investigate the perceptual influence of each gestures parameter separately. Here, the influence of the velocity profile on the mental representation of the gesture induced by a friction sound was investigated through 3 experiments. The results reveal the perceptual relevance of this parameter, and particularly a specific morphology corresponding to biological movements, the so-called 1/3-power law. The experiments are discussed according to the sensorimotor theory and the invariant taxonomy of the ecological approach.","2013","2023-07-06 01:06:12","2023-07-20 00:06:20","2023-07-06 01:06:12","130-153","","","7900","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-41248-6_8","","/Users/minsik/Zotero/storage/NKHQTYLA/Thoret et al. - 2013 - Reenacting Sensorimotor Features of Drawing Moveme.pdf","","","","Aramaki, Mitsuko; Barthet, Mathieu; Kronland-Martinet, Richard; Ystad, Sølvi","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BXM7Z5X3","journalArticle","2021","Cibrian, Franceli L.; Ley-Flores, Judith; Newbold, Joseph W.; Singh, Aneesha; Bianchi-Berthouze, Nadia; Tentori, Monica","Interactive sonification to assist children with autism during motor therapeutic interventions","Personal and Ubiquitous Computing","","1617-4909, 1617-4917","10.1007/s00779-020-01479-z","http://link.springer.com/10.1007/s00779-020-01479-z","Interactive sonification is an effective tool used to guide individuals when practicing movements. Little research has shown the use of interactive sonification in supporting motor therapeutic interventions for children with autism who exhibit motor impairments. The goal of this research is to study if children with autism understand the use of interactive sonification during motor therapeutic interventions, its potential impact of interactive sonification in the development of motor skills in children with autism, and the feas ibility of using it in specialized schools for children with autism. We conducted two deployment studies in Mexico using Go-withthe-Flow, a framework to sonify movements previously developed for chronic pain rehabilitation. In the first study, six children with autism were asked to perform the forward reach and lateral upper-limb exercises while listening to three different sound structures (i.e., one discrete and two continuous sounds). Results showed that children with autism exhibit awareness about the sonification of their movements and engage with the sonification. We then adapted the sonifications based on the results of the first study, for motor therapy of children with autism. In the next study, nine children with autism were asked to perform upper-limb lateral, cross-lateral, and push movements while listening to five different sound structures (i.e., three discrete and two continues) designed to sonify the movements. Results showed that discrete sound structures engage the children in the performance of upper-limb movements and increase their ability to perform the movements correctly. We finally propose design considerations that could guide the design of projects related to interactive sonification","2021-04","2023-07-06 01:06:12","2023-07-21 04:52:43","2023-07-06 01:06:12","391-410","","2","25","","Pers Ubiquit Comput","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/3IY33WVZ/Cibrian et al. - 2021 - Interactive sonification to assist children with a.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8W68SHFH","bookSection","2010","Geier, Matthias; Spors, Sascha; Weinzierl, Stefan","The Future of Audio Reproduction","Adaptive Multimedia Retrieval. Identifying, Summarizing, and Recommending Image and Music","978-3-642-14757-9 978-3-642-14758-6","","","http://link.springer.com/10.1007/978-3-642-14758-6_1","The introduction of new techniques for audio reproduction such as binaural technology, Wave Field Synthesis and Higher Order Ambisonics is accompanied by a paradigm shift from channel-based to object-based transmission and storage of spatial audio. The separate coding of source signal and source location is not only more efficient considering the number of channels used for reproduction by large loudspeaker arrays, it will also open up new options for a user-controlled soundfield design. The paper describes the technological change from stereophonic to array-based audio reproduction techniques and introduces a new proposal for the coding of spatial properties related to auditory objects.","2010","2023-07-06 01:06:12","2023-07-19 11:09:30","2023-07-06 01:06:12","1-17","","","5811","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-14758-6_1","","","","","","Detyniecki, Marcin; Leiner, Ulrich; Nürnberger, Andreas","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I74UV2Q8","journalArticle","2009","Hwang, Sungmok; Park, Youngjin; Park, Youn-sik","Analysis on perceptual sensitivity to head-related impulse responses in the median plane","Journal of Mechanical Science and Technology","","1738-494X, 1976-3824","10.1007/s12206-009-0925-z","http://link.springer.com/10.1007/s12206-009-0925-z","This study deals with the perceptual sensitivity to Head-Related Impulse Responses (HRIRs) in the median plane based on a series of subjective listening tests using a pair of headphones. First, the non-individualized HRIRs were modeled from 12 principal components (PCs) extracted from Principal Components Analysis (PCA) of the CIPIC HRTF database. The Just Noticeable Difference (JND) in weight of PCs (PCWs) at each elevation was estimated. It was not observed the common elevation-dependent tendency or PCW-dependent tendency of JND in PCWs across the five subjects who participated in the tests, and the inter-subject variation of JND in PCWs was large. The JND in HRIRs can be estimated indirectly from the JND in PCWs because the HRIRs can be represented by a linear summation of the PCs weighted by PCWs. The common elevation-dependent tendency of JND in Directional Impulse Responses (DIRs), which are the mean-subtracted HRIRs, across the five subjects can be found. The change in PCWs does not seem to contribute to our perception of sound source characteristics; however, the resulting change in HRIRs due to the change in PCWs seems to contribute. The subjects showed larger JND in DIRs in the frontal region than in the rear region. This means that our perception of sound source characteristics is more sensitive for frontal sources than rear sources.","2009-12","2023-07-06 01:06:12","2023-07-20 06:47:45","2023-07-06 01:06:12","3340-3348","","12","23","","J Mech Sci Technol","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U74AASR7","bookSection","2009","Mannheimer, Steve; Ferati, Mexhid; Bolchini, Davide; Palakal, Mathew","Educational Sound Symbols for the Visually Impaired","Universal Access in Human-Computer Interaction. Addressing Diversity","978-3-642-02706-2 978-3-642-02707-9","","","http://link.springer.com/10.1007/978-3-642-02707-9_12","Acoustic-based computer interactivity offers great potential [1], particularly with blind and visually impaired users [2]. At Indiana University’s School of Informatics at IUPUI, we have developed an innovative educational approach relying on “audemes,” short, nonverbal sound symbols made up of 2-5 individual sounds lasting 3-7 seconds - like expanded “earcons”[3] - to encode and prompt memory. To illustrate: An audeme for “American Civil War” includes a 3-second snippet of the song Dixie partially overlapped by a snippet of Battle Hymn of the Republic, followed by battle sounds, together lasting 5 seconds. Our focus on non-verbal sound explores the mnemonic impact of metaphoric rather than literal signification. Working for a year with BVI students, we found audemes improved encoding and long-term memory of verbal educational content, even after five months, and engaged the students in stimulating ways.","2009","2023-07-06 01:06:12","2023-07-21 05:08:24","2023-07-06 01:06:12","106-115","","","5614","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-02707-9_12","","","","","","Stephanidis, Constantine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BC5LH3FV","bookSection","2011","White, Sean; Feiner, Steven","Dynamic, Abstract Representations of Audio in a Mobile Augmented Reality Conferencing System","Recent Trends of Mobile Collaborative Augmented Reality Systems","978-1-4419-9844-6 978-1-4419-9845-3","","","https://link.springer.com/10.1007/978-1-4419-9845-3_12","We describe a wearable audio conferencing and information presentation system that represents individual participants and audio elements through dynamic, visual abstractions, presented on a tracked, see-through head-worn display. Our interest is in communication spaces, annotation, and data that are represented by auditory media with synchronistic or synesthetic visualizations. Representations can transition between different spatial modalities as audio elements enter and exit the wearer’s physical presence. In this chapter, we discuss the user interface and infrastructure, SoundSight, which uses the Skype Internet telephony API to support wireless conferencing, and describe our early experience using the system.","2011","2023-07-06 01:06:12","2023-07-21 04:56:49","2023-07-06 01:06:12","149-160","","","","","","","","","","","Springer New York","New York, NY","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-1-4419-9845-3_12","","","","","","Alem, Leila; Huang, Weidong","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6W62HP6X","bookSection","2021","Mangiron, Carme","Game Accessibility: Taking Inclusion to the Next Level","Universal Access in Human-Computer Interaction. Design Methods and User Experience","978-3-030-78091-3 978-3-030-78092-0","","","https://link.springer.com/10.1007/978-3-030-78092-0_17","This paper provides an overview of game accessibility. It describes what game accessibility is and highlights the need to combine usability and adaptability, as a “one-size-fits-all” approach is not possible due to the interactive and dynamic nature of the video game medium and the array of (dis)abilities different users may have. The main accessibility barriers different groups of users face are also presented. Then, the main focus shifts to the current state of the art in game accessibility, including existing guidelines and research carried out in this field. Finally, future perspectives are outlined, emphasizing the need for a user-centred approach and for collaboration between users, academia, and the industry.","2021","2023-07-06 01:06:12","2023-07-21 05:09:51","2023-07-06 01:06:12","269-279","","","12768","","","Game Accessibility","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-78092-0_17","","","","","","Antona, Margherita; Stephanidis, Constantine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JBM33QCV","journalArticle","2011","Stefik, Andreas; Gellenbeck, Ed","Empirical studies on programming language stimuli","Software Quality Journal","","0963-9314, 1573-1367","10.1007/s11219-010-9106-7","http://link.springer.com/10.1007/s11219-010-9106-7","Comprehending and debugging computer programs are inherently difficult tasks. The current approach to building program execution and debugging environments is to use exclusively visual stimuli on programming languages whose syntax and semantics has often been designed without empirical guidance. We present an alternative: Sodbeans, an open-source integrated development environment designed to output carefully chosen spoken auditory cues to supplement empirically evaluated visual stimuli. Originally designed for the blind, earlier work suggested that Sodbeans may benefit sighted programmers as well. We evaluate Sodbeans in two experiments. First, we report on a formal debugging experiment comparing (1) a visual debugger, (2) an auditory debugger, and (3) a multimedia debugger, which includes both visual and auditory stimuli. The results from this study indicate that while auditory debuggers on their own are significantly less effective for sighted users when compared with visual and multimedia debuggers, multimedia debuggers might benefit sighted programmers under certain circumstances. Specifically, we found that while multimedia debuggers do not provide instant usability, once programmers have some practice, their performance in answering comprehension questions improves. Second, we created and evaluated a pilot survey analyzing individual elements in a custom programming language (called HOP) to garner empirical metrics on their comprehensibility. Results showed that some of the most widely used syntax and semantics choices in commercial programming languages are extraordinarily unintuitive for novices. For example, at an aggregate level, the word for , as in a for loop, was rated reliably worse than repeat by more than 673% by novices. After completing our studies, we implemented the HOP programming language and integrated it into Sodbeans.","2011-03","2023-07-06 01:06:12","2023-07-21 05:00:51","2023-07-06 01:06:12","65-99","","1","19","","Software Qual J","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M2E924PN","bookSection","2010","Sciabica, Jean-François; Bezat, Marie-Céline; Roussarie, Vincent; Kronland-Martinet, Richard; Ystad, Sølvi","Towards Timbre Modeling of Sounds Inside Accelerating Cars","Auditory Display","978-3-642-12438-9 978-3-642-12439-6","","","http://link.springer.com/10.1007/978-3-642-12439-6_19","Quality investigations and design of interior car sounds constitute an important challenge for the car industry. Such sounds are complex and time-varying, inducing considerable timbre variations depending on the driving conditions. An interior car sound is indeed a mixture between several sound sources, with two main contributions, i.e. the engine noise on the one hand and the aerodynamic and tire-road noise on the other. Masking phenomena occur between these two components and should be considered when studying perceptive attributes of interior car sounds in order to identify relevant signal parameters. By combining sensory analysis and signal analysis associated with an auditory model, a relation between a reduced number of signal parameters and perceptive attributes can be found. This approach has enabled us to propose timbre descriptors based on the tristimulus criterion that reflect the dynamic behavior of a sound inside an accelerating car.","2010","2023-07-06 01:06:12","2023-07-19 11:38:33","2023-07-06 01:06:12","377-391","","","5954","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-12439-6_19","","","","","","Ystad, Sølvi; Aramaki, Mitsuko; Kronland-Martinet, Richard; Jensen, Kristoffer","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WKQ3IP3N","bookSection","2007","Reiter, Ulrich; Jumisko-Pyykkö, Satu","Watch, Press, and Catch – Impact of Divided Attention on Requirements of Audiovisual Quality","Human-Computer Interaction. HCI Intelligent Multimodal Interaction Environments","978-3-540-73108-5 978-3-540-73110-8","","","http://link.springer.com/10.1007/978-3-540-73110-8_104","Many of today’s audiovisual application systems offer some kind of interactivity. Yet, quality assessments of these systems are often performed without taking into account the possible effects of divided attention caused by interaction or user task. We present a subjective assessment performed among 40 test subjects to investigate the impact of divided attention on the perception of audiovisual quality in interactive application systems. Test subjects were asked to rate the overall perceived audiovisual quality in an interactive 3D scene with varying degrees of interactive tasks to be performed by the subjects. As a result we found that the experienced overall quality did not vary with the degree of interaction. The results of our study make clear that in the case where interactivity is offered in an audiovisual application, it is not generally possible to technically lower the signal quality without perceptual effects.","2007","2023-07-06 01:07:23","2023-07-20 06:31:51","2023-07-06 01:07:23","943-952","","","4552","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73110-8_104","","","","","","Jacko, Julie A.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SS8XAYU9","bookSection","2007","Miyashita, Hisashi; Takagi, Hironobu; Sato, Daisuke; Asakawa, Chieko","Making Multimedia Internet Content Accessible and Usable","Universal Access in Human-Computer Interaction. Applications and Services","978-3-540-73282-2 978-3-540-73283-9","","","http://link.springer.com/10.1007/978-3-540-73283-9_12","Although multimedia content containing streaming media is now widely used on the World Wide Web, there exist considerable difficulties for blind users to access such content, due to its dynamic changes, keyboard inoperability, and audio interference with the speech from assistive software. In particular, the third problem of audio interference is serious for blind users, since multimedia content often contains streaming media such as video and music which continuously play sounds, and thus they cannot hear the speech, which is masked by the loud media. In this paper, we propose a new accessible browser that can directly manipulate such multimedia content. In order to control Flash contents, our browser relies on a transcoding HTTP proxy to inject special scripts into the Flash content and then manipulates the embedded streaming media and sound objects via the injected scripts. By using our browser, users can easily turn the volume up or down, play, stop, or pause the streaming media with shortcut keys. Since the users do not need to focus on buttons or sliders for these operations, they can immediately stop or fade out the intrusive media when listening to speech from assistive software.","2007","2023-07-06 01:07:23","2023-07-21 05:09:31","2023-07-06 01:07:23","98-107","","","4556","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73283-9_12","","/Users/minsik/Zotero/storage/HEU3K3MU/Miyashita et al. - 2007 - Making Multimedia Internet Content Accessible and .pdf","","","","Stephanidis, Constantine","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3HE3R4LE","bookSection","2008","Yamaguchi, Tokuo; Asai, Kazuhiro; Kitamura, Yoshifumi; Kishino, Fumio","Interactive Multimedia Contents in the IllusionHole","Entertainment Computing - ICEC 2008","978-3-540-89221-2 978-3-540-89222-9","","","http://link.springer.com/10.1007/978-3-540-89222-9_13","This paper proposes a system of interactive multimedia contents that allows multiple users to participate in a face-to-face manner and share the same time and space. It provides an interactive environment where multiple users can see and manipulate stereoscopic animation with individual sound. Two application examples are implemented; one is location-based content design and the other is user-based content design. Both effectively use a unique feature of the IllusionHole, i.e., a location-sensitive display device that provides a stereoscopic image with multiple users around the table. Keywords: 3D user interface, entertainment computi","2008","2023-07-06 01:07:23","2023-07-19 23:59:35","2023-07-06 01:07:23","116-121","","","5309","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-89222-9_13","","/Users/minsik/Zotero/storage/SBEX2DGX/Yamaguchi et al. - 2008 - Interactive Multimedia Contents in the IllusionHol.pdf","","","","Stevens, Scott M.; Saldamarco, Shirley J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6RFZEF4S","journalArticle","2007","Gygi, Brian; Kidd, Gary R.; Watson, Charles S.","Similarity and categorization of environmental sounds","Perception & Psychophysics","","0031-5117, 1532-5962","10.3758/BF03193921","http://link.springer.com/10.3758/BF03193921","Four experiments investigated the acoustical correlates of similarity and categorization judgments of environmental sounds. In Experiment 1, similarity ratings were obtained from pairwise comparisons of recordings of 50 environmental sounds. A three-dimensional multidimensional scaling (MDS) solution showed three distinct clusterings of the sounds, which included harmonic sounds, discrete impact sounds, and continuous sounds. Furthermore, sounds from similar sources tended to be in close proximity to each other in the MDS space. The orderings of the sounds on the individual dimensions of the solution were well predicted by linear combinations of acoustic variables, such as harmonicity, amount of silence, and modulation depth. The orderings of sounds also correlated significantly with MDS solutions for similarity ratings of imagined sounds and for imagined sources of sounds, obtained in Experiments 2 and 3—as was the case for free categorization of the 50 sounds (Experiment 4)—although the categorization data were less well predicted by acoustic features than were the similarity data.","2007-08","2023-07-06 01:07:23","2023-07-21 04:43:41","2023-07-06 01:07:23","839-855","","6","69","","Perception & Psychophysics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/ASNWFGG5/Gygi et al. - 2007 - Similarity and categorization of environmental sou.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"43T5WJYU","bookSection","2021","Kariyado, Yuta; Arevalo, Camilo; Villegas, Julián","Auralization of Three-Dimensional Cellular Automata","Artificial Intelligence in Music, Sound, Art and Design","978-3-030-72913-4 978-3-030-72914-1","","","https://link.springer.com/10.1007/978-3-030-72914-1_11","An auralization tool for exploring three-dimensional cellular automata is presented. This proof-of-concept allows the creation of a sound field comprising individual sound events associated with each cell in a three-dimensional grid. Each sound-event is spatialized depending on the orientation of the listener relative to the three-dimensional model. Users can listen to all cells simultaneously or in sequential slices at will. Conceived to be used as an immersive Virtual Reality (VR) scene, this software application also works as a desktop application for environments where the VR infrastructure is missing. Subjective evaluations indicate that the proposed sonification increases the perceived quality and immersability of the system with respect to a visualization-only system. No subjective differences between the sequential or simultaneous presentations were found.","2021","2023-07-06 01:07:23","2023-07-19 11:33:38","2023-07-06 01:07:23","161-170","","","12693","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-72914-1_11","","","","","","Romero, Juan; Martins, Tiago; Rodríguez-Fernández, Nereida","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GEQ46RMA","bookSection","2017","Matsuo, Masaki; Miura, Takahiro; Sakajiri, Masatsugu; Onishi, Junji; Ono, Tsukasa","Inclusive Side-Scrolling Action Game Securing Accessibility for Visually Impaired People","Human-Computer Interaction – INTERACT 2017","978-3-319-68058-3 978-3-319-68059-0","","","https://link.springer.com/10.1007/978-3-319-68059-0_41","Though many computer games have recently become accessible for gamers with visual impairments, these players still face difficulty in manipulating game characters and acquiring visual information. It is true that although an increasing number of games for visually impaired people called audio games are being developed, many of these games cannot satisfy their basic needs because of the shortage of contents and are difficult for sighted people because of no visual information. Based on this situation, we have been developing accessible games for visually impaired people that feature enriched materials and multimodal information presentation. However, the needs of real-time action on accessible games remain unsolved. In this article, our objective is to develop an inclusive side scroller game with high real-time performance and accessibility functions for visually impaired people, and be available to play with more than one person including sighted persons.","2017","2023-07-06 01:07:23","2023-07-21 07:44:16","2023-07-06 01:07:23","410-414","","","10516","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-68059-0_41","","/Users/minsik/Zotero/storage/3SBWEXJX/Matsuo et al. - 2017 - Inclusive Side-Scrolling Action Game Securing Acce.pdf","","","","Bernhaupt, Regina; Dalvi, Girish; Joshi, Anirudha; K. Balkrishan, Devanuj; O’Neill, Jacki; Winckler, Marco","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SZLVY62F","bookSection","2013","Wilkie, Sonia; Stockman, Tony","The Perception of Auditory-Visual Looming in Film","From Sounds to Music and Emotions","978-3-642-41247-9 978-3-642-41248-6","","","http://link.springer.com/10.1007/978-3-642-41248-6_21","Auditory-visual looming (the presentation of objects moving in depth towards the viewer) is a technique used in film (particularly those in 3D) to assist in drawing the viewer into the created world. The capacity of a viewer to perceptually immerse within the multidimensional world and interact with moving objects can be affected by the sounds (audio cues) that accompany these looming objects. However the extent to which sound parameters should be manipulated remains unclear. For example, the amplitude, spectral components, reverb and spatialisation can all be altered, but the degree of their alteration and the resulting perception generated need greater investigation. Building on a previous study analysing the physical properties of the sounds, we analyse people’s responses to the complex sounds which use multiple audio cues for film looming scenes, reporting which conditions elicited a faster response to contact time, causing the greatest amount of underestimation.","2013","2023-07-06 01:07:23","2023-07-20 00:06:32","2023-07-06 01:07:23","378-386","","","7900","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-41248-6_21","","","","","","Aramaki, Mitsuko; Barthet, Mathieu; Kronland-Martinet, Richard; Ystad, Sølvi","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z2Y4MB4E","bookSection","2010","Okada, Noriko; Miki, Mitsunori; Hiroyasu, Tomoyuki; Yoshimi, Masato","Classified-Chime Sound Generation Support System Using an Interactive Genetic Algorithm","Artifical Intelligence and Soft Computing","978-3-642-13231-5 978-3-642-13232-2","","","http://link.springer.com/10.1007/978-3-642-13232-2_21","This research proposes a chime sound generation support system to readily generate intercom chime sounds that are agreeable to individual persons and to associate the chime sounds with visitors. In the proposed system, an interactive genetic algorithm (IGA) is used. Based on the melodies created by users, chime sounds are automatically generated in accordance with rules. The effectiveness of the proposed system is verified by an experiment using the system.","2010","2023-07-06 01:07:23","2023-07-19 11:32:40","2023-07-06 01:07:23","173-180","","","6114","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-13232-2_21","","","","","","Rutkowski, Leszek; Scherer, Rafał; Tadeusiewicz, Ryszard; Zadeh, Lotfi A.; Zurada, Jacek M.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5A7BXSW6","bookSection","2003","Marila, Juha; Ronkainen, Sami","Time-Out in Mobile Text Input: The Effects of Learning and Feedback","Human-Computer Interaction with Mobile Devices and Services","978-3-540-40821-5 978-3-540-45233-1","","","http://link.springer.com/10.1007/978-3-540-45233-1_8","In many user interfaces with restricted input/output capabilities, a time-out is used to automatically change the UI from one mode into another. In this paper we studied the learning of time-outs and the effect of feedback on it in mobile phone text entry. The effects of three different feedback schemes (auditory/visual/no feedback) on learning of two different time-out lengths were compared. We measured the response time from the time-out occurrence to the time of user’s reaction. Error rates and the development of the response times in different schemes were used as measures of learning. We also studied if the users learned to estimate the time-out lengths, or if they just reacted to the available feedback. There were three main findings. Without feedback, response times had great variation. Auditory feedback enabled faster response times than visual. Finally, we found evidence of short-term learning, but not as much of a lasting effect.","2003","2023-07-06 01:07:23","2023-07-20 06:29:58","2023-07-06 01:07:23","91-103","","","2795","","","Time-Out in Mobile Text Input","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-45233-1_8","","","","","","Chittaro, Luca","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YMD2JNHT","journalArticle","2019","Ben-Hur, Zamir; Alon, David Lou; Rafaely, Boaz; Mehra, Ravish","Loudness stability of binaural sound with spherical harmonic representation of sparse head-related transfer functions","EURASIP Journal on Audio, Speech, and Music Processing","","1687-4722","10.1186/s13636-019-0148-x","https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-019-0148-x","In response to renewed interest in virtual and augmented reality, the need for high-quality spatial audio systems has emerged. The reproduction of immersive and realistic virtual sound requires high resolution individualized head-related transfer function (HRTF) sets. In order to acquire an individualized HRTF, a large number of spatial measurements are needed. However, such a measurement process requires expensive and specialized equipment, which motivates the use of sparsely measured HRTFs. Previous studies have demonstrated that spherical harmonics (SH) can be used to reconstruct the HRTFs from a relatively small number of spatial samples, but reducing the number of samples may produce spatial aliasing error. Furthermore, by measuring the HRTF on a sparse grid the SH representation will be order-limited, leading to constrained spatial resolution. In this paper, the effect of sparse measurement grids on the reproduced binaural signal is studied by analyzing both aliasing and truncation errors. The expected effect of these errors on the perceived loudness stability of the virtual sound source is studied theoretically, as well as perceptually by an experimental investigation. Results indicate a substantial effect of truncation error on the loudness stability, while the added aliasing seems to significantly reduce this effect.","2019-12","2023-07-06 01:07:23","2023-07-20 00:01:41","2023-07-06 01:07:23","5","","1","2019","","J AUDIO SPEECH MUSIC PROC.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/FU924GLQ/Ben-Hur et al. - 2019 - Loudness stability of binaural sound with spherica.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AGFDYFTL","bookSection","2005","McCarthy, John; Wright, Peter","Technology in Place: Dialogics of Technology, Place and Self","Human-Computer Interaction - INTERACT 2005","978-3-540-28943-2 978-3-540-31722-7","","","http://link.springer.com/10.1007/11555261_72","Ubiquitous and ambient computing – computationally enhanced built environments and portable products that aim to make computing available anytime-anywhere – has somewhat paradoxically put place at the heart of Interaction Design. In this paper, foundations are laid for a dialogical approach to place as an expression of the experienced relationship between people and space. Building on McCarthy and Wright’s dialogical conceptualisation of technology as experience, place is described in terms of the plurality of histories, interactions and meanings that characterise people’s different engagements with particular spaces. Implications of a dialogical approach to place are considered with respect to the further development within Interaction Design of concepts such as context, engagement, and interactivity.","2005","2023-07-06 01:07:23","2023-07-20 05:55:40","2023-07-06 01:07:23","914-926","","","3585","","","Technology in Place","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11555261_72","","/Users/minsik/Zotero/storage/TVNAM9N4/McCarthy and Wright - 2005 - Technology in Place Dialogics of Technology, Plac.pdf","","","","Costabile, Maria Francesca; Paternò, Fabio","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8ZZEXHA2","bookSection","2009","Zhang, Jingjing; Lotto, Beau; Bergstom, Ilias; Andreou, Lefkothea; Miyadera, Youzou; Yokoyama, Setsuo","Development of a Visualised Sound Simulation Environment: An e-Approach to a Constructivist Way of Learning","Human-Computer Interaction. Interacting in Various Application Domains","978-3-642-02582-2 978-3-642-02583-9","","","http://link.springer.com/10.1007/978-3-642-02583-9_30","In this paper, the design and implementation of a visualised sound simulation environment is presented as an initial step to further laboratory experimentation. Preliminary laboratory experiments showed a positive learning curve in human auditory perception. Learning occurred when new information was processed with relevant existing knowledge in this simulation environment. While the work towards the truth of the empirical hypothesis is still under discussion, this project has been expanded beyond the scope that was originally envisaged and the developed environment showed its potential to be adopted on mobile devices for many educational purposes. This initiative not only brings scientists and educators together, but it is also hoped that it represents a possible e-approach to a constructivist way of learning.","2009","2023-07-06 01:07:23","2023-07-20 06:32:15","2023-07-06 01:07:23","266-275","","","5613","","","Development of a Visualised Sound Simulation Environment","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-02583-9_30","","/Users/minsik/Zotero/storage/D8WKHDPA/Zhang et al. - 2009 - Development of a Visualised Sound Simulation Envir.pdf","","","","Jacko, Julie A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7BF2VXTE","bookSection","2004","Dorin, Alan","The Virtual Ecosystem as Generative Electronic Art","Applications of Evolutionary Computing","978-3-540-21378-9 978-3-540-24653-4","","","http://link.springer.com/10.1007/978-3-540-24653-4_48","This paper proposes four desirable attributes of processes to be applied in generative electronic art. By example, it then demonstrates that the virtual ecosystem in its entirety is a process with many of these desirable attributes. The paper contrasts this process with the use of cellular automata. It outlines a number of generative artworks with which the author has been involved that utilize the virtual ecosystem, and discusses their pros and cons in the context of generative art. The paper suggests means by which the application of the four desirable attributes may extend the creative possibilities for these works.","2004","2023-07-06 01:07:23","2023-07-19 11:30:56","2023-07-06 01:07:23","467-476","","","3005","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-24653-4_48","","","","","","Raidl, Günther R.; Cagnoni, Stefano; Branke, Jürgen; Corne, David Wolfe; Drechsler, Rolf; Jin, Yaochu; Johnson, Colin G.; Machado, Penousal; Marchiori, Elena; Rothlauf, Franz; Smith, George D.; Squillero, Giovanni","Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PNKUKK8N","bookSection","2009","Ferati, Mexhid; Bolchini, Davide; Mannheimer, Steve","Towards a Modeling Language for Designing Auditory Interfaces","Universal Access in Human-Computer Interaction. Applications and Services","978-3-642-02712-3 978-3-642-02713-0","","","http://link.springer.com/10.1007/978-3-642-02713-0_53","Auditory applications are systems that communicate content, navigation capabilities and functionality mainly via the aural channel, or via a combination of the aural and visual channels, and can support the user interaction in a multimodal fashion as well (e.g. through touch or speech). In this paper, we present the preliminary results of an exploratory research effort aimed at establishing a design modeling language for auditory applications, by extending an existing interactive application design model (IDM, Interactive Dialogue Model) used in the area of hypermedia and information-intensive applications. Our exploratory research capitalizes on previous experience in hypermedia modeling, aural information architectures, and design of auditory applications. We use an auditory application, the Acoustic Edutainment Interface (AEDIN), as a real case study to inform and exemplify the use of the modeling language.","2009","2023-07-06 01:08:42","2023-07-21 05:09:03","2023-07-06 01:08:42","502-511","","","5616","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-02713-0_53","","/Users/minsik/Zotero/storage/97RQBNKZ/Ferati et al. - 2009 - Towards a Modeling Language for Designing Auditory.pdf","","","","Stephanidis, Constantine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GAUDA9ZR","journalArticle","2007","Baldwin, Carryl L.","Cognitive implications of facilitating echoic persistence","Memory & Cognition","","0090-502X, 1532-5946","10.3758/BF03193314","http://link.springer.com/10.3758/BF03193314","Seventeen participants performed a tone-pattern-matching task at different presentation levels while concurrently engaged in a simulated-driving task. Presentation levels of 60, 65, and 70 dBC (SPL) were combined factorially with tone-matching delays of 2, 3, and 4 sec. Intensity had no effect on performance in single-task conditions and short-delay conditions. However, when the participants were engaged concurrently in the driving task, a significant interaction between presentation level and delay was observed. In the longest delay condition, the participants performed the tone-pattern-matching task more efficiently (more quickly and without additional errors) as presentation intensity increased. These findings demonstrate the interaction between sensory and cognitive processes and point to a direct-intensity relationship where intensity affects the persistence of echoic memory. Implications for facilitating auditory processing and improving auditory interfaces in complex systems (i.e., transportation environments), particularly for older and hearing-impaired listeners, are discussed.","2007-06","2023-07-06 01:08:42","2023-07-21 04:29:23","2023-07-06 01:08:42","774-780","","4","35","","Memory & Cognition","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/XDYW7WE4/Baldwin - 2007 - Cognitive implications of facilitating echoic pers.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z4Z36GS9","journalArticle","1970","Cuddy, Lola L.","Training the absolute identification of pitch","Perception & Psychophysics","","0031-5117, 1532-5962","10.3758/BF03212589","http://link.springer.com/10.3758/BF03212589","Two methods for training the absolute judgment of pitch, reference trainillg and series training, were studied. Reference training concentrated during training on the identification of three reference tones in a set of nine pure tones, while series training gave equal weight during training to the idelltification of all nine tones. Results of flre- and posttraining tests, scored for the number of correct judgmellts, showed that reference training was more effective than series training for listeners wit!: musical experience. In addition. aiscriminability (a') scaling ofpreand posttest performance indicated that reference training was particular(v effective for training listeners with musical experienCe' when the nine tones of aset were grouped into three pitch classes-high, medium, and low pitch. Listeners without musical experience benefited from buill training methods, but their overall improvement was less than that for musical listeners.","1970-09","2023-07-06 01:08:42","2023-07-21 04:43:23","2023-07-06 01:08:42","265-269","","5","8","","Perception & Psychophysics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/I5AA6FDS/Cuddy - 1970 - Training the absolute identification of pitch.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DNCXWDHT","journalArticle","2009","Tolentino, Lisa; Birchfield, David; Megowan-Romanowicz, Colleen; Johnson-Glenberg, Mina C.; Kelliher, Aisling; Martinez, Christopher","Teaching and Learning in the Mixed-Reality Science Classroom","Journal of Science Education and Technology","","1059-0145, 1573-1839","10.1007/s10956-009-9166-2","http://link.springer.com/10.1007/s10956-009-9166-2","As emerging technologies become increasingly inexpensive and robust, there is an exciting opportunity to move beyond general purpose computing platforms to realize a new generation of K-12 technology-based learning environments. Mixed-reality technologies integrate real world components with interactive digital media to offer new potential to combine best practices in traditional science learning with the powerful affordances of audio/visual simulations. This paper introduces the realization of a learning environment called SMALLab, the Situated Multimedia Arts Learning Laboratory. We present a recent teaching experiment for high school chemistry students. A mix of qualitative and quantitative research documents the efficacy of this approach for students and teachers. We conclude that mixed-reality learning is viable in mainstream high school classrooms and that students can achieve significant learning gains when this technology is co-designed with educators.","2009-12","2023-07-06 01:08:42","2023-07-20 06:49:49","2023-07-06 01:08:42","501-517","","6","18","","J Sci Educ Technol","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SIFZIR4U","journalArticle","2006","Chen, Xiaoyu; Tremaine, Marilyn; Lutz, Robert; Chung, Jae-woo; Lacsina, Patrick","AudioBrowser: a mobile browsable information access for the visually impaired","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-006-0019-y","http://link.springer.com/10.1007/s10209-006-0019-y","Although a large amount of research has been conducted on building interfaces for the visually impaired that allows users to read web pages and generate and access information on computers, little development addresses two problems faced by the blind users. First, sighted users can rapidly browse and select information they find useful, and second, sighted users can make much useful information portable through the recent proliferation of personal digital assistants (PDAs). These possibilities are not currently available for blind users. This paper describes an interface that has been built on a standard PDA and allows its user to browse the information stored on it through a combination of screen touches coupled with auditory feedback. The system also supports the storage and management of personal information so that addresses, music, directions, and other supportive information can be readily created and then accessed anytime and anywhere by the PDA user. The paper describes the system along with the related design choices and design rationale. A user study is also reported.","2006-06","2023-07-06 01:08:42","2023-07-21 05:11:00","2023-07-06 01:08:42","4-22","","1","5","","Univ Access Inf Soc","AudioBrowser","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7TDSSBIY","journalArticle","2022","Sekhavat, Yoones A.; Azadehfar, Mohammad Reza; Zarei, Hossein; Roohi, Samad","Sonification and interaction design in computer games for visually impaired individuals","Multimedia Tools and Applications","","1380-7501, 1573-7721","10.1007/s11042-022-11984-3","https://link.springer.com/10.1007/s11042-022-11984-3","Video games are changing how we interact and communicate with each other. They can provide an authentic and collaborative platform for building new communities and connecting people. Since video games generally rely on visual elements that must be recognized by the players, most of them are not accessible by visually impaired people. In this research, we study the sonification and interaction issues in the design of computer games for visually impaired individuals. We have proposed an audio game called GrandEscape with the focus on the special needs of visually impaired people while playing. A comprehensive set of user studies has been performed to evaluate different interaction and sonification techniques in terms of providing a sense of presence and gaming experience.","2022-03","2023-07-06 01:08:42","2023-07-21 04:33:18","2023-07-06 01:08:42","7847-7871","","6","81","","Multimed Tools Appl","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z7FSYVAZ","journalArticle","2006","Tong, Kam-pang Maya; Wong, Kam-wah","Schematic interface of sound creation for computer animators","Journal of Zhejiang University-SCIENCE A","","1673-565X, 1862-1775","10.1631/jzus.2006.A1141","http://link.springer.com/10.1631/jzus.2006.A1141","As an audiovisual medium, computer animations require superior image quality and professional soundtrack to lead audiences into their fascinating virtual world. Without sound, the impact of storytelling is reduced or the story is even not understandable. Despite the importance of sound, most animators are unfamiliar with sound editing software. Limited budget projects such as independent or student works have difficulty hiring sound professionals to create tailor-made soundtrack. Therefore, we need a suitable sound tool to express their individual ideas. In this paper, we propose an approach using schematic for both computer animation and sound. Our approach provides (1) physical simulation on sound through animation parameters and (2) a new channel for animators to add aesthetic values in sound through their experience of using schematics in existing animation software. We demonstrate our idea through several examples, such as Doppler shift, obstacle effect, importance, energetic, and sputtering effect.","2006-07","2023-07-06 01:08:42","2023-07-20 06:50:29","2023-07-06 01:08:42","1141-1151","","7","7","","J. Zhejiang Univ. - Sci. A","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7ASRKWHF","bookSection","2012","McGookin, David; Brewster, Stephen A.","Understanding Auditory Navigation to Physical Landmarks","Haptic and Audio Interaction Design","978-3-642-32795-7 978-3-642-32796-4","","","http://link.springer.com/10.1007/978-3-642-32796-4_1","We present two studies that seek to better understand the role spatialised (3D) audio can play in supporting effective pedestrian navigation. 24 participants attempted to navigate and locate physical landmarks in a local botanical gardens using a gpsTunes [1] based auditory navigation system coupled with a map. Participants were significantly better at locating prominent than non-prominent physical landmarks. However, no significant quantative difference was found between the use of a map only and map + audio. Qualitative analysis revealed significant issues when physical landmarks are used, and common strategies when combining audio and map navigation. We highlight the implications of these in relation to existing work, and provide guidelines for future designers to employ.","2012","2023-07-06 01:08:42","2023-07-20 00:15:11","2023-07-06 01:08:42","1-10","","","7468","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-32796-4_1","","","","","","Magnusson, Charlotte; Szymczak, Delphine; Brewster, Stephen","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6XJBZ57H","bookSection","2001","Murphy, David; Pitt, Ian","Spatial Sound Enhancing Virtual Story Telling","Virtual Storytelling Using Virtual Reality Technologies for Storytelling","978-3-540-42611-0 978-3-540-45420-5","","","http://link.springer.com/10.1007/3-540-45420-9_3","Spatial sound information/cues may enhance the sense of immersiveness in virtual story telling. However, their role within complex, loosely-structured narratives is little understood. This paper describes a virtual heritage project that aims to convey a factual story using interactive virtual environments. Sound was added to the existing project in an effort to enhance the virtual experience. The use of sound is assessed through a user-study in order to assess its effectiveness and suggest methods of improvement.","2001","2023-07-06 01:10:52","2023-07-21 05:14:54","2023-07-06 01:10:52","20-29","","","2197","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-45420-9_3","","","","","","Balet, Olivier; Subsol, Gérard; Torguet, Patrice","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X6TEBY7U","bookSection","2007","Bergstrom, Tony; Karahalios, Karrie","Seeing More: Visualizing Audio Cues","Human-Computer Interaction – INTERACT 2007","978-3-540-74799-4 978-3-540-74800-7","","","http://link.springer.com/10.1007/978-3-540-74800-7_3","Using audio visualization, we seek to demonstrate how natural interaction is augmented with the addition of interaction history. Our Conversation Clock visualization captures and represents audio in a persistent and meaningful representation to provide social cues not available in an otherwise ephemeral conversation. In this paper we present user study evaluation of the Conversation Clock as utilized by familiar groups and demonstrate how individuals use the salient cues to evaluate their own interaction.","2007","2023-07-06 01:10:52","2023-07-20 05:55:49","2023-07-06 01:10:52","29-42","","","4663","","","Seeing More","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-74800-7_3","","/Users/minsik/Zotero/storage/3NWLNWRY/Bergstrom and Karahalios - 2007 - Seeing More Visualizing Audio Cues.pdf","","","","Baranauskas, Cécilia; Palanque, Philippe; Abascal, Julio; Barbosa, Simone Diniz Junqueira","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2B6L9R9J","journalArticle","2006","Mion, Luca; D’Incà, Gianluca","Analysis of expression in simple musical gestures to enhance audio in interfaces","Virtual Reality","","1359-4338, 1434-9957","10.1007/s10055-006-0029-3","http://link.springer.com/10.1007/s10055-006-0029-3","Expression could play a key role in the audio rendering of virtual reality applications. Its understanding is an ambitious issue in the scientific environment, and several studies have investigated the analysis techniques to detect expression in music performances. The knowledge coming from these analyses is widely applicable: embedding expression on audio interfaces can drive to attractive solutions to emphasize interfaces in mixed-reality environments. Synthesized expressive sounds can be combined with real stimuli to experience augmented reality, and they can be used in multi-sensory stimulations to provide the sensation of first-person experience in virtual expressive environments. In this work we focus on the expression of violin and flute performances, with reference to sensorial and affective domains. By means of selected audio features, we draw a set of parameters describing performers’ strategies which are suitable both for tuning expressive synthesis instruments and enhancing audio in human–computer interfaces.","2006-05","2023-07-06 01:10:52","2023-07-21 05:14:28","2023-07-06 01:10:52","62-70","","1","10","","Virtual Reality","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L6T6GYXE","journalArticle","2013","Usher, Raymond; Robertson, Paul; Sloan, Robin","Physical responses (arousal) to audio in games","The Computer Games Journal","","2052-773X","10.1007/BF03392340","http://link.springer.com/10.1007/BF03392340","This study investigates the role that audio plays in the video gaming experience. Two groups of participants played three games (of different styles). One group played the games with the audio switched on; the other group played the games with the audio switched off. A bioharness was used to measure the heartbeat and respiration rates of the participants as they played the game. The results showed that for all three games, the heartbeat and respiration rates of the participants in the group (playing the games with the audio switched on) were higher than the heartbeat and respiration rates of participants in the other group. This suggests that audio effects in video games provide a measurable enhancement of the game-playing experience.","2013-08","2023-07-06 01:10:52","2023-07-21 05:05:03","2023-07-06 01:10:52","5-13","","2","2","","Comput Game J","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K3799PS7","journalArticle","1988","Miyazaki, Ken’ichi","Musical pitch identification by absolute pitch possessors","Perception & Psychophysics","","0031-5117, 1532-5962","10.3758/BF03207484","http://link.springer.com/10.3758/BF03207484","Musical pitch identification was investigated in two experiments in which absolute pitch (AP) possessors and nonpossessors categorized tones presented in isolation into predetermined pitch classes. Stimuli consisted of 60 different tones per octave (at intervals of 20 cents). The experiments were designed to minimize the possibility that subjects could use strategies other than AP in performing the task. The results clearly differentiated AP possessors from nonpossessors in accuracy and speed of responding. Those subjects who had AP could categorize the tones quite consistently by using musical qualities of the tones (tone chroma). However, they did not respond uniformly to all stimuli; they responded more accurately and quickly to some musically important tones in a C-major mode (C, E, or G). On the other hand, those who had no AP showed almost random response patterns. In the absence of a tonal context, they could not use tone chroma, but only tone height. It isargued that tone chroma should be defined as the musical characteristics of tones in a tonal context, and that AP possessors are unique in that they can perceive it absolutely in the absence of any musical context. Although AP was believed to be very rare, it was proved here that aphenomenally large proportion of the subjects tested had AP. The correlation was observed between AP possession and early musical training that started at the age of 3 to 5.","1988-11","2023-07-06 01:10:52","2023-07-21 04:44:09","2023-07-06 01:10:52","501-512","","6","44","","Perception & Psychophysics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/WR5RV7TS/Miyazaki - 1988 - Musical pitch identification by absolute pitch pos.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UA5ZYT8Q","bookSection","2002","McElligott, Lisa; Dillon, Michelle; Leydon, Krispin; Richardson, Bruce; Fernström, Mikael; Paradiso, Joseph A.","‘ForSe FIElds’ - Force Sensors for Interactive Environments","UbiComp 2002: Ubiquitous Computing","978-3-540-44267-7 978-3-540-45809-8","","","http://link.springer.com/10.1007/3-540-45809-3_13","In this paper we discuss the development of ‘Z-Tiles’ in conjunction with a sister project, ‘Self-Organising Sensors’ (SOS). Combined, these projects will result in a pressure sensitive, self-organising, interactive sensor design that can be embedded into appropriate environments. The shared objective of these projects is to further our understanding of movement and gesture. In this paper, we discuss the design and behaviour of a force sensing material, the physical design of the sensor encasement and the software that allows the sensors to communicate and self-organise. The issues of modularity and portability are also discussed in this paper, while consideration has also been given to the conceptualisation and development of a variety of prototypes; ranging from entertainment to potential therapeutic applications. Essentially, the Z-tiles sensor can be used in control surfaces where force, weight distribution or motion is used as control parameters.","2002","2023-07-06 01:10:52","2023-07-21 05:07:51","2023-07-06 01:10:52","168-175","","","2498","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-45809-3_13","","","","","","Borriello, Gaetano; Holmquist, Lars Erik","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BTW9XBWE","bookSection","2019","Ty, Jayzon; Inoue, Naoki; Plopski, Alexander; Okahashi, Sayaka; Sandor, Christian; Hsu, Hsiu-Yun; Kuo, Li-Chieh; Su, Fong-Chin; Kato, Hirokazu","Integration of Augmented Reality with Pressing Evaluation and Training System for Finger Force Training","Human Aspects of IT for the Aged Population. Social Media, Games and Assistive Environments","978-3-030-22014-3 978-3-030-22015-0","","","http://link.springer.com/10.1007/978-3-030-22015-0_45","One major concern for the elderly is the decline in their ability to control their hands, which can significantly affect their ability to perform activities of daily living. One of the important hand functions that deteriorate over time is the ability to control finger force exertion, due to the gradual decrease in finger muscle strength as people age. Previous studies have shown that with proper training, it is possible to regain finger strength. However, when designing training systems for finger force control, visualization of the finger forces plays an important role in its effectiveness. In this paper, we describe the development of the augmented reality pressing and evaluation system (AR-PETS), an augmented reality based prototype system for finger force control training. We discuss the development of the system, as well as the design considerations during the development of the system.","2019","2023-07-06 01:10:52","2023-07-20 05:50:35","2023-07-06 01:10:52","575-587","","","11593","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-22015-0_45","","","","","","Zhou, Jia; Salvendy, Gavriel","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VFD28Z9V","journalArticle","1997","Kaiser, Mary K.; Montegut, Michael J.","Of red planets and indigo computers: Mars database visualization as an example of platform downsizing","Behavior Research Methods, Instruments, &amp Computers","","0743-3808, 1532-5970","10.3758/BF03200566","http://link.springer.com/10.3758/BF03200566","The last decade has witnessed tremendous advancements in the computer hardware and software used to perform scientific visualization. In this paper, we consider how the visualization of a particular data set, the digital terrain model derived from the Viking orbiter imagery,has been realized in four distinct projects over this period. These examples serve to demonstrate how the vast improvements in computational performance both decrease the cost of such visualization efforts and permit an increasing level of interactivity. Wethen consider how even today's graphical systems require the visualization designer to make intelligent choices and tradeoffs in database rendering. Finally,we discuss how insights gleaned from an understanding of human visual perception can guide these design decisions, and suggest new options for visualization hardware and software.","1997-03","2023-07-06 01:10:52","2023-07-19 23:34:44","2023-07-06 01:10:52","48-53","","1","29","","Behavior Research Methods, Instruments, & Computers","Of red planets and indigo computers","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/SG9SCWQ4/Kaiser and Montegut - 1997 - Of red planets and indigo computers Mars database.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E4IDCSPK","bookSection","2021","Kailas, Ganesh; Tiwari, Nachiketa","Design for Immersive Experience: Role of Spatial Audio in Extended Reality Applications","Design for Tomorrow—Volume 2","9789811601187 9789811601194","","","https://link.springer.com/10.1007/978-981-16-0119-4_69","The incredible growth of extended reality (XR) applications will be leading us to a world beyond our imaginations in the coming decades. Extended reality is an umbrella term that encompasses different categories of immersive technologies like virtual reality (VR), augmented reality (AR), and mixed reality (MR). From the traditional applications like entertainment and training, XR has been spreading its wings into a large number of applications in health care, aerospace, product design and prototyping, e-commerce, workspace productivity, architecture, and building industries. Immersibility of the virtual reality scene into the physical world will be crucial for its acceptance by mainstream industries and future development. In addition to the virtual scene's visual perception, spatial audio is a key feature in designing truly immersive XR. Hearing is the fastest sense of humans, which makes virtual auditory display (VAD) an ineluctable part of any XR application. In this work, the importance of three-dimensional spatial audio in XR applications is explored in the user perspective approach. User experience (UX) is improved to a large extent when the applications make use of spatial audio compared to direction-less hearing experience. Spatial sound has a crucial role in giving information regarding actions in the background and beyond the field of view (FOV), and thus in making proper three-dimensional realism. However, designing user-dependent virtual audio reality is challenging because of its parametric dependence on human anthropometric features. This work also suggests the possibilities of utilizing computer-aided designing (CAD) and computer-aided engineering (CAE) tools in producing personalized virtual audio reality. While the immersive extended reality experience evolves as the next frontier in user experience designing, a sophisticated 3D audio experience will be there at the heart of it.","2021","2023-07-06 01:10:52","2023-07-19 23:54:56","2023-07-06 01:10:52","853-863","","","222","","","Design for Immersive Experience","","","","","Springer Singapore","Singapore","en","","","","","DOI.org (Crossref)","","Series Title: Smart Innovation, Systems and Technologies DOI: 10.1007/978-981-16-0119-4_69","","","","","","Chakrabarti, Amaresh; Poovaiah, Ravi; Bokil, Prasad; Kant, Vivek","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SZGUPRHM","journalArticle","2003","Pittarello, Fabio","Accessing information through multimodal 3D environments: towards universal access","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-003-0044-z","http://link.springer.com/10.1007/s10209-003-0044-z","3D environments represent a great opportunity for universal access to information, as they offer an intuitive interaction paradigm, similar to what is experienced by humans in their everyday lives. In spite of that, several 3D interfaces are characterized by poor structures and are hard to navigate. This paper presents the multimodal concept of the Interaction Locus (IL) as a means to give structure to 3D scenes, helping the user to interact with and access information inside them. The concept was initially developed with particular reference to desktop virtual reality (2.5 D virtual reality), but it is general enough to be extended to other contexts, such as real 3D scenes. The final part of this work shows how the IL concept addresses the need for a unified authoring methodology, capable of allowing access to different target user groups from a variety of different devices.","2003-06-01","2023-07-06 01:10:52","2023-07-21 05:12:31","2023-07-06 01:10:52","189-204","","2","2","","Universal Access in the Information Society","Accessing information through multimodal 3D environments","","","","","","","","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TYEFVHBL","journalArticle","1984","O’Leary, Ann; Rhodes, Gillian","Cross-modal effects on visual and auditory object perception","Perception & Psychophysics","","0031-5117, 1532-5962","10.3758/BF03205954","http://link.springer.com/10.3758/BF03205954","Cross-modal influences on perceptual organization were demonstrated using adisplay that combined astimulus for auditory stream segregation with its visual apparent movement analogue. Both phenomena give rise to the perception of either one or two objects, depending on the rate of presentation of the stimuli. At slower rates, one object is perceived, while two are perceived at faster rates. Subjects indicated the stimulus onset asynchrony (SOA) between successive stimuli at which the perceptual shift occurred in each modality. Then visual and auditory stimuli were presented concurrently and subjects responded to the ""target"" modality sequence. Two intergroup separations for the nontarget stimuli were used. Distances were chosen, based on the subject's calibration data, which represented one and two objects, respectively, at the stream segregation point for the target sequence. Segregation occurred at larger SOAs when the nontarget stimulation indicated two objects than when it represented one. This was true for both visual and auditory target sequences.","1984-11","2023-07-06 01:10:52","2023-07-21 04:44:21","2023-07-06 01:10:52","565-569","","6","35","","Perception & Psychophysics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/AF5P4DC9/O’Leary and Rhodes - 1984 - Cross-modal effects on visual and auditory object .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J993ZY8H","bookSection","2010","Bakker, Saskia; Van Den Hoven, Elise; Eggen, Berry","Exploring Interactive Systems Using Peripheral Sounds","Haptic and Audio Interaction Design","978-3-642-15840-7 978-3-642-15841-4","","","http://link.springer.com/10.1007/978-3-642-15841-4_7","Our everyday interaction in and with the physical world, has facilitated the development of auditory perception skills that enable us to selectively place one auditory channel in the center of our attention and simultaneously monitor others in the periphery. We search for ways to leverage these auditory perception skills in interactive systems. In this paper, we present three working demonstrators that use sound to subtly convey information to users in an open office. To qualitatively evaluate these demonstrators, each of them has been implemented in an office for three weeks. We have seen that such a period of time, sounds can start shifting from the center to the periphery of the attention. Furthermore, we found several issues to be addressed when designing such systems, which can inform future work in this area.","2010","2023-07-06 01:10:52","2023-07-20 00:14:17","2023-07-06 01:10:52","55-64","","","6306","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-15841-4_7","","/Users/minsik/Zotero/storage/ZMRRTALN/Bakker et al. - 2010 - Exploring Interactive Systems Using Peripheral Sou.pdf","","","","Nordahl, Rolf; Serafin, Stefania; Fontana, Federico; Brewster, Stephen","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4XSCJTIW","bookSection","2012","Kim, Hee-Cheol","An Experimental Study to Explore Usability Problems of Interactive Voice Response Systems","Intelligent Information and Database Systems","978-3-642-28492-2 978-3-642-28493-9","","","http://link.springer.com/10.1007/978-3-642-28493-9_19","While interactive voice response (IVR) systems employing touch tone interface (TTI) are popularly used these days, they are generally known for their inconvenience. This is not only because of the characteristics that TTI inherently has, but also because of lack of understanding of IVR system users. This study is aimed at contributing to capture an understanding of the users, which eventually leads to better system design. In particular, we have developed an IVR system simulator to enable efficient, flexible, and rich usability tests for IVR systems. This paper presents an experimental study on usability of IVR systems utilizing the simulator. In the experiment, 41 subjects performed three different tasks concerning phone charges using the simulator to identify various usability problems. The analytic results are hopefully a basis for user-centered IVR systems design.","2012","2023-07-06 01:12:37","2023-07-20 06:35:11","2023-07-06 01:12:37","169-177","","","7198","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-28493-9_19","","","","","","Pan, Jeng-Shyang; Chen, Shyi-Ming; Nguyen, Ngoc Thanh","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I2DXKVE8","bookSection","1998","Brewster, Stephen","Using Earcons to Improve the Usability of a Graphics Package","People and Computers XIII","978-3-540-76261-4 978-1-4471-3605-7","","","http://link.springer.com/10.1007/978-1-4471-3605-7_18","This paper describes how non–speech sounds can be used to improve the usability of a graphics package. Sound was specifically used to aid problems with tool palettes and finding the current mouse coordinates when drawing. Tool palettes have usability problems because users need to see the information they present but they are often outside the area of visual focus. An experiment was conducted to investigate the effectiveness of adding sound to tool palettes. Earcons were used to indicate the current tool and when tool changes occurred. Results showed a significant reduction in the number of tasks performed with the wrong tool. Therefore users knew what the current tool was and did not try to perform tasks with the wrong tool. All of this was not at the expense of making the interface any more annoying to use.","1998","2023-07-06 01:12:37","2023-07-21 04:41:52","2023-07-06 01:12:37","287-302","","","","","","","","","","","Springer London","London","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-1-4471-3605-7_18","","/Users/minsik/Zotero/storage/PZG5K4EX/Brewster - 1998 - Using Earcons to Improve the Usability of a Graphi.pdf","","","","Johnson, Hilary; Nigay, Lawrence; Roast, Christopher","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I99E9DEG","bookSection","2008","Mendels, Philip; Frens, Joep","The Audio Adventurer: Design of a Portable Audio Adventure Game","Fun and Games","978-3-540-88321-0 978-3-540-88322-7","","","http://link.springer.com/10.1007/978-3-540-88322-7_5","In this paper we describe the design of a portable device for playing audio adventure games. This device enables the player to explore an audio world, interact with it, and solve challenges while a narrative evolves. To avoid the difficulties that can arise when freely navigating open spaces in audio-only worlds, we structured our audio world as a network of paths. Different ways of panning the audio to fit this model are proposed. Two initial rotational devices for navigating the audio world were created and evaluated: a relative and an absolute one. The relative one was worked out to a final prototype. Inventory functionality was added to increase the interactive possibilities and to make the device more expressive. Initial reactions were positive, but additional content and experiments are needed to investigate whether the Audio Adventurer can offer a long-lasting immersive and engaging experience.","2008","2023-07-06 01:12:37","2023-07-20 00:07:08","2023-07-06 01:12:37","46-58","","","5294","","","The Audio Adventurer","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-88322-7_5","","","","","","Markopoulos, Panos; De Ruyter, Boris; IJsselsteijn, Wijnand; Rowland, Duncan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FRRQSGEX","journalArticle","1975","Fulgosi, Ante; Zaja, Bozo","Information transmission of 3.1 bits in absolute identification of auditory pitch","Bulletin of the Psychonomic Society","","0090-5054","10.3758/BF03333207","http://link.springer.com/10.3758/BF03333207","Eleven tones (from 50 to 11,000 Hz), differing only in pitch, were presented to five subjects 108 times each. The subjects were told that in the ""training"" phase of the experiment, they should reach a definite, but unspecified, ""level"" of performance before they could advance to the final or ""test"" phase of the experiment. In the final quarter of the experiment, the median value of the identification performance was 3.11 bits.","1975-10","2023-07-06 01:12:37","2023-07-19 23:37:37","2023-07-06 01:12:37","379-380","","4","6","","Bull. Psychon. Soc.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/D6EFCRUA/Fulgosi and Zaja - 1975 - Information transmission of 3.1 bits in absolute i.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5ATY4GIU","journalArticle","2021","Salselas, Inês; Penha, Rui; Bernardes, Gilberto","Sound design inducing attention in the context of audiovisual immersive environments","Personal and Ubiquitous Computing","","1617-4909, 1617-4917","10.1007/s00779-020-01386-3","https://link.springer.com/10.1007/s00779-020-01386-3","Sound design has been a fundamental component of audiovisual storytelling in linear media. However, with recent technological developments and the shift towards non-linear and immersive media, things are rapidly changing. More sensory information is available and, at the same time, the user is gaining agency upon the narrative, being offered the possibility of navigating or making other decisions. These new characteristics of immersive environments bring new challenges to storytelling in interactive narratives and require new strategies and techniques for audiovisual narrative progression. Can technology offer an immersive environment where the user has the sensation of agency, of choice, where her actions are not mediated by evident controls but subliminally induced in a way that it is ensured that a narrative is being followed? Can sound be a subliminal element that induces attentional focus on the most relevant elements for the narrative, inducing storytelling and biasing search in an immersive non-linear audiovisual environment? Herein, we present a literature review that has been guided by this prospect. With these questions in view, we present our exploration process in finding possible answers and potential solution paths. We point out that consistency, in terms of coherency across sensory modalities and emotional matching may be a critical aspect. Finally, we consider that this review may open up new paths for experimental studies that could, in the future, provide new strategies in the practice of sound design in the context of non-linear media.","2021-08","2023-07-06 01:12:37","2023-07-21 04:54:19","2023-07-06 01:12:37","737-748","","4","25","","Pers Ubiquit Comput","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FNF4V7W4","journalArticle","2022","Mitre-Ortiz, Andres; Muñoz-Arteaga, Jaime; Cardona-Reyes, Héctor","Developing a model to evaluate and improve user experience with hand motions in virtual reality environments","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-022-00882-y","https://link.springer.com/10.1007/s10209-022-00882-y","In video games, the evaluation of the user experience (UX) mainly refers to two main groups of aspects, those that refer to the player that is mainly oriented to make the player feel good while playing and those that refer to the video game that is oriented to make the video game easy to understand and play. The aspects considered that are related to the player are engagement, enjoyment, and flow; the aspects related to video game, usability, and dependability. Virtual reality environments today have changed the paradigm in various fields of application, such as health, education, entertainment, among others. Therefore, it is important to observe the effects of handedness with hand movements in virtual reality environments. This work proposes a model to evaluate and improve the user experience considering player and video game aspects, taking into account handedness with hand movements in virtual reality environments. Player and video game aspects can be added to evaluations of the effect of handedness, especially in virtual reality environments, in order to know the user’s behavior in terms of skill, performance, and accuracy, among other features by using a particular hand to perform specific tasks. Next, a case study is presented with two groups of users using a virtual reality environment to perform several user tasks considering the dominant and non-dominant hand. By evaluating the user tasks it is possible to know the levels of engagement, enjoyment, motivation, and usability in a virtual reality environment. Finally, an analysis of results is presented in which several improvements of UX are presented.","2022-05-26","2023-07-06 01:12:37","2023-07-21 05:12:09","2023-07-06 01:12:37","","","","","","Univ Access Inf Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/AZ72SMY5/Mitre-Ortiz et al. - 2022 - Developing a model to evaluate and improve user ex.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3YB7BJ6K","bookSection","2005","Adachi, Kazuya; Iwai, Ken’ichiro; Yamada, Eiji; Cohen, Michael","Multimodal Wayfinding in a Driving Simulator for the $^{\rm {\bf S}}_{\rm {\bf c}}{\rm {\bf ha}}_{\rm {\bf i}}{\rm {\bf r}}^{\rm {\bf e}}$ Internet Chair, a Networked Rotary Motion Platform","Entertainment Computing - ICEC 2005","978-3-540-29034-6 978-3-540-32054-8","","","http://link.springer.com/10.1007/11558651_50","We are exploring idss (intelligent driver support systems), especially including way-finding presented via spatial audio. (“Way-finding” refers to giving a driver directions, as via car navigation [“Car-Nabi”] gps/gis systems.) We have developed a networked driving simulator as a virtual-reality based interface (control/display system) featuring integration with the Schaire rotary motion platform for azimuth-display, stereographic display for 3d graphics, and spatial audio (sound spatialization) way-finding cues.","2005","2023-07-06 01:12:37","2023-07-19 23:59:26","2023-07-06 01:12:37","511-514","","","3711","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11558651_50","","","","","","Kishino, Fumio; Kitamura, Yoshifumi; Kato, Hirokazu; Nagata, Noriko","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YQ2IX2CC","journalArticle","2022","Norwood, Michael Francis; Lakhani, Ali; Watling, David Phillip; Marsh, Chelsea Hannah; Zeeman, Heidi","Efficacy of Multimodal Sensory Therapy in Adult Acquired Brain Injury: A Systematic Review","Neuropsychology Review","","1040-7308, 1573-6660","10.1007/s11065-022-09560-5","https://link.springer.com/10.1007/s11065-022-09560-5","Abstract                            Adults who experience an acquired brain injury often experience disorders of consciousness, physical difficulties, and maladaptive behaviours. Multimodal sensory therapy may benefit brain injured patients, however the extent this therapy can facilitate rehabilitation is not well understood. This systematic review aimed to synthesize multimodal sensory therapy research for adults affected by acquired brain injury. PRISMA guidelines were followed and searches for work published up until July 2021 were undertaken in 5 databases, finding 1054 articles. 43 articles were included in the study. Results describe 29 studies related to coma following an acquired brain injury and 14 to no coma studies (mostly stroke). Multimodal sensory therapy was mostly used as a coma arousal technique following traumatic brain injury, finding positive effects. Multimodal sensory therapy was less applied in stroke, no coma rehabilitation, where most studies found improvement in somatosensory sensation and motor control in an affected limb. In several no coma studies, effects were maintained after several months. The most common senses stimulated in coma studies were audio (               N                = 30), tactile (               N                = 28), visual (               N                = 26), olfactory (               N                = 22), and gustatory (               N                = 17), while the most common senses stimulated in stroke, no coma studies were proprioception (               N                = 7), tactile (               N                = 8), and stereognosis (               N                = 4). Multimodal sensory therapy can be beneficial for patients, especially those in a minimally conscious state or attempting physical rehabilitation following stroke. Negative findings are infrequent in the current literature base. Multimodal sensory therapy appears to be a low-risk intervention with positive outcomes.","2022-09-02","2023-07-06 01:12:37","2023-07-06 01:12:37","2023-07-06 01:12:37","","","","","","Neuropsychol Rev","Efficacy of Multimodal Sensory Therapy in Adult Acquired Brain Injury","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/38IS75QI/Norwood et al. - 2022 - Efficacy of Multimodal Sensory Therapy in Adult Ac.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LKHCV6Q7","journalArticle","2017","Yuan, Yougen; Xie, Lei; Fu, Zhong-Hua; Xu, Ming; Cong, Qi","Sound image externalization for headphone based real-time 3D audio","Frontiers of Computer Science","","2095-2228, 2095-2236","10.1007/s11704-016-6182-2","http://link.springer.com/10.1007/s11704-016-6182-2","3D audio effects can provide immersive auditory experience, but we often face the so-called in-head localization (IHL) problem in headphone sound reproduction. To address this problem, we propose an effective sound image externalization approach. Specifically, we consider several important factors related to sound propagation, which include image-source model based early reflections with distance decay, wall absorption and air absorption, late reverberation and other dynamic factors like head movement. We apply our sound image externalization approach to a headphone based real-time 3D audio system. Subjective listening tests show that the sound image externalization performance is significantly improved and the sound source direction is preserved as well. A/B preference test further shows that, as compared with a recent popular approach, the proposed approach is mostly preferred by the listeners.","2017-06","2023-07-06 01:12:37","2023-07-20 00:06:59","2023-07-06 01:12:37","419-428","","3","11","","Front. Comput. Sci.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZE9LK3WP","bookSection","2011","Rhee Oh, Yoo; Kim, Hong Kook; Choi, Seung Ho","An Efficient Approach to Machine Control Sound Generation for Interfacing Real and Virtual Environments","Informatics Engineering and Information Science","978-3-642-25326-3 978-3-642-25327-0","","","http://link.springer.com/10.1007/978-3-642-25327-0_43","This paper presents the motivations and methods to generate the machine control sound for the virtual environments. The factors of machine control sound are investigated to handle the sound efficiently in the virtual systems and some of the controllable factors are implemented. First, we propose a new sound file format to find or generate the proper sound with the sound factors in the virtual systems that cause various kind of events and sounds. Then, we apply the proposed sound generating technique to the virtual system for a reality model, especially focused on the MP player.","2011","2023-07-06 01:12:37","2023-07-20 06:33:56","2023-07-06 01:12:37","507-516","","","251","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Communications in Computer and Information Science DOI: 10.1007/978-3-642-25327-0_43","","","","","","Abd Manaf, Azizah; Zeki, Akram; Zamani, Mazdak; Chuprat, Suriayati; El-Qawasmeh, Eyas","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CXYF83H5","bookSection","1999","Fenton-Kerr, Tom","GAIA: An Experimental Pedagogical Agent for Exploring Multimodal Interaction","Computation for Metaphors, Analogy, and Agents","978-3-540-65959-4 978-3-540-48834-7","","","http://link.springer.com/10.1007/3-540-48834-0_9","This paper discusses GAIA (Graphic-Audio Interface Agent), an experimental interface agent used in a pedagogical simulation program, REM(the Re-mapping Europa Mission), where the learning task is the discrimination of specific locations on a series of unlabelled maps. The agent’s task is to enhance the learning experience by providing timely, contextual clues mediated through a graphic/audio interface. Factors that influence such an agent’s ability to provide effective help, such as modes of agent representation, are discussed in the context of differing uses requiring alternative mode choices. The experimental context is explored with an in-depth look at the REM program. The paper concludes with comments on audio interfaces, suggestions for multimodal agent design and likely future directions for multimodal agent interfaces.","1999","2023-07-06 01:12:37","2023-07-19 23:47:17","2023-07-06 01:12:37","154-164","","","1562","","","GAIA","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-48834-0_9","","","","","","Nehaniv, Chrystopher L.","Goos, G.; Hartmanis, J.; Van Leeuwen, J.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5NZKJ5CJ","bookSection","2010","Nordahl, Rolf; Berrezag, Amir; Dimitrov, Smilen; Turchet, Luca; Hayward, Vincent; Serafin, Stefania","Preliminary Experiment Combining Virtual Reality Haptic Shoes and Audio Synthesis","Haptics: Generating and Perceiving Tangible Sensations","978-3-642-14074-7 978-3-642-14075-4","","","http://link.springer.com/10.1007/978-3-642-14075-4_18","We describe a system that can provide combined auditory and haptic sensations that arise while walking on different grounds. The simulation is based on a physical model that drives both haptic transducers embedded in sandals and headphones. The model is able to represent walking interactions with solid surfaces that can creak, be covered with crumpling material. The simulation responds to pressure on the floor by a vibrotactile signal felt by the feet. In a preliminary discrimination experiment, 15 participants were asked to recognize four different surfaces in a list of sixteen possibilities and under three different conditions, haptics only, audition only and combined haptic-audition. The results indicate that subjects are able to recognize most of the stimuli in the audition only condition, and some of the material properties such as hardness in the haptics only condition. The combination of auditory and haptic cues does not significantly improve recognition.","2010","2023-07-06 01:12:37","2023-07-20 00:17:04","2023-07-06 01:12:37","123-129","","","6192","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-14075-4_18","","/Users/minsik/Zotero/storage/GT94Z256/Nordahl et al. - 2010 - Preliminary Experiment Combining Virtual Reality H.pdf","","","","Kappers, Astrid M. L.; Van Erp, Jan B. F.; Bergmann Tiest, Wouter M.; Van Der Helm, Frans C. T.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XAXYS2C7","bookSection","2005","Bologna, Guido; Vinckenbosch, Michel","Eye Tracking in Coloured Image Scenes Represented by Ambisonic Fields of Musical Instrument Sounds","Mechanisms, Symbols, and Models Underlying Cognition","978-3-540-26298-5 978-3-540-31672-5","","","http://link.springer.com/10.1007/11499220_34","We present our recent project on visual substitution by Ambisonic 3D-sound fields. Ideally, our system should be used by blind or visually impaired subjects having already seen. The original idea behind our targeted prototype is the use of an eye tracker and musical instrument sounds encoding coloured pixels. The role of the eye tracker is to activate the process of attention inherent in the vision and to restore by simulation the mechanisms of central and peripheral vision. Moreover, we advocate the view that cerebral areas devoted to the integration of information will play a role by rebuilding a global image of the environment. Finally, the role of colour itself is to help subjects distinguishing coloured objects or perceiving textures, such as sky, walls, grass and trees, etc ...","2005","2023-07-06 01:12:37","2023-07-21 04:28:55","2023-07-06 01:12:37","327-337","","","3561","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11499220_34","","","","","","Mira, José; Álvarez, José R.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RFI6NRC7","bookSection","2007","Kimura, Atsunobu; Ihara, Masayuki; Kobayashi, Minoru; Manabe, Yoshitsugu; Chihara, Kunihiro","Visual Feedback: Its Effect on Teleconferencing","Human-Computer Interaction. HCI Applications and Services","978-3-540-73109-2","","","http://link.springer.com/10.1007/978-3-540-73111-5_67","We present shared visual feedback for supporting conversations in contingent auditory environments like teleconferences. To facilitate the initiation of conversations in such environments, it is critical that the caller be able to grasp the auditory channel between the caller’s mouth and the receiver’s ear, and to vocalize at the voice level proper for the receiver. To achieve this goal, feedback of the voice level as measured at the receiver’s ear is needed. Our starting points were a first generation prototype that displays visual feedback on the caller’s screen and a second generation prototype that projects visual feedback onto the floor in the receiver’s room. To enhance the speaker’s assurance and to make installation easier, a third prototype is implemented as a LED device and a microphone. An experiment is conducted on the third prototype and its effectiveness in supporting natural conversations in telecommunication sessions in daily use environments is confirmed.","2007","2023-07-06 01:14:13","2023-07-20 06:31:17","2023-07-06 01:14:13","591-600","","","4553","","","Visual Feedback","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-73111-5_67","","","","","","Jacko, Julie A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FDCNTN8U","journalArticle","2022","Kwon, Nahyun; Lee, Yunjung; Oh, Uran","Supporting a crowd-powered accessible online art gallery for people with visual impairments: a feasibility study","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-021-00814-2","https://link.springer.com/10.1007/s10209-021-00814-2","While people with visual impairments are interested in artwork as much as their sighted peers, their experience is limited to few selective artworks that are exhibited at certain museums. To enable people with visual impairments to access and appreciate as many artworks as possible at ease, we propose an online art gallery that allows users to explore different parts of a painting displayed on their touchscreen-based devices while listening to corresponding verbal descriptions of the touched part on the screen. To investigate the scalability of our approach, we first explored if anonymous crowd who may not have expertise in art are capable of providing visual descriptions of artwork as a preliminary study. Then we conducted a user study with 9 participants with visual impairments to explore the potential of our system for independent artwork appreciation by assessing if and how well the system supports 4 steps of Feldman Model of Criticism. The findings suggest that visual descriptions of artworks produced by an anonymous crowd are sufficient for people with visual impairments to interpret and appreciate paintings with their own judgments which is different from existing approaches that focused on delivering descriptions and opinions written by art experts. Based on the lessons learned from the study, we plan to collect visual descriptions of a greater number of artwork and distribute our online art gallery publicly to make more paintings accessible for people with visual impairments.","2022-11","2023-07-06 01:14:13","2023-07-21 05:11:41","2023-07-06 01:14:13","967-982","","4","21","","Univ Access Inf Soc","Supporting a crowd-powered accessible online art gallery for people with visual impairments","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/SWX8Z9AG/Kwon et al. - 2022 - Supporting a crowd-powered accessible online art g.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J5Y8HEUJ","journalArticle","2022","Chai, Yujin; Weng, Yanlin; Wang, Lvdi; Zhou, Kun","Speech-driven facial animation with spectral gathering and temporal attention","Frontiers of Computer Science","","2095-2228, 2095-2236","10.1007/s11704-020-0133-7","https://link.springer.com/10.1007/s11704-020-0133-7","In this paper, we present an efficient algorithm that generates lip-synchronized facial animation from a given vocal audio clip. By combining spectral-dimensional bidirectional long short-term memory and temporal attention mechanism, we design a light-weight speech encoder that learns useful and robust vocal features from the input audio without resorting to pre-trained speech recognition modules or large training data. To learn subject-independent facial motion, we use deformation gradients as the internal representation, which allows nuanced local motions to be better synthesized than using vertex offsets. Compared with state-of-the-art automatic-speech-recognition-based methods, our model is much smaller but achieves similar robustness and quality most of the time, and noticeably better results in certain challenging cases.","2022-06","2023-07-06 01:14:13","2023-07-20 00:06:50","2023-07-06 01:14:13","163703","","3","16","","Front. Comput. Sci.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LINLKGUS","bookSection","2010","Graf, Christian","Verbally Annotated Tactile Maps – Challenges and Approaches","Spatial Cognition VII","978-3-642-14748-7 978-3-642-14749-4","","","http://link.springer.com/10.1007/978-3-642-14749-4_26","Survey knowledge of spatial environments can be successfully conveyed by visual maps. For visually impaired people, tactile maps have been proposed as a substitute. The latter are hard to read and to understand. This paper proposes how the cognitive disadvantages can be compensated for by Verbally Annotated Tactile (VAT) maps. VAT maps combine two representational components: a verbal annotation system as a propositional component and a tactile map as a spatial component. It is argued that users will benefit from the cross-modal interaction of both. In a pilot study it is shown that using tactile You-Are-Here maps that only implement the spatial component is not optimal. I argue that some of the problems observed can be compensated for by incorporating verbal annotations. Research questions on cross-modal interaction in VAT maps are formulated that address the challenges that have to be overcome in order to benefit from propositional and spatial representations induced by VAT maps.","2010","2023-07-06 01:14:13","2023-07-21 05:02:28","2023-07-06 01:14:13","303-318","","","6222","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-14749-4_26","","","","","","Hölscher, Christoph; Shipley, Thomas F.; Olivetti Belardinelli, Marta; Bateman, John A.; Newcombe, Nora S.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JLXLCGIH","bookSection","2015","Yonezawa, Tomoko","Auditory Browsing Interface of Ambient and Parallel Sound Expression for Supporting One-to-many Communication","Distributed, Ambient, and Pervasive Interactions","978-3-319-20803-9 978-3-319-20804-6","","","http://link.springer.com/10.1007/978-3-319-20804-6_21","In this paper, we introduce an auditory browsing system for supporting one-to-many communication in parallel with an ongoing discourse, lecture, or presentation. The live reactions of audiences should reflect the main speech from the viewpoint of active participation. In order to browse numerous live comments from audiences, the speaker stretches her/his neck toward a particular section of the virtual audience group. We adopt the metaphor of “looking inside” toward the direction of the seating position with repositioned and overlaid audiences’ voices corresponding to the length of the voice regardless of the seating of real audiences. As a result, the speaker could browse the comments of the audience and show the communicative behaviors when she/he was interested in a particular group of the audience’s utterances.","2015","2023-07-06 01:14:13","2023-07-19 23:58:12","2023-07-06 01:14:13","224-233","","","9189","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-20804-6_21","","/Users/minsik/Zotero/storage/39WJEZH4/Yonezawa - 2015 - Auditory Browsing Interface of Ambient and Paralle.pdf","","","","Streitz, Norbert; Markopoulos, Panos","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I8KEESWQ","bookSection","2018","Ustek, Dilan; Chow, Kevin; Zhang, Haihua; MacLean, Karon","A Multimodal Illusion of Force Improves Control Perception in Above-Surface Gesture: Elastic Zed-Zoom","Haptics: Science, Technology, and Applications","978-3-319-93444-0 978-3-319-93445-7","","","http://link.springer.com/10.1007/978-3-319-93445-7_26","Emerging above-surface technology is an opportunity to exploit interaction spaces above a device’s surface; however, the resulting loss of the proprioceptive feedback available from on-surface interactions degrades the user’s sense of control and precision. We asked whether a pseudohaptic illusion (PHI) could help: a sense of force in the absence of actual contact, induced by manipulating the relation of body motion to graphical and auditory cues. To examine the value of above-surface PHIs, we used a zooming microtask, because finger occlusion impedes current implementations on small displays such as smartwatches. In a qualitative study (N = 12), we were able to trigger a physical illusion most often described as elasticity in 92% of participants through physical control/graphical display (C/D) manipulation, and that audio cues significantly strengthened the illusion. Participants experiencing this PHI reported improved sense of control when zooming, and found the interaction’s physicality natural.","2018","2023-07-06 01:14:13","2023-07-20 00:17:17","2023-07-06 01:14:13","295-308","","","10893","","","A Multimodal Illusion of Force Improves Control Perception in Above-Surface Gesture","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-93445-7_26","","","","","","Prattichizzo, Domenico; Shinoda, Hiroyuki; Tan, Hong Z.; Ruffaldi, Emanuele; Frisoli, Antonio","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VUYPTH23","bookSection","2007","Brodersen, Christina; Iversen, Ole Sejer","Dressing up for school work: Supporting a collaborative environment with heterogeneous technologies","ECSCW 2007","978-1-84800-030-8","","","http://link.springer.com/10.1007/978-1-84800-031-5_14","This paper approaches heterogeneity and heterogeneous technology as assets, rather than limitations, in the development of computer supported cooperative work. We demonstrate how heterogeneous technologies sustain teachers’ and students’ school work by presenting four different prototypes (the HyConExplorer, the eCell, the iGame- Floor and the eBag) that complement one another because they offer different functionalities and are, at the same time, designed with the wholeness of school activities, particularly group-based ones, in mind. Thus, they provide teachers and students with a broad range of IT support to aid them in and outside of the classroom. We take the school domain as our point of departure, but argue that the focus on heterogeneous technologies is applicable for the general area of CSCW.","2007","2023-07-06 01:14:13","2023-07-19 23:58:23","2023-07-06 01:14:13","251-270","","","","","","Dressing up for school work","","","","","Springer London","London","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-1-84800-031-5_14","","","","","","Bannon, Liam J.; Wagner, Ina; Gutwin, Carl; Harper, Richard H. R.; Schmidt, Kjeld","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3V9ZX7T5","bookSection","2017","Song, Yucheng; Wang, Xiaochen; Yang, Cheng; Gao, Ge; Chen, Wei; Tu, Weiping","Frame-Independent and Parallel Method for 3D Audio Real-Time Rendering on Mobile Devices","MultiMedia Modeling","978-3-319-51813-8 978-3-319-51814-5","","","http://link.springer.com/10.1007/978-3-319-51814-5_19","As 3D audio is a fundamental medium of virtual reality (VR), 3D audio real-time rendering technique is essential for the implementation of VR, especially on the mobile devices. While constrained by the limited computational power, the computation load is too high to implement 3D audio real-time rendering on the mobile devices. To solve this problem, we propose a frame-independent and parallel method of framing convolution, to parallelize process of 3D audio rendering using head-related transfer function (HRTF). In order to refrain from the dependency of overlap-add convolution over the adjacent frames, the data of convolution result is added on the final results of the two adjacent frames. We found our method could reduce the calculation time of 3D audio rendering significantly. The results were 0.74 times, 0.5 times and 0.36 times the play duration of si03.wav (length of 27 s), with Snapdragon 801, Kirin 935 and Helio X10 Turbo, respectively.","2017","2023-07-06 01:14:13","2023-07-21 04:30:58","2023-07-06 01:14:13","221-232","","","10133","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-51814-5_19","","","","","","Amsaleg, Laurent; Guðmundsson, Gylfi Þór; Gurrin, Cathal; Jónsson, Björn Þór; Satoh, Shin’ichi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YPQHDGGX","bookSection","2005","Bentley, Peter J.; Novakovic, Gordana; Ruto, Anthony","Fugue: An Interactive Immersive Audiovisualisation and Artwork Using an Artificial Immune System","Artificial Immune Systems","978-3-540-28175-7 978-3-540-31875-0","","","http://link.springer.com/10.1007/11536444_1","The role of T-cells within the immune system is to confirm and assess anomalous situations and then either respond to or tolerate the source of the effect. To illustrate how these mechanisms can be harnessed to solve real-world problems, we present the blueprint of a T-cell inspired algorithm for computer security worm detection. We show how the three central T-cell processes, namely T-cell maturation, differentiation and proliferation, naturally map into this domain and further illustrate how such an algorithm fits into a complete immune inspired computer security system and framework.","2005","2023-07-06 01:14:13","2023-07-19 11:32:50","2023-07-06 01:14:13","1-12","","","3627","","","Fugue","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/11536444_1","","/Users/minsik/Zotero/storage/A6MQ9TEN/Bentley et al. - 2005 - Fugue An Interactive Immersive Audiovisualisation.pdf","","","","Jacob, Christian; Pilat, Marcin L.; Bentley, Peter J.; Timmis, Jonathan I.","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2IJH7HJ2","journalArticle","2020","Patrick, Rafael N. C.; Letowski, Tomasz R.; McBride, Maranda E.","A multimodal auditory equal-loudness comparison of air and bone conducted sounds","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-020-00320-4","http://link.springer.com/10.1007/s12193-020-00320-4","The term ‘multimodal’ typically refers to the combination of two or more sensory modalities; however, through the advancement of technology, modality variations within specific sensory systems are being discovered and compared in regards to physiological perception and response. The ongoing evaluation of air vs bone conduction auditory perception modalities is one such comparison. Despite an increased awareness of the potential benefits of utilizing bone conduction pathways, a complete understanding of the human auditory system, more specifically, the relationship between air conducted and bone conducted sound remains a critical deficiency hindering the development of advanced multimodal auditory displays. Conduction equivalency ratios (CERs), which were defined as the difference in sound intensity levels (in dB) between equally loud signals transmitted in air conduction (AC) (sound field) and bone conduction (BC) modes provided a link between these two modes of hearing by determining the relationship between spectral content of AC and BC sound. The current report aims to describe, in depth, the establishment of such CERs at three BC transducer contact locations on a listener’s head over a range of audible frequencies presented over three signal intensities within a controlled free-field listening environment. Results indicated the AC–BC relationship is not unique and depends on sound intensity, frequency, and BC transducer location. In addition, in terms of head sensitivity, results support similar findings which indicate that the Mastoid and Condyle locations can be considered interchangeable in terms of their frequency-related sensitivity while the Forehead was found to be considerably less sensitive compared to the other locations.","2020-06","2023-07-06 01:14:13","2023-07-20 07:04:01","2023-07-06 01:14:13","199-206","","2","14","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PFT2GLIP","bookSection","2020","Lyu, Yanjun; Mechtley, Brandon; Hayes, Lauren; Sha, Xin Wei","Tableware: Social Coordination Through Computationally Augmented Everyday Objects Using Auditory Feedback","HCI International 2020 – Late Breaking Papers: Digital Human Modeling and Ergonomics, Mobility and Intelligent Environments","978-3-030-59986-7 978-3-030-59987-4","","","http://link.springer.com/10.1007/978-3-030-59987-4_24","This research develops a novel way of rethinking cultural and social behavior using computationally augmented artifacts. These ‘instruments’ provide various types of auditory feedback when manipulated by certain actions within social contexts, such as a bar or dining space. They foster affective social engagement through the habitual and explorative actions that they afford in everyday contexts, and their resulting auditory feedback. The goal is not only to observe how social interactions are affected by the manipulation of augmented artifacts, but also to observe how the sounds and manipulations affect psycho-sociological [1] changes towards more collaborative social relations during the processes of participatory sense-making [2]. In this paper, we present: a) a study of dynamic social interaction and how we instrumented tangible artifacts to reflect and induce engagement, b) a literature review that provides background for our design methodology, c) ‘vocal prototyping’–a responsive media technique for developing action-sonic mappings, d) our experimental prototype based on this design methodology.","2020","2023-07-06 01:14:13","2023-07-20 05:50:04","2023-07-06 01:14:13","332-349","","","12429","","","Tableware","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-59987-4_24","","","","","","Stephanidis, Constantine; Duffy, Vincent G.; Streitz, Norbert; Konomi, Shin'ichi; Krömker, Heidi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GJLZXH79","bookSection","1999","Mikhak, Bakhtiar; Martin, Fred; Resnick, Mitchel; Berg, Robbie; Silverman, Brian","The Children’s Machines: Handheld and Wearable Computers Too","Handheld and Ubiquitous Computing","978-3-540-66550-2 978-3-540-48157-7","","","http://link.springer.com/10.1007/3-540-48157-5_5","In this paper we describe the material of a construction kit designed to allow children to build their own handheld and wearable devices to meet their interests and passions. Children don’t work with these machines, they learn, play and grow with them. Informed by the types of projects that children have done with this material in the context of educational research in science and engineering, we present a few scenarios for why children would build their own handheld or wearable computational devices. We believe that these application scenarios and their appeal to children are strong evidence for why we should rethink the design of computational devices, particularly for children.","1999","2023-07-06 01:14:13","2023-07-20 00:09:31","2023-07-06 01:14:13","31-43","","","1707","","","The Children’s Machines","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-48157-5_5","","/Users/minsik/Zotero/storage/PYER3B6A/Mikhak et al. - 1999 - The Children’s Machines Handheld and Wearable Com.pdf","","","","Gellersen, Hans-W.","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5SLLETHL","journalArticle","2010","Norman, Neil L.","Feasts in Motion: Archaeological Views of Parades, Ancestral Pageants, and Socio-Political Process in the Hueda Kingdom, 1650–1727 AD","Journal of World Prehistory","","0892-7537, 1573-7802","10.1007/s10963-010-9037-z","http://link.springer.com/10.1007/s10963-010-9037-z","This paper examines the socio-political processes surrounding spectacular religious parades and more private acts of veneration and supplication within the Hueda Kingdom (c. 1650–1727 AD) in the Republic of Bénin, West Africa. The first goal of this paper is to posit the role that such public and private ceremonies played in framing negotiations of political and moral authority. It argues that ceremonial hosts and assembled audience members worked to channel toward their own interests the social, political, and economic outpouring that resulted from ritually sanctioned performances of wealth. The second goal of the paper is to illustrate material and spatial dimensions of such ceremonial spaces. These themes, drawn from a historically well-known polity, work together to aid in comparative theory building on feasting and the politics of spectacle in the deeper archaeological past.","2010-12","2023-07-06 01:14:13","2023-07-20 06:50:12","2023-07-06 01:14:13","239-254","","4","23","","J World Prehist","Feasts in Motion","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5E5BJ5E3","bookSection","2015","Jeon, Myounghoon; Croschere, Jayde","Sorry, I’m Late; I’m Not in the Mood: Negative Emotions Lengthen Driving Time","Engineering Psychology and Cognitive Ergonomics","978-3-319-20372-0 978-3-319-20373-7","","","http://link.springer.com/10.1007/978-3-319-20373-7_22","A considerable amount of research has shown that anger degenerates driving performance [e.g., 1, 2, 3], but little research has empirically shown other affective effects on driving. To investigate angry and sad effects on driving, we conducted a driving simulation study with induced affective states. In cognitive psychology, there is the “sadder but wiser” phenomenon, but given that driving is a complex, dynamic task that engages not only basic cognitive processes, but also other critical elements such as decision making, action selection, and motor control, it might result in different outcomes. Thirty-two participants were induced into sad, angry, or neutral affective states and asked to complete a driving task using a medium fidelity driving simulator. Measures included driving performance, subjective mood ratings, and a NASA-TLX workload index. Results showed that participants in the angry and sad conditions took significantly more time to complete the driving task compared to the neutral condition.","2015","2023-07-06 01:14:13","2023-07-19 23:58:44","2023-07-06 01:14:13","237-244","","","9174","","","Sorry, I’m Late; I’m Not in the Mood","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-20373-7_22","","","","","","Harris, Don","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EZ7Z6S9J","journalArticle","2011","Zhou, Feng; Xu, Qianli; Jiao, Roger Jianxin","Fundamentals of product ecosystem design for user experience","Research in Engineering Design","","0934-9839, 1435-6066","10.1007/s00163-010-0096-z","http://link.springer.com/10.1007/s00163-010-0096-z","Recognizing the importance of user experience (UX) in product ecosystems, this paper examines the fundamental issues underlying product ecosystem design, including implications of a product ecosystem, the notion of the ambience, as well as UX in terms of affect and cognition. A conceptual model is outlined to elucidate the critical factors and the operational mechanism of product ecosystem design for user satisfaction. A technical framework of product ecosystem design for UX is presented, consisting of three consecutive and iterative stages, namely, affective-cognitive need acquisition, affective-cognitive analysis, and affective-cognitive fulfillment. An application to subway station design is reported to illustrate the key techniques for product ecosystem design, including ambient intelligence, data mining, Petri-net modeling, and simulation.","2011-01","2023-07-06 01:15:52","2023-07-21 04:57:14","2023-07-06 01:15:52","43-61","","1","22","","Res Eng Design","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y92X6F6G","bookSection","2015","Tanuma, Motoki; Oka, Makoto; Mori, Hirohiko","Human Characteristics of Figure Recognition in Tactile Feedback","Human Interface and the Management of Information. Information and Knowledge Design","978-3-319-20611-0 978-3-319-20612-7","","","http://link.springer.com/10.1007/978-3-319-20612-7_44","In car, information presented to drivers is increasing and most of information is done using the visual and auditory displays. Presenting the information only to visual and auditory modes must cause drivers’ cognitive overloads in the near future and it is necessary to find the way using other modes to reduce them. In this study, we especially focus on human tactile figure recognition of the train of sticking stimuli and examine the human characteristics of what kinds of figures people can recognize as the tactile feedback. We developed tactile device that expresses four figures. We found there are interactions between the interval time of each sticking and the figures and human has quite different mechanisms between the cases of the simultaneous sticking and the consecutive sticking in recognizing the figures.","2015","2023-07-06 01:15:52","2023-07-20 05:53:03","2023-07-06 01:15:52","458-465","","","9172","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-20612-7_44","","","","","","Yamamoto, Sakae","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HRU4M56U","bookSection","2004","Lyon, Kirstin; Nürnberg, Peter J.","Interface Design – Use of Audio as an Output","Metainformatics","978-3-540-22010-7 978-3-540-24647-3","","","http://link.springer.com/10.1007/978-3-540-24647-3_7","This paper analyses a number of audio interface models that are currently in use or being developed. It describes a space for describing various models of interfaces that could be used by both visually impaired (VI) and mobile computer users. This paper is concerned only with the use of audio as an output cue. Visualisation is an increasingly important method for people to understand complex information and to navigate around structured information. Computer-based visualisation techniques often depend almost entirely on high-resolution graphics. There are several situations in which this is insufficient.","2004","2023-07-06 01:15:52","2023-07-21 04:29:42","2023-07-06 01:15:52","79-88","","","3002","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-24647-3_7","","","","","","Hicks, David L.","Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Dough; Vardi, Moshe Y.; Weikum, Gerhard","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3Y7U6IA9","bookSection","2021","Marcondes, Francisco S.; Durães, Dalila; Gonçalves, Filipe; Fonseca, Joaquim; Machado, José; Novais, Paulo","In-Vehicle Violence Detection in Carpooling: A Brief Survey Towards a General Surveillance System","Distributed Computing and Artificial Intelligence, 17th International Conference","978-3-030-53035-8 978-3-030-53036-5","","","http://link.springer.com/10.1007/978-3-030-53036-5_23","Violence is a word that encompasses several meanings ranging from an actual fight to theft and several types of harassment. Therefore, violence detection through surveillance systems can be a quite difficult yet important task. The increasing use of carpooling services and vehicle sharing brought the need to implement a sufficient general surveillance system for monitoring these vehicles for assuring the passengers’ safety during the ride. This paper raised the literature for this matter, finding fewer research papers than it was expected for the in-vehicle perspective, noticeably to sexual harassment. Most of the research papers focused on out-vehicle issues such as runs over and vehicle theft. In-vehicle electronic components security and cockpit user experience were perceived as major concern areas. This paper discusses these findings and presents some insights about in-vehicle surveillance.","2021","2023-07-06 01:15:52","2023-07-19 23:57:39","2023-07-06 01:15:52","211-220","","","1237","","","In-Vehicle Violence Detection in Carpooling","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Advances in Intelligent Systems and Computing DOI: 10.1007/978-3-030-53036-5_23","","","","","","Dong, Yucheng; Herrera-Viedma, Enrique; Matsui, Kenji; Omatsu, Shigeru; González Briones, Alfonso; Rodríguez González, Sara","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IAJE6CYI","bookSection","2016","Karam, M.; Langdon, P. M.","Designing Human Somatosensory System Interactions: Not Just for Haptics Any More!","Designing Around People","978-3-319-29496-4 978-3-319-29498-8","","","http://link.springer.com/10.1007/978-3-319-29498-8_19","We present an elaboration and application of a proposed framework highlighting the somatosensory system in our understanding of the design and development of computer interactions for the human body. The Somatosensory system encompasses the entire range of sensations, organisms, and mechanisms relating to the human sense of touch, and this framework is intended to serve as a tool for broadening our understanding of the multidisciplinary aspects that influence all interactions designed for the body. The framework illustrates a preliminary approach to organizing all touch-based systems into four categories of critical parameters that can enable the effective comparison of different technologies and systems applications, and approaches to human somatosensory system interactions (HSI). In this paper, the framework is applied to evaluate and compare four speech-to-touch (ST) systems towards informing the design of a novel system that uses tactile-acoustic devices to improve speech comprehension.","2016","2023-07-06 01:15:52","2023-07-19 23:56:00","2023-07-06 01:15:52","187-196","","","","","","Designing Human Somatosensory System Interactions","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-3-319-29498-8_19","","","","","","Langdon, Pat; Lazar, Jonathan; Heylighen, Ann; Dong, Hua","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MD3B9AK3","bookSection","2020","Regimbal, Juliette; Radi, Nusaiba; Weill–Duflos, Antoine; Cooperstock, Jeremy R.","Single-Actuator Simultaneous Haptic Rendering for Multiple Vital Signs","HCI International 2020 - Late Breaking Papers: Multimodality and Intelligence","978-3-030-60116-4 978-3-030-60117-1","","","http://link.springer.com/10.1007/978-3-030-60117-1_19","Haptic displays have been investigated as a possible way to reduce the effects of alarm fatigue in clinical environments. Previous displays have employed multiple vibrotactile actuators, using the spatial dimension to aid in conveying information of a number of vital signs. However, inspired by prior work investigating multidimensional tactons, we wished to examine the effectiveness of a single actuator to communicate information regarding multiple vital signs simultaneously. The results of our evaluation suggest that this is not only feasible, but that with a carefully designed encoding strategy, we may be able obtain perception performance comparable to that achievable with multi-actuator displays.","2020","2023-07-06 01:15:52","2023-07-20 05:49:46","2023-07-06 01:15:52","261-270","","","12424","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-60117-1_19","","","","","","Stephanidis, Constantine; Kurosu, Masaaki; Degen, Helmut; Reinerman-Jones, Lauren","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ETGVZRRX","bookSection","2004","Zhang, Qiong; Chen, Taiyi","A Progressive Sounding Object Model in Virtual Environment","Entertainment Computing – ICEC 2004","978-3-540-22947-6 978-3-540-28643-1","","","http://link.springer.com/10.1007/978-3-540-28643-1_75","Realistic audio is a requisite part of an immersive VR system. Previous research primarily focused on sound transmission modeling, e.g. room acoustics modeling. A progressive sounding object model based on modal synthesis is proposed in this article to integrate sound source modeling with sound transmission simulation. It is characterized by direct construction from geometry data plus a handful of material properties of the virtual object, progressive representation in multiple levels and natural binaural modeling.","2004","2023-07-06 01:15:52","2023-07-19 23:59:56","2023-07-06 01:15:52","571-576","","","3166","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-28643-1_75","","","","","","Rauterberg, Matthias","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2Y5YBFLQ","journalArticle","1972","Aiken, Leona S.; Griffin, Lawrence R.","Visual and auditory processing of common pattern class structure","Perception & Psychophysics","","0031-5117, 1532-5962","10.3758/BF03210942","http://link.springer.com/10.3758/BF03210942","Visual and auditory classification of equivalent class-structured patterns were examined. Underlying patterns from two classes were translated into auditory tone sequences and visual polygons. All Ss classified 50 visual patterns and their direct auditory analogs. Visual classification accuracy exceeded auditory accuracy (p < .01); however, auditory accuracy improved when auditory classification was preceded by the visual task (p < .01). Based on group data, classification strategies appeared similar across modalities, with accuracy of classification of individual patterns predicted to the same degree by common measures of physical class structure across modalities. Ss' drawings of the prototypes also suggested a common strategy across modalities. While group data suggest some consistency of classification strategy across modalities, individual Ss were not at all consistent in their visual and auditory classifications.","1972-11","2023-07-06 01:15:52","2023-07-21 04:43:04","2023-07-06 01:15:52","492-496","","6","12","","Perception & Psychophysics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/DI7LVED5/Aiken and Griffin - 1972 - Visual and auditory processing of common pattern c.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"75JBB4P2","bookSection","2003","Ilmonen, Tommi; Kontkanen, Janne","Software Architecture for Multimodal User Input - FLUID","Universal Access Theoretical Perspectives, Practice, and Experience","978-3-540-00855-2 978-3-540-36572-3","","","http://link.springer.com/10.1007/3-540-36572-9_25","Traditional ways to handle user input in software are uncomfortable when an application wishes to use novel input devices. This is especially the case in gesture based user interfaces. In this paper we describe these problems and as a solution we present an architecture and an implementation of a user input toolkit. We show that the higher level processing of user input such as gesture recognition requires a whole newkind of paradigm. The system we designed and implemented — FLexible User Input Design (FLUID) — is a lightweight library that can be used in different kinds of software. The potential application areas include all system where novel input devices are in use: virtual reality, entertainment systems and embedded systems.","2003","2023-07-06 01:15:52","2023-07-21 05:13:31","2023-07-06 01:15:52","319-338","","","2615","","","","","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/3-540-36572-9_25","","","","","","Carbonell, Noëlle; Stephanidis, Constantine","Goos, Gerhard; Hartmanis, Juris; Van Leeuwen, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BLTNJDC3","bookSection","2001","Ramloll, Rameshsharma; Brewster, Stephen; Yu, Wai; Riedel, Beate","Using Non-speech Sounds to Improve Access to 2D Tabular Numerical Information for Visually Impaired Users","People and Computers XV—Interaction without Frontiers","978-1-85233-515-1 978-1-4471-0353-0","","","http://link.springer.com/10.1007/978-1-4471-0353-0_32","Weinvestiga ted twosolutions for numerical (2D)tab ular data discovery and overview for visually impaired and blind users. One involved accessing information in tables (26 rows x10 columns containing integers between and including 0and 100)by this target user group using both speech and non-speech sounds. The other involved accessing similar information in tables of the same size through speech only by the same user group. Wefound that opportunities to access data through non-speech sounds result in ahighly significant decrease in the overall subjectiveworkloa d, morespecifically in the mental, temporal, performance and frustration workload categories. This subjectiveworkloa dassessment was supported by our quantitativeresults which showed ahighly significant decrease in the averagetime taken to complete agiven data comprehension task and a significant increase in the number of successfully completed tasks.","2001","2023-07-06 01:15:52","2023-07-21 04:42:32","2023-07-06 01:15:52","515-529","","","","","","","","","","","Springer London","London","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-1-4471-0353-0_32","","/Users/minsik/Zotero/storage/SYBKNHN7/Ramloll et al. - 2001 - Using Non-speech Sounds to Improve Access to 2D Ta.pdf","","","","Blandford, Ann; Vanderdonckt, Jean; Gray, Phil","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WXWJJ9PZ","journalArticle","1974","Leshowltz, Barry; Hanzi, Raquel","Serial position effects for tonal stimuli","Memory & Cognition","","0090-502X, 1532-5946","10.3758/BF03197500","http://link.springer.com/10.3758/BF03197500","Serial position effects for tones were studied in a recognition memory experiment. The S was given a stimulus list consisting 01' several tone bursts followed by a number 01' test tones. Accuracy 01' recognition 01' stimulus items as a function 01' input position followed the c1assical bowed serial position curve. Memory strength was a monotonically decreasing function 01' position in the test list. The da ta were fitted with a strength theory model 01' memory. The fit yielded decay parameters corresponding to stimulus- and response-induced interference, wh ich were comparable to the parameters reported for meaningful verbal material.","1974-01","2023-07-06 01:15:52","2023-07-21 04:29:32","2023-07-06 01:15:52","112-116","","1","2","","Memory & Cognition","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/RI3HHGRN/Leshowltz and Hanzi - 1974 - Serial position effects for tonal stimuli.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MBFV9W6Y","journalArticle","1978","Syrdal-Lasky, Ann","Effects of intensity on the categorical perception of stop consonants and isolated second formant transitions","Perception & Psychophysics","","0031-5117, 1532-5962","10.3758/BF03204146","http://link.springer.com/10.3758/BF03204146","Identification and discrimination of two-formant [bae-dae-gae] and [pae-tae-kae] synthetic speech stimuli and discrimination of corresponding isolated second formant transitions (chirps) were performed by six subjects. Stimuli were presented at several intensity levels such that the intensity of the F2 transition was equated between speech and nonspeech stimuli, or the overall intensity of the stimulus was equated. At higher intensity (92 dB), b-d-g and p-t-k identification and between-category discrimination performance declined and bilabial-alveolar phonetic boundaries shifted in location on the continuum towards the F2 steady-state frequency. Between-category discrimination improved from performance at 92 dB when 92-dB speech stimuli were simultaneously masked by 60-dB speech noise; alveolar-velar boundaries shifted to a higher frequency location in the 92-dB-plus-noise condition. Chirps were discriminated categorically when presented at 58 dB, but discrimination peaks declined at higher intensities. Perceptual performance for chirps and p-t-k stimuli was very similar, and slightly inferior to performance for b-d-g stimuli, where simultaneous masking by Fl resulted in a lower effective intensity of F2. The results were related to a suggested model involving pitch comparison and transitional quality perceptual strategies.","1978-09","2023-07-06 01:15:52","2023-07-21 04:44:45","2023-07-06 01:15:52","420-432","","5","23","","Perception & Psychophysics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/SHM82UPG/Syrdal-Lasky - 1978 - Effects of intensity on the categorical perception.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4YB2XSEL","bookSection","2018","Morgan, Phillip L.; Voinescu, Alexandra; Williams, Craig; Caleb-Solly, Praminda; Alford, Chris; Shergold, Ian; Parkhurst, Graham; Pipe, Anthony","An Emerging Framework to Inform Effective Design of Human-Machine Interfaces for Older Adults Using Connected Autonomous Vehicles","Advances in Human Aspects of Transportation","978-3-319-60440-4 978-3-319-60441-1","","","http://link.springer.com/10.1007/978-3-319-60441-1_33","Connected autonomous vehicles (CAVs) represent an exciting opportunity for wider access to mobility; especially for individuals unable to drive manual vehicles. Interaction with CAVs will be through human-machine interfaces (HMIs) providing journey-related and other information with some interactivity. These should be designed with potential users as part of a co-design process to maximize acceptance, engagement, and trust. This paper presents an emerging framework to inform the design of in-vehicle CAV HMIs with a focus on older adults (70-years+). These could be amongst early adopters of CAVs and tend to have the highest level of cognitive, sensory, and physical impairments. Whilst there are numerous principles on HMI design for older adults there are fewer on HMIs for AVs, and a need for research on CAV HMI design principles for older adults. Our emerging framework is novel and important for designers of CAV HMIs for older adults and other potential users.","2018","2023-07-06 01:15:52","2023-07-19 11:11:50","2023-07-06 01:15:52","325-334","","","597","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Advances in Intelligent Systems and Computing DOI: 10.1007/978-3-319-60441-1_33","","/Users/minsik/Zotero/storage/99BN8XFZ/Morgan et al. - 2018 - An Emerging Framework to Inform Effective Design o.pdf","","","","Stanton, Neville A","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DF5KRXKD","bookSection","2000","Pai, Dinesh K.","Robotics in Reality-Based Modeling","Robotics Research","978-1-4471-1254-9 978-1-4471-0765-1","","","http://link.springer.com/10.1007/978-1-4471-0765-1_43","Building realistic models of physical objects presents both a significant challenge for robotics and a potentially important area of applications. These models must support interactive simulation in rich, multimodal virtual environments with not only 3D graphics, but also haptic force feedback and auditory displays. In this paper we discuss the problems and opportunities of acquiring such comprehensive models using robotic measurement facilities. We also describe ACME, the UBC Active Measurement Facility, a telerobotic system for building reality-based models.","2000","2023-07-06 01:15:52","2023-07-21 04:59:09","2023-07-06 01:15:52","353-358","","","","","","","","","","","Springer London","London","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-1-4471-0765-1_43","","","","","","Hollerbach, John M.; Koditschek, Daniel E.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J4KHCDRG","journalArticle","1977","McGovern, Katharine; Strange, Winifred","The perception of /r/ and /l/ in syllable-initial and syllable-final position","Perception & Psychophysics","","0031-5117, 1532-5962","10.3758/BF03198720","http://link.springer.com/10.3758/BF03198720","American English liquids Irl and III have been considered intermediate between stop consonants and vowels acoustically, articulatorily, phonologically, and perceptually. Cutting (l947a) found position-dependent ear advantages for liquids in a dichotic listening task: syllable-initial liquids produced significant right ear advantages, while syllable-final liquids produced no reliable ear advantages. The present study employed identification and discrimination tasks to determine whether Irl and III are perceived differently depending on syllable position when perception is tested by a different method. Fifteen subjects listened to two synthetically produced speech series-/lil to Iril and lilI to lir/-in which stepwise variations of the third formant cued the difference in consonant identity. The results indicated that: (1) perception did not differ between syllable positions (in contrast to the dichotic listening results), (2) liquids in both syllable positions were perceived categorically, and (3) discrimination of a nonspeech control series did not account for the perception of the speech sounds.","1977-03","2023-07-06 01:15:52","2023-07-21 04:44:01","2023-07-06 01:15:52","162-170","","2","21","","Perception & Psychophysics","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/8ID89L6D/McGovern and Strange - 1977 - The perception of r and l in syllable-initial .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DVKX7PW9","journalArticle","2007","Persad, Umesh; Langdon, Patrick; Clarkson, John","Characterising user capabilities to support inclusive design evaluation","Universal Access in the Information Society","","1615-5289, 1615-5297","10.1007/s10209-007-0083-y","http://link.springer.com/10.1007/s10209-007-0083-y","Designers require knowledge and data about users to effectively evaluate product accessibility during the early stages of design. This paper addresses this problem by setting out the sensory, cognitive and motor dimensions of user capability that are important for product interaction. The relationship between user capability and product demand is used as the underlying conceptual model for product design evaluations and for estimating the number of people potentially excluded from using a given product.","2007-08-17","2023-07-06 01:15:52","2023-07-21 05:12:17","2023-07-06 01:15:52","119-135","","2","6","","Univ Access Inf Soc","","","","","","","","en","","","","","DOI.org (Crossref)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BGM6CUSL","bookSection","2017","Xu, Jiawang; Wang, Xiaochen; Zhang, Maosheng; Yang, Cheng; Gao, Ge","Binaural Sound Source Distance Reproduction Based on Distance Variation Function and Artificial Reverberation","MultiMedia Modeling","978-3-319-51813-8 978-3-319-51814-5","","","http://link.springer.com/10.1007/978-3-319-51814-5_9","In this paper, a method combining the distance variation function (DVF) and image source method (ISM) is presented to generate binaural 3D audio with accurate feeling of distance. The DVF is introduced to indicate the change in intensity and inter-aural difference when the distance between listener and source changes. Then an artificial reverberation simulated by ISM is added. The reverberation introduces the energy ratio of direct-to-reverberant, which provides an absolute cue to distance perception. The distance perception test results indicate improvement for distance perception when sound sources located within 50 cm. In addition, the variance of perceptual distance was much smaller than that using DVF only. The reduction of variance is a proof that the method proposed in this paper can generate 3D audio with more accurate and steadier feeling of distance.","2017","2023-07-06 01:15:52","2023-07-21 04:31:05","2023-07-06 01:15:52","101-111","","","10133","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-51814-5_9","","","","","","Amsaleg, Laurent; Guðmundsson, Gylfi Þór; Gurrin, Cathal; Jónsson, Björn Þór; Satoh, Shin’ichi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3K59YW7X","bookSection","2015","Erkut, Cumhur; Rajala-Erkut, Anu; Dahl, Sofia","Exploring Felt Qualities of Embodied Interaction with Movement and Sound","Arts and Technology","978-3-319-18835-5 978-3-319-18836-2","","","http://link.springer.com/10.1007/978-3-319-18836-2_10","We present approaches for teaching and designing embodied interaction in collaboration with a contemporary dance choreographer. Our approaches are based on the felt qualities of movement, providing a shared experience, vocabulary for self-expression, and appreciation for movement as a design material for interaction design practitioners. In parallel, such activities provide art professionals competencies for new contexts. We present two workshops conducted at different times. The first workshop, back in 2009, brought about novel sonic interaction paradigms, technologies, and artifacts. The second workshop was carried out in March 2014, and we are in the process of developing interactive sketches by pairing our observations with motion tracking. In this paper, the activities in these workshops are presented, and reflected upon. In particular, we are investigating whether or not these activities guided the participants from the prevailing notion of command/control in embodied interaction towards experiences related to the felt qualities of movement.","2015","2023-07-06 01:15:52","2023-07-19 11:35:03","2023-07-06 01:15:52","77-85","","","145","","","","","","","","Springer International Publishing","Cham","","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering DOI: 10.1007/978-3-319-18836-2_10","","","","","","Brooks, Anthony Lewis; Ayiter, Elif; Yazicigil, Onur","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4FXGQGXN","journalArticle","2020","Mathew, Justin D.; Huot, Stéphane; Katz, Brian F. G.","Comparison of spatial and temporal interaction techniques for 3D audio trajectory authoring","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-019-00314-x","http://link.springer.com/10.1007/s12193-019-00314-x","With the popularity of immersive media, developing usable tools for content development is important for the production process. In the context of 3D audio production, user interfaces for authoring and editing 3D audio trajectories enable content developers, composers, practitioners, and recording and mixing engineers to define how audio sources travel in time. However, common interaction techniques in 3D audio production tools can make the workflow of this task tedious and difficult to accomplish. This study investigates this problem by classifying the atomic tasks (spatially and temporally) of a general composite task of authoring 3D audio trajectories and then evaluating different interaction techniques across these tasks. Common graphical user interfaces were compared with input devices having varying degrees-of-freedom for spatial atomic tasks in order to investigate the effect of direct manipulation and integrality of interaction techniques. Continuous and discrete interaction techniques were compared for temporal tasks in order to investigate the effect of direct manipulation. Results suggest that interaction techniques with high degrees of integrality and direct manipulation reduce task completion time compared to standard GUI techniques. The design of temporal tasks can create a visual bias, and discrete-time controls can be a suitable method for traversing a small number of control points. These results and further observations provide directions on the study of interaction technique design for 3D audio tools, which in turn should improve workflows of 3D audio content creation.","2020-03","2023-07-06 01:16:15","2023-07-20 07:01:58","2023-07-06 01:16:15","83-100","","1","14","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/648YFS96/Mathew et al. - 2020 - Comparison of spatial and temporal interaction tec.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FZT5WNQ7","journalArticle","2017","Taffou, Marine; Ondřej, Jan; O’Sullivan, Carol; Warusfel, Olivier; Viaud-Delmon, Isabelle","Judging crowds’ size by ear and by eye in virtual reality","Journal on Multimodal User Interfaces","","1783-7677, 1783-8738","10.1007/s12193-016-0228-5","http://link.springer.com/10.1007/s12193-016-0228-5","Judging the size of a group of people is an everyday task, on which many decisions are based. In the present study, we investigated whether judgment of size of different groups of people depended on whether they were presented through the auditory channel, through the visual channel, or through both auditory and visual channels. Groups of humanoids of different sizes (from 8 to 128) were presented within a virtual environment to healthy participants. They had to judge whether there was a lot of people in each group and rate their discomfort in relation to the stimuli with Subjective Units of Distress. Our groups of 96 and 128 virtual humans were judged as crowds regardless of their sensory presentation. The sensory presentation influenced participants’ judgment of virtual human group size ranging from 8 to 48. Moreover, while the quantity judgments in the auditory condition increased linearly with the group size, participants judged the quantity of people in a logarithmic manner in the two other sensory conditions. These results suggest that quantity judgment based on auditory information in a realistic context may often involve implicit arithmetic. Even though our participants were not phobic of crowds, our findings are of interest for the field of virtual reality-based therapy for diverse disorders because they indicate that quantity judgment can potentially be altered in a sensory-specific manner in patients with fear of crowds.","2017-03","2023-07-06 01:16:15","2023-07-20 07:04:56","2023-07-06 01:16:15","57-65","","1","11","","J Multimodal User Interfaces","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/minsik/Zotero/storage/672LHLEU/Taffou et al. - 2017 - Judging crowds’ size by ear and by eye in virtual .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""