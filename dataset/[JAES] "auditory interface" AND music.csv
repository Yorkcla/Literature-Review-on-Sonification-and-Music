"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"A36SRT6M","journalArticle","2012","Stewart, Rebecca; Sandler, Mark","Spatial Auditory Display in Music Search and Browsing Applications","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16637","","2012","2023-07-12 06:29:39","2023-07-12 06:29:39","","936–946","","11","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QKKVD953","journalArticle","2012","Altinsoy, M. Ercan","The Quality of Auditory-Tactile Virtual Environments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16164","In our daily lives, we usually perceive an event via more than one sensory modality (e.g., vision, hearing, touch). Therefore, multimodal integration and interactions play an important role when we use objects and for event recognition in our environment. A virtual environment (VE) is a computer simulation of a realistic-looking and interactive world. VEs should take into account the multisensory nature of humans and communicate with the user not only through vision but also through other modalities. In addition to vision, hearing and touch are the most commonly used communication channels. Recently, a variety of products with additional tactile input and output capabilities have been developed (e.g., Apple iPhone and other touch-screen devices, NintendoWii, etc.). Some of these devices provide new possibilities for interacting with a computer, including the auditory modality. Binaural synthesis and rendering are becoming key technologies for multimedia products. Virtual environments are no longer limited to academic research; they have commercial applications, particularly in medicine, game, and entertainment industries. Thus, the quality of VEs is becoming increasingly important. User interaction with a VE is a key issue in the perception of its quality. Several studies have discussed the quality of displays, input and output devices (for different modalities) as well as software and hardware issues; however, multimodal user interaction should also be examined. This paper focuses on the parameters that influence the quality of audio-tactile VEs.","2012","2023-07-12 06:29:43","2023-07-19 03:36:11","","38–46","","1/2","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5F23X2GN","journalArticle","2012","Peres, S. Camille","A Comparison of Sound Dimensions for Auditory Graphs: Pitch Is Not So Perfect","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16367","","2012","2023-07-12 06:29:48","2023-07-12 06:29:48","","561–567","","7/8","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WR2UGGN4","journalArticle","2012","Jeon, Myounghoon; Gupta, Siddharth; Davison, Benjamin K.; Walker, Bruce N.","Auditory Menus Are Not Just Spoken Visual Menus: A Case Study of “Unavailable” Menu Items","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16362","","2012","2023-07-12 06:29:52","2023-07-12 06:29:52","","505–518","","7/8","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U2M8WD2K","journalArticle","2014","Begault, Durand R.; Bittner, Rachel M.; Anderson, Mark R.","Multimodal Information Management: Evaluation of Auditory and Haptic Cues for NextGen Communication Displays","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17333","Auditory communication displays within the Next Generation Air Transport System (currently under development in the United States) will likely require an improvements in the user interface for selecting amongst multiple incoming messages. Interface design can impact both user performance and preference. Two design factors were evaluated: physical pressure-sensitive switches versus flatpanel ""virtual switches,"" and auditory feedback from switch contact. Performance with stimuli using physical switches was 1.2 s faster than virtual switches (2.0 s vs. 3.2 s); auditory feedback provided a 0.6 s performance advantage (2.3 s vs. 2.9 s). The subjective results show a significant preference and superior performance for physical pressure-sensitive switches having audio feedback, compared to touch-panel virtual switches. The correlation between objective measures of performance and subjective ratings of preference and performance was shown to be high. Overall, the results indicate that any replacement of physical controls by virtual touch screens must be considered carefully, and should include audio feedback.","2014","2023-07-12 06:29:57","2023-07-19 03:40:54","","375–385","","6","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NXJQ75NP","journalArticle","2012","Jensen, Kristoffer; Hjortkjær, Jens","An Improved Dissonance Measure Based on Auditory Memory","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16325","Because dissonance is an important part of music analysis and psychoacoustic research, improving the model for predicting dissonance is useful. In this new model, musical events are assumed to reside in short-term memory, lasting on the range of 3 to 5 seconds. The total dissonance is the sum of the local dissonance from the new event and the interaction with elements in memory. In subjective tests with different kinds of music, the model with memory consistently better predicts listeners’ reports of dissonance. Dissonant sounds in music give rise to a physiological arousal response in listeners.","2012","2023-07-12 06:30:01","2023-07-19 04:08:30","","350–354","","5","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4I4SH6F6","journalArticle","1995","Crispien, Kai; Ehrenberg, Tasso","Evaluation of the -Cocktail-Party Effect- for Multiple Speech Stimuli within a Spatial Auditory Display","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=7922","","1995","2023-07-12 06:30:05","2023-07-12 06:30:05","","932–941","","11","43","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RBAJVWIA","journalArticle","2012","Parseihian, Gaëtan; Katz, Brian F. G.","Morphocons: A New Sonification Concept Based on Morphological Earcons","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16355","","2012","2023-07-12 06:30:11","2023-07-12 06:30:11","","409–418","","6","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FRNBMGKT","journalArticle","2018","Cuadrado, Francisco","Touch the Sound: Design and Development of a Tangible System for Sound Experimentation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19587","“Touch the Sound” is a technological tool specifically designed for children training in experimentation of sound. It is based on the interaction with physical and tangible elements (passive objects that children can organize and move inside a two-axis space). A tablet based app uses computer-vision procedures to recognize each object and its position and movement to playback audio files and modify different sound parameters in real time. The technological approach and design adopted according to the philosophy and objectives of the Touch the Sound project has proved to be effective, despite the drawbacks and technical problems encountered during the development process. Programming based on native Android OS tools has enabled the design of a system that achieves optimal performance results on devices with limited hardware resources. The system contributes to the media literacy of children, making them aware of the narrative possibilities of sound, and teaching them to control and modify its parameters.","2018","2023-07-12 06:30:18","2023-07-19 03:50:46","","478–485","","6","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9SS6VWIS","journalArticle","2012","Grond, Florian; Kramer, Oliver; Hermann, Thomas","Balancing Salience and Unobtrusiveness in Auditory Monitoring of Evolutionary Optimization","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16364","","2012","2023-07-12 06:30:26","2023-07-12 06:30:26","","531–539","","7/8","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6D7EUGA9","journalArticle","2012","Merchel, Sebastian; Altinsoy, M. Ercan; Stamm, Maik","Touch the Sound: Audio-Driven Tactile Feedback for Audio Mixing Applications","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16165","In this study experiments were conducted to determine if a person could distinguish percussive audio loops by their fingertips using audio-driven tactile feedback. The audio signal was adapted to generate a vibration signal (tactile feedback) taking into account the limited capabilities of the tactile modality. A systematic approach to find the different adaptation parameters is discussed. The vibrations were created by an electrodynamic shaker mounted behind a touch-sensitive screen. Results indicate percussive loops are best distinguished if the source features (e.g., frequency spectrum) and sequence features (e.g., rhythm) are maintained.","2012","2023-07-12 06:30:31","2023-07-19 04:31:17","","47–53","","1/2","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P9EQJTQP","journalArticle","2019","Wühle, Tom; Merchel, Sebastian; Altinsoy, M. Ercan","The Precedence Effect in Scenarios with Projected Sound","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19890","One solution to realize spatial sound reproduction without a large number of distributed loudspeakers is to create virtual sources in the desired directions by using highly focusing real sources that project sound on reflective boundaries. Auditory events are then ideally located in the direction of the virtual sources created by sound projection rather than the direction of the real sources. However, due to physically limited focusing capabilities of real sources, the perception of the listener is also influenced by sound that is directly radiated from the real source and, therefore, arrives earlier at the position of the listener. This study showed how emerging precedence caused by the leading direct sound affected auditory perception in scenarios with lagging projected sound. The level range from which the direct sound caused first noticeable changes in auditory events until it finally dominated the localization was found to be approximately 20 dB. It was shown that localization dominance of the projected sound did not immediately occur after localization dominance of the direct sound vanished. However, localization dominance of the projected sound occurred even though the presence of the direct sound was still perceptible. The results indicate that the temporal structure of the direct sound plays an important perceptual role in scenarios with projected sound. Real playback signals with complex temporal structures causing impulsive loudness fluctuations were shown to be more favorable for the perceptual dominance of the leading direct sound. Such structures are therefore critical in terms of sound projection.","2019","2023-07-12 06:30:35","2023-07-19 10:57:25","","92–100","","3","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TZ6HUC6S","journalArticle","2014","Sanz, Pablo Revuelta; Mezcua, Belén Ruiz; Pena, José M. Sánchez; Walker, Bruce N.","Scenes and Images into Sounds: A Taxonomy of Image Sonification Methods for Mobility Applications","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17130","","2014","2023-07-12 06:30:38","2023-07-12 06:30:38","","161–171","","3","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MR4DAAHT","journalArticle","2016","Lafay, Grégoire; Misdariis, Nicolas; Lagrange, Mathieu; Rossignol, Mathias","Semantic Browsing of Sound Databases without Keywords","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18371","With the growing capability of recording and storage devices, the problem of indexing large audio databases has been the object of much attention. Most of this effort is dedicated to automatic inferences from indexed metadata. In contrast, browsing audio databases in an effective manner has been less considered. This report studies the relevance of a semantic organization of sounds to ease the browsing of a sound database. For such a task, semantic access to data is traditionally implemented by a keyword selection process. However, various limitations of written language, such as word polysemy, ambiguities, or translation issues, may bias the browsing process. Two sound presentation strategies organized sounds spatially to reflect an underlying semantic hierarchy. For the sake of comparison, the authors also considered a display whose spatial organization was only based on acoustic cues. Those three displays were evaluated in terms of search speed in a crowdsourcing experiment using two different corpora: environmental sounds from urban environments and sounds produced by musical instruments. Coherent results demonstrate the usefulness of an implicit semantic organization for representing sounds in terms of both search speed and of learning efficiency.","2016","2023-07-12 06:30:46","2023-07-19 04:16:53","","628–635","","9","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SFEMZ42S","journalArticle","2006","Väljamäe, Aleksander; Larsson, Pontus; Västfjäll, Daniel; Kleiner, Mendel","Vibrotactile Enhancement of Auditory-Induced Self-Motion and Spatial Presence","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13883","The entertainment industry frequently uses vibroacoustic stimulation, where chairs with embedded loudspeakers and shakers enhance the experience. Scientific investigations of the effect of such enhancers on illusory self-motion (vection) and spatial presence are largely missing. The current study examined whether auditory-induced vection (AIV) may be further augmented by the simultaneous presentation of additional vibrotactile cues delivered via mechanical shakers and low-frequency sound. It was found that mechanically induced vibrations increase AIV and spatial presence responses significantly. This cross-modal enhancement was stronger for stimuli containing an auditory–tactile simulation of a vehicle engine, demonstrating the benefits of the multisensory representation of virtual environments.","2006","2023-07-12 06:30:50","2023-07-19 04:54:47","","954–963","","10","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SAAHAC8V","journalArticle","2018","Burloiu, Grigore; Mihai, Valentin; Damian, Stefan","Layered Motion and Gesture Sonification in an Interactive Installation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19860","","2018","2023-07-12 06:30:54","2023-07-12 06:30:54","","770–778","","10","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9G5AFW5S","journalArticle","2018","Stolfi, Ariane; Sokolovskis, Janis; Goródscy, Fábio; Iazzetta, Fernando; Barthet, Mathieu","Audio Semantics: Online Chat Communication in Open Band Participatory Music Performances","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19868","","2018","2023-07-12 06:30:58","2023-07-12 06:30:58","","910–921","","11","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I28HAY5N","journalArticle","2012","Bujacz, Michal; Skulimowski, Piotr; Strumillo, Pawel","Naviton—A Prototype Mobility Aid for Auditory Presentation of Three-Dimensional Scenes to the Visually Impaired","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16374","","2012","2023-07-12 06:31:02","2023-07-12 06:31:02","","696–708","","9","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8XLYEJ6M","journalArticle","2012","Wersényi, György","Virtual Localization by Blind Persons","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16368","","2012","2023-07-12 06:31:06","2023-07-12 06:31:06","","568–579","","7/8","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PUNKHMSI","journalArticle","2013","Lech, Michal; Kostek, Bozena","Testing A Novel Gesture-Based Mixing Interface","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16822","","2013","2023-07-12 06:31:09","2023-07-12 06:31:09","","301–313","","5","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8Q8SPGNP","journalArticle","2015","Kim, Sungyoung; Okumura, Hiraku; Otani, Makoto","Near-Field Sound Control Using a Planar Loudspeaker","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17566","","2015","2023-07-12 06:31:19","2023-07-12 06:31:19","","54–62","","1/2","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KCNNB8B5","journalArticle","2013","Colmenares, Juan A.; Peters, Nils; Eads, Gage; Saxton, Ian; Jacquez, Israel; Kubiatowicz, John D.; Wessel, David","A Multicore Operating System with QoS Guarantees for Network Audio Applications","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16703","","2013","2023-07-12 06:31:23","2023-07-12 06:31:23","","174–184","","4","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5LCI59SG","journalArticle","1994","Begault, Durand R.; Erbe, Tom","Multichannel Spatial Auditory Display for Speech Communications","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=6924","A spatial auditory display for multiple speech communications was developed at NASA/Ames Research Center. Input is spatialized by the use of simplified head-related transfer functions, adapted for FIR filtering on Motorola 56001 digital signal processors. Hardware and firmware design implementations are overviewed for the initial prototype developed for NASA-Kennedy Space Center. An adaptive staircase method was used to determine intelligibility levels of four-letter call signs used by launch personnel at NASA against diotic speech babble. Spatial positions at 30° azimuth increments were evaluated. The results from eight subjects showed a maximum intelligibility improvement of about 6-7 dB when the signal was spatialized to 60 or 90° azimuth positions.","1994","2023-07-12 06:31:26","2023-07-19 03:41:03","","819–826","","10","42","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PQQEFLWP","journalArticle","2012","Côté, Nicolas; Koehl, Vincent; Möller, Sebastian; Raake, Alexander; Wältermann, Marcel; Gautier-Turbin, Valérie","Diagnostic Instrumental Speech Quality Assessment in a Super-Wideband Context","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16213","A new speech quality model, DIAL (Diagnostic Instrumental Assessment of Listening), provides diagnostic information in both narrow-band and super wide-band contexts. It is “intrusive,” assuming that the original source audio is available. Because many quality-measuring techniques collapse all degradations into a single score, they do not help developers to diagnose the basis of that score. In contrast, the proposed DIAL model uses four quality dimensions: coloration, continuity, noisiness, and loudness.","2012","2023-07-12 06:31:35","2023-07-19 03:50:11","","156–164","","3","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2QCVM727","journalArticle","1998","Begault, Durand R.","Virtual Acoustics, Aeronautics, and Communications","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12140","An optimal approach to auditory display design for commercial aircraft would utilize both spatialized (3-D) audio techniques and active noise cancellation for safer operations. Results from several aircraft simulator studies conducted at NASA Ames Research Center are reviewed, including Traffic alert and Collision Avoidance System (TCAS) warnings, spoken orientation 'beacons' for gate identification and collision avoidance on the ground, and hardware for improved speech intelligibility. The implications of hearing loss among pilots is also considered.","1998","2023-07-12 06:31:38","2023-07-19 03:40:45","","520–530","","6","46","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X4GF795Z","journalArticle","2022","Elmosnino, Stephane","A Review of Literature in Critical Listening Education","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21737","This paper reviews the literature on critical listening education. Broadly speaking, academic research in this field is often limited to qualitative descriptions of curriculum and studies on the effectiveness of technical ear training. Furthermore audio engineering textbooks often view critical listening as secondary to technical concepts. To provide a basis for the development of curriculum and training, this paper investigates both academic and non-academic work in the field. Consequently a range of common curriculum topics is advanced as the focus areas in current practice. Moreover this paper uncovers pedagogical best practice for training sequence and the use of sounds/sight within instruction. A range of specific instructional activities, such as technical ear training, is also explored, thus providing insights into training in this field. Beyond a direct benefit to pedagogues, it is hoped that this review of the literature can provide a starting point for research in critical listening education.","2022","2023-07-12 06:31:41","2023-07-19 03:54:27","","328–339","","5","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IK2DL2KC","journalArticle","2016","Mo, Ronald; Wu, Bin; Horner, Andrew","The Effects of Reverberation on the Emotional Characteristics of Musical Instruments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18055","Previous research has shown that reverberation influences the perception of clarity, spaciousness, and other aspects of music. But the degree to which reverberation influences the emotional experience of musical instrument sounds is still not known. The authors conducted a listening test to compare the effect of reverberation on the emotional characteristics of eight instrument sounds representing the wind and bowed string families. The subjects compared paired stimuli for eight emotional categories: Happy, Sad, Heroic, Scary, Comic, Shy, Romantic, and Mysterious. For simple parametric reverberation, the results showed the following: a significant effect on Mysterious and Romantic for the back of a large hall; a medium effect on Sad, Scary, and Heroic for the back of a large hall; a mild effect on Happy for the front of a small hall; relatively little effect on Shy; and the opposite effect on Comic, with listeners judging anechoic sounds most Comic. These results give audio engineers and musicians an interesting perspective on simple parametric artificial reverberation. The motivation for this research was to understand how emotional characteristics vary with reverberation length and amount in simple parametric reverberation, which are equivalent to the hall size and the listeners distance to the front.","2016","2023-07-12 06:31:44","2023-07-19 04:32:15","","966–979","","12","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"83IYBR56","journalArticle","2014","Hjortkjær, Jens; Walther-Hansen, Mads","Perceptual Effects of Dynamic Range Compression in Popular Music Recordings","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17084","The belief that the use of dynamic range compression in music mastering deteriorates sound quality needs to be formally tested. In this study normal hearing listeners were asked to evaluate popular music recordings in original versions and in remastered versions with higher levels of dynamic range compression. Surprisingly, the results failed to reveal any evidence of the effects of dynamic range compression on subjective preference or perceived depth cues. Perceptual data suggest that listeners are less sensitive than commonly believed to even high levels of compression. As measured in terms of differences in the peak-to-average ratio, compression has little perceptual effect other than increased loudness or clipping effects that only occur at high levels of compression. One explanation for the inconsistency between data and belief might result from the fact that compression is frequently accompanied by additional processing such as equalization and stereo enhancement.","2014","2023-07-12 06:31:47","2023-07-19 04:06:18","","37–41","","1/2","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3EEP4QC9","journalArticle","2019","Pulkki, Ville; Pöntynen, Henri; Santala, Olli","Spatial Perception of Sound Source Distribution in the Median Plane","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20700","Modern spatial audio reproduction techniques with headphones or loudspeakers seek to control the perceived spatial image as accurately as possible in three dimensions. The mechanisms of spatial perception have been studied mainly in the horizontal plane, and this article attempts to shed some light on the corresponding phenomena in the median plane. Spatial perception of concurrently active sound sources was investigated in an exploratory listening experiment. Incoherent noise source distributions of varying spatial characteristics were presented from loudspeaker arrays in anechoic conditions. The arrays were coinciding with the ±45° angular sectors in the frontal median and horizontal planes. The task for immobile subjects was to report the directions of loudspeakers they perceived emitting sound. The results from median plane distributions suggest that two concurrent sources located along the vertical midline can be perceived individually without resorting to head movements when they are separated in elevation by 60° or more. With source pairs separated by less than 60°, and with more complex physical distributions, the distributions were perceived inaccurately, biased, and spatially compressed but nevertheless not as point-like auditory images.","2019","2023-07-12 06:31:51","2023-07-19 04:39:35","","855–870","","11","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LLBRRDXY","journalArticle","2006","Karjalainen, Matti; Mäki-patola, Teemu; Kanerva, Aki; Huovilainen, Antti","Virtual Air Guitar","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13884","[Engineering Report] A combination of handheld controllers and a guitar synthesizer is called “virtual air guitar” (VAG). The name refers to playing an “air” guitar, that is, just acting the playing with music playback, and the term virtual refers to making a playable synthetic instrument. Sensing of the left-to-right-hand distance is used for pitch control, the right-hand movements are used for plucking, and in advanced versions of the VAG the finger positions of both hands can be used for other features of sound production. Three different hand gesture controllers are discussed. The sound synthesis algorithm simulates the electric guitar, augmented with sound effects such as tube amplifier distortion, as well as intelligent mapping from playing gestures to synthesis parameters. The realization of the virtual instrument is described, and sound demonstrations are available on a Web site.","2006","2023-07-12 06:31:55","2023-07-19 04:10:14","","964–980","","10","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L3E8IDGD","journalArticle","2014","Evreinova, Tatiana V.; Evreinov, Grigori; Raisamo, Roope","An Exploration of Volumetric Data in Auditory Space","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17131","","2014","2023-07-12 06:32:00","2023-07-12 06:32:00","","172–187","","3","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EP34DRFC","journalArticle","2001","Blesser, Barry A.","An Interdisciplinary Synthesis of Reverberation Viewpoints","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=10176","","2001","2023-07-12 06:32:04","2023-07-12 06:32:04","","867–903","","10","49","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CU4KYMI6","journalArticle","2022","Lindetorp, Hans; Falkenberg, Kjetil","Evaluating Web Audio for Learning, Accessibility, and Distribution","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22019","","2022","2023-07-12 06:32:08","2023-07-12 06:32:08","","951–961","","11","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4PUH7M83","journalArticle","2016","Woodcock, James; Davies, William J.; Cox, Trevor J.; Melchior, Frank","Categorization of Broadcast Audio Objects in Complex Auditory Scenes","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18297","","2016","2023-07-12 06:32:11","2023-07-12 06:32:11","","380–394","","6","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RT9Z6Y34","journalArticle","2012","Grosshauser, T.; Bläsing, B.; Spieth, C.; Hermann, T.","Wearable Sensor-Based Real-Time Sonification of Motion and Foot Pressure in Dance Teaching and Training","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16369","","2012","2023-07-12 06:32:16","2023-07-12 06:32:16","","580–589","","7/8","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N8AIKQYG","journalArticle","2021","Wühle, Tom; Merchel, Sebastian; Altinsoy, M. Ercan","Localization Masking—Reducing the Influence of the Direct Sound on Localization in Sound Projection by the Additional Generation of One or More Leading Sounds","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21464","In spatial audio reproduction with sound projection, instead of placing loudspeakers in specific directions, virtual sources are created by projecting sound on reflective boundaries. The projected sound should dominate the localization. One limiting factor is the leading direct sound occurring because of physical limitations of the focusing capabilities of sound projectors. In this paper, localization masking, a method to reduce the influence of this direct sound on localization, is introduced. Localization masking was investigated in an anechoic chamber with cascaded lead-lag pairs representing the sounds involved. The sounds were reproduced via individual loudspeakers. Natural percussion signals with transient temporal structures were used. The lag localization dominance threshold, defined as the maximum lead level at which the direction of the auditory events is in the direction of the lag, was measured using a method of adjustment. Localization masking caused this threshold to shift to up to a 7-dB higher lead level. Therefore localization masking reduced the influence of the initial lead, representing the direct sound, on localization. In practical sound projection scenarios, localization masking may improve the projection of signals with transient structures or reduce the requirements on the focusing capabilities of sound projectors that are used to project such signals.","2021","2023-07-12 06:32:19","2023-07-19 10:57:33","","683–693","","9","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"44J4DUMN","journalArticle","2004","Härmä, Aki; Jakka, Julia; Tikander, Miikka; Karjalainen, Matti; Lokki, Tapio; Hiipakka, Jarmo; Lorho, Gaëtan","Augmented Reality Audio for Mobile and Wearable Appliances","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13010","","2004","2023-07-12 06:32:23","2023-07-12 06:32:23","","618–639","","6","52","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZXBVZW65","journalArticle","2014","Merchel, Sebastian; Altinsoy, M. Ercan","The Influence of Vibrations on Musical Experience","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17134","The coupled perception of sound and vibration is a well-known phenomenon during live rock and pop concerts. Measurements in concert halls and churches have confirmed that sound can excite perceivable vibrations on the surface of the body even during classical performances. This research explores if vibrations have an influence on the quality of the listening experiences. Therefore, sound and seat vibrations were controlled separately in an audio reproduction scenario. Vibrations were generated from audio recordings using various approaches. Different parameters during this process were examined in relation to their perceptual consequences. It can be concluded that vibrations play a significant role in the perception of music. Real concert halls might benefit from amplifying the vibrations (passively or actively) in the auditorium.","2014","2023-07-12 06:32:27","2023-07-19 04:31:08","","220–234","","4","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J2IIK4RJ","journalArticle","2015","Manor, Ella; Martens, William; Marui, Atsushi; Cabrera, Densil","Nearfield Crosstalk Increases Listener Preferences for Headphone-Reproduced Stereophonic Imagery","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17638","Although final mixing and mastering is monitored over loudspeakers, the majority of music listeners use headphones on mobile devices. Preferences for spatial process depend on the method of reproduction. For a variety of program material using headphones, listeners often prefer a stereophonic image that is created by simulating nearfield crosstalk compared to the biphonic spatial image. This novel approach, called Nearfield Crosstalk Simulation, describes crosstalk that simulates closely located loudspeakers. Previous work used farfield crosstalk simulation in an effort to produce an enhanced stereophonic effect, but such results were less preferred. The primary difference between the more conventional farfield crosstalk and the novel nearfield crosstalk developed for this study was the introduction of a level and a time difference at low frequency, consistent with what actually occurs for sound sources very close to a listener’s head.","2015","2023-07-12 06:32:33","2023-07-19 04:26:51","","324–335","","5","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MYD9BU5I","journalArticle","2009","Korhola, Henri; Karjalainen, Matti","Perceptual Study and Auditory Analysis on Digital Crossover Filters","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14825","The extensive research on the perceptual attributes of analog filters used for loudspeaker crossover networks does not necessarily apply to digital filters. In this study finite-impulse response (FIR) and Linkwitz–Riley (LR) digital crossover filters were examined for their perceptual artifacts. Subjective tests with headphones and loudspeakers showed that for LR filters the audibility of phase distortion can be predicted by group delay errors. But FIR filters of high order produce audible artifacts because of time smear created by extensive ringing. LR filters of order 8 or less and FIR filters of about 600 were without problems. These safety limits should be respected.","2009","2023-07-12 06:32:37","2023-07-19 04:16:09","","413–429","","6","57","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7NP8A589","journalArticle","1994","Stuart, J. Robert","Noise: Methods for Estimating Detectability and Threshold","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=6959","It is often necessary to estimate or compare the detectability or significance of noises. A review and development of the underlying psychoacoustics of noise detection and threshold phenomena are presented. Particular attention is paid to noises of arbitrary spectral shape. Various measures of human auditory frequency selectivity are contrasted, including critical band, critical ratio, auditory filter, and active process considerations. Estimation techniques based on weighting are compared with those given by detection criteria applied to more sophisticated auditory modeling. Some definitive recommendations are made.","1994","2023-07-12 06:32:46","2023-07-19 04:50:12","","124–140","","3","42","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GFC9ZW4X","journalArticle","2017","Shirley, Ben Guy; Meadows, Melissa; Malak, Fadi; Woodcock, James Stephen; Tidball, Ash","Personalized Object-Based Audio for Hearing Impaired TV Viewers","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18562","Age demographics have led to an increase in the proportion of the population suffering from some form of hearing loss. The introduction of object-based audio to television broadcasting has the potential to improve the viewing experience for millions of hearing impaired people. Personalization of object-based audio can assist in overcoming difficulties in understanding speech and the narrative audio. This research presented describes a Multi-Dimensional Audio (MDA) implementation of object-based clean audio that presents independent object streams based on object-category elicitation. Evaluations were carried out with hearing impaired people, and participants were able to personalize audio levels independently for four object-categories using an on-screen menu: speech, music, background effects, and foreground effects related to on-screen events. Results show considerable preference variation across subjects but nevertheless the expanding object-category personalization beyond a binary speech/nonspeech categorization can substantially improve the viewing experience for some hearing impaired people.","2017","2023-07-12 06:32:49","2023-07-19 04:47:49","","293–303","","4","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VZFLGLQQ","journalArticle","2019","Geronazzo, Michele; Peruch, Enrico; Prandoni, Fabio; Avanzini, Federico","Applying a Single-Notch Metric to Image-Guided Head-Related Transfer Function Selection for Improved Vertical Localization","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20483","","2019","2023-07-12 06:32:53","2023-07-12 06:32:53","","414–428","","6","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M8HLAYUL","journalArticle","2012","Metatla, Oussama; Bryan-Kinns, Nick; Stockman, Tony","The Effects of Using Headphones and Speakers on Collaboration in an Audio-Only Workspace","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16365","","2012","2023-07-12 06:32:56","2023-07-12 06:32:56","","540–550","","7/8","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4GRGUDQI","journalArticle","2006","Choisel, Sylvain; Wickelmaier, Florian","Extraction of Auditory Features and Elicitation of Attributes for the Assessment of Multichannel Reproduced Sound","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13903","","2006","2023-07-12 06:32:59","2023-07-12 06:32:59","","815–826","","9","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LR7PKK6A","journalArticle","2016","Mo, Ronald; Choi, Ga Lam; Lee, Chung; Horner, Andrew","The Effects of MP3 Compression on Perceived Emotional Characteristics in Musical Instruments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18523","Musical instrument sounds have distinct timbral and emotional characteristics that can change when audio processing is applied. This paper investigates the effects of MP3 compression on the emotional characteristics of eight sustained instrument sounds using listening tests. The experimental paradigm involved a pairwise comparison of compressed and uncompressed samples at several bit rates over ten emotional categories. The results showed that MP3 compression strengthened neutral and negative emotional characteristics such as Mysterious, Shy, Scary, and Sad, and weakened positive emotional characteristics such as Happy, Heroic, Romantic, Comic, and Calm. Angry was relatively unaffected by MP3 compression, probably because the background “growl” artifacts added by MP3 compression decreased positive emotional characteristics and increased negative characteristics such as Mysterious and Scary. Compression effected some instruments more and others less; trumpet was the most effected and the horn the least.","2016","2023-07-12 06:33:03","2023-07-19 04:31:51","","858–867","","11","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2YPX7JJ4","journalArticle","2021","Kim, Sungyoung; Howie, Will","Influence of the Listening Environment on Recognition of Immersive Reproduction of Orchestral Music Sound Scenes","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21533","","2021","2023-07-12 06:33:06","2023-07-12 06:33:06","","834–848","","11","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RYLXEUMA","journalArticle","2022","Yang, Jing; Barde, Amit; Billinghurst, Mark","Audio Augmented Reality: A Systematic Review of Technologies, Applications, and Future Research Directions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22008","","2022","2023-07-12 06:33:09","2023-07-12 06:33:09","","788–809","","10","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UATCP3PU","journalArticle","2020","Kim, Chungeun; Lim, Veranika; Picinali, Lorenzo","Investigation Into Consistency of Subjective and Objective Perceptual Selection of Non-individual Head-Related Transfer Functions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20996","The binaural technique uses a set of direction-dependent filters known as Head-Related Transfer Functions (HRTFs) in order to create 3D soundscapes through a pair of headphones. Although each HRTF is unique to the person it ismeasured from, due to the cost and complexity of the measurement process pre-measured non-individual HRTFs are generally used. This study investigates whether it is possible for a listener to perceptually select the best-fitting non-individual HRTFs in a consistent manner, using both subjective and objective methods. 16 subjects participated in 3 repeated sessions of binaural listening tests. During each session, participants firstly listened tomoving sound sources spatialized using 7 different non-individual HRTFs and ranked them according to perceived plausibility and externalization (subjective selection). They then performed a localization task with sources spatialized using the same HRTFs (objective selection). In the subjective selection, 3 to 9 participants showed test-retest reliability levels that could be regarded as good or excellent depending on the attribute under question, the source type, and the trajectory. The reliability was better for participants with musical training and critical audio listening experience. In the objective selection, it was not possible to find significant differences between the tested HRTFs based on localization-related performances.","2020","2023-07-12 06:33:12","2023-07-19 04:11:25","","819–831","","11","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F97CEIJW","journalArticle","2021","Turchet, Luca; Rinaldo, Edoardo","Technical Performance Assessment of the Ableton Link Protocol Over Wi-Fi","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21469","To date, Ableton Link is the most widely adopted synchronization protocol for musical applications based on Wi-Fi networks. However the limitations of Link over Wi-Fi in terms of scalability are not known, an understanding that may be useful to designers of musical ecosystems involving many nodes to be synchronized. In this paper we present four experiments aiming to investigate how the protocol performance is affected by the number of connected devices, kind of Wi-Fi access point utilized, and connection or disconnection of nodes. Results showed the reliability of the protocol only for a limited number of nodes, which was 22 for a consumer-grade portable router and 41 for a mesh network created by two high-end access points. The protocol performances were found to decrease with the number of devices and when nodes connected or disconnected. Furthermore, the performances of Link are tightly bounded to that of Wi-Fi, which can vary significantly from day to day depending on network load and interferences. Taken together, our findings indicate that Link over Wi-Fi is not suitable for ensuring synchronization in ecosystems with a high number of nodes, and we call for new wireless technologies suitable for large scale synchronizations in co-located settings.","2021","2023-07-12 06:33:15","2023-07-19 04:53:57","","748–756","","10","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FD8MEDIZ","journalArticle","2006","Faller, Christof","Multiple-Loudspeaker Playback of Stereo Signals","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13886","A perceptually motivated spatial decomposition for two-channel stereo audio signals, capturing the information about the virtual sound stage, is proposed. The spatial decomposition allows resynthesizing audio signals for playback over sound systems other than twochannel stereo. With the use of more front loudspeakers the width of the virtual sound stage can be increased beyond ±30° and the sweet-spot region is extended. Optionally, lateral independent sound components can be played back separately over loudspeakers on the sides of a listener to increase listener envelopment. It is also explained how the spatial decomposition can be used with surround sound and wavefield synthesis–based audio systems.","2006","2023-07-12 06:33:19","2023-07-19 03:55:28","","1051–1064","","11","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8XKCN68J","journalArticle","2016","Rummukainen, Olli; Romblom, David; Guastavino, Catherine","Diffuse Field Modeling Using Physically-Inspired Decorrelation Filters and B-Format Microphones: Part II Evaluation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18128","The Diffuse Field Model (DFM) described in Part 1 is perceptually evaluated in this article. Two experiments were conducted. In first experiment, sound recording professionals rated different treatments of DFM presented on a 20-channel array. This evaluation included the geometric modeling of reflections, strategies involving the early portion of the B-Format Room Impulse Response (RIR), and a comparison between 0th- and 1st-order RIR. Results indicate that it is necessary to model the earliest reflections and to use all four channels of the B-Format room impulse response. In the second experiment, musicians and sound recording professionals were asked to rate DFM and common microphone techniques presented on 3/2 stereophonic setup. DFM was found to be perceptually comparable to the Hamasaki Square technique. DFM approach used in this study is part of a physically-plausible virtual acoustic model for sources that were captured with close microphone placement. This model replaces the panning, delay, and reverberation that would typically be used. DFM is a perceptually viable method to create room impression that allows free placement of anechoic point sources in arbitrary multichannel loudspeaker setups.","2016","2023-07-12 06:33:22","2023-07-19 04:42:36","","194–207","","4","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3EHJAYK6","journalArticle","2016","Mulder, Johannes","Amplified Music and Sound Level Management: A Discussion of Opportunities and Challenges","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18123","Because music-induced hearing disorders and noise pollution from concerts emerge at the crossroads of technology, culture, and society, this paper argues that multidisciplinary approaches are required to address these two issues. Even though noise regulations and hearing risk-mitigation policies may be different, best practices originate from procedures and policies that are developed from an understanding of the multiple stakeholder perspectives. Each stakeholder has a unique perspective but these conflicting differences must be reconciled one way or another. Technology may offer options.","2016","2023-07-12 06:33:25","2023-07-19 04:34:06","","124–131","","3","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IW326I3K","journalArticle","2022","Lladó, Pedro; Mckenzie, Thomas; Meyer-Kahlen, Nils; Schlecht, Sebastian J.","Predicting Perceptual Transparency of Head-Worn Devices","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21825","Acoustically transparent head-worn devices are a key component of auditory augmented reality systems, in which both real and virtual sound sources are presented to a listener simultaneously. Head-worn devices can exhibit high transparency simply through their physical design but in practice will always obstruct the sound field to some extent. In this study, a method for predicting the perceptual transparency of head-worn devices is presented using numerical analysis of device measurements, testing both coloration and localization in the horizontal and median plane. Firstly, listening experiments are conducted to assess perceived coloration and localization impairments. Secondly, head-related transfer functions of a dummy head wearing the head-worn devices are measured, and auditory models are used to numerically quantify the introduced perceptual effects. The results show that the tested auditory models are capable of predicting perceptual transparency and are therefore robust in applications that they were not initially designed for.","2022","2023-07-12 06:33:30","2023-07-19 04:21:16","","585–600","","7/8","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2X6F9HGH","journalArticle","2021","Shier, Jordie; McNally, Kirk; Tzanetakis, George; Brooks, Ky Grace","Manifold Learning Methods for Visualization and Browsing of Drum Machine Samples","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21015","The use of electronic drum samples is widespread in contemporary music productions, with music producers having an unprecedented number of samples available to them. The task of organizing and selecting from these large collections can be challenging and time consuming, which points to the need for improved methods for user interaction. This paper presents a system that computationally characterizes and organizes drum machine samples in two dimensions based on sound similarity. The goal of the work is to support the development of intuitive drum sample browsing systems. The methodology presented explores time segmentation, which isolates temporal subsets from the input signal prior to audio feature extraction, as a technique for improving similarity calculations. Manifold learning techniques are compared and evaluated for dimensionality reduction tasks, and used to organize and visualize audio collections in two dimensions. This methodology is evaluated using a combination of objective and subjective methods including audio classification tasks and a user listening study. Finally, we present an open-source audio plug-in developed using the JUCE software framework that incorporates the findings from this study into an application that can be used in the context of a music production environment.","2021","2023-07-12 06:33:34","2023-07-19 04:47:40","","40–53","","1/2","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DBUXRZD8","journalArticle","2014","Pihlajamäki, Tapani; Santala, Olli; Pulkki, Ville","Synthesis of Spatially Extended Virtual Source with Time-Frequency Decomposition of Mono Signals","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17339","Auditory displays, driven by nonauditory data, are often used to present a sound scene to a listener. Typically, the sound field places sound objects at different locations, but the scene becomes aurally richer if the perceived sonic objects have a spatial extent (size), called volumetric virtual coding. Previous research in virtual-world Directional Audio Coding has shown that spatial extent can be synthesized from monophonic sources by applying a time-frequency-space decomposition, i.e., randomly distributing time-frequency bins of the source signal. This technique does not guarantee a stable size and the timbre can degrade. This study explores how to optimize volumetric coding in terms of timbral and spatial perception. The suggested approach for most types of audio uses an STFT window size of 1024 samples and then distributes the frequency bands from lowest to highest using the Halton sequence. The results from two formal listening experiments are presented.","2014","2023-07-12 06:33:39","2023-07-19 04:38:17","","467–484","","7/8","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CSTZ55J3","journalArticle","2015","Francombe, Jon; Mason, Russell; Dewhirst, Martin; Bech, Søren","A Model of Distraction in an Audio-on-Audio Interference Situation with Music Program Material","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17567","","2015","2023-07-12 06:33:46","2023-07-12 06:33:46","","63–77","","1/2","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PLJSW8DJ","journalArticle","1993","Persterer, Alexander; Opitz, Martin; Koppensteiner, Christian; Nefjodova, M.; Müller, Christian; Berger, Meinhard","AUDIMIR: Directional Hearing at Microgravity","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=7005","In October 1991 the first Austrian cosmonaut spent one week on board the Soviet space station MIR. One of the 14 experiments carried out there was AKG's AUDIMIR, a psychoacoustics technological experiment. AUDIMIR was designed to investigate the accuracy of directional hearing and its role as part of the human orientation system at microgravity.","1993","2023-07-12 06:33:53","2023-07-19 04:37:52","","239–247","","4","41","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZU2RNVBQ","journalArticle","2012","Herre, Jürgen; Purnhagen, Heiko; Koppens, Jeroen; Hellmuth, Oliver; Engdegård, Jonas; Hilper, Johannes; Villemoes, Lars; Terentiv, Leon; Falch, Cornelia; Hölzer, Andreas; Valero, María Luis; Resch, Barbara; Mundt, Harald; Oh, Hyen-O","MPEG Spatial Audio Object Coding—The ISO/MPEG Standard for Efficient Coding of Interactive Audio Scenes","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16371","In 2010 the ISO/MPEG Audio standardization group issued the Spatial Audio Object Coding (SAOC) specification to define technology for parametric low bit-rate coding of audio object signals with a mono or stereo downmix. This paper provides an overview of MPEG SAOC technology, discussing recent verification tests. The authors examine operation modes for typical application scenarios by taking advantage of object-based processing. Most important, SAOC enables transmission of multi-object signals at data rates of the same order of magnitude as those used to represent two-channel audio. The important application scenarios are envisaged to be high-quality spatial teleconferencing, personal audio, interactive gaming, and rich media. Because the SAOC representation is independent of any particular loudspeaker setup, SAOC signals can be rendered efficiently on either a target loudspeaker configuration or portable device.","2012","2023-07-12 06:33:56","2023-07-19 04:05:32","","655–673","","9","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IFCRQYDI","journalArticle","2015","Rottondi, Cristina; Buccoli, Michele; Zanoni, Massimiliano; Garao, Dario; Verticale, Giacomo; Sarti, Augusto","Feature-Based Analysis of the Effects of Packet Delay on Networked Musical Interactions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18047","When musicians at multiple physical locations attempt to play together, the limiting factor is unavoidable packet delays and jitter introduced by the IP network that connects them together. This research investigates the musical tolerance of adverse network conditions as a function of rhythmic complexity and tempo. Results show that with higher network latency: (a) musicians exhibit a more pronounced tendency to decelerate with more rhythmically complex pieces; (b) rhythmical complexity does not significantly worsen musician perception of the delay and interaction quality; (c) among the timbral features, instruments with a higher spectral entropy and spectral flatness (such as guitars and drums) lead to larger tempo slowdown. Low-latency networks promise to revolutionize interactive music such as remote rehearsals and music teaching.","2015","2023-07-12 06:33:59","2023-07-19 04:42:12","","864–875","","11","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RMY8KQX4","journalArticle","2001","Begault, Durand R.; Wenzel, Elizabeth M.; Anderson, Mark R.","Direct Comparison of the Impact of Head Tracking, Reverberation, and Individualized Head-Related Transfer Functions on the Spatial Perception of a Virtual Speech Source","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=10175","","2001","2023-07-12 06:34:03","2023-07-12 06:34:03","","904–916","","10","49","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q8LB4FBH","journalArticle","2000","Shlien, Seymour","Auditory Models for Gifted Listeners","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12038","","2000","2023-07-12 06:34:06","2023-07-12 06:34:06","","1032–1044","","11","48","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M24IF9QK","journalArticle","2019","Woodcock, James; Davies, William J.; Cox, Trevor J.","Influence of Visual Stimuli on Perceptual Attributes of Spatial Audio","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20494","Although audio is often reproduced with a visual counterpart, the audio technology for these systems is often researched and evaluated in isolation from the visual component. Previous research indicates that the auditory and visual modalities are not processed separately by the brain. For example, visual stimuli can influence ratings of audio quality and vice versa. This paper presents an experiment to investigate the influence of visual stimuli on a set of attributes relevant to the perception of spatial audio. Eighteen participants took part in a paired comparison listening test where they were asked to judge pairs of stimuli rendered with fourteen-, five-, and two-channel systems using ten perceptual attributes. The stimuli were presented in both audio only and audiovisual conditions. The results show a significant and large effect of the loudspeaker configuration for all the tested attributes other than overall spectral balance and depth of field. The effect of visual stimuli was found to be small and significant for the attributes realism, sense of space, and spatial clarity. These results suggest that evaluations of audiovisual technologies that are aimed to evoke a sense of realism or presence should consider the influence of both the audio and visual modalities.","2019","2023-07-12 06:34:10","2023-07-19 10:56:47","","557–567","","7/8","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F8RA5M76","journalArticle","2011","Jansen, Reinier J.; Özcan, Elif; van Egmond, René","PSST! Product Sound Sketching Tool","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15936","Designing and evaluating a product’s sound during the conceptual phase is both more effective and efficient. Many products produce sounds that are intentional (such as the ring of a phone) and as artifacts (such as the motor noise of a vacuum cleaner). In most cases, the sonic aspects of a product are considered after the design is mostly complete where the choices are limited. A tool that allows sounds to be “sketched” is suggested as a means for industrial designers to begin the sound design process at the earliest stages. A preliminary analysis suggests that an inexperienced sound designer can successfully sketch a sound with the appropriate adjectives.","2011","2023-07-12 06:34:14","2023-07-19 04:08:14","","396–403","","6","59","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"92V2TZIC","journalArticle","2020","Çakmak, Cem; Hamilton, Rob","od: Composing Spatial Multimedia for the Web","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20991","Composers have been exploring multi-channel sound field spatialization since the early days of electronic music. However, reproduction of such works outside of specialized concert spaces and research facilities or even their accurate reproduction within those spaces remain difficult and unpredictable at best. Combining the reach and simplicity of web browsers with ambisonic to binaural rendering, Web Audio-based tools can ensure greater accessibility for existing spatial works as well as acting as a platform upon which new ones can be implemented. At times with such practices the developing technologies become deprecated or obsolete during the period of making the work. This paper describes the technical design and artistic conception of od, a spatial multimedia production for binaural listening on the web. The project has led us to develop a workflow without relying on specific tools that can be of use as a framework for documenting existing spatial works or novel browser-based creative applications.","2020","2023-07-12 06:34:17","2023-07-19 03:47:04","","747–755","","10","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NKTP6BJD","journalArticle","2023","Dupré, Théophile; Denjean, Sébastien; Aramaki, Mitsuko; Kronland-Martinet, Richard","Spatial Integration of Dynamic Auditory Feedback in Electric Vehicle Interior","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22142","","2023","2023-07-12 06:34:20","2023-07-12 06:34:20","","349–362","","6","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ESDDA7LG","journalArticle","2000","Baileyi, Nicholas J.; Cooper, David","Sculptor: Exploring Timbral Spaces in Real Time","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=10265","","2000","2023-07-12 06:34:24","2023-07-12 06:34:24","","174–180","","3","48","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3R2PLLMI","journalArticle","1975","Richmond, Charles","A Practical Theatrical Sound Console","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=2715","The requirements for a sound control console for the legitimate theater are unique and in many ways the reverse of those of a recording board. A stock unit is described which solves most of these requirements simply and economically, allowing a single operator to effectively adjust over 100 controls simultaneously without the need for computer automation interfacing.","1975","2023-07-12 06:34:27","2023-07-19 04:40:35","","36–40","","1","23","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G2DS54H6","journalArticle","2022","Allan, Jon; Leijonhufvud, Susanna","Listener Preferences in Streamed Music","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21557","A cross-disciplinary study between the two research areas of Audio Technology and Music Education was performed to assess how different aspects of education and experience may influence the experience of music listening given a typical streaming service---Spotify.1 The point of departure is that streamed media facilitates a plenitude of versions of the same song. The paper focuses on the differences that these different songs yield from various mastering processes and production choices motivated by the end distribution media and user settings in the playback system that aim to alter the sound. These variations may all lead to differences in musical dynamics and timbre. A listening test was conducted to examine listeners' preferences, the assessed audio quality, and subjects' reports on how the music content affected them when given the possibility to compare versions in a controlled environment. The test subjects (n = 76) represented populations with various educational backgrounds and experience within music and audio technology. Among the results, it was found that education and experience in some cases do affect preferences.","2022","2023-07-12 06:34:30","2023-07-19 03:36:02","","156–176","","3","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8W9DVZPI","journalArticle","2017","Ronan, Malachy; Ward, Nicholas; Sazdov, Robert; Lee, Hyunkook","The Perception of Hyper-Compression by Mastering Engineers","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19176","Hyper-compressed popular music is associated with the overuse of dynamic range processing in an effort to gain a competitive advantage in music production. This behavior should be unnecessary given the availability of loudness normalization algorithms across the industry; the practice has been denounced by mastering engineers as generating audible artefacts. However, the audibility of these artefacts to mastering engineers has not been examined. This study probes this question using an ABX listening experiment with 20 mastering engineers. On average, mastering engineers correctly discriminated 17 out of 24 conditions, suggesting that the sound quality artefacts generated by hyper-compression are difficult to perceive. The findings in the study suggest that audibility depends on the crest factor (CF) of the music rather than the amount of CF reduction, thus proposing the existence of a threshold of audibility.","2017","2023-07-12 06:34:33","2023-07-19 04:41:56","","613–621","","7/8","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PAN6R573","journalArticle","2021","Lefford, M. Nyssim; Bromham, Gary; Fazekas, György; Moffat, David","Context-Aware Intelligent Mixing Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21022","","2021","2023-07-12 06:34:36","2023-07-12 06:34:36","","128–141","","3","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HHNAT55Z","journalArticle","2019","Melchior, Vicki R.","High-Resolution Audio: A History and Perspective","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20455","Exploration of audio at digital resolutions higher than CD began in the late 1980s, arising from a wealth of interdependent sources including listening experiences, rapid technical advances, an appreciation of psychoacoustics, acoustic measurement, and the ethos that music recording should capture the full range of audible sound. High-resolution formats were developed and incorporated into successive generations of distribution media from DVD, SACD, and Blu-ray, to internet downloads and now to streaming. A continuing debate throughout has been whether, and especially why, higher resolutions should be audibly superior. This review covers the history of the period, focusing on the main drivers of experimentation and development up to the present, and then seeks to explain the current view that, beyond dynamic range, the most likely technical sources differentiating the sound of digital formats are the filtering chains that are ubiquitous in traditional digital sampling and reconstruction of analog music sources.","2019","2023-07-12 06:34:39","2023-07-19 04:30:18","","246–257","","5","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7H9FYTNP","journalArticle","2011","Sabin, Andrew Todd; Rafii, Zafar; Pardo, Bryan","Weighted-Function-Based Rapid Mapping of Descriptors to Audio Processing Parameters","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15938","User interfaces of audio tools and processors can be difficult for novices to use because technical parameters provide little guidance about their sonic manifestation. Borrowing a technique from psychoacoustics, the authors explore an efficient means for mapping users’ descriptors (target adjectives) to technical parameters. Weighting functions are created based on the relative influence of a parameter in influencing the adjective descriptor, such as warm, bright, and full. This approach was tested on two common types of processing: equalization and reverberation. Despite the relative simplicity of the approach, the results are promising.","2011","2023-07-12 06:34:42","2023-07-19 04:45:58","","419–430","","6","59","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3LKJKQ3F","journalArticle","2022","Moffat, David; De Man, Brecht; Reiss, Joshua D.","Semantic Music Production: A Meta-Study","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21823","This paper presents a systematic review of semantic music production, including a meta-analysis of three studies into how individuals use words to describe audio effects within music production. Each study followed different methodologies and stimuli. The SAFE project created audio effect plug-ins that allowed users to report suitable words to describe the perceived result. SocialFX crowdsourced a large data set of how non-professionals described the change that resulted from an effect applied to an audio sample. The Mix Evaluation Data Set performed a series of controlled studies in which students used natural language to comment extensively on the content of different mixes of the same groups of songs. The data sets provided 40,411 audio examples and 7,221 unique word descriptors from 1,646 participants. Analysis showed strong correlations between various audio features, effect parameter settings, and semantic descriptors. Meta-analysis not only revealed consistent use of descriptors among the data sets but also showed key differences that likely resulted from the different participant groups and tasks. To the authors' knowledge, this represents the first meta-study and the largest-ever analysis of music production semantics.","2022","2023-07-12 06:34:45","2023-07-19 04:32:23","","548–564","","7/8","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YGI6QI9G","journalArticle","2013","Xie, Bosun; Zhong, Xiaoli; Yu, Guangzheng; Guan, Shanquin; Rao, Dan; Liang, Zhiqiang; Zhang, Chengyun","Report on Research Projects on Head-Related Transfer Functions and Virtual Auditory Displays in China","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16826","In recent years, audio researchers in China have been devoting significant resources to the study of head-related transfer functions (HRTFs) and virtual auditory displays (VADs) in such applications as acoustics, computer games, signal processing, hearing aids, spatial sound reproductions, and many others. With financial support from the National Nature Science Fund of China, South China University of Technology and the Beijing University of Posts and Telecommunications have launched a long-term research project on HRTFs and VADs. This engineering report presents highlights of these research projects, including studies on HRTF measurements and database construction, statistical analysis on measured HRTFs, a dynamic VAD system, and an algorithm for reducing timbre coloration on virtual surround sound reproduction.","2013","2023-07-12 06:34:49","2023-07-19 10:58:15","","314–326","","5","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V9RHFEHU","journalArticle","2015","Karandreas, Alex; Christensen, Flemming","Influence of Visual Appearance on Loudspeaker Sound Quality Evaluation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17872","This study investigates the importance of both the auditory and visual modalities when evaluating subjective quality. Bimodal experiments comprising audiovisual and unimodal presentations were used to explore the interaction between modalities. Audio stimuli of varied degradation were coupled with both actual loudspeakers of different visual appearance and scaled photographs of the same loudspeakers. As would be expected, the factor audio had the strongest influence on quality in the audiovisual session. However, in visual only presentations, the factor visual was statistically significant. This indicates that when presented in isolation, the differences between the visual stimuli are perceived clearly and are judged to be substantial but become obscure in the presence of audio stimuli. From a product design perspective, the results suggest that the modalities are independent and that a change in subjective quality in either modality would combine linearly.","2015","2023-07-12 06:34:52","2023-07-19 04:09:54","","684–697","","9","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TAA374LI","journalArticle","2021","Brezina, Pavol","Perspectives of Advanced Ear Training Using Audio Plug-Ins","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21039","Technical ear training is currently gaining more and more attention during the training of professional sound engineers. In recent years, a number of web applications and standalone programs have been created to provide a basic training interface for ear training. However, with the greater accessibility of audio plug-ins and the rising demand to use them in production, the question arises as to how these tools could be integrated into the ear training process. The aim of this article is to point out the specifics and possibilities of using audio plug-ins as training tools through the unique prototype of a proprietary, standalone host.","2021","2023-07-12 06:34:55","2023-07-19 03:45:00","","351–358","","5","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HJZV3Y5L","journalArticle","2014","Kim, Sungyoung; Ikeda, Masahiro; Martens, William L.","Reproducing Virtually Elevated Sound via a Conventional Home-Theater Audio System","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17244","While adding a height dimension to a surround sound system implies the addition of loudspeakers on the ceiling, this study explores the means for synthesizing the 3rd dimension by signal processing. Creating a virtual elevated auditory image by using the conventional lateral 5.1 loudspeaker configuration greatly simplifies the burden on the consumer. The proposed system has been implemented using a crosstalk-cancellation method optimized for three of the five channels in a home-theater system: center, left-surround, and right-surround. This hybrid method, based on earlier work by Klepko, required the calculation of two inverse filters. Preliminary listening tests showed that loudspeakers at ear level could render sound sources perceived to be at higher elevations, and that the perceived elevation angles increased monotonically with the target elevation angles.","2014","2023-07-12 06:35:01","2023-07-19 04:11:53","","337–344","","5","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DFYPTSNU","journalArticle","2004","Andersen, Tue Haste; Jensen, Kristoffer","Importance and Representation of Phase in the Sinusoidal Model","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13029","Work is presented on the representation and perceptual importance of phase. Based on a standard sinusoidal analysis/synthesis system, the phase alignment of the sound components is analyzed. A novel phase representation, partial-period phase, is introduced, which characterizes phase evolution over time with an almost stationary parameter for many musical sounds. The proposed partial-period phase representation is used to control the phase when synthesizing sounds. Sounds synthesized with varying amounts of phase information are compared in a listening experiment with 11 subjects. It is shown that phase is of great importance to the perception of the sound quality of common harmonic musical sounds, but indications are found that phase is not of importance to the slightly inharmonic piano sounds. In particular, the sound degradation is large for low-pitched sounds, approaching ""slightly annoying"" when no phase information is used. In addition, a model based on the partialperiod phase representation has a significantly better perceived sound quality than sounds with random phase shifts.","2004","2023-07-12 06:35:14","2023-07-19 03:36:21","","1157–1169","","11","52","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WIVWR8LM","journalArticle","2019","Lund, Thomas; Mäkivirta, Aki; Naghian, Siamäk","Time for Slow Listening","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20547","Conscious perception is influenced by long-term experience and learning, to an extent that it might be more accurately understood and studied as primarily a reach-out phenomenon, at least in adults. Considering human hearing, time is a deciding factor on several scales, and the sensory information flow rate, otherwise termed the perceptual bandwidth, is modest. We introduce the term “slow listening” and discuss how new findings from other fields of science should be taken into account in pro audio, for instance when conducting subjective tests, and when preserving content for future generations to enjoy.","2019","2023-07-12 06:35:17","2023-07-19 04:25:34","","636–640","","9","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V79VZT7N","journalArticle","2014","Terrell, Michael; Simpson, Andrew; Sandler, Mark","The Mathematics of Mixing","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17081","Although audio mixing has always been viewed as the artistic task of either a conductor balancing the musicians in a live performance or a mixing engineer combining multiple tracks in a sound studio, this research considers mixing as a mathematical optimization problem. Using an auditory model, the authors demonstrated how numerical optimization can be used to pose and solve a mix problem. There is interplay between artistic objectives, perceptual constraints, and engineering methods. Taking loudness as an example, it is shown that the nonlinearity in the perceptual model leads to complex behavior, which can be overcome by careful choice of optimization strategies and parameters.","2014","2023-07-12 06:35:20","2023-07-19 04:52:17","","4–13","","1/2","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VVR8JD2J","journalArticle","2021","Gori, Matteo; Ceccarelli, Andrea","Benchmarking Networked Music Performance Tools With the NMP-Bench Model and Architecture","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21466","Networked Music Performance (NMP) aims at establishing a live interaction between musicians remotely connected that performs as if they were in the same room. While several NMP tools have been proposed through the last 20 years, a benchmark that aims at measuring and comparing their quality is currently not present. In this paper we propose the Networked Music Performance Benchmark (NMP-Bench), the first approach to systematically analyze and compare the performance of NMP tools. Focused on server-based NMP and its auditory component, NMP-Bench provides a comprehensive approach to measure and quantitatively compare NMP tools, encompassing network, music, and effectiveness metrics, with the goal of understanding the technical gaps that may reduce the experience of the performers. The paper presents the NMP-Bench model and architecture, which is then applied to benchmark two NMP tools (in simulated settings with no actual musicians) over three music pieces of different music styles. Results show differences between the two tools; this supports our statement that NMP-Bench can be used to select the most suitable tool and highlight strengths and weaknesses of NMP tools.","2021","2023-07-12 06:35:23","2023-07-19 04:02:40","","708–719","","10","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8QA8VZWX","journalArticle","2023","Pedersen, Rasmus Lundby; Picinali, Lorenzo; Kajs, Nynne; Patou, François","Virtual-Reality-Based Research in Hearing Science: A Platforming Approach","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22144","The lack of ecological validity in clinical assessment, as well as the challenge of investigating multimodal sensory processing, remain key challenges in hearing science. Virtual Reality (VR) can support hearing research in these domains by combining experimental control with situational realism. However, the development of VR-based experiments is traditionally highly resource demanding, which places a significant entry barrier for basic and clinical researchers looking to embrace VR as the research tool of choice. The Oticon Medical Virtual Reality (OMVR) experiment platform fast-tracks the creation or adaptation of hearing research experiment templates to be used to explore areas such as binaural spatial hearing, multimodal sensory integration, cognitive hearing behavioral strategies, auditory-visual training, etc. In this paper, the OMVR's functionalities, architecture, and key elements of implementation are presented, important performance indicators are characterized, and a use-case perceptual evaluation is presented.","2023","2023-07-12 06:35:28","2023-07-19 04:37:28","","374–389","","6","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G2Z2PAZT","journalArticle","2016","Wilson, Alex; Fazenda, Bruno M.","Perception of Audio Quality in Productions of Popular Music","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18102","In the context of recorded sound there is great debate over which parameters influence the perception of quality. To gain insight into the dimensions of quality perception, subjective and objective evaluation of musical program material, extracted from commercial CDs, was undertaken. It was observed that perception of audio quality and liking of the music can be affected by separate factors. Familiarity with stimuli affected like ratings, while quality ratings were most associated with signal features related to perceived loudness and dynamic range compression. Additionally, the sonic attributes describing quality ratings indicate a diverse lexicon relating to timbre, space, defects, and other concepts. The results also suggest that, while the perceived quality of popular music may have decreased over recent years, like ratings were unaffected. Like ratings were strongly influenced by song familiarity, implying that aspects of preference and liking are distinct from the interpretation of quality and might not be the best descriptors for studies where technical quality is the percept being sought. Quality in music production is revealed as a perceptual construct distinct from hedonic, musical preference. Audio quality can be predicted from objective features in the signal, and can be adequately and consensually described using verbal attributes.","2016","2023-07-12 06:35:31","2023-07-19 10:56:09","","23–34","","1/2","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UKCSI35G","journalArticle","2015","Hafezi, Sina; Reiss, Joshua D.","Autonomous Multitrack Equalization Based on Masking Reduction","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17637","In multitrack music production, some sounds get masked by other sounds and the listener has less ability to fully hear and distinguish the sound sources in the mix. The authors designed a simplified measure of masking based on best practices, and then implemented both an off-line and real-time, autonomous multitrack equalization system that reduces masking in multitrack audio. The system used objective measures of spectral masking in the resultant mixes. Listening tests provided a subjective comparison between the mix results of different implementations of the system, a raw mix, and manual mixes made by an amateur and a professional mix engineer. The results showed that autonomous systems reduce both the perceived and objective masking. The offline semi-autonomous system is capable of improving the raw mix better than an amateur and close to a professional mix by simply controlling one user parameter. The results also suggest that existing objective measures of masking are ill-suited for quantifying perceived masking in multitrack musical audio.","2015","2023-07-12 06:35:34","2023-07-19 04:03:11","","312–323","","5","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YF6DWJTG","journalArticle","2021","Hupke, Robert; Nophut, Marcel; Preihs, Stephan; Peissig, Jürgen","Toward Professional Distributed Performances: Effects of a Global Metronome on Networked Musical Ensemble Interactions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21467","Modern communication networks enable audiovisual interaction between geographically distant locations in near real time, leading to an increasing interest in networked music performance (NMP) and growing availability of related tools and applications. An important point of such distributed performances is the nature of interaction between the performers, which poses challenges toward, e.g., the network latency in the communication chain. Extensive research in the field of NMPs has shown that it is possible to achieve stabilization of synchrony and tempo deviation by providing a global time reference signal at each location of an NMP. In this study, for the first time, both an auditory and visual global metronome were integrated into the ecosystem of a physical NMP to evaluate the objective musical outcome and perceived benefit of the metronome with a professional music ensemble of five musicians between Munich and Hanover. The objective analysis shows that the metronome has a positive effect in terms of tempo stability at high latency levels, whereas synchrony strongly depends on the individual coping strategy of each musician. The subjective analysis suggests that a perceivable positive effect of the metronome is discernible for the musicians at all latency levels.","2021","2023-07-12 06:35:39","2023-07-19 04:07:29","","720–736","","10","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"59L8BPBP","journalArticle","2022","Fyfe, Lawrence; Bedoya, Daniel; Chew, Elaine","Annotation and Analysis of Recorded Piano Performances on the Web","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22020","","2022","2023-07-12 06:35:42","2023-07-12 06:35:42","","962–978","","11","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CMYVWN86","journalArticle","2016","Aguilera, Emanuel; Lopez, Jose J.; Cooperstock, Jeremy R.","Spatial Audio for Audioconferencing in Mobile Devices: Investigating the Importance of Virtual Mobility and Private Communication and Optimizations","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18138","Audioconferencing systems are becoming increasingly sophisticated, seeking to improve immersion, intelligibility, and sense of presence. Various opportunities exist to make mobile multiparty conference calls more realistic, interactive, and immersive. These include spatial audio, interactive manipulation of avatar positions, and whisper mode. The authors analyze the utility of avatar movement and sidebar whisper-mode functionality within which a subset of participants can engage in an ad-hoc sidebar conversation privately from the remaining participants. Participants found that avatar movement was useful, but approximately half the participants only used this feature for initial positioning of the avatars in a desired configuration in the first task. The ability to maintain sidebar conversations was used frequently and rated highly in negotiation activity. However, participants made almost no use of this feature in the conversation activity for which negotiation was not involved.","2016","2023-07-12 06:35:47","2023-07-19 03:34:59","","332–341","","5","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IR372ZDT","journalArticle","2016","Zacharakis, Asterios; Pastiadis, Konstantinos","Revisiting the Luminance-Texture-Mass Model for Musical Timbre Semantics: A Confirmatory Approach and Perspectives of Extension","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18372","This study describes a listening experiment designed to further examine the previously proposed luminance-texture-mass (LTM) model for timbral semantics. Thirty two musically trained listeners rated twenty four instrument tones on six predefined semantic scales: brilliance, depth, roundness, warmth, fullness, and richness. These six scales were analyzed with Principal Component Analysis (PCA) and Multidimensional Scaling (MDS) to produce two different timbre spaces. These timbre spaces were subsequently compared for their configurational and dimensional similarity with the LTM semantic space and the direct MDS perceptual space obtained from the same stimuli. The results showed that the selected semantic scales are adequately representing the LTM model and are fair at predicting the configurations of the sounds that result from pairwise dissimilarity ratings.","2016","2023-07-12 06:35:50","2023-07-19 10:58:59","","636–645","","9","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZMP23PZA","journalArticle","2016","Fan, Jianyu; Thorogood, Miles; Pasquier, Philippe","Automatic Soundscape Affect Recognition Using A Dimensional Approach","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18373","Soundscape studies have demonstrated a variety of approaches for investigating how soundscapes affect is part of immersive experiences. This research tries to develop an automatic affect recognition system that soundscape composers can use to create emotional compositions to evoke a target mood. In addition, this system can offer sound designers a more streamlined workflow for creating suitable sound effects for films and can offer engineers a way to design mood-enabled recommendation systems for retrieval of soundscape recordings. This research uses ground truth data collected from an online survey, and an analysis of the corpus shows that participants have a high level of agreement on the valence and arousal of soundscapes. The authors then generated a gold standard by averaging user responses. The propose system obtained better results than an expert-user model.","2016","2023-07-12 06:35:53","2023-07-19 03:55:47","","646–653","","9","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RTI3IZ6Q","journalArticle","2021","Yeoward, Christopher; Shukla, Rishi; Stewart, Rebecca; Sandler, Mark; Reiss, Joshua D.","Real-Time Binaural Room Modelling for Augmented Reality Applications","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21532","This paper proposes and evaluates an integrated method for real-time, head-tracked, 3D binaural audio with synthetic reverberation. Virtual vector base amplitude panning is used to position the sound source and spatialize outputs from a scattering delay network reverb algorithm running in parallel. A unique feature of this approach is its realization of interactive auralization using vector base amplitude panning and a scattering delay network, within acceptable levels of latency, at low computational cost. The rendering model also allows direct parameterization of room geometry and absorption characteristics. Varying levels of reverb complexity can be implemented, and these were evaluated against two distinct aspects of perceived sonic immersion. Outcomes from the evaluation provide benchmarks for how the approach could be deployed adaptively, to balance three real-time spatial audio objectives of envelopment, naturalness, and efficiency, within contrasting physical spaces.","2021","2023-07-12 06:35:57","2023-07-19 10:58:42","","818–833","","11","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RIKQA6NB","journalArticle","2003","Chen, Fang","Localization of 3-D Sound Presented through Headphone - Duration of Sound Presentation and Localization Accuracy","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12188","The relationship between the duration of a sound presentation and the accuracy of human localization is investigated. The three-dimensional sound is presented via headphones. The head-tracking system was integrated together with the sound presentation. Generalized headrelated transfer functions (HRTFs) are used in the experiment. Six different types of sounds with durations of 0.5, 2, 4, and 6 seconds were presented in random order on any azimuth in the horizontal plane. Thirty subjects participated in the study. A special location indication system called DINC (directional indication compass) was developed. With DINC the judged location of every test can be recorded accurately. The results showed that the localization accuracy is significantly related to the duration of the sound presentation. As long as the sound has a broad frequency bandwidth, the sound type has little effect on the localization accuracy. A presentation of at least 4-second duration is recommended. There is no significant difference between male and female subjects in the accuracy of detection.","2003","2023-07-12 06:36:00","2023-07-19 03:47:52","","1163–1171","","12","51","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8KIMMPXY","journalArticle","2019","Wilson, Alex; Fazenda, Bruno M.","User-guided Rendering of Audio Objects Using an Interactive Genetic Algorithm","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20490","One of the advantages of object-based audio/broadcast over traditional channel-based delivery is that it allows for the rendering of personalized content when delivered to the listeners. The methods by which personalization are achieved often require an in-depth understanding of the problem domain. This paper describes the design and evaluation of an interactive audio renderer, which is used to optimize an audio mix based on the feedback of the listener. A panel of 14 trained participants was recruited to try the system. When using the proposed system in a simple music mixing task, participants were able to create a range of mixes of audio objects comparable to those made using the conventional fader-based system. This suggests that the system is not an obstacle to the creation of desired content, and does not impose noticeable limits on what content can be created. Evaluation using the System Usability Scale showed a low level of physical and mental burden and so is predicted that the system would be suitable for a variety of applications where physical interaction is to be kept low, such as an interface for users with vision and/or mobility impairments.","2019","2023-07-12 06:36:03","2023-07-19 10:56:20","","522–530","","7/8","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TS3EEV5R","journalArticle","2019","Fenton, Steven; Lee, Hyunkook","A Perceptual Model of “Punch” Based on Weighted Transient Loudness","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20484","This paper proposes and evaluates a perceptual model for the measurement of “punch” in musical signals based on a novel algorithm. Punch is an attribute that is often used to characterize music or sound sources that convey a sense of dynamic power or weight to the listener. A methodology is explored that combines signal separation, onset detection, and low-level feature measurement to produce a perceptually weighted punch score. The model weightings are derived through a series of listening tests using noise bursts, which reveal the perceptual relevance of the onset time and frequency components of the signal across octave bands. The punch score is determined by a weighted sum of these parameters using coefficients derived through regression analysis. The model outputs are evaluated against subjective scores obtained through a pairwise comparison listening test using a wide variety of musical stimuli and against other computational models. The model output PM95 outperformed the other models showing a “very strong” correlation with punch perception.","2019","2023-07-12 06:36:07","2023-07-19 03:57:19","","429–439","","6","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5PDVBNLP","journalArticle","2020","Malecki, Pawel; Piotrowska, Magdalena; Sochaczewska, Katarzyna; Piotrowski, Szymon","Electronic Music Production in Ambisonics-Case Study","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20723","This paper presents a case study of an original electronic music production in stereo and means for then creating an Ambisonic remix. The main goal of this work was to explore the potential for extending all dimensions into extended space. The stereo and Ambisonic mixes, as well as the stereo and binaural renders, were subjectively evaluated by experts performers. When compared, the stereo and Ambisonic mixes differ not only in terms of space but also with regard to the timbre and dynamics. In general, the listeners preferred the spaciousness and selectivity of the Ambisonic version over the stereo. The obtained results are consistent with the outcomes from other studies that focused on the differences between stereo and multichannel reproduction. The most interesting conclusion can be formulated from a comparison between the stereo and binaural renders of the Ambisonic mix. The results clearly show the preference of spaciousness of the binaural version, and the general preference also indicated binaural as preferred. Due to the popularity of headphone playback, the obtained results show the potential of Ambisonic productions with targeted binaural playback.","2020","2023-07-12 06:36:10","2023-07-19 04:26:33","","87–94","","1/2","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3JH678ZE","journalArticle","2020","Vowels, M.J.; Mason, R.","Comparison of Pairwise Dissimilarity and Projective Mapping Tasks With Auditory Stimuli","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20895","","2020","2023-07-12 06:36:13","2023-07-12 06:36:13","","638–648","","9","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CHR5ZRM5","journalArticle","2021","Riionheimo, Janne; Lokki, Tapio","Movie Sound, Part 2: Preference and Attribute Ratings of Six Listening Environments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21017","In this study, the assessors evaluated the alterations in the sound field of six movie listening environments. The sound fields of the listening environments were auralized to an anechoic listening room with 45 loudspeakers so that assessors could compare the rooms with each other directly. 31 experienced listeners evaluated five descriptive attributes on a continuous scale for each room with two program material items, dialogue and music. The preference ratings for the rooms were also collected. The perceptual evaluations were compared to the objective electroacoustic data of the rooms. The sense of space, clarity, and distance match the measured clarity C50 at the middle frequencies, while the brightness matches the level of the high frequencies in the electroacoustic response above 4 kHz. No psychoacoustical support was found for the current standards, according to which the high frequencies should be attenuated more in large cinemas with longer reverberation than in small cinemas. It turned out that the movie sound professionals do not prefer either too dead or too live listening environments.","2021","2023-07-12 06:36:17","2023-07-19 04:40:55","","68–79","","1/2","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"43K8M446","journalArticle","2012","Schönstein, David; Katz, Brian F.G.","Variability in Perceptual Evaluation of HRTFs","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16552","","2012","2023-07-12 06:36:21","2023-07-12 06:36:21","","783–793","","10","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AIXF8I55","journalArticle","2017","Volk, Christer P.; Bech, Søren; Pedersen, Torben H.; Christensen, Flemming","Modeling Perceptual Characteristics of Loudspeaker Reproduction in a Stereo Setup","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18729","Loudspeaker specifications traditionally describe their physical characteristics rather than the perceptual properties of the sound reproduction. This study explores three metrics for predicting the perceived characteristics of loudspeakers’ sound in a stereo setup evaluated in a standardized listening room. Perceptual evaluations of eleven loudspeakers were conducted on the basis of six selected sensory descriptors chosen by trained listeners during consensus meetings. Four of these descriptors were found suitable for modeling metrics that predicted Bass depth, Punch, Brilliance, and Dark-Bright respectively; Bass depth and Punch were however combined because of a high correlation between them. The input for the metrics was recordings made using a head-and-torso simulator and processed using a loudness model. The prediction models were trained on a subset of seven sets of loudspeakers and validated on four others. The range of correlation coefficients between perceptual evaluations and outputs of the metrics were r = 0.85-0.96.","2017","2023-07-12 06:36:24","2023-07-19 10:53:38","","356–366","","5","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HBWH5E4E","journalArticle","2017","Kaplanis, Neofytos; Bech, Søren; Tervo, Sakari; Pätynen, Jukka; Lokki, Tapio; Waterschoot, Toon; Jensen, Søren Holdt","A Rapid Sensory Analysis Method for Perceptual Assessment of Automotive Audio","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18550","As today’s automotive audio systems rapidly evolve, it is unclear if the current perceptual assessment protocols fully capture the human sensations evoked by such new systems. The highly complex and acoustically hostile environment of the automobile cabin hinders the effectiveness of standard objective metrics, while lacking robustness, repeatability, and perceptual relevance. This report examines the current assessment protocols and their identified limitations. A new design of an assessment protocol is proposed. It uses the Spatial Decomposition Method for acquiring, analyzing, and reproducing the sound field in a laboratory over loudspeakers, thereby allowing instant comparisons of automotive audio systems. A rapid sensory analysis protocol, the Flash Profile, is employed for evaluating the perceptual experience using individually elicited attributes, in a time-efficient manner. A pilot experiment is described, where experts, experienced, and naive assessors followed the procedure and evaluated three sound fields. Current findings suggest that this method allows for the assessment of both spatial and timbral properties of automotive sound.","2017","2023-07-12 06:36:27","2023-07-19 04:09:44","","130–146","","1/2","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2KQIK3RZ","journalArticle","2021","Cairns, Patrick; Daffern, Helena; Kearney, Gavin","Parametric Evaluation of Ensemble Vocal Performance Using an Immersive Network Music Performance Audio System","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21541","This paper describes an immersive audio Network Music Performance (NMP) system designed for group singing. A prototype of this design (audio only) was deployed to ten singers across Europe, who participated in a duet vocal performance study, operating the system from their home networks. Parametric evaluation of these vocal performances was conducted in order to provide characterization of musical interactivity between performers and explore the challenges and opportunities presented for immersive audio NMP systems in practical usecase settings. Results demonstrate that it is possible to achieve performance that conforms to expectations of live interactivity and estimate the conditions under which this may be achieved. Significant effect of latency, and in one case virtual room ""type,"" is observed across performances. Informal questionnaire responses present discussion of the potential for virtual acoustics and latency to impact the perceptual experience of networked performers.","2021","2023-07-12 06:36:31","2023-07-19 03:46:47","","924–933","","12","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J9C5I74R","journalArticle","2016","Williams, Duncan","Toward Emotionally-Congruent Dynamic Soundtrack Generation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18374","Real-time control of the emotional content of sound has utility in video game soundtracking where the player controls the narrative trajectory, and the affective attributes of the sound should ideally match this trajectory. Perceived emotions can be represented in a 2-dimensional space composed of valence (positivity, e.g. happy, sad, fearful) and arousal (intensity, e.g. mild vs strong). This report is a speculative exploration of measuring and manipulating sound effects to achieve emotional congruence. An initial study suggests that timbral features can exert an influence on the perceived emotional response of a listener. A panel of listeners responded to stimuli in a set with varying timbres, while maintaining pitch, loudness, and other musical and acoustic features such as key, melodic contour, rhythm and meter, reverberant environment etc. The long term goal is to create an automated system that utilizes timbre morphing in real time to manipulate perceived affect in soundtrack generation.","2016","2023-07-12 06:36:34","2023-07-19 10:56:01","","654–663","","9","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8ZLWJRML","journalArticle","2017","Zhou, Tingting; Zeng, Yumin; Wang, Rongrong","Single-Channel Speech Enhancement Based on Psychoacoustic Masking","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18560","Speech enhancement processing can improve the performance of speech communication systems in noisy environments, such as in mobile communication systems, speech recognition, or hearing aids. Single-channel speech enhancement is more difficult than it is with multiple channels since there is no independent source of information that can help separate the speech and noise signals. This paper addresses single-channel speech enhancement based on the masking properties of the human auditory system. A complete implementation of speech enhancement using psychoacoustic masking is presented. The incorporation of temporal masking along with simultaneous masking (as compared to using only simultaneous masking) produces results that are more consistent with human auditory characteristics. The combined masking is then used to adapt the subtraction parameters to obtain the best trade-off among noise reduction, speech distortion, and the level of residual perceptual noise. The application of objective measures and subjective listening tests demonstrate that the proposed algorithm outperforms comparable speech enhancement algorithms.","2017","2023-07-12 06:36:37","2023-07-19 10:59:30","","272–284","","4","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TBCRZ3JV","journalArticle","2012","Bahne, Adrian","Perceived Sound Quality of Small Original and Optimized Loudspeaker Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16163","The perceived sound quality of small loudspeaker systems with and without digital optimization was empirically evaluated in a listening experiment. Further, it was investigated how the presentation order in the performed paired comparisons influenced the results, as well as whether a self-evaluation was of potential use for variance reduction. The systems were optimized by means of FIR filters. The two versions of each loudspeaker system were rated in a paired comparison test for music stimuli. For the purpose of analysis a linear Gaussian model was applied, resulting in an interval scale revealing interesting information about certainty and discrimination ability of the listeners. The test investigated whether linear pre-compensation of small and inexpensive loudspeaker systems results in a significant improvement of the perceived audio quality in a typical listening situation. The results indicated a significant preference for the optimized version and a significant dependency on the presentation order was detected. The self-evaluation was found to be uncorrelated to the test results.","2012","2023-07-12 06:36:40","2023-07-19 03:37:41","","29–37","","1/2","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q3MXF4C6","journalArticle","2017","Fleßner, Jan-Hendrik; Huber, Rainer; Ewert, Stephan D.","Assessment and Prediction of Binaural Aspects of Audio Quality","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19361","","2017","2023-07-12 06:36:44","2023-07-12 06:36:44","","929–942","","11","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L4BL2LTJ","journalArticle","2022","Hill, Adam J.; Mulder, Johannes; Burton, Jon; Kok, Marcel; Lawrence, Michael","Sound Level Monitoring at Live Events, Part 3–Improved Tools and Procedures","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21552","This is the final installment in a series of three papers looking into the subject of sound level monitoring at live events. The first two papers revealed how practical shortcomings and audience and neighbor considerations (in the form of sound level limits) can impact the overall live experience. This paper focuses on an improved set of tools for sound engineers to ensure a high-quality and safe live event experience while maintaining compliance with local sound level limits. This includes data processing tools to predict future limit violations and guidelines for improved user interface design. Practical procedures, including effective sound level monitoring practice, alongside resourceful mixing techniques are presented to provide a robust toolset that can allow sound engineers to perform their best without compromising the listening experience in response to local sound level limits.","2022","2023-07-12 06:36:47","2023-07-19 04:06:07","","73–82","","1/2","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FVVMIPYY","journalArticle","1987","Fielder, Louis D.","Evaluation of the Audible Distortion and Noise Produced by Digital Audio Converters","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=5193","","1987","2023-07-12 06:36:50","2023-07-12 06:36:50","","517–535","","7/8","35","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8DDH2325","journalArticle","2016","Drossos, Konstantinos; Kaliakatsos-Papakostas, Maximos; Floros, Andreas; Virtanen, Tuomas","On the Impact of The Semantic Content of Sound Events in Emotion Elicitation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18338","Sound events are known to have an influence on the listener’s emotions, but the reason for this influence is less clear. Take for example the sound produced by a gun firing. Does the emotional impact arise from the fact that the listener recognizes that a gun produced the sound (semantic content) or does it arise from the attributes of the sound created by the firing gun? This research explores the relation between the semantic similarity of the sound events and the elicited emotions. Results indicate that the semantic content seems to have a limited role in the conformation of the listener’s affective states. However, when the semantic content is matched to specific areas in the Arousal-Valence space or when the source’s spatial position is considered, the effect of the semantic content is higher, especially for the cases of medium to low valence and medium to high arousal or when the sound source is at the lateral positions of the listener’s head.","2016","2023-07-12 06:36:53","2023-07-19 03:53:53","","525–532","","7/8","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8CVB35AH","journalArticle","2023","Corcuera, Andrea; Chatziioannou, Vasileios; Ahrens, Jens","Perceptual Significance of Tone-Dependent Directivity Patterns of Musical Instruments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22132","Musical instruments are complex sound sources that exhibit directivity patterns that not only vary depending on the frequency, but can also change as a function of the played tone. It is yet unclear whether the directivity variation as a function of the played tone leads to a perceptible difference compared to an auralization that uses an averaged directivity pattern. This paper examines the directivity of 38 musical instruments from a publicly available database and then selects three representative instruments among those with similar radiation characteristics (oboe, violin, and trumpet). To evaluate the listeners' ability to perceive a difference between auralizations of virtual environments using tone-dependent and averaged directivities, a listening test was conducted using the directivity patterns of the three selected instruments in both anechoic and reverberant conditions. The results show that, in anechoic conditions, listeners can reliably detect differences between the tone-dependent and averaged directivities for the oboe but not for the violin or the trumpet. Nevertheless, in reverberant conditions, listeners can distinguish tone-dependent directivity from averaged directivity for all instruments under study.","2023","2023-07-12 06:37:01","2023-07-19 03:49:54","","293–302","","5","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XE4FPZQJ","journalArticle","2021","Battello, Riccardo; Comanducci, Luca; Antonacci, Fabio; Cospito, Giovanni; Sarti, Augusto","Experimenting With Adaptive Metronomes in Networked Music Performances","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21468","The increasing attention toward interactions at a distance and the improvement of digital communications networks have steadily increased the interest toward Networked Music Performance both regarding entertainment and education. Unfortunately the unavoidable network latency remains one of the main issues that prevents a satisfiable remote performance. In this work we propose three different techniques that try to contrast this issue by relying on adaptive metronomes, i.e., metronomes that are able to track the tempo of the musicians through a beat tracking technique. We present a series of preliminary experiments with both professional and amateur musicians that demonstrate that these techniques could be a promising approach as an additional tool for contrasting the impact of latency.","2021","2023-07-12 06:37:05","2023-07-19 03:39:07","","737–747","","10","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TB8ZEWYT","journalArticle","2020","Hansen, Brian; Burchett, Joseph N.; Forbes, Angus G.","Quasar Spectroscopy Sound: Analyzing Intergalactic and Circumgalactic Media via Data Sonification","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21000","","2020","2023-07-12 06:37:08","2023-07-12 06:37:08","","865–875","","11","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZDH4GHEX","journalArticle","2004","Algazi, V. Ralph; Duda, Richard O.; Thompson, Dennis M.","Motion-Tracked Binaural Sound","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13028","","2004","2023-07-12 06:37:11","2023-07-12 06:37:11","","1142–1156","","11","52","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H2Z5MEJE","journalArticle","2018","Axon, Louise; Goldsmith, Michael; Creese, Sadie","Sonification Mappings: Estimating Effectiveness, Polarities and Scaling in an Online Experiment","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19875","","2018","2023-07-12 06:37:14","2023-07-12 06:37:14","","1016–1032","","12","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7AGZ4JTL","journalArticle","2021","Rovithis, Emmanouel; Moustakas, Nikolaos; Vogklis, Konstantinos; Drossos, Konstantinos; Floros, Andreas","Design Recommendations for a Collaborative Game of Bird Call Recognition Based on Internet of Sound Practices","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21544","","2021","2023-07-12 06:37:17","2023-07-12 06:37:17","","956–966","","12","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UDPIN4H6","journalArticle","2013","Kim, Chungeun; Mason, Russell; Brookes, Tim","Head Movements Made by Listeners in Experimental and Real-Life Listening Activities","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16833","","2013","2023-07-12 06:37:21","2023-07-12 06:37:21","","425–438","","6","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BMC4BJ59","journalArticle","2016","Thorogood, Miles; Fan, Jianyu; Pasquier, Philippe","Soundscape Audio Signal Classification and Segmentation Using Listeners Perception of Background and Foreground Sound","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18334","A soundscape recording captures the sonic environment at a given location at a given time using one or more fixed or moving microphones. In most cases, the soundscape is uncontrolled and unscripted. Human listeners experience sonic components as being either background or foreground depending on their salient perceptual characteristics, such as proximity, repetition, and spectral attributes. Analyzing soundscapes in research tasks requires the classification and segmentation of the important sonic components, but that process is time consuming when done manually. This research establishes the background and foreground classification task within a musicological and soundscape context and then presents a method for the automatic segmentation of soundscape recordings. Using a soundscape corpus with ground truth data obtained from a human perception study, the analysis shows that participants have a high level of agreement on the category assigned to background samples (92.5%), foreground samples (80.8%), and background with foreground samples (75.3%). Experiments demonstrate how smaller window sizes affect the performance of the classifier.","2016","2023-07-12 06:37:25","2023-07-19 04:52:43","","484–492","","7/8","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KAWK342P","journalArticle","2017","Stitt, Peter; Bertet, Stéphanie; van Walstijn, Maarten","Off-Center Listening with Third-Order Ambisonics: Dependence of Perceived Source Direction on Signal Type","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18554","In multichannel reproduction, listening at off-center positions involves differences in the arrival times of the signals from different loudspeakers, which influences localization. The difference between transient or nontransient signals was found to have an influence on the localization of an auditory source image with third-order Ambisonics when the listener is located off-center. The transients were found to have larger errors on the placement of the Ambisonic pointer because of stronger localization dominance of the earlier arriving loudspeakers. Subjects performed consistently over a number of repetitions, but there was a larger difference among different subjects. Results were compared to those obtained with several prediction models, including an extended version of the energy vector model that incorporates the precedence effect. Compared to two binaural models, the extended vector model is shown to provide the best predictions over all conditions. The results confirm that the type of signal must be taken into account in predictive modeling. Furthermore, the extended energy vector exhibits about 50% less error than the standard energy vector.","2017","2023-07-12 06:37:28","2023-07-19 04:49:36","","188–197","","3","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QHNFXNS6","journalArticle","2022","Hayes, Ben; Saitis, Charalampos; Fazekas, György","Disembodied Timbres: A Study on Semantically Prompted FM Synthesis","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21740","","2022","2023-07-12 06:37:32","2023-07-12 06:37:32","","373–391","","5","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2EGSHFU3","journalArticle","2017","Paasonen, Juhani; Karapetyan, Aleksandr; Plogsties, Jan; Pulkki, Ville","Proximity of Surfaces — Acoustic and Perceptual Effects","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19365","When simulating a virtual reality, surfaces near the subject location can play a perceptual role. The authors studied the acoustic effect of nearby objects with both binaural response measurements and subjective listening tests. With a large, smooth, and flat surface, significant acoustical effects exist when the distance is less than about 50 cm. The largest effects can be attributed to comb filtering caused by the interaction between direct sound and reflected sound. At very short distances, some frequency-dependent resonances and shadowing effects that were evoked by the sound field between the subject and the surface could also be observed. Measurements showed some effects that could not be attributed to either simple comb filtering or resonance effects. In a listening test the subjects were asked to sort three binaurally-rendered samples in growing order of perceived distance. With relatively small distance triples, i.e., 1-11-21 cm, 3-13-23 cm, or 7-17-27 cm, the subjects performed better than guessing. However, even with shortest tested distances the proportion of correct answers was of the order of 50%. Two of 12 subjects consistently reported the distances in reverse order, which shows that the acoustic effect was significant, but it did not lead to the correct perception of distance of the surface. The results suggest that the acoustic effect of objects being close to an avatar’s ear may in some cases improve realism and sound quality in an acoustic virtual reality both in anechoic and in reverberant conditions.","2017","2023-07-12 06:37:37","2023-07-19 04:36:35","","997–1004","","12","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PB4MQ5A7","journalArticle","2016","Chau, Chuck-jee; Mo, Ronald; Horner, Andrew","The Emotional Characteristics of Piano Sounds with Different Pitch and Dynamics","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18528","Previous research has shown that both sustained and nonsustained musical instrument sounds have strong emotional characteristics. This report explores how the effects of pitch and dynamics influence the emotional characteristics of isolated one-second piano sounds. Listeners compared the sounds pairwise over ten emotion categories. The results showed that all ten emotional categories were significantly affected by pitch and nine of them by dynamics. In particular, the emotional characteristics Happy, Romantic, Comic, Calm, Mysterious, and Shy generally increased with pitch, but sometimes decreased at the highest pitches. The characteristics Heroic, Angry, and Sad generally decreased with pitch. Scary was strong in the extreme low and high registers. With regard to dynamics, the results showed that the characteristics Heroic, Comic, Angry, and Scary were stronger for loud notes, while Romantic, Calm, Mysterious, Shy, and Sad were stronger for soft notes. Surprisingly, Happy was not affected by dynamics. These results help quantify the emotional characteristics of piano sounds.","2016","2023-07-12 06:37:52","2023-07-19 03:47:43","","918–932","","11","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V2QEZRJC","journalArticle","2013","Kunka, Bartosz; Kostek, Bozena","New Aspects of Virtual Sound Source Localization Research–Impact of Visual Angle and 3-D Video Content on Sound Perception","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16824","","2013","2023-07-12 06:37:55","2023-07-12 06:37:55","","280–289","","5","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YGQJQMXE","journalArticle","2003","Laroche, Jean","Efficient Tempo and Beat Tracking in Audio Recordings","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12235","Automatic beat tracking consists of estimating the number of beats per minutes at which a music track is played and identifying exactly when these beats occur. Applications range from music analysis, sound-effect synchronization, and audio editing to automatic playlist generation and deejaying. An off-line beat-tracking technique for estimating a time-varying tempo in an audio track is presented. The algorithm uses an MMSE estimation of local tempo and beat location candidates, followed by a dynamic programming stage used to determine the optimum choice of candidate in each analysis frame. The algorithm is efficient in its use of computation resource, yet provides very good results on a wide range of audio tracks. The algorithm details are presented, followed by a discussion of the performance and suggestions for further improvements.","2003","2023-07-12 06:37:58","2023-07-19 04:17:21","","226–233","","4","51","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D3SYKPNG","journalArticle","2023","Anemüller, Carlotta; Adami, Alexander; Herre, Jürgen","Efficient Binaural Rendering of Spatially Extended Sound Sources","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22131","","2023","2023-07-12 06:38:01","2023-07-12 06:38:01","","281–292","","5","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KCGPE7J8","journalArticle","2012","Möller, Sebastian; Kettler, Frank; Gierlich, Hans-Wilhelm; Poschen, Silvia; Côté, Nicolas; Raake, Alexander; Wältermann, Marcel","Extending the E-Model for Capturing Noise Reduction and Echo Canceller Impairments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16214","The E-model for predicting speech quality mouth-to-ear can be extended with additional parameters to describe the effect of imperfect noise reduction and echo cancellation. As shown by subjective tests, quality-prediction accuracy of noise reduction and echo cancelling improves. Future work is planned to better refine the proposed approach.","2012","2023-07-12 06:38:04","2023-07-19 04:32:58","","165–175","","3","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"54TDHKKH","journalArticle","2018","Francombe, Jon; Woodcock, James; Hughes, Richard J.; Mason, Russell; Franck, Andreas; Pike, Chris; Brookes, Tim; Davies, William J.; Jackson, Philip J. B.; Cox, Trevor J.; Fazi, Filippo M.; Hilton, Adrian","Qualitative Evaluation of Media Device Orchestration for Immersive Spatial Audio Reproduction","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19581","The challenge of installing and setting up dedicated spatial audio systems can make it difficult to deliver immersive listening experiences to the general public. However, the proliferation of smart mobile devices and the rise of the Internet of Things mean that there are increasing numbers of connected devices capable of producing audio in the home. “Media device orchestration” (MDO) is the concept of utilizing an ad hoc set of devices to deliver or augment a media experience. In this paper, the concept is evaluated by implementing MDO for augmented spatial audio reproduction using object-based audio with semantic metadata. A system that augmented a stereo pair of loudspeakers with an ad hoc array of connected devices is described. The MDO approach aims to optimize aspects of the listening experience that are closely related to listener preference rather than attempting to recreate sound fields as devised during production. A thematic analysis of positive and negative listener comments about the system revealed three main categories of responses: perceptual, technical, and content-dependent aspects. MDO performed particularly well in terms of immersion/envelopment, but the quality of listening experience was partly dependent on loudspeaker quality and listener position.","2018","2023-07-12 06:38:09","2023-07-19 04:00:44","","414–429","","6","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W3LMXGMP","journalArticle","2006","Pulkki, Ville; Merimaa, Juha","Spatial Impulse Response Rendering II: Reproduction of Diffuse Sound and Listening Tests","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13664","Spatial impulse response rendering (SIRR) is a method for reproducing room impulse responses over multichannel loudspeaker setups. The applied analysis and synthesis methods were introduced in a companion paper. Time–frequency analysis is used to obtain directional and diffuseness information from the recorded sound field. Nondiffuse sound is then reproduced as pointlike virtual sources, and diffuse sound is synthesized with a decorrelation technique. The proposed synthesis methods for diffuse sound are examined in more detail and a hybrid method is derived. The relationship between diffuseness and interaural coherence is also studied. In addition, results of two listening tests are presented. It is shown that with a large loudspeaker setup under anechoic conditions, SIRR reproduction is at best indistinguishable from the original sample. Furthermore, in a listening test conducted in a standard listening room with real measured responses, SIRR reproduction is evaluated as the most natural one of the systems studied.","2006","2023-07-12 06:38:12","2023-07-19 04:39:26","","3–20","","1/2","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KB3PD9FD","journalArticle","2020","Lee, Jake Ryan Rajjayabun; Reiss, Joshua D.","Real-Time Sound Synthesis of Audience Applause","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20732","We investigate a procedural model for synthesizing applause sounds that contains novel aspects to ensure high quality and usability. Synthesis of a single clap is generated as a result of filtering a noise source and applying an envelope with exponential decay, based on prior art and existing experimental data. An ensemble approach is introduced to simulate many clappers in a spatially distributed environment. This renders how applause interacts with the space in which it is hosted, including the room impulse response, and where each clap is situated relative to the listener’s position. The applause features realistic build-up and fadeout based on natural audience response. The implementation contains meaningful parameters that allow a user to configure and change the sound to achieve a multitude of different types of applause, such as an “enthusiasm parameter” to simulate the greater perceived intensity from an enthusiastic audience. Subjective evaluation was performed to compare our method against recorded samples and four other popular sound synthesis techniques. It showed that the pro- posed implementation produced significantly more realistic results than other forms of applause synthesis, and it was almost indistinguishable from real-life recordings.","2020","2023-07-12 06:38:17","2023-07-19 04:19:49","","261–272","","4","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A5XT3Z36","journalArticle","2019","McGinnity, Siobhan; Mulder, Johannes; Beach, Elizabeth Francis; Cowan, Robert","Management of Sound Levels in Live Music Venues","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20710","With the increased recognition of potential damage to listeners when subjected to excessively loud sound, software-based sound level management systems can be viewed as a component of a strategy for reducing sound exposure to patrons and staff in live music venues. However, the use of level management tools in small indoor music venues, which represent a unique environment, has not been systematically explored. In an experimental approach for sound level management, a software system was tried in six indoor live-music venues in Melbourne. Comparing a control (without sound level management software) and the experimental condition (using the software), there was no reduction in mean LAeq,T, although there was a reduction in the number of events with extreme volume levels. Subjective questionnaires indicated that one-fifth of the patrons preferred lower sound levels than they experienced. The findings suggest that modifications to the software system may be necessary if the aim of the system is to reduce patron and staff sound exposure rather than simply to avoid exceeding legislative sound level limits. Recommended alterations could include greater flexibility in choice of target, matching with context of the performance, or changes to the system's visual display so that staying below, not at target, is positively reinforced.","2019","2023-07-12 06:38:21","2023-07-19 04:29:45","","972–985","","12","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SXYPLPNE","journalArticle","2023","Cairns, Patrick; Hunt, Anthony; Johnston, Daniel; Cooper, Jacob; Lee, Ben; Daffern, Helena; Kearney, Gavin","Evaluation of Metaverse Music Performance With BBC Maida Vale Recording Studios","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22139","","2023","2023-07-12 06:38:24","2023-07-12 06:38:24","","313–325","","6","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y2GZFTZD","journalArticle","2017","Chau, Chuck-jee; Gilburt, Samuel J. M.; Mo, Ronald; Horner, Andrew","The Emotional Characteristics of Bowed String Instruments with Different Pitch and Dynamics","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19173","This paper investigates how emotional characteristics vary with pitch and dynamics within the bowed string instrument family. Listening tests compared the effects of pitch and dynamics on emotional characteristics of the violin, viola, cello, and double bass. Listeners compared the sounds pairwise over ten emotional categories. Results showed that the emotional characteristics Happy, Heroic, Romantic, Comic, and Calm generally increased with pitch, but decreased at the highest pitches. Angry and Sad generally decreased with pitch. Scary was strong in the extreme low and high registers, while Shy and Mysterious were unaffected by pitch. For dynamics, the results showed that Heroic, Comic, and Angry were stronger for loud notes, while Romantic, Calm, Shy, Sad, and the high register for Happy were stronger for soft notes. Scary and Mysterious were unaffected by dynamics. The results also showed significant differences between different bowed string instruments on notes of the same pitch and dynamic level. The results provide audio engineers and musicians with suggestions for emphasizing emotional characteristics of bowed strings in sound recordings and performances.","2017","2023-07-12 06:38:27","2023-07-19 03:47:34","","573–588","","7/8","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BSKFSEDJ","journalArticle","2021","Riionheimo, Janne; Lokki, Tapio","Movie Sound, Part 1: Perceptual Differences of Six Listening Environments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21016","","2021","2023-07-12 06:38:30","2023-07-12 06:38:30","","54–67","","1/2","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4VJQU972","journalArticle","2020","Liew, Kongmeng; Lindborg, PerMagnus","A Sonification of Cross-Cultural Differences in Happiness-Related Tweets","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20715","","2020","2023-07-12 06:38:33","2023-07-12 06:38:33","","25–33","","1/2","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6TTAK2FL","journalArticle","2019","Thuillier, Etienne; Lähdeoja, Otso; Välimäki, Vesa","Feedback Control in an Actuated Acoustic Guitar using Frequency Shifting","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20480","Recent research demonstrated that the classical guitar can be advantageously augmented using a pickup to drive an actuator mounted on the guitar’s back plate. This allows enrichment of the instrument’s timbral palette with audio effect processors in the loop. The feedback problem that results from such a setup is similar to what occurs in live music performance setups where the sound of a guitar is amplified using a loudspeaker. In the present case, measurements of the augmented guitar’s open-loop response demonstrate that instabilities are susceptible to occurring from the string’s modes and not from the guitar’s sound-box. In particular, the shape of the magnitude response suggests frequency shifting as a viable solution to string instability. Introduction of an upward frequency shift in the forward path is proposed as a means for stabilizing the closed-loop system. Experimental results demonstrate that the proposed solution leads to improved stability even for a modest frequency shift of 3 Hz. The achieved gain margin improvement, which is shown to be of at least 3 dB, then comes at the cost of a clearly perceptible amplitude modulation, which may be acceptable in conjunction with other audio effects chosen by the performer.","2019","2023-07-12 06:38:36","2023-07-19 04:52:51","","373–381","","6","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HQ2WA4E2","journalArticle","2014","Le Bagousse, Sarah; Paquier, Mathieu; Colomes, Catherine","Categorization of Sound Attributes for Audio Quality Assessment—A Lexical Study","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17549","For the evaluation of perceived quality in audio coding, two well-known subjective test methods, both of which are based on Basic Audio Quality (BAQ), are recommended by the International Telecommunication Union. Although a predictor of quality, BAQ is likely to be multidimensional. Listening tests can be used to evaluate other attributes that contribute to impairments created by coding. The goal of this study is to define categories of additional attributes, thereby providing a complement to the single BAQ metric. When quality attributes are sorted, there appears to be three groups: one related to space, a second related to defects, and a third split into timbre and quality.","2014","2023-07-12 06:38:39","2023-07-19 04:17:49","","736–747","","11","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3QXDZUVP","journalArticle","2006","Phatak, Sandeep A.; Ratnam, Rama; Wheeler, Bruce C.; O’brien, William D., Jr.; Feng, Albert","Effect of Reflectors on Sound-Source Localization with Two Microphones","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13689","[Engineering Report] The localization performance of the source localization algorithms degrades in reverberant conditions. The performance of one such localization algorithm, the localization–extraction (LE) algorithm, was measured systematically as a function of the number of reflecting surfaces in a cubical enclosure. Localization was qualitatively measured using a localization plot and quantized using two objective parameters. A broad-band noise burst and a speech signal were used as stimuli. The degradation of the localization performance was monotonic but not uniform with an increase in the number of reflectors. The performance was found to be proportional to the bandwidth of the stimulus. The performance of the LE algorithm was benchmarked against that of a commonly used signal-subspace technique—multiple signal classification (MUSIC). The LE algorithm was less affected by reflections than the MUSIC algorithm. Degradation of the source localization under high reverberation was found to be more severe at low frequencies, which resulted in the detection of a “phantom” source at 0° for the speech signal.","2006","2023-07-12 06:38:42","2023-07-19 04:38:01","","512–524","","6","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IF9HLX5H","journalArticle","2012","Mendonça, Catarina; Campos, Guilherme; Dias, Paulo; Vieira, José; Ferreira, João P.; Santos, Jorge A.","On the Improvement of Localization Accuracy with Non-Individualized HRTF-Based Sounds","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16555","","2012","2023-07-12 06:38:45","2023-07-12 06:38:45","","821–830","","10","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MQXTFVJJ","journalArticle","2021","Bown, Oliver; Ferguson, Sam; Dos Santos, Augusto Dias Pereira; Mikolajczyk, Kurt","Supporting Creative Practice in Wireless Distributed Sound Installations Given Technical Constraints","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21470","","2021","2023-07-12 06:38:48","2023-07-12 06:38:48","","757–767","","10","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"47ZAI3H4","journalArticle","2017","Howie, Will; King, Richard; Martin, Denis","Listener Discrimination Between Common Speaker-Based 3D Audio Reproduction Formats","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19353","Over the last decade, numerous three-dimensional audio playback formats have been introduced and standardized for cinema, broadcast, and home theater environments. They differ in terms of number of speakers, speaker positions in the horizontal and vertical planes, and workflow strategies: channel-based, object-based, or some hybrid of the two. Each system possesses inherent pros and cons. This research attempts to determine whether listeners could discriminate among four currently standardized three-dimensional audio formats for reproduction of acoustic music. Double-blind listening tests showed that listeners could discriminate between NHK 22.2 Multichannel Sound (22.2) and several lower-channel-count 3D reproduction formats with a high degree of success, regardless of the musical stimulus. Listeners were also able to discriminate between three relatively similar 3D audio formats: ATSC 11.1, KBS 10.2, and Auro 9.1, although with significantly less success than with the 22.2. This suggests each of these formats deliver a perceptually different listening experience, with 22.2 being particularly different from the other formats under investigation.","2017","2023-07-12 06:38:52","2023-07-19 04:06:53","","796–805","","10","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y26SXWW3","journalArticle","2007","Bai, Mingsian R.; Chen, Meng-chun","Intelligent Preprocessing and Classification of Audio Signals","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14164","An audio processor that integrates intelligent classification and preprocessing algorithms is presented. Audio features in the time and frequency domains are extracted and processed prior to classification. Classification algorithms, including the nearest neighbor rule (NNR), artificial neural networks (ANN), fuzzy neural networks (FNN), and hidden Markov models (HMM), are used to classify and identify singers and musical instruments. A training phase is required to establish a feature space template, followed by a test phase in which the audio features of the test data are calculated and matched to the feature space template. In addition to audio classification, the proposed system provides several independent component analysis (ICA)-based preprocessing functions for blind source separation, voice removal, and noise reduction. The proposed techniques were applied to process various kinds of audio program materials. The test results reveal that the performance of the methods is satisfactory, but varies slightly with the algorithm and program materials used in the tests.","2007","2023-07-12 06:38:55","2023-07-19 03:37:50","","372–384","","5","55","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6E7YQ6KZ","journalArticle","2016","Bolaños, Javier Gómez; Mäkivirta, Aki; Pulkki, Ville","Automatic Regularization Parameter for Headphone Transfer Function Inversion","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18517","Binaural synthesis enables headphone presentation of the same auditory impression that a listener would perceive if present in the original sound field. While the presentation of a binaural signal requires a head-related impulse response and/or a binaural room response, it also requires compensation for the headphone response. A method is proposed for automatically regularizing the inversion of a headphone transfer function for headphone equalization. The problem arises from the fact that the inversion cannot treat peaks and notches as being perceptually equivalent. Notch inversion can create high-Q resonances that can be very unpleasant. Evaluation of the proposed method indicates that it provides an inversion filter that can maintain the accuracy of the conventional regularized inverse method while limiting the inversion of notches in a perceptually acceptable manner. The results show that the proposed method can produce perceptually better equalization than the regularized inverse method used with a fixed regularization factor or the complex smoothing method used with a half-octave smoothing window.","2016","2023-07-12 06:38:58","2023-07-19 03:43:36","","752–761","","10","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XUA3HN9T","journalArticle","2022","Lazaro, May Jorella; Kim, Sungho; Choi, Minsik; Kim, Kichang; Park, Dongchul; Moon, Soyoun; Yun, Myung Hwan","Design and Evaluation of Electric Vehicle Sound Using Granular Synthesis","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21566","Electric vehicles (EVs) generally do not have the natural engine sound; thus, sound design for such vehicles is more crucial. This study focuses on proposing an EV interior sound design through utilizing the granular synthesis method. Moreover, it aims to investigate the effect of each granular synthesis parameter on the drivers' affective experience. In the model proposed, four granular synthesis parameters (sample source, grain duration, grain envelope, and revolution per minute [RPM] range) with three levels each were selected for synthesis. By combining different values of each sound parameter in an orthogonal array, 27 EV sound samples were generated. A jury test was conducted with 32 participants (20 male, 12 female), which evaluated each sound sample based on three EV-related affective adjectives (""refined,"" ""sporty,"" and ""futuristic"") and ratings of overall satisfaction. The results showed that each granular synthesis parameter has a different impact on the perception of EV-related affect and satisfaction. Moreover, it also found that combining different values of each parameter may result in inducing a specific emotion or experience. The EV sound design methodology proposed in this study can contribute to the development of future EV sound to increase its effectiveness in improving the drivers' auditory experience.","2022","2023-07-12 06:39:01","2023-07-19 04:17:40","","294–304","","4","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MKV94KYJ","journalArticle","2019","Howie, Will; Martin, Denis; Kim, Sungyoung; Kamekawa, Toru; King, Richard","Effect of Audio Production Experience, Musical Training, and Age on Listener Performance in 3D Audio Evaluation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20696","","2019","2023-07-12 06:39:04","2023-07-12 06:39:04","","782–794","","10","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6WFMWNJS","journalArticle","2022","Kim, Taeho; Pöntynen, Henri; Pulkki, Ville","Vertical Direction Control Using Difference-Spectrum Filters in Stereophonic Loudspeaker Reproduction","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21555","This paper introduces difference-spectrum filters that can be used to control the perceived vertical direction of a sound source presented from ear-level loudspeakers. The difference-spectrum filter was designed to mimic the macroscopic changes in the spectral envelope of head-related transfer functions (HRTFs) between a target elevation angle and the ear-level elevation (0?), where the HRTF envelopes were obtained from averaging an extensive collection of individual HRTFs in a database. Localization tests were conducted to evaluate the effectiveness of difference-spectrum filters on elevation perception, which showed a promising result in the two-channel stereophonic condition for the virtual sound source. The perceived elevation correlated well with the target elevation angle of difference-spectrum filters in the stereophonic condition, although a weak correlation was observed in the monophonic condition. Thus, the test results show that difference-spectrum filters can create robust illusory elevation perception and enable vertical direction control over a wide range of elevation angles in stereophonic loudspeaker reproduction.","2022","2023-07-12 06:39:08","2023-07-19 04:12:20","","128–139","","3","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AFNZ8JVK","journalArticle","2018","Lai, Wen-Hsing; Liang, Sen-Fu","Control of Fundamental Frequency Fluctuation in Taiwanese Singing Synthesis","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19565","","2018","2023-07-12 06:39:10","2023-07-12 06:39:10","","343–359","","5","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N9L8GA5H","journalArticle","2023","Lee, Ben; Rudzki, Tomasz; Skoglund, Jan; Kearney, Gavin","Context-Based Evaluation of the Opus Audio Codec for Spatial Audio Content in Virtual Reality","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22037","","2023","2023-07-12 06:39:15","2023-07-12 06:39:15","","145–154","","4","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QL88GJBB","journalArticle","2023","Willemsen, Silvin; Nuijens, Helmer; Lasickas, Titas; Serafin, Stefania","The Sonic Interactions in Virtual Environments (SIVE) Toolkit","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22143","","2023","2023-07-12 06:39:18","2023-07-12 06:39:18","","363–373","","6","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GKYDMRGE","journalArticle","2016","Rocchesso, Davide; Mauro, Davide Andrea; Drioli, Carlo","Organizing a sonic space through vocal imitations","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18333","This research investigates how the vocal mimicking capabilities of humans may be exploited to access and explore a given sonic space. Experiments showed that prototype vocal sounds can be represented in a two-dimensional space and still remain perceptually distinct from each other. Experiments provide a measure of how meaningful the machine distribution and grouping of vocal sounds are to humans, and confirms that humans are able to effectively use the acoustic and articulatory cues at their disposal to associate sounds to given prototypes. When used in an automatic clustering process, these cues are sufficiently consistent with those used by humans when categorizing acoustic phenomena. The procedure of dimensionality reduction and clustering is demonstrated in the case of imitations of engine sounds, which then represent the sonic space of a motor sound model. A two-dimensional space is particularly attractive for sound design because it can be used as a sonic map where the landmarks contain both a synthetic sound and its vocal imitation.","2016","2023-07-12 06:39:21","2023-07-19 04:41:11","","474–483","","7/8","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LXUTXB5X","journalArticle","2018","Francombe, Jon; Brookes, Tim; Mason, Russell","Determination and Validation of Mix Parameters for Modifying Envelopment in Object-Based Audio","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19381","","2018","2023-07-12 06:39:24","2023-07-12 06:39:24","","127–145","","3","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6PPDREHK","journalArticle","2010","Tzanetakis, George; Martins, Luis Gustavo; McNally, Kirk; Jones, Randy","Stereo Panning Information for Music Information Retrieval Tasks","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15454","A music information retrieval system can extract information that arises from how various sound sources are panned between channels during the mixing and recording process. The authors propose augmenting standard audio features, which are based on the source music, with one of two methods for extracting panning and contrast features. These additional features provide statistically important information for nontrivial audio classifications tasks. Traditional classifications focus on information about pitch, rhythm, and timbre. Other types of mixing parameters are proposed for future work.","2010","2023-07-12 06:39:27","2023-07-19 04:54:15","","409–417","","5","58","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"34CY6QQF","journalArticle","2017","Geravanchizadeh, Masoud; Avanaki, Hadi Jamshidi; Dadvar, Paria","Binaural Speech Intelligibility Prediction in the Presence of Multiple Babble Interferers Based on Mutual Information","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18561","This paper describes a predictor for binaural speech intelligibility that computes speech reception thresholds (SRT) without the need to perform subjective listening tests. Although listening tests are considered to be the most reliable indicators of performance, such tests are time consuming and costly. The proposed model computes SRTs in two stages. First, it calculates the binaural advantage. Then, it derives the SRTs based on the computed mutual information of the speech and mixture envelopes. Listening tests were conducted with 13 normal-hearing listeners in 15 spatial configurations, covering one, two, and three babble interferers. The proposed predictor performs as well as the baseline model in predicting the intelligibility of binaural vowel-consonant-vowel signals contaminated by multiple nonstationary babble noise sources. The model is evaluated in anechoic conditions and compared with subjective data as well as with the predictions obtained from a baseline binaural speech intelligibility model.","2017","2023-07-12 06:39:31","2023-07-19 04:02:15","","285–292","","4","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7IMSQMNF","journalArticle","1988","Fielder, Louis D.; Benjamin, Eric M.","Subwoofer Performance for Accurate Reproduction of Music","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=5147","The spectra and the maximum output levels for accurately reproducing low-frequency musical signals are determined from published research and new measurements. Analysis of commercial recordings shows substantial musical information in the octave from 32 to 16 Hz and some down to 12 Hz. Psychoacoustic data are used to establish to what degree errors (such as total harmonic distortion, FM distortion, modulation noise, and bandwidth limits) are perceptible. Criteria are set for proper subwoofer performance at peak sound pressure levels of 110 dB.","1988","2023-07-12 06:39:34","2023-07-19 03:58:02","","443–456","","6","36","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GI3YL3E3","journalArticle","2004","Burred, Juan José; Lerch, Alexander","Hierarchical Automatic Audio Signal Classification","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13015","The design, implementation, and evaluation of a system for automatic audio signal classification is presented. The signals are classified according to audio type, differentiating between three speech classes, 13 musical genres, and background noise. A large number of audio features are evaluated for their suitability in such a classification task, including MPEG-7 descriptors and several new features. The selection of the features is carried out systematically with regard to their robustness to noise and bandwidth changes, as well as to their ability to distinguish a given set of audio types. Direct and hierarchical approaches for the feature selection and for the classification are evaluated and compared.","2004","2023-07-12 06:39:37","2023-07-19 03:46:04","","724–739","","7/8","52","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C25EGXP6","journalArticle","2016","Stitt, Peter; Bertet, Stéphanie; van Walstijn, Maarten","Extended Energy Vector Prediction of Ambisonically Reproduced Image Direction at Off-Center Listening Positions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18135","Spatial audio techniques, such as Ambisonics and Wave Field Synthesis, aim to reproduce the sound field in the limited area of the sweet spot, which is approximately equidistant from all loudspeakers. This research extends the energy vector technique to improved localization at off-centered positions. In determining the source direction, a perceptual weight is assigned to each loudspeaker gain that takes into account the relative arrival times, levels, and directions of the loudspeaker signals. Conversely, uncompensated differences in arrival time can trigger the precedence effect. The proposed model was evaluated and compared to the original energy vector model and two binaural models. The extended energy vector version was at least 50% more accurate than the second best predictor with an average error of about 4°.","2016","2023-07-12 06:39:39","2023-07-19 04:49:27","","299–310","","5","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DXYQDQU6","journalArticle","2009","Hiekkanen, Timo; Mäkivirta, Aki; Karjalainen, Matti","Virtualized Listening Tests for Loudspeakers","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14816","When comparing loudspeakers and trying to eliminate the influence of their location, problems arise due to listeners’ short auditory memory. But if a virtual loudspeaker in a virtual room using headphones was equivalent to the real environment, comparison testing would avoid the problem of memory. Switching could be done instantaneously. Subjective tests showed that the quality of virtual loudspeakers depended highly on the test signal and upon the difficulty of creating accurate room- and head-related transfer functions at high frequencies. Nevertheless, virtualized loudspeakers can be imperceptible from reality in many cases.","2009","2023-07-12 06:39:42","2023-07-19 04:05:49","","237–251","","4","57","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5FS6SVDQ","journalArticle","2020","Kritsis, Kosmas; Garoufis, Christos; Zlatintsi, Athanasia; Bouillon, Manuel; Acosta, Carlos; Martín-Albo, Daniel; Piechaud, Robert; Maragos, Petros; Katsouros, Vassilis","iMuSciCA Workbench: Web-based Music Activities For Science Education","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20990","","2020","2023-07-12 06:39:45","2023-07-12 06:39:45","","738–746","","10","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2DV8WZAR","journalArticle","2006","Heusdens, Richard; Jensen, Jesper; Kleijn, W. Bastiaan; Kot, Valery; Niamut, Omar A.; Van De Par, Steven; Van Schijndel, Micholle H.","Bit-Rate Scalable Intraframe Sinusoidal Audio Coding Based on Rate-Distortion Optimization","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13673","A coding methodology that aims at rate-distortion optimal sinusoid + noise coding of audio and speech signals is presented. The coder divides the input signal into variable-length time segments and distributes sinusoidal components over the segments such that the resulting distortion (as measured by a perceptual distortion measure) is minimized subject to a prespecified rate constraint. The coder is bit-rate scalable. For a given target bit budget it automatically adapts the segmentation and distribution of sinusoids in a rate-distortion optimal manner. The coder uses frequency-differential coding techniques in order to exploit intrasegment correlations for efficient quantization and encoding of the sinusoidal model parameters. This technique makes the coder more robust toward packet losses when used in a lossy-packet channel environment as compared to time-differential coding techniques, which are commonly used in audio or speech coders. In a subjective listening experiment the present coder showed similar or better performance than a set of four MPEG-4 coders operating at bit rates of 16, 24, 32, and 48 kbit/s, each of which was state of the art for the given target bit rate.","2006","2023-07-12 06:39:48","2023-07-19 04:05:40","","167–188","","3","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EHKYTHJ3","journalArticle","2016","Mo, Ronald; So, Richard H. Y.; Horner, Andrew","An Investigation into How Reverberation Effects the Space of Instrument Emotional Characteristics","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18533","","2016","2023-07-12 06:39:57","2023-07-12 06:39:57","","988–1002","","12","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VYKT92G9","journalArticle","2016","Bitzer, Joerg; Kissner, Sven; Holube, Inga","Privacy-Aware Acoustic Assessments of Everyday Life","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18298","In order to enhance people’s ability to interact with their acoustic environments, hearing devices are common tools. However, it is difficult to evaluate the benefit of those tools or to measure acoustically challenging situations in natural environments. This paper proposes a way to measure the most important features of everyday acoustics environments by extracting a limited set of features while not compromising the privacy of partners and bystanders. The respective national laws on how to deal with audio privacy are very different among countries. The authors proposed using a smartphone as the source recorder but splitting the feature extraction into two phases: an initial feature processing in the smartphone and a later processing on a more powerful computer. For a given feature set, a statistical analysis shows comparable results from the extracted data when using either the original audio or the new privacy-aware extraction methods. A comparison shows that different scenarios result in separable features using the new extraction method.","2016","2023-07-12 06:40:00","2023-07-19 03:42:40","","395–404","","6","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BRCZN7AI","journalArticle","1992","Begault, Durand R.","Perceptual Effects of Synthetic Reverberation on Three-Dimensional Audio Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=7027","A psychoacoustic investigation was conducted in which five subjects gave localization judgments for headphone-delivered speech stimuli processed by nonindividual head-related transfer functions, with and without synthetic ""spatial"" reverberation added to the stimuli. Spatial reverberation minimized intracranially heard stimuli, but increased the magnitude of azimuth and elevation localization errors. The results are applicable to three-dimensional sound systems and spatial sound field processors designed to increase the sensation of auditory ""spaciousness.""","1992","2023-07-12 06:40:04","2023-07-19 03:40:34","","895–904","","11","40","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LD6HAV7R","journalArticle","2019","Su, Hengwei; Marui, Atsushi; Kamekawa, Toru","The Auditory Source Widening Effect in Binaural Synthesis with Spatial Distribution of Frequency Bands","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20545","A binaural technique (involving direct control of signals transferred into both ears of listeners), not only can solve the problem of spatial impression of headphone reproduction but also has the ability to provide realistic auditory experiences, especially in 3D spatial acoustic reproduction. In this study, monophonic source signals were processed by frequency-band decomposition and distribution to achieve spatially widened perceived source widths in binaural synthesis. Stimuli with different widths were synthesized, and the perceived widths were evaluated by conducting a listening experiment to investigate the relationship of the perceived width and the synthesized width. Three different bandwidths of frequency bands and two center positions of synthesized widths were used in the processing, and the relevant effects on perception of source width were investigated. The results of the listening experiment suggested that under proper processing conditions the perceived width could increase with increasing synthesized widths. However, dependencies of source signal characteristics and variations between participants were observed. Degradations of timbre and spatial quality were also evaluated. The results suggested that this method suffered less degradation than a conventional decorrelation method while it achieved comparable widening effects for binaural reproduction. For example, for a cello source signal with 1/12-octave bandwidth, the perceived width increased with increasing synthesis width. This suggests that under appropriate conditions this method could control the perceived width of a monophonic source in binaural synthesis.","2019","2023-07-12 06:40:07","2023-07-19 04:50:31","","691–704","","9","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WEN6YKXX","journalArticle","2005","Sanchez-bote, Jose-luis; Gonzalez-rodriguez, Joaquin; Ortega-garcia, Javier","Audible Noise Suppression with a Real-Time Broad-Band Superdirective Microphone Array","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13419","A novel audible noise suppression (ANS) multichannel processor is proposed and implemented for real-time processing of data from a 15-microphone nested linear array. The ANS processor, based on the masking properties of the human auditory system, has been used successfully in single-channel systems. An enhanced multichannel version has now been developed, taking advantage of the extra information available in the acoustic spatial samples from the microphone array. This is used to improve the clean speech signal estimates used to calculate dynamically the noise hearing thresholds for ANS filtering, which will benefit the perceived and objective quality of the processed signal. While off-line experiments with a multichannel recorded database under different noise and reverberation conditions have previously assessed the performance of the system, several on-line experiments assessing the real-time prototype are described.","2005","2023-07-12 06:40:10","2023-07-19 04:46:15","","403–418","","5","53","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CBM6EKK4","journalArticle","2012","Raake, Alexander; Wältermann, Marcel; Wüstenhagen, Ulf; Feiten, Bernhard","How to Talk about Speech and Audio Quality with Speech and Audio People","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16212","Typically, audio quality assessment uses MUSHRA (Multi Stimulus with Hidden Reference and Anchors), while speech quality assessment uses ACR (Absolute Category Rating). Since many applications are transporting both speech and music, such as mobile devices, a conversion technique between the two types of ratings would be useful. Two speech and two audio quality listening tests are compared with different content types. The results illustrate when and how the two types of measurements are consistent, complementary, and inconsistent.","2012","2023-07-12 06:40:13","2023-07-19 04:39:44","","147–155","","3","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VY6JYZGU","journalArticle","2010","Moore, Alastair H.; Tew, Anthony I.; Nicol, Rozenn","An Initial Validation of Individualized Crosstalk Cancellation Filters for Binaural Perceptual Experiments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15240","","2010","2023-07-12 06:40:16","2023-07-12 06:40:16","","36–45","","1/2","58","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DIEJERI6","journalArticle","2013","Igumbor, Osedum P.; Foss, Richard","Control Protocol Command Translation for Device Interoperability on Ethernet AVB Networks","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16707","","2013","2023-07-12 06:40:17","2023-07-12 06:40:17","","224–234","","4","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6CUMZA74","journalArticle","2009","Vilkamo, Juha; Lokki, Tapio; Pulkki, Ville","Directional Audio Coding: Virtual Microphone-Based Synthesis and Subjective Evaluation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14838","Directional Audio Coding (DirAC) is a perceptual method for reproducing spatial audio using existing microphones and surround sound configurations. In the analysis phase, the direction and diffuseness are continuously estimated in frequency bands. This information, together with input signals remapped into a set of virtual microphones corresponding to the reproduction configuration, is used to recreate the spatial audio experience in the listening setup. Subjective listening tests in both an anechoic chamber and in a standard listening room showed that the mean opinion scores were almost always good to excellent, which is higher quality than traditional techniques.","2009","2023-07-12 06:40:20","2023-07-19 04:55:31","","709–724","","9","57","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DDB4R7FU","journalArticle","1991","Hansen, Villy; Munch, Gert","Making Recordings for Simulation Tests in the Archimedes Project","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=5961","Archimedes is a psychoacoustic research project with three partners aiming to use experimental results from free-field simulation of room acoustics to improve sound-reproduction systems. The preparatory work of one partner is presented, involving monophonic recording of voice and various solo instruments under anechoic and semi-reverberant conditions. These recording are for comprehensive experiments using simulation of a standard listening room.","1991","2023-07-12 06:40:30","2023-07-19 04:03:37","","768–774","","10","39","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PN6UK434","journalArticle","2022","Fela, Randy Frans; Zacharov, Nick; Forchhammer, Søren","Assessor Selection Process for Perceptual Quality Evaluation of 360 Audiovisual Content","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22010","","2022","2023-07-12 06:40:34","2023-07-12 06:40:34","","824–842","","10","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WC47D6JM","journalArticle","1987","Strawn, John","Editing Time-Varying Spectra","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=10283","","1987","2023-07-12 06:40:39","2023-07-12 06:40:39","","337–352","","5","35","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LXY8JRNW","journalArticle","2014","Tervo, Sakari; Laukkanen, Perttu; Pätynen, Jukka; Lokki, Tapio","Preferences of Critical Listening Environments Among Sound Engineers","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17241","A study of preferred listening environments among fifteen sound engineers illustrates the universal principle that “one size does not fit everyone.” By using the measured impulse responses of nine studio control rooms that were then encoded using the Spatial Decomposition Method, each space was simulated in an anechoic chamber with a 30-channel reproduction system. Preferences depended on the occupation of the sound engineer and on the nature of the song. While mixing engineers preferred acoustically dry environments with high clarity, mastering engineers preferred more reverberant environments with less clarity. Reverberation and clarity appear to be the dominant dimensions for preference. Extensive interviews with the subjects provided more nuanced explanations of how the sound engineers experience a listening space.","2014","2023-07-12 06:40:42","2023-07-19 04:52:26","","300–314","","5","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3JPSDD9J","journalArticle","2018","Baird, Alice; Jørgensen, Stina Hasse; Parada-Cabaleiro, Emilia; Cummins, Nicholas; Hantke, Simone; Schuller, Björn","The Perception of Vocal Traits in Synthesized Voices: Age, Gender, and Human Likeness","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19393","","2018","2023-07-12 06:40:45","2023-07-12 06:40:45","","277–285","","4","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6JBZPU9S","journalArticle","2015","Drossos, Konstantinos; Floros, Andreas; Kermanidis, Katia L.","Evaluating the Impact of Sound Events’ Rhythm Characteristics to Listener’s Valence","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17573","While modern sound researchers generally focus on speech and music, mammalian hearing arose from the need to sense those events in the environment that produced sound waves. Such unorganized sound stimuli, referred to as Sound Events (SEs), can also produce an affective and emotional response. In this research, the investigators explore valence recognition of SEs utilizing rhythm-related acoustics cues. A well-known data set with emotionally annotated SEs was employed; various rhythm-related attributes were then extracted and several machine-learning experiments were conducted. The results portray that the rhythm of a SE can affect the listener’s valence up to an extent and, combined with previous works on SEs, could lead to a comprehensive recognition of the rhythm’s effect on the emotional state of the listener.","2015","2023-07-12 06:40:48","2023-07-19 03:53:44","","139–153","","3","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RSGU6NE6","journalArticle","2022","Pretto, Niccolò; Pozza, Nadir Dalla; Padoan, Alberto; Chmiel, Anthony; Werner, Kurt James; Micalizzi, Alessandra; Schubert, Emery; Roda, Antonio; Milani, Simone; Canazza, Sergio","A Workflow and Digital Filters for Correcting Speed and Equalization Errors on Digitized Audio Open-Reel Magnetic Tapes","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21798","This paper presents a workflow and digital filters for compensating speed and equalization errors that can impact digitized audio open-reel tapes. Thirty cases of mismatch between recording and reproducing speed (3.75, 7.5, 15, and 30 in/s) and equalization standards [National Association of Broadcasters (NAB), Consultative Committee for International Radio (CCIR), and Audio Engineering Society] were considered. For three frequent cases of mismatch (NAB 3.75 in/s---CCIR 7.5 in/s; NAB 3.75 in/s---CCIR 15 in/s; and NAB 7.5 in/s---CCIR 15 in/s), MUltiple Stimuli with Hidden Reference and Anchor--inspired tests with =21 participants assessed the workflow and digital filters, using excerpts of music and voice. Two different correction filters were used, both of which provided promising results. Following this, subsequent analyses examined predictive variables for correct and incorrect MUltiple Stimuli with Hidden Reference and Anchor performance, as well as spectral and numerical differences between filters, which provide key insights and recommendations for further related work.","2022","2023-07-12 06:40:52","2023-07-19 04:39:10","","495–509","","6","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"922XZHYC","journalArticle","2015","Mu, Hao; Gan, Woon-Seng","Perceptual Quality Improvement and Assessment for Virtual Bass Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18050","Because of size limitation and cost constraints, small loudspeakers cannot efficiently reproduce the sound in the low-frequency range, resulting in poor bass perception and a lack of strong rhythms. A psychoacoustic bass enhancement system, known as the virtual bass system (VBS), enhances the perception of bass reproduced by small loudspeakers by tricking the human auditory system into perceiving bass that does not physically exit. A VBS is based on the psychoacoustic phenomenon called the missing fundamental effect, wherein the higher harmonics of the fundamental frequency can produce the sensation of the fundamental frequency. However, these harmonics can result in perceived distortion. This report describes two techniques to improve the audio quality of the VBS, including an improved hybrid VBS and the timbre matching weighting. Objective and subjective tests are presented to compare the performance of the proposed and the conventional VBS techniques.","2015","2023-07-12 06:40:55","2023-07-19 04:33:56","","900–913","","11","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RTXGP74U","journalArticle","2018","Vryzas, Nikolaos; Kotsakis, Rigas; Liatsou, Aikaterini; Dimoulas, Charalampos A.; Kalliris, George","Speech Emotion Recognition for Performance Interaction","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19585","This research explores the relevance of machine-driven Speech Emotion Recognition (SER) as a way to augment theatrical performances and interactions, such as controlling stage color/light, stimulating active audience engagement, actors’ interactive training, etc. It is well known that the meaning of a speech utterance arises from more than the linguistic content. Emotional affect can dramatically change meaning. As the basis for classification experiments, the authors developed the Acted Emotional Speech Dynamic Database (AESDD, which contains spoken utterances from 5 actors with 5 emotions. Several audio features and various classification techniques were implemented and evaluated using this database, as well comparing results with the Surrey Audio-Visual Expressed Emotion (SAVEE) database. The training classified was integrated into a novel application that performed live SER, fitting the needs of actor training.","2018","2023-07-12 06:41:00","2023-07-19 10:53:55","","457–467","","6","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K7MIZKQI","journalArticle","2011","Sarroff, Andy M.; Bello, Juan P.","Toward a Computational Model of Perceived Spaciousness in Recorded Music","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15975","A validated computational model of perceived spaciousness in sound recordings serves to represent the subjective experience of listeners. Three dimensions of spaciousness form the basis of the model: width of source ensemble, extent of reverberation, and extent of immersion. The model is trained and tested to learn the audio parameters that contribute to these three dimensions. The resulting model predicted spaciousness 32% above that of a baseline predictor. The model can be used to show an audio engineer the value of the three dimensions in real time.","2011","2023-07-12 06:41:03","2023-07-19 04:46:40","","498–513","","7/8","59","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TNZT3B77","journalArticle","2003","Soulodre, Gilbert A.; Lavoie, Michel C.; Norcross, Scott G.","Objective Measures of Listener Envelopment in Multichannel Surround Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12205","A common goal in multichannel musical recordings is to create a better approximation of the concert-hall experience than can be achieved with a traditional stereo reproduction system. Listener envelopment (LEV) is known to be an important part of good concert-hall acoustics and is therefore desirable in multichannel reproduction. In the present study a series of subjective tests were conducted to determine which acoustic parameters are important to the creation of LEV. It is shown that LEV can be controlled systematically in a home listening environment by varying the level and angular distribution of the late arriving sound. While the perceptual transition point between early and late energy has traditionally been set to 80 ms when predicting LEV, this matter has not been investigated rigorously. Subjective tests were conducted wherein the temporal and spatial distributions of the late energy were varied. A new frequency-dependent objective measure GSperc was derived, and it was shown to outperform other objective measures significantly.","2003","2023-07-12 06:41:06","2023-07-19 04:48:52","","826–840","","9","51","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DVCSEJL4","journalArticle","2016","Reiss, Joshua D.","A Meta-Analysis of High Resolution Audio Perceptual Evaluation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18296","","2016","2023-07-12 06:41:10","2023-07-12 06:41:10","","364–379","","6","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YCK49YKX","journalArticle","2019","Ward, Lauren A.; Shirley, Ben G.","Personalization in Object-based Audio for Accessibility: A Review of Advancements for Hearing Impaired Listeners","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20496","Hearing loss is widespread and significantly impacts an individual’s ability to engage with broadcast media. Access for people with impaired hearing can be improved through new object-based audio personalization methods. Utilizing the literature on hearing loss and intelligibility, this paper develops three dimensions that have the potential to improve intelligibility: spatial separation, speech-to-noise ratio, and redundancy. These can be personalized, individually or concurrently, using object-based audio. A systematic review of all work in object-based audio personalization is then undertaken. These dimensions are utilized to evaluate each project’s approach to personalization, identifying successful approaches, commercial challenges, and the next steps required to ensure continuing improvements to broadcast audio for hard-of-hearing individuals. Although no single solution will address all problems faced by individuals with hearing impairments when accessing broadcast audio, several approaches covered in this review show promise.","2019","2023-07-12 06:41:15","2023-07-19 10:54:39","","584–597","","7/8","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ED46SJ88","journalArticle","2018","Woodcock, James; Davies, William J.; Melchior, Frank; Cox, Trevor J.","Elicitation of Expert Knowledge to Inform Object-Based Audio Rendering to Different Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19375","","2018","2023-07-12 06:41:18","2023-07-12 06:41:18","","44–59","","1/2","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W4RIDB95","journalArticle","2017","Francombe, Jon; Brookes, Tim; Mason, Russell; Woodcock, James","Evaluation of Spatial Audio Reproduction Methods (Part 2): Analysis of Listener Preference","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18556","A paired-comparison preference rating experiment was performed in combination with a free-elicitation task for eight reproduction methods (consumer and professional systems with a wide range of expected quality) and seven program items (representative of potential broadcast material). The experiment was performed by groups of experienced and inexperienced listeners. Both groups preferred systems with increased spatial content; nine- and five-channel systems were most preferred. The use of elicited attributes was analyzed alongside the preference ratings, resulting in an approximate hierarchy of attribute importance. Three attributes (amount of distortion, output quality, and bandwidth) were found to be important for differentiating systems where there was a large preference difference; sixteen were always important (most notably enveloping and horizontal width); and seven were used alongside small preference differences. Although the presence of more spatial content increases preference, adding loudspeaker channels does not necessarily give a corresponding increase in preference.","2017","2023-07-12 06:41:21","2023-07-19 03:58:53","","212–225","","3","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HP96QIXQ","journalArticle","2017","Adami, Alexander; Taghipour, Armin; Herre, Jürgen","On Similarity and Density of Applause Sounds","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19359","With applause an audience expresses its enthusiasm and appreciation; it is therefore a vital part of live performances and thus live recordings. Applause sounds range from very sparse sounds with easily distinguishable individual claps in small crowds to extremely dense and almost noise-like sounds in very large crowds. Commonly used perceptual attributes such as loudness, pitch, and timbre seem insufficient to characterize different types of applause, while “density,” which was recently introduced, is an important perceptual attribute for characterizing applause-like sounds. In this paper, the perceptual properties of applause sounds are investigated with a focus on how spectral equalization affects their perception. Two experiments are presented: the extent to which spectral equalization influences the perception of density and the impact of spectral equalization on the perceived similarity of applause sounds with different densities. Additionally, the data of both experiments were jointly evaluated to explore the effect of perceived density and spectral equalization on the perceived similarity of applause sounds. A statistical analysis of the experimental data suggests that spectral equalization has no statistically significant effect on density and only a small but significant effect on similarity. A linear mixed effects model fitted to the experimental data revealed that perceived density differences as well as spectral equalization significantly predict applause similarity. Density differences were found to be the dominating factor. Finally, the results appear applicable to other impulsive sounds with a high rate of transient events.","2017","2023-07-12 06:41:26","2023-07-19 03:33:57","","897–913","","11","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YJ4MSSTG","journalArticle","1984","Ingebretsen, Robert B.; Stockham, Thomas G., Jr.","Random-Access Editing of Digital Audio","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=4522","","1984","2023-07-12 06:41:29","2023-07-12 06:41:29","","114–122","","3","32","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QCYAYR8L","journalArticle","2023","Ackermann, David; Domann, Julian; Brinkmann, Fabian; Arend, Johannes M.; Schneider, Martin; Pörschmann, Christoph; Weinzier, Stefan","Recordings of a Loudspeaker Orchestra With Multichannel Microphone Arrays for the Evaluation of Spatial Audio Methods","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22032","For live broadcasting of speech, music, or other audio content, multichannel microphone array recordings of the sound field can be used to render and stream dynamic binaural signals in real time. For a comparative physical and perceptual evaluation of conceptually different binaural rendering techniques, recordings are needed in which all other factors affecting the sound (such as the sound radiation of the sources, the room acoustic environment, and the recording position) are kept constant. To provide such a recording, the sound field of an 18- channel loudspeaker orchestra fed by anechoic recordings of a chamber orchestra was captured in two rooms with nine different receivers. In addition, impulse responses were recorded for each sound source and receiver. The anechoic audio signals, the full loudspeaker orchestra recordings, and all measured impulse responses are available with open access in the Spatially Oriented Format for Acoustics (SOFA 2.1, AES69-2022) format. The article presents the recording process and processing chain as well as the structure of the generated database.","2023","2023-07-12 06:41:32","2023-07-19 03:33:33","","62–73","","1/2","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G4JTHMAL","journalArticle","2019","Brinkmann, Fabian; Dinakaran, Manoj; Pelzer, Robert; Grosche, Peter; Voss, Daniel; Weinzierl, Stefan","A Cross-Evaluated Database of Measured and Simulated HRTFs Including 3D Head Meshes, Anthropometric Features, and Headphone Impulse Responses","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20546","The individualization of head related transfer functions (HRTFs) can make an important contribution to improving the quality of binaural technology applications. One approach to individualization is to exploit the relationship between the shape of HRTFs and the anthropometric features of the ears, head, and torso of the corresponding listeners. To identify statistically significant relationships between the two sets of variables, a relatively large database is required. For this purpose full-spherical HRTFs of 96 subjects were acoustically measured and numerically simulated. A detailed cross-evaluation showed a good agreement to previous data between repeated measurements and between measured and simulated data. In addition to 96 HRTFs, the database includes high-resolution head-meshes, a list of 25 anthropometric features per subject, and headphone transfer functions for two headphone models.","2019","2023-07-12 06:41:35","2023-07-19 03:45:08","","705–718","","9","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MYNQIG9X","journalArticle","2014","Wu, Bin; Horner, Andrew; Lee, Chung","The Correspondence of Music Emotion and Timbre in Sustained Musical Instrument Sounds","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17544","While melody, rhythm, and harmony are important emotional triggers in music, there has been little consideration of timbre. The authors designed a series of listening tests to compare the emotionality of sounds from eight wind and bowed stringed instruments. The violin, trumpet, and clarinet were best at evoking the emotions of happy, joyful, heroic, and comic. Conversely, the horn and flute evoked the emotions of sad and depressed. The oboe was emotionally neutral. Emotions correlated with average spectral centroid and spectral centroid deviation. The results suggest that the even/odd harmonic ratio is perhaps the most salient timbral feature after attack time and brightness. This research has direct implications for musicians and audio engineers who are doing orchestration for such applications as computer games, film sound, and stage music.","2014","2023-07-12 06:41:39","2023-07-19 10:57:16","","663–675","","10","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YSP47RBH","journalArticle","2015","Wallis, Rory; Lee, Hyunkook","The Effect of Interchannel Time Difference on Localization in Vertical Stereophony","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18040","When listeners localize in the median plane (vertical), binaural cues are absent because the sound in the two ears is the same; median plane localization depends solely on spectral cues. In order to analyze the localization of band-limited stimuli in vertical stereophony, listening tests were conducted using seven octave bands of pink noise centered at frequencies from 125 to 8000 Hz as well as broadband pink noise. Experimental results showed that localization is generally governed by the so-called “pitch-height” effect, with the high-frequency stimuli generally being localized significantly higher than the low-frequency stimuli for all conditions. The relationship between pitch and height was found to be nonlinear. As frequency increased, subjective judgments appeared to become more erratic because of interchannel time differences.","2015","2023-07-12 06:41:43","2023-07-19 10:54:21","","767–776","","10","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MX7JF7B2","journalArticle","1991","Wöhr, Martin; Theile, Günther; Goeres, Hans-Jürgen; Persterer, Alexander","Room-Related Balancing Technique: A Method for Optimizing Recording Quality","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=5968","In contrast to natural sounds, in the case of stereophonic reproduction the ears localize the loudspeakers as the source of the sound. The spatial depth of a recording can therefore not be reproduced adequately with conventional microphone balancing techniques. It is described how to simulate spatial depth artificially. Listening tests demonstrate that stereophonic reflections clearly improve the quality of a sound image, making it more natural. It was shown that the favorable imaging characteristics found for an appropriate main microphone can be transferred consistently to the spot-microphone signals with the aid of modern computer technology.","1991","2023-07-12 06:41:47","2023-07-19 10:56:29","","623–631","","9","39","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FB6JUPGD","journalArticle","2023","McKenzie, Thomas; Meyer-Kahlen, Nils; Hold, Christoph; Schlecht, Sebastian J.; Pulkki, Ville","Auralization of Measured Room Transitions in Virtual Reality","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22140","","2023","2023-07-12 06:41:50","2023-07-12 06:41:50","","326–337","","6","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BVFKEIYZ","journalArticle","2007","Wun, Simon; Horner, Andrew","Evaluation of Weighted Principal-Component Analysis Matching for Wavetable Synthesis","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14194","[Engineering Report] Wavetable matching of musical instrument tones using principal-component analysis (PCA) takes advantage of spectral correlation information to find the basis spectra. Although PCA matching is efficient, it usually matches the low-amplitude parts of a tone poorly because of its inherent statistical bias. Weighted PCA methods are described, which normalize the tone prior to PCA to fairly weight its different parts. Matching results for a range of instruments show that the PCA of a frame-weighted spectrum with a roughly constant loudness throughout improves on unweighted PCA by an average of about 4% relative spectral error. Listening test results show that frame-weighted PCA gives some perceptual improvements for most of the instruments.","2007","2023-07-12 06:41:53","2023-07-19 10:57:50","","762–774","","9","55","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QDQSTSUF","journalArticle","2014","Goetze, Stefan; Albertin, Eugen; Rennies, Jan; Habets, Emanuël A.P.; Kammeyer, Karl-Dirk","Speech Quality Assessment for Listening-Room Compensation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17334","There is extensive research on listening-room compensation algorithms to remove the unwanted degradations of speech quality. Objective measures for quality assessment are required in order to evaluate such algorithms. They exist in two classes: reverberation-cancellation algorithms that equalize the room impulse response of the acoustic channel, and reverberation-suppression algorithms that remove the reverberant part of the speech signal by calculating the spectral weight for time-frequency coefficients. This paper focuses on evaluating the former. Algorithms are analyzed regarding their capability to assess reverberation, coloration, spectral distortion, perceived distance, and overall quality of the signals. An evaluation of the sound quality of the dereverberated signals is derived from subjective listening tests and then compared to objective measures.","2014","2023-07-12 06:41:56","2023-07-19 04:02:32","","386–399","","6","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GI28658I","journalArticle","2017","Rasumow, Eugen; Blau, Matthias; Doclo, Simon; Par, Steven van de; Hansen, Martin; Püschel, Dirk; Mellert, Volker","Perceptual Evaluation of Individualized Binaural Reproduction Using a Virtual Artificial Head","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18778","In binaural recordings, spatial information can be captured by using an artificial head that emulates a real human head having average anthropometric geometries and with ear microphones. Because artificial heads are generic without the individual characteristics of the actual listener, recordings often produce perceptual deficiencies such as front-back confusion and internalized source images. Alternatively, individually measured head-related transfer functions (HRTFs) can be approximately synthesized using a microphone array in conjunction with a filter-and-sum beamformer, called a virtual artificial head (VAH). This approach allows for the possibility of adapting a recording to an individual’s HRTF in the recording studio by an appropriate modification of the directivity pattern of the VAH. In this study, binaural reproductions using the VAH, two traditional artificial heads, and individual HRTFs were perceptually evaluated in the horizontal plane with respect to the original free-field presentation. The results show that individual HRTFs in conjunction with individually equalized headphone transfer function result in the best subjective appraisals. The ratings obtained for the VAH-setup indicate a high level of acceptance among the subjects. Mean ratings were often good to excellent.","2017","2023-07-12 06:42:00","2023-07-19 04:40:19","","448–459","","6","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M78RJSNM","journalArticle","2009","Martin, Aengus; Jin, Craig; Schaik, André van","Psychoacoustic Evaluation of Systems for Delivering Spatialized Augmented-Reality Audio","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15234","Spatialized augmented-reality audio systems superimpose virtual sound sources onto sounds from a real acoustic environment. Applications include assistance for visually impaired users, guidance systems, teleconferencing, attention-focusing systems, and electronic games. In the proposed system, acoustically transparent earpieces are used to allow environmental sounds to be heard directly while the virtual reality is created electronically. The results show that the listener can localize virtual sound sources as accurately as when using earphones, which are standard for virtual auditory space presentation, while there is only minor degradation in his ability to localize real sound sources.","2009","2023-07-12 06:42:03","2023-07-19 04:28:29","","1016–1027","","12","57","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NS7P272N","journalArticle","2013","Lee, Hyunkook; Rumsey, Francis","Level and Time Panning of Phantom Images for Musical Sources","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17075","The localization behaviors of panning based on interchannel level difference (ICLD) and interchannel time difference (ICTD) at different target image positions were investigated using musical sources with different spectral and temporal characteristics as well as a wideband speech source. The results indicate that a level panning can perform robustly regardless of the spectral and temporal characteristics of source signals, whereas time panning is not suitable for a continuous source with a high fundamental frequency. Statistical differences between the data obtained for different sources were found to be insignificant, and a unified set of ICLD and ICTD values for 10°, 20°, and 30° image positions was derived. Linear level and time panning functions for the two separate panning regions of 0°–20° and 21°–30° are further proposed, and their applicability to arbitrary loudspeaker base angle is also considered. These perceptual panning functions are expected to be more accurate than the theoretical sine or tangent law in terms of matching between predicted and actually perceived image positions.","2013","2023-07-12 06:42:07","2023-07-19 04:19:40","","978–988","","12","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R8P689L8","journalArticle","2023","Meyer-Kahlen, Nils; Kastemaa, Miranda; Schlecht, Sebastian J.; Lokki, Tapio","Measuring Motion-to-Sound Latency in Virtual Acoustic Rendering Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22145","","2023","2023-07-12 06:42:10","2023-07-12 06:42:10","","390–398","","6","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B7N3XC9A","journalArticle","2015","Kendrick, Paul; Li, Francis; Fazenda, Bruno; Jackson, Iain; Cox, Trevor","Perceived Audio Quality of Sounds Degraded by Nonlinear Distortions and Single-Ended Assessment Using HASQI","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17873","For field recordings and user-generated content recorded on phones, tablets, and other mobile devices, poor audio quality arises in part from nonlinear distortions caused by clipping and limiting at pre-amplification stages and by dynamic range control. Based on the Hearing Aid Sound Quality Index (HASQI), a single-ended method to quantify perceived audio quality in the presence of nonlinear distortions has been developed. Validations on music and soundscapes yielded single-ended estimates within ±0.19 of HASQI on a quality range from 0.0 and 1.0. Perceptual tests were carried out to validate the method for music and soundscapes. HASQI has also been shown to predict quality degradations for processes other than nonlinear distortions including additive noise, linear filtering, and spectral changes. By including these other causes of quality degradations, the current model for nonlinear distortion assessment could be expanded.","2015","2023-07-12 06:42:13","2023-07-19 04:10:32","","698–712","","9","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R4ICTTXX","journalArticle","2018","Eichas, Felix; Zölzer, Udo","Gray-Box Modeling of Guitar Amplifiers","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19874","Musical distortion circuits, especially guitar amplifiers, have been the subject of virtual analog modeling for years. There exists two main modeling approaches: white-box modeling, where the internal properties are fully known, and gray-box modeling, where only the input and output are available. This work proposes a gray-box modeling approach for analog guitar amplifiers using iterative optimization to adjust the parameters of a block-based model. The only assumption made about the reference system is its basic structure. The digital model is an extended Wiener–Hammerstein model consisting of a linear time-invariant (LTI) block, a nonlinear block with a nonlinear mapping function, and another LTI block connected in series. The model is adapted in two steps: first the filters are measured, and then the parameters for the nonlinear part of the digital model are optimized with the Levenberg–Marquardt method to minimize a cost-function describing the error between the digital model and the analog reference system. A small number of guitar amplifiers were modeled, the adapted model was evaluated with objective scores, and a listening test was performed to rate its quality.","2018","2023-07-12 06:42:25","2023-07-19 03:54:19","","1006–1015","","12","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SJTTYEA4","journalArticle","1980","Johnson, George Philip","New Audio Transformations in Acoustical Transduction Based on Optical Principles","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=3997","The historical development of mathematical transformations in audio systems analysis is examined as the changes in sound recording practice and the consequent insight into the operation of the ears are described. The progress in the formal modeling of the audio transformations which the ears seem to perform on sound appears to have reached a theoretical limit in information which may be derived from a set of single point pressure intercepts. Principles of optical interferometry are examined broadly as a possible basis for new acoustical intercepts which will provide more information about acoustic phenomena for audio analysis. New audio transformations developed from experience with acoustical transducers operating onoptical principles may lead to important discoveries about the operation of the ear, which in turn may improve the audio illusion well beyond present standards.","1980","2023-07-12 06:42:28","2023-07-19 04:09:00","","140–149","","3","28","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8V8ZLT69","journalArticle","2020","Howie, Will; Martin, Denis; Kim, Sungyoung; Kamekawa, Toru; King, Richard","Effect of Skill Level on Listener Performance in 3D Audio Evaluation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20894","","2020","2023-07-12 06:42:31","2023-07-12 06:42:31","","628–637","","9","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I9M6JPF8","journalArticle","1999","Savioja, Lauri; Huopaniemi, Jyri; Lokki, Tapio; Väänänen, Ritta","Creating Interactive Virtual Acoustic Environments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12095","","1999","2023-07-12 06:42:34","2023-07-12 06:42:34","","675–705","","9","47","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VYRVKACS","journalArticle","2016","Cecchi, Stefania; Virgulti, Marco; Primavera, Andrea; Piazza, Francesco; Bettarelli, Ferruccio; Li, Junfeng","Investigation on Audio Algorithms Architecture for Stereo Portable Devices","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18106","Due to the size constraints of portable stereo listening devices, their small and closely spaced loudspeakers lead to a poor spatial sound image. An audio algorithms architecture is proposed to compensate for the weak sound image. The system is composed of a spatializer based on an improved version of the recursive ambiophonics crosstalk elimination algorithm, integrated with a combined quasi-anechoic equalization approach, and a virtual bass algorithm capable of enhancing the loudspeakers performance. The proposed solution has been successfully implemented on both Android and iOS operating systems. Listening tests show positive results. Although the spatializer is capable of enhancing the spatial impression, the main audio quality suffers, which therefore requires a compensating equalizer and virtual bass algorithm.","2016","2023-07-12 06:42:37","2023-07-19 03:47:25","","75–88","","1/2","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2NMCTDB6","journalArticle","2015","Conetta, Robert; Brookes, Tim; Rumsey, Francis; Zielinski, Slawomir; Dewhirst, Martin; Jackson, Philip; Bech, Søren; Meares, David; George, Sunish","Spatial Audio Quality Perception (Part 1): Impact of Commonly Encountered Processes","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17557","","2015","2023-07-12 06:42:47","2023-07-12 06:42:47","","831–846","","12","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3R6V7AJP","journalArticle","2015","Zhou, Tingting; Zhang, Ming; Li, Chen","A Model for Calculating Psychoacoustical Fluctuation Strength","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17874","When sound is modulated at frequencies up to 20 Hz, the sensation is that of fluctuation strength. At faster amplitude variations, the sensation is that of roughness. This report describes a new model for calculating fluctuation strength based on equivalent rectangular bandwidth. By using 75 filter channels on the ERB number scale, the total fluctuation strength is calculated by weighting, filtering, and adding the generalized modulation depth (GMD) in each channel. The model changes the way that GMD is converted into specific fluctuation strength. Using an ERB number scale instead of Bark scale provides other advantages. The calculated results using the new model are more consistent with subjective ratings with an RMS error that is decreased by 90% and correlation coefficients are increased by 20%. The proposed model can be used for both narrow and wideband noises.","2015","2023-07-12 06:42:51","2023-07-19 10:59:39","","713–724","","9","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PFEAL9FA","journalArticle","2011","George, Sunish; Zielinski, Slawomir; Rumsey, Francis; Jackson, Philip; Conetta, Robert; Dewhirst, Martin; Meares, David; Bech, Søren","Development and Validation of an Unintrusive Model for Predicting the Sensation of Envelopment Arising from Surround Sound Recordings","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15743","","2011","2023-07-12 06:42:55","2023-07-12 06:42:55","","1013–1031","","12","58","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HY6498GG","journalArticle","2015","Ntalampiras, Stavros","Audio Pattern Recognition of Baby Crying Sound Events","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17641","Infants can communicate their internal state (such as pain, hunger, fear, fatigue, or stress) by the nature of their crying. Experts in linguistics suggest that the cry comprises the first speech manifestations. This article describes the design methodology for classifying baby crying sound events according to the pathological status of the infant. Such an automated system can be an aid to an attending physician performing a diagnosis. In order to address this challenge, a great variety of audio parameters (Perceptual Linear Prediction, Mel Frequency Cepstral Coefficients, Perceptual Wavelet Packets, Teager Energy Operator, Temporal Modulation) were considered. Classification techniques, including Multilayer Perception, Support Vector Machine, Random Forest, Reservoir Network, Gaussian Mixture model, and Hidden Markov model were customized. The goal is to provide an automatic and noninvasive framework for monitoring infants and helping inexperienced/trainee pediatricians, parents, and baby caregivers to identify the baby’s pathological status.","2015","2023-07-12 06:42:58","2023-07-19 04:35:15","","358–369","","5","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YCG4YPTB","journalArticle","2020","McCormack, Leo; Pulkki, Ville; Politis, Archontis; Scheuregger, Oliver; Marschall, Marton","Higher-Order Spatial Impulse Response Rendering: Investigating the Perceived Effects of Spherical Order, Dedicated Diffuse Rendering, and Frequency Resolution","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20852","","2020","2023-07-12 06:43:01","2023-07-12 06:43:01","","338–354","","5","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BYLE2SRN","journalArticle","2019","Silzle, Andreas; Schmidt, Rebekka; Bleisteiner, Werner; Epain, Nicolas; Ragot, Martin","Quality of Experience Tests of an Object-based Radio Reproduction App on a Mobile Device","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20495","Object-based audio (OBA) provides many enhancements and new features; yet, many of these require the user to be active in choosing and selecting the functionalities in visual representations and graphical interfaces. Basic investigations of the user experience of OBA within the EU research project OPRHEUS helped to identify the necessary criteria and dimensions. The user experience in object-based media comprises three dimensions: audio, information, and usability experience. During the project, a radio app for mobile devices was designed, developed, and tested. It includes many of the end-user features available with OBA. A first Quality of Experience (QoE) test to evaluate the radio app was carried out at JOSEPHS, an open innovation lab located in Nuremberg, Germany. The second QoE test took place at b<>com’s user experience lab in Rennes, France. For both investigations, the main objective was to find out how users can access, interact, and appreciate the various new features of OBA. For the first test, two typical user and listening scenarios were simulated: mobile listening and at home. The general acceptance of the new features and functions that come along with OBA is very high. The usability is rated high. Further possibilities for improvements were provided by the test users. The very good perceived sound quality with surround sound over loudspeakers or binaural reproduction over headphones impressed the listeners most. The second test focused mainly on the approach of comparing and evaluating the features from acceptability to acceptance, or from expectations to fulfillment. In the second test, the most appreciated feature was to set fore-to-background balance. This feature was number two in the first test. The importance of speech intelligibility for Radio and TV is a known and well discussed issue. Now, with OBA and the Next Generation Audio (NGA) codec MPEG-H, solutions are at hand to address it.","2019","2023-07-12 06:43:04","2023-07-19 04:48:27","","568–583","","7/8","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RMVZ26EV","journalArticle","2015","Lee, Hyunkook","2D-to-3D Ambience Upmixing based on Perceptual Band Allocation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18044","","2015","2023-07-12 06:43:07","2023-07-12 06:43:07","","811–821","","10","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9NFI6AIA","journalArticle","2015","Sun, Shuyuan; Shen, Yong; Liu, Ziyun; Feng, Xuelei","The Effects of Recording and Playback Methods in Virtual Listening Tests","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17843","To better control confounding variables found in many listening test, researchers often use virtual listening tests, an accepted method in psychoacoustic research. A virtual listening test method can insure that listening conditions are the same for every test subject, but the choices of recording and playback methods now play an important role. In this report, the results of live and virtual listening tests with four different loudspeakers were compared. The analyses on listeners’ reliability, dispersion of data, preference rating, and Least Significant Difference (LSD) comparison results are presented. The experimental evidence showed the significant effects of recording and playback methods in the virtual listening tests. These results provide evidence that there are significant effects of recording and playback methods in virtual listening tests. A reliable and authentic evaluation can be obtained from virtual listening tests more easily and efficiently by the comprehensive consideration and selection of recording and playback methods.","2015","2023-07-12 06:43:10","2023-07-19 04:50:40","","570–582","","7/8","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EA3Z9QGH","journalArticle","2021","Rund, František; Vencovský, Václav; Semanský, Marek","An Evaluation of Click Detection Algorithms Against the Results of Listening Tests","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21123","This paper evaluates the ability of several algorithms to detect impulse distortions (clicks) in audio signals. The systems are evaluated against data from a listening test conducted using real audio signals provided by a vinyl manufacturer. Some of the signals contained clicks due to damage during the manufacturing process. An evaluation of click detection algorithms against listening test results focuses on the ability of the click-detection algorithms to detect perceptible clicks. The results presented in this paper show that an algorithm that employs a hearing model detected audible clicks with a lower false detection rate than the other algorithms in the test and that the wavelet transform–based algorithm with a dynamic threshold outperformed the other algorithms.","2021","2023-07-12 06:43:13","2023-07-19 04:45:50","","586–593","","7/8","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z676A7LX","journalArticle","2017","Adrian, Jens-Alrik; Gerkmann, Timo; van de Par, Steven; Bitzer, Joerg","Synthesis of Perceptually Plausible Multichannel Noise Signals Controlled by Real World Statistical Noise Properties","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19360","Algorithms in speech and audio applications are often evaluated under adverse conditions to evaluate their robustness against additive noise. This research describes a method to generate artificial but perceptually plausible acoustic disturbances, thereby providing a controlled and repeatable context for evaluating algorithms. This allows for control of such noise parameters as coloration, modulation, and amplitude distribution independently of each other, while also providing the means to define the amount of coherence among all the signal channels. Results of a listening test in a monaural setup show no significant difference in naturalness between synthesized and original signal. It is not always obvious how to create natural noise. For example, it was observed that white Gaussian noise is often an inappropriate noise. Frequency-dependent modulations on a short time scale appear to contribute to naturalness. Synthesizing vinyl/shellac, which has a particular type of impulse character, requires a unique approach to synthesis. Rain and applause synthesis proved to be challenging.","2017","2023-07-12 06:43:16","2023-07-19 03:34:05","","914–928","","11","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N3PF63AK","journalArticle","1993","Kleiner, Mendel; Dalenbäck, Bengt-Inge; Svensson, Peter","Auralization-An Overview","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=6976","Auralization is a term introduced to be used in analogy with visualization to describe rendering audible (imaginary) sound fields. Several modeling methods are available in architectural acoustics for this purpose. If auralization is done by computer modeling, it can be thought of as ""true"" acoustical computer-aided design. Together with new hardware implementation of signal processing routines, auralization forms the basis of a powerful new technoogy for room simulation and aural event generation. The history, trends, problems, and possibilities of auralization are described. The discussion primarily deals with auralization of auditorium acoustics and loudspeaker installations. The advantages and disadvantages of various approaches are discussed, as are possible testing and verification techniques. The possibility of using acoustic scale models for auralization is also discussed.: Demonstrations of auralizations have been made, but still the technology's ability to reproduce the subjective impression of the audible characteristics of a hall accurately remains to be verified. This limits the credibility of auralization as a design tool, and verification of auralization the foremost problem to be attacked at this time. The verification problem also applies to the basic room impulse response prediction programs. The combination of auralization with transaural reproduction, room equalization, and active noise control could make it possible to expand the applications of the technology beyond the laboratory and beyond simple headphone reproduction. A large number of interesting applications outside the room and psychoacoustics reearch are conceivable, the most interesting of which are probably its use in information, education, and entertainment.","1993","2023-07-12 06:43:19","2023-07-19 04:15:21","","861–875","","11","41","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WYC4VZGL","journalArticle","2021","Arend, Johannes M.; Brinkmann, Fabian; Pörschmann, Christoph","Assessing Spherical Harmonics Interpolation of Time-Aligned Head-Related Transfer Functions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21019","","2021","2023-07-12 06:43:23","2023-07-12 06:43:23","","104–117","","1/2","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6GD7ANHH","journalArticle","2020","Malecki, Pawel; Sochaczewska, Katarzyna; Wiciak, Andjerzy","Settings of Reverb Processors from the Perspective of Room Acoustics","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20735","The issues of reverberation in acoustic architecture and music production share the same theoretical core; nevertheless the first one has been scientifically researched in depth while the second one remains at technical and experimental crossroads. It could be stated that the ISO 3382 parameters were proposed in the “analog” era for room acoustics (actual halls) whereasthe “digital” parameters introduced in software, artificial reverbs, are not standardized in any way but help to create desired reverberation for music or audio effects. The interest herein is to bind these two disciplines together and analyze some of the significant descriptors of room acoustics (RT, C50, C80, BR, ER, CT) applied in plug-in reverberant processors to observe how the virtual space is affected by changing the values of different parameters. Psychoacoustic ranges of JND were applied to conclude their relevance (or rather influence) and whether it is possible to perceive alteration. Five of the selected popular and commercial VST reverbs are juxtaposed with five similar settings and the results of analysis might be useful for sound mixers and automated mixing algorithms.","2020","2023-07-12 06:43:26","2023-07-19 04:26:42","","292–301","","4","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6QFTBBSP","journalArticle","2019","Franck, Andreas; Francombe, Jon; Woodcock, James; Hughes, Richard; Coleman, Philip; Menzies, Dylan; Cox, Trevor J.; Jackson, Philip J.B.; Fazi, Filippo Maria","A System Architecture for Semantically Informed Rendering of Object-Based Audio","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20488","Object-based audio promises format-agnostic reproduction and extensive personalization of spatial audio content. However, in practical listening scenarios, such as in consumer audio, ideal reproduction is typically not possible. To maximize the quality of listening experience, a different approach is required, for example modifications of metadata to adjust for the reproduction layout or personalization choices. This paper proposes a novel system architecture for semantically informed rendering (SIR), that combines object audio rendering with high-level processing of object metadata. In many cases, this processing uses novel, advanced metadata describing the objects to optimally adjust the audio scene to the reproduction system or listener preferences. The proposed system is evaluated with several adaptation strategies, including semantically motivated downmix to layouts with few loudspeakers, manipulation of perceptual attributes, perceptual reverberation compensation, and orchestration of mobile devices for immersive reproduction. These examples demonstrate how SIR can significantly improve the media experience and provide advanced personalization controls, for example by maintaining smooth object trajectories on systems with few loudspeakers, or providing personalized envelopment levels. An example implementation of the proposed system architecture is described and provided as an open, extensible software framework that combines object-based audio rendering and high-level processing of advanced object metadata.","2019","2023-07-12 06:43:29","2023-07-19 03:58:28","","498–509","","7/8","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L77P5252","journalArticle","2015","Hendrickx, Etienne; Paquier, Mathieu; Koehl, Vincent","Audiovisual Spatial Coherence for 2D and Stereoscopic-3D Movies","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18049","","2015","2023-07-12 06:43:32","2023-07-12 06:43:32","","889–899","","11","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VE7AB7SI","journalArticle","2021","Lee, Hyunkook; Johnson, Dale","3D Microphone Array Comparison: Objective Measurements","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21536","This paper describes a set of objective measurements carried out to compare various types of 3D microphone arrays, comprising OCT-3D, PCMA-3D, 2L-Cube, Decca Cuboid, Eigenmike EM32 (i.e., spherical microphone system), and Hamasaki Square with 0-m and 1-m vertical spacings of the height layer. Objective parameters that were measured comprised interchannel and spectral differences caused by interchannel crosstalk (ICXT), fluctuations of interaural level and time differences (ILD and ITD), interchannel correlation coefficient (ICC), interaural cross-correlation coefficient (IACC), and direct-to-reverberant energy ratio (DRR). These were chosen as potential predictors for perceived differences among the arrays. The measurements of the properties of ICXT and the time-varying ILD and ITD suggest that the arrays would produce substantial perceived differences in tonal quality as well as locatedness. The analyses of ICCs and IACCs indicate that perceived differences among the arrays in spatial impression would be larger horizontally rather than vertically. It is also predicted that the addition of the height channel signals to the base channel ones in reproduction would produce little effect on both source-image spread and listener envelopment, regardless of the array type. Finally, differences between the ear-input signals in DRR were substantially smaller than those observed among microphone signals.","2021","2023-07-12 06:43:35","2023-07-19 04:19:31","","871–887","","11","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UDNABYVY","journalArticle","2006","Neher, Tobias; Brookes, Tim; Rumsey, Francis","A Hybrid Technique for Validating Unidimensionality of Perceived Variation in a Spatial Auditory Stimulus Set","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13675","","2006","2023-07-12 06:43:38","2023-07-12 06:43:38","","259–275","","4","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9VMFDBPT","journalArticle","2008","Beracoechea, J. A.; Torres-Guijarro, Soledad; García, L.; Casajús-Quirós, Francisco J.; Ortiz, L.","Subjective Intelligibility Evaluation in Multiple-Talker Situation for Virtual Acoustic Opening-Based Audio Environments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14388","A virtual conferencing system, which couples two separated rooms as if they were a single virtual space connected by an open window, can be implemented in a variety of ways. This study examines the performance of four approaches using the metric of intelligibility with multiple simultaneous talks as the criterion. The problem is constrained by the amount of computation required, limitations on channel capacity, reverberation in the source space, and the difficulty using beamforming to isolate talkers and reduce acoustics contamination. Many improvements were not linear once a threshold level of performance was achieved.","2008","2023-07-12 06:43:41","2023-07-19 03:41:40","","339–356","","5","56","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7L7VITD7","journalArticle","2019","Ma, Xiaohui; Hohnerlein, Christoph; Ahrens, Jens","Concept and Perceptual Validation of Listener-Position Adaptive Superdirective Crosstalk Cancellation Using a Linear Loudspeaker Array","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20701","","2019","2023-07-12 06:43:47","2023-07-12 06:43:47","","871–881","","11","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"25F7MM6U","journalArticle","1999","Algazi, V. Ralph; Avendano, Carlos; Thompson, Dennis","Dependence of Subject and Measurement Position in Binaural Signal Acquisition","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12085","","1999","2023-07-12 06:43:50","2023-07-12 06:43:50","","937–947","","11","47","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UC74E6BM","journalArticle","2013","Berg, Jan; Bustad, Christofer; Jonsson, Lars; Mossberg, Lars; Nyberg, Dan","Perceived Audio Quality of Realistic FM and DAB+ Radio Broadcasting Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16969","The perceived audio quality of a digital broadcasting system (such as DAB+) is dependent on the type of coding and bit rates selected. Because of bandwidth constraints, the required number of channels, and conflicting auxiliary services, audio quality is sometimes degraded. In designing a broadcast system, it is necessary to have well-defined criteria for minimally acceptable quality. Two studies explored quality criteria and how quality degrades for various bit rates. For DAB+ the subchannel rate should not be less than the currently available maximum of 192 kbits/s for a stereo signal, which would be comparable to the quality of a modern FM system. Rates below 160 kbit/s can significantly degrade certain types of program material. To be truly perceptually transparent, bits rates of close to 300 kbits/s may be needed when using the current generation of coders.","2013","2023-07-12 06:43:53","2023-07-19 03:41:48","","755–777","","10","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AGGZ2WGC","journalArticle","2012","Herre, Jürgen; Falch, Cornelia; Mahane, Dirk; Del Galdo, Giovanni; Kallinger, Markus; Thiergart, Oliver","Interactive Teleconferencing Combining Spatial Audio Object Coding and DirAC Technology","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16155","In teleconferencing applications with multiple participants, it would be desirable to create the perception of a real meeting with each speaker in a specific perceived location. This paper describes how Directional Audio Coding (DirAC) can be used as a front end to MPEG Spatial Audio Object Coding (SAOC) to achieve that goal. A novel parameter transcoder provides an efficient way of combining the two technologies. Reproduction of the virtual environment, where each talker exists in a virtual location, can be achieved using headphones or loudspeakers. Subjective tests on different simple and optimized versions were performed, many of which were rated as “good” on the MUSHRA scale.","2012","2023-07-12 06:43:56","2023-07-19 04:05:13","","924–935","","12","59","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QU4LUUVD","journalArticle","1982","Clark, David","High-Resolution Subjective Testing Using a Double-Blind Comparator","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=3839","","1982","2023-07-12 06:43:59","2023-07-12 06:43:59","","330–338","","5","30","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8UUT4IU3","journalArticle","2005","Hawksford, Malcolm J.","System Measurement and Identification Using Pseudorandom Filtered Noise and Music Sequences","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13411","","2005","2023-07-12 06:44:02","2023-07-12 06:44:02","","275–296","","4","53","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8FUY35DV","journalArticle","2015","Baykaner, Khan; Coleman, Philip; Mason, Russell; Jackson, Philip J. B.; Francombe, Jon; Olik, Marek; Bech, Søren","The Relationship Between Target Quality and Interference in Sound Zone","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17568","Sound zone systems aim to control sound fields in such a way that multiple listeners can enjoy different audio programs within the same room with minimal acoustic interference. Often, there is a trade-off between the acoustic contrast achieved between the zones and the fidelity of the reproduced audio program in the target zone. A listening test was conducted to obtain subjective measures of distraction, target quality, and overall quality of listening experience for ecologically valid programs within a sound zoning system. Sound zones were reproduced using acoustic contrast control, planarity control, and pressure matching applied to a circular loudspeaker array. The highest mean overall quality was a compromise between distraction and target quality. The results showed that the term “distraction” produced good agreement among listeners, and that listener ratings made using this term were a good measure of the perceived effect of the interferer.","2015","2023-07-12 06:44:05","2023-07-19 03:39:26","","78–89","","1/2","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UB4E7LSI","journalArticle","1999","Beerends, John G.; De Caluwe, Frank E.","The Influence of Video Quality on Perceived Audio Quality and Vice Versa","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12105","Three questions are being studied. Question one: 'To what extent is the perceived audio quality of an audiovisual stream influenced by the video quality?' Question two is the reverse: 'To what extent is the perceived video quality of an audiovisual stream influenced by the audio quality?' Finally: 'How do audio and video quality contribute to the overall perceived audiovisual quality?' The quality ranges from broadcast audio and video quality to standard videophone (telephone) quality. The main conclusion is that when subjects are asked to judge the audio quality of an audiovisual stimulus, the video quality will contribute significantly to the subjectively perceived audio quality. The effect is about 1.2 points on a nine-point quality scale. The reversed effect is much smaller, about 0.2 point. Furthermore a simple mapping from the audio and video quality to the overall audiovisual quality given, and it is shown that the video quality dominates the overall perceived audiovisual quality in nonconversational experiments.","1999","2023-07-12 06:44:09","2023-07-19 03:39:56","","355–362","","5","47","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H6FK7XUR","journalArticle","1969","Luce, David; Clark, Melville, Jr.","A Real-Time Multipartial Waveform Analyzer-Synthesizer","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=1567","A real-time analyzer-synthesizer is described that can process complex signals whose waveforms change with time to the limits of the uncertainty principle. Overlap integrals between the input signal and user-selected basis functions are automatically recorded on (during analysis) or read from (during synthesis) film. Special converters permit using trigonometric amplitudes and phases for analysis and synthesis. Frequency tracking with signal-controlled bandwidth may be used during analysis. Frequency tracking uses a coherence detector to distinguish signals to which a frequency may be ascribed from noise. Various constant fractional or constant absolute bandwidths may be selected. The synthesizer can work with user-provided graphs; any of these curves and those on the film may be used as the modulation function for any selected basis functions.","1969","2023-07-12 06:44:13","2023-07-19 04:25:08","","439–444","","4","17","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NUH8JBQS","journalArticle","2017","Francombe, Jon; Brookes, Tim; Mason, Russell","Evaluation of Spatial Audio Reproduction Methods (Part 1): Elicitation of Perceptual Differences","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18555","","2017","2023-07-12 06:44:38","2023-07-12 06:44:38","","198–211","","3","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CQ8TMGUA","journalArticle","2019","Paulus, Jouni; Torcoli, Matteo; Uhle, Christian; Herre, Jürgen; Disch, Sascha; Fuchs, Harald","Source Separation for Enabling Dialogue Enhancement in Object-based Broadcast with MPEG-H","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20489","Low intelligibility of narration or dialogue resulting from high background level is one of the most common complaints in broadcasting. Even when the intelligibility is not compromised, listeners may have personal preferences that differ from the mix being broadcast. Dialogue Enhancement (DE) enables the delivery of optimal dialogue mixing to each listener, be it in terms of intelligibility or for aesthetic preference. This makes DE one of the most promising applications of user interactivity enabled by object-based audio broadcasting, such as MPEG-H. This paper investigates the use of source separation methods to extract dialogue and background from the complex sound mixture for enabling object-based broadcasting when dialogue is not available from the production process, as for example, with legacy content. The presented source separation technology integrates several separation approaches with known limitations into a more powerful overall architecture. In addition, the paper evaluates the subjective benefit of DE using the Adjustment/Satisfaction Test in which the listeners made extensive use of the dialogue level personalization. The fact that the preferred dialogue level had a high variance among the listeners indicates the need for this functionality. Even when an imperfect separation result was used for enabling DE, the possibility for personalizing the dialogue level lead to increased listener satisfaction.","2019","2023-07-12 06:44:41","2023-07-19 04:37:20","","510–521","","7/8","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EKPVZ5QX","journalArticle","2023","Engel, Isaac; Daugintis, Rapolas; Vicente, Thibault; Hogg, Aidan O. T.; Pauwels, Johan; Tournier, Arnaud J.; Picinali, Lorenzo","The SONICOM HRTF Dataset","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22128","","2023","2023-07-12 06:44:45","2023-07-12 06:44:45","","241–253","","5","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"II5KAD63","journalArticle","2012","Huckvale, Mark; Hilkhuysen, Gaston","Performance-Based Measurement of Speech Quality with an Audio Proof-Reading Task","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16358","Speech communication systems need to be evaluated across a wide range of signal qualities. However, when signal quality is high, evaluations focus on listener acceptance rather than on task performance because conventional intelligibility tests reach ceiling values. This research considers whether measures of cognitive load could be used to measure the effect of signal quality on communication performance even when intelligibility is high. It is shown that differences in signal quality can affect the ability of listeners to detect transcription errors in an audio proof-reading task. The research also shows that a noise reduction system, which elsewhere has been said to improve listener acceptance, gives no improvement in terms of cognitive load on this task.","2012","2023-07-12 06:44:49","2023-07-19 04:07:20","","444–451","","6","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"84R4YG5B","journalArticle","2020","Geary, David; Torcoli, Matteo; Paulus, Jouni; Simon, Christian; Straninger, Davide; Travaglini, Alessandro; Shirley, Ben","Loudness Differences for Voice-Over-Voice Audio in TV and Streaming","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20995","Voice-over-Voice (VoV) is a common mixing practice observed in news reports and documentaries, where a foreground voice is mixed on top of a background voice, e.g., to translate an interview. This is achieved by ducking the background voice so that the foreground voice is more intelligible, while still allowing the listener to perceive the presence and tone of the background voice. Currently there is little published research on ducking practices for VoV or on technical details such as the Loudness Difference (LD) between foreground and background speech. This paper investigates the ducking practices of nine expert audio engineers and the preferred LDs of 13 non-expert listeners of ages 57 years and older. Results highlight a clear difference between the LDs used by the experts and those preferred by the non-expert listeners. Experts tended toward LDs of 11.5–17 LU, while non-experts preferred a range of 20–30 LU. Based on these results, a minimum LD of 20 LU is recommended for VoV. High inter-subject variance due to personal preference was observed. This variance makes a substantial case for the introduction of personalization in broadcast and streaming. The audiovisual material used for the tests is provided at https://www.audiolabs-erlangen.de/resources/2020-VoV-DB.","2020","2023-07-12 06:44:53","2023-07-19 04:01:47","","810–818","","11","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"43YTF639","journalArticle","2021","Kamaris, Gavriil; Zachos, Panagiotis; Mourjopoulos, John","Low Filter Order Digital Equalization for Mobile Device Earphones","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21035","This work examines the feasibility and acceptability of digitally correcting the response irregularities of typical earphones used for listening with cell phones and other mobile devices. A novel adaptive low filter order response equalization method is introduced since for these applications digital signal processing resources are limited; hence such filters are restricted in order. The method employs parallel infinite impulse response (IIR) filter sections with 5–9 pole pairs that match well a target frequency response known to be suitable for earphone listening. The tests with earphones of varying specifications and price indicate that these short filters can be effective for response equalization and that such processing improves listener preference.","2021","2023-07-12 06:44:56","2023-07-19 04:09:34","","297–308","","5","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QJ9NHQP3","journalArticle","1997","Pulkki, Ville","Virtual Sound Source Positioning Using Vector Base Amplitude Panning","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=7853","A vector-based reformulation of amplitude panning is derived, which leads to simple and computationally efficient equations for virtual sound source positioning. Using the method, vector base amplitude panning (VBAP), it is possible to create two- or three-dimensional sound fields where any number of loudspeakers can be placed arbitrarily. The method produces virtual sound sources that are as sharp as is possible with current loudspeaker configuration and amplotude panning methods. A digital tool that implements two- and three-dimensional VBAP with eight inputs and outputs has been realized.","1997","2023-07-12 06:45:00","2023-07-19 04:39:18","","456–466","","6","45","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SAYYZBER","journalArticle","2011","Oo, Nay; Gan, Woon-Seng; Hawksford, Malcolm O. J.","Perceptually-Motivated Objective Grading of Nonlinear Processing in Virtual-Bass Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16149","The RCA Victor DYNAGROOVE system is a comprehensive system of improvements in sound recording by means of disc records. All aspects of the process are taken into consideration, starting with the artist's conception of the music and ending with the reproduction of the sound in the listener's home.","2011","2023-07-12 06:45:03","2023-07-19 04:36:17","","804–824","","11","59","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LBIACGKJ","journalArticle","2016","Lee, Hyunkook","Perceptual Band Allocation (PBA) for the Rendering of Vertical Image Spread with a Vertical 2D Loudspeaker Array","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18534","Two subjective experiments were conducted to examine a new vertical image-rendering method called Perceptual Band Allocation (PBA), using octave bands of pink noise presented from main and height loudspeaker pairs. The PBA attempts to control the perceived degree of vertical image spread (VIS) by a flexible mapping between frequency band and loudspeaker layer based on the desired positioning of the band in the vertical plane. The first experiment measured the perceived vertical location of the phantom image of octave-band stimuli for the main and height loudspeaker layers individually. Results showed significant differences among the frequency bands in perceived image location. Based on the localization data from this experiment, six different PBA stimuli were created in such a way that each frequency band was mapped to either the main or height loudspeaker layer depending on the target degree of VIS. The second experiment conducted a listening test to grade the perceived magnitudes of VIS for the six stimuli. The results indicated that PBA could significantly increase the perceived magnitude of VIS compared to that of a sound presented only from the main layer. It was also found that the different PBA schemes produced various degrees of perceived VIS with statistically significant differences.","2016","2023-07-12 06:45:07","2023-07-19 04:18:34","","1003–1013","","12","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"46G5GRL3","journalArticle","1997","Olive, Sean E.; Schuck, Peter L.; Ryan, James G.; Sally, Sharon L.; Bonneville, Marc E.","The Detection Thresholds of Resonances at Low Frequencies","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=7868","New experimental data on the detection thresholds of low-frequency resonances and antiresonances are presented. Using an adaptive procedure known as the up-down transformed response (UDTR) rule, the 70.7% detection thresholds were measured for a single added resonance (peak and notch) for different Q values and center frequencies. The signals included pink noise and pulses auditioned through earphones. The results show that detection thresholds are affected in complicated ways by Q, center frequency, and signal type. This makes their detection difficult to predict using current hearing models and frequency response measurements.","1997","2023-07-12 06:45:11","2023-07-19 04:35:42","","116–128","","3","45","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GHVWNC93","journalArticle","1996","Välimäki, Vesa; Huopaniemi, Jyri; Karjalainen, Matti; Jánosy, Zoltán","Physical Modeling of Plucked String Instruments with Application to Real-Time Sound Synthesis","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=7902","An efficient approach for real-time synthesis of plucked string instruments using physical modeling and DSP techniques is presented. Results of model-based resynthesis are illustrated to demonstrate that high-quality synthetic sounds of several string instruments can be generated using the proposed modeling principles. Real-time implementation using a signal processor is described, and several aspects of controlling physical models of plucked string instruments are studied.","1996","2023-07-12 06:45:16","2023-07-19 04:54:38","","331–353","","5","44","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KL8SASIC","journalArticle","2015","Lee, Hyunkook; Gribben, Christopher","Effect of Vertical Microphone Layer Spacing for a 3D Microphone Array","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17560","Subjective listening tests were conducted to investigate how the spacing between main (lower) and height (upper) microphone layers in a 3D main microphone array affects perceived spatial impression and overall preference. It was generally found that layer spacing of 0.5 m, 1 m, and 1.5 m did not produce significant differences in either perceived spatial impression or preference. The 0-m layer had slightly higher ratings than the spaced layers in both spatial impression and preference, depending on the type of source. The four configurations were compared with trumpet, acoustic guitar, percussion quartet, and string quartet using a 9-channel loudspeaker setup. It is suggested that the perceived results were mainly associated with vertical interchannel crosstalk in the signals of each height layer and the magnitude and pattern of spectral change at the listener’s ear caused by each layer. Informal comments suggested that the main preference attributes were tonal quality, as well as spatial quality.","2015","2023-07-12 06:45:20","2023-07-19 04:19:22","","870–884","","12","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IM4PYATA","journalArticle","2010","Bispo, Bruno C.; Esquef, Paulo A. A.; Biscainho, Luiz W. P.; Lima, Amaro A. de; Freeland, Fabio P.; Jesus, Rafael A. de; Said, Amir; Lee, Bowon; Schafer, Ronald W.; Kalker, Ton","EW-PESQ: A Quality Assessment Method for Speech Signals Sampled at 48 kHz","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15252","","2010","2023-07-12 06:45:24","2023-07-12 06:45:24","","251–268","","4","58","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F7WWWY9P","journalArticle","2022","Andreopoulou, Areti; Katz, Brian F. G.","Perceptual Impact on Localization Quality Evaluations of Common Pre-Processing for Non-Individual Head-Related Transfer Functions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21738","This article investigates the impact of two commonly used Head-Related Transfer Function (HRTF) processing/modeling methods on the perceived spatial accuracy of binaural data by monitoring changes in user ratings of non-individualized HRTFs. The evaluated techniques are minimum-phase approximation and Infinite-Impulse Response (IIR) modeling. The study is based on the hypothesis that user-assessments should remain roughly unchanged, as long as the range of signal variations between processed and unprocessed (reference) HRTFs lies within ranges previously reported as perceptually insignificant. Objective assessments of the degree of spectral variations between reference and processed data, computed using the Spectral Distortion metric, showed no evident perceptually relevant variations in the minimum-phase data and spectral differences marginally exceeding the established thresholds for the IIR data, implying perceptual equivalence of spatial impression in the tested corpus. Nevertheless analysis of user responses in the perceptual study strongly indicated that variations introduced in the data by the tested methods of HRTF processing can lead to inversions in quality assessment, resulting in the perceptual rejection of HRTFs that were previously characterized in the ratings as the ""most appropriate"" or alternatively in the preference of datasets that were previously dismissed as ""unfit."" The effect appears more apparent for IIR processing and is equally evident across the evaluated horizontal and median planes.","2022","2023-07-12 06:45:28","2023-07-19 03:36:31","","340–354","","5","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"39977QYV","journalArticle","2011","Parodi, Yesenia Lacouture; Rubak, Per","A Subjective Evaluation of the Minimum Channel Separation for Reproducing Binaural Signals over Loudspeakers","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15974","","2011","2023-07-12 06:45:31","2023-07-12 06:45:31","","487–497","","7/8","59","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3ZJF9LZ5","journalArticle","2023","Brinkmann, Fabian; Kreuzer, Wolfgang; Thomsen, Jeffrey; Dombrovskis, Sergejs; Pollack, Katharina; Weinzierl, Stefan; Majdak, Piotr","Recent Advances in an Open Software for Numerical HRTF Calculation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22155","Mesh2HRTF 1.x is an open-source and fully scriptable end-to-end pipeline for the numerical calculation of head-related transfer functions (HRTFs). The calculations are based on 3D meshes of listener's body parts such as the head, pinna, and torso. The numerical core of Mesh2HRTF is written in C++ and employs the boundary-element method for solving the Helmholtz equation. It is accelerated by a multilevel fast multipole method and can easily be parallelized to further speed up the computations. The recently refactored framework of Mesh2HRTF 1.x contains tools for preparing the meshes as well as specific post-processing and inspection of the calculatedHRTFs. The resultingHRTFs are saved in the spatially oriented format for acoustics being directly applicable in virtual and augmented reality applications and psychoacoustic research. The Mesh2HRTF 1.x code is automatically tested to assure high quality and reliability. A comprehensive online documentation enables easy access for users without in-depth knowledge of acoustic simulations.","2023","2023-07-12 06:45:36","2023-07-19 03:45:17","","502–514","","7/8","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F3ZGMVTK","journalArticle","2017","Sena, Enzo De; Brookes, Mike; Naylor, Patrick A.; Waterschoot, Toon van","Localization Experiments with Reporting by Head Orientation: Statistical Framework and Case Study","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19364","This research focuses on sound localization experiments in which subjects report the position of an active sound source by turning toward it. A statistical framework for the analysis of the data is presented together with a case study from a large-scale listening experiment. The statistical framework is based on a model that is robust to the presence of front/back confusions and random errors. Closed-form natural estimators are derived, and one-sample and two-sample statistical tests are described. The framework is used to analyze the data of an auralized experiment undertaken by nearly nine hundred subjects. The objective was to explore localization performance in the horizontal plane in an informal setting and with little training, which are conditions that are similar to those typically encountered in consumer applications of binaural audio. Results show that responses had a rightward bias and that speech was harder to localize than percussion sounds, which are results consistent with the literature. Results also show that it was harder to localize sound in a simulated room with a high ceiling despite having a higher direct-to-reverberant ratio than other simulated rooms.","2017","2023-07-12 06:45:40","2023-07-19 04:47:22","","982–996","","12","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NDLL68A2","journalArticle","2020","Ackermann, David; Fiedler, Felicitas; Brinkmann, Fabian; Schneider, Martin; Weinzierl, Stefan","On the Acoustic Qualities of Dynamic Pseudo-Binaural Recordings","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20858","The motion-tracked binaural (MTB) technique allows the dynamic, pseudobinaural rendering of spatial sound scenes recorded by a circular array of microphones on a rigid sphere. The system provides a multichannel live audio transmission from which a head-related signal with approximated interaural time and level differences can be derived and played via headphones, head tracking, and a corresponding rendering software. The latter is mainly calculating imperceptible interpolation between channel pairs during head movements. This contribution evaluates the potential of this format for the creation of virtual acoustic envi- ronments. Based on the technical realization of a 16-channel MTB array with omnidirectional diffuse field-corrected electret condenser microphone capsules, the plausibility of 8 and 16-channel recordings was tested against a physical sound source. Furthermore, the sound quality of the pseudobinaural rendering was assessed based on different items of the Spatial Audio Quality Inventory (SAQI) compared to a true dynamic binaural reference. The results show that the overall plausibility of the MTB signal with optimal interpolation is close to the reference. Even if there are small differences with respect to tone color and spatial sound source attributes, the degree of externalization and even the perceived source elevation were, despite the absence of pinna cues, well comparable to the true binaural reference.","2020","2023-07-12 06:45:43","2023-07-19 03:33:47","","418–427","","6","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8EPB8NI7","journalArticle","2013","Vilkamo, Juha; Pulkki, Ville","Minimization of Decorrelator Artifacts in Directional Audio Coding by Covariance Domain Rendering","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16932","Directional Audio Coding (DirAC) is a perceptually motivated microphone technique that models the sound field as a combination of a plane wave and a surrounding diffuse field with a time–frequency resolution that approximates that of the human spatial hearing. In this paper a recently proposed covariance domain spatial-sound rendering method was applied to optimize the DirAC reproduction by minimizing the amount of the decorrelated sound energy. When several semi-independent microphone signals were available, this procedure was shown to improve the overall perceived sound quality, especially with audio content that has an impulsive fine structure, such as applause and speech. In all tests, the covariance rendering method performed similarly or better than the legacy rendering method, making it the preferred choice for performing DirAC synthesis.","2013","2023-07-12 06:45:47","2023-07-19 10:53:00","","637–646","","9","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"79BDFFGX","journalArticle","2022","Ragano, Alessandro; Benetos, Emmanouil; Hines, Andrew","Automatic Quality Assessment of Digitized and Restored Sound Archives","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21562","","2022","2023-07-12 06:45:51","2023-07-12 06:45:51","","252–270","","4","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DDVEB2QW","journalArticle","2012","Vilkamo, Juha; Neugebauer, Bernhard; Plogsties, Jan","Sparse Frequency-Domain Reverberator","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16156","There are many topologies for implementing the late part of artificial reverberation, each of which is a unique trade-off among perceived quality, computational cost, and parameter flexibility. A new approach, which assumes that the late reverberation can be described as a stochastic process, applies the reverberation in frequency bands using a combination of a feedback loop and an efficient sparse decorrelator. Listening tests with ten experts confirmed that the new reverberator topology has a perceived quality that was mostly equivalent to the idealized reverberation using decaying Gaussian noise response in frequency bands.","2012","2023-07-12 06:45:57","2023-07-19 10:52:53","","936–943","","12","59","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IE4YCV6L","journalArticle","1998","Lipshitz, Stanley P.","Dawn of the Digital Age","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12175","This paper attempts to give the reader a concise survey of the history of the development of digital audio technology in the Audio Engineering Society, as seen through its publications.","1998","2023-07-12 06:46:08","2023-07-19 04:20:44","","37–42","","1/2","46","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VA9Z5WNC","journalArticle","1971","Eargle, John M.","Multichannel Stereo Matrix Systems: An Overview","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=2155","In increasing profusion, -4-2-4- matrix playback systems are being introduced to an already confused market place. All of these systems fall short of the -four-channel- performance claimed for them, but many of them are surprisingly accurate in their recreation of auditory perspective around the compass. This paper will examine the basic properties of 4-2-4 matrices along with some of the embellishments which have been used to improve their performance. Finally, higher order matrix systems will be examined briefly.","1971","2023-07-12 06:46:12","2023-07-19 03:54:10","","552–559","","7","19","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LTK8YLYK","journalArticle","2021","Bernier, Antoine; Bouserhal, Rachel E.; Voix, Jérémie; Herzog, Philippe","Design and Assessment of an Active Musician's Hearing Protection Device With Occlusion Effect Reduction","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21459","","2021","2023-07-12 06:46:15","2023-07-12 06:46:15","","618–631","","9","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JE2KPKLN","journalArticle","2003","Moore, Brian C. J.; Glasberg, Brian R.; Stone, Michael A.","Why Are Commercials so Loud? ' Perception and Modeling of the Loudness of Amplitude-Compressed Speech*","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12190","","2003","2023-07-12 06:46:19","2023-07-12 06:46:19","","1123–1132","","12","51","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VHUS6GPL","journalArticle","1996","Craven, Peter G.; Gerzon, Michael A.","Lossless Coding for Audio Discs","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=7887","Strategies are presented that are being used to achieve efficient lossless compression or packing of PCM audio waveform data, allowing storage and transmission at greatly reduced data rates without any alteration of the signal. The disadvantages of simpler difference schemes and the benefits of more advanced schemes using IIR prediction with Huffman coding are explained and described, particularly with regard to the unique requirements of the future high-quality audio disc (HQAD) standard using high-density CD media.","1996","2023-07-12 06:46:23","2023-07-19 03:50:28","","706–720","","9","44","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"289GYVU3","journalArticle","2013","Neuendorf, Max; Multrus, Markus; Rettelbach, Nikolaus; Fuchs, Guillaume; Robilliard, Julien; Lecomte, Jérémie; Wilde, Stephan; Bayer, Stefan; Disch, Sascha; Helmrich, Christian; Lefebvre, Roch; Gournay, Philippe; Bessette, Bruno; Lapierre, Jimmy; Kjörling, Kristofer; Purnhagen, Heiko; Villemoes, Lars; Oomen, Werner; Schuijers, Erik; Kikuiri, Kei; Chinen, Toru; Norimatsu, Takeshi; Chong, Kok Seng; Oh, Eunmi; Kim, Miyoung; Quackenbush, Schuyler; Grill, Bernhard","The ISO/MPEG Unified Speech and Audio Coding Standard—Consistent High Quality for All Content Types and at All Bit Rates","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17074","With the advent of devices that unite a multitude of functionalities, the industry has an increased demand for an audio codec that can deal equally well with all types of audio content. In early 2012 the ISO/IEC JTC1/SC29/WG11 (MPEG) finalized the new MPEG-D Unified Speech and Audio Coding standard, bringing together the previously separated worlds of general audio and speech coding. It does so by integrating elements from audio coding and speech coding into a unified system over a wide range of bit rates. The present publication outlines all aspects of this standardization effort, starting with the history and motivation of the MPEG work. Technical features of the final system are described. Listening test results and performance numbers show the advantages of the new system over current state-of-the-art codecs.","2013","2023-07-12 06:46:27","2023-07-19 04:34:49","","956–977","","12","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CQJTN66G","journalArticle","2000","Evans, Michael J.; Tew, Anthony I.; Angus, James A. S.","Perceived Performance of Loudspeaker-Spatialized Speech for Teleconferencing","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12051","Spatial audio reproduction using loudspeakers can enhance teleconferencing applications by increasing the perceptual realism or telepresence, of participants' interactions. However, it is critical that any additional perceptual realism afforded by spatial audio does not compromise the perceived performance of the telecommunications in terms of participants' understanding of the words being conveyed. Formal subjective testing methods are used to verify that the spatial reproduction of speech does not compromise perceived performance relative to nonspatialized reproduction. Indeed, significant improvements in the performance as perceived by listeners have been found. Additional tests identify apparent discrepancies between which method permits the most accurate localization and which has the highest perceived performance.","2000","2023-07-12 06:46:30","2023-07-19 03:55:02","","771–785","","9","48","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KW6P3YJE","journalArticle","2003","Smithers, Michael J.; Crockett, Brett G.; Fielder, Louis D.","Ultra-High Quality Video Frame Synchronous Audio Coding","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12196","Two methods of coding and delivering ultra-high-quality audio are presented. Both methods are video frame synchronous and editable at common video frame rates (23.98, 24, 25, 29.97, and 30 frames per second) without the use of sample-rate converters. The first is an ultra-high-quality audio coder that exceeds 4.8 on the ITU-R five-point audio impairment scale at a bit rate of 256 kbit/s per channel and at up to three generations of encoding/decoding. The second is an enhanced method of video frame synchronous PCM packing. Specifically the problem of transmitting 48-kHz audio in 29.97-Hz frames is examined.","2003","2023-07-12 06:46:35","2023-07-19 04:48:35","","1032–1045","","11","51","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L7BNKPW4","journalArticle","2020","Pörschmann, Christoph; Arend, Johannes M.","A Method for Spatial Upsampling of Voice Directivity by Directional Equalization","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20896","","2020","2023-07-12 06:46:39","2023-07-12 06:46:39","","649–663","","9","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MZFZP4QZ","journalArticle","2019","Stuart, J. Robert; Craven, Peter G.","The Gentle Art of Dithering","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20457","There is still disagreement over the ways in which sound quality might benefit from higher sampling rates or wider bit depths in a digital path. The authors show that if a digital pathway includes any unintended or undithered quantizations, then several types of errors will be created whose nature will change with increased sampling rate and word size. Although dither methods for ameliorating quantization error have been well understood in the literature for some time, these insights are not always applied in practice. It is rare for an audio performance to be captured, produced, and played back with a flawless chain. The paper includes a tutorial overview of digital sampling and quantization with additive, subtractive, and noise-shaped dither. The discussions also include more advanced topics, such as cascaded quantizers, fixed and floating-point arithmetic, and time-domain aspects of quantization errors. Guidelines and recommendations are presented, including for the design of listening tests.","2019","2023-07-12 06:46:43","2023-07-19 04:50:22","","278–299","","5","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EHGIUHTW","journalArticle","2015","Politis, Archontis; Laitinen, Mikko-Ville; Ahonen, Jukka; Pulkki, Ville","Parametric Spatial Audio Processing of Spaced Microphone Array Recordings for Multichannel Reproduction","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17580","There are two major categories of recording approaches for multichannel sound reproduction: coincident and spaced microphone arrays. Coincident techniques assume that the microphones are essentially at the same point in space and they provide stable localization of discrete sound events. In contrast, spaced techniques provide a better enveloping sense of the ambient reverberant sound, at the expense of localization performance. In this research a parametric processing method is presented for spaced microphone array recordings using the principles of Directional Audio Coding (DirAC), knowledge of the array configuration, and the directional response of the microphones. The method achieves equal or better quality relative to the standard high-quality version of DirAC and it improves the common direct playback of such recordings by offering improved and stable localization cues and reduction of coloration issues at all frequencies.","2015","2023-07-12 06:46:47","2023-07-19 04:38:36","","216–227","","4","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7CQCFBCM","journalArticle","2022","Majdak, Piotr; Zotter, Franz; Brinkmann, Fabian; De Muynke, Julien; Mihocic, Michael; Noisternig, Markus","Spatially Oriented Format for Acoustics 2.1: Introduction and Recent Advances","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21824","Spatially oriented acoustic data can range from a simple set of impulse responses, such as head-related transfer functions, to a large set of multiple-input multiple-output spatial room impulse responses obtained in complex measurements with a microphone array excited by a loudspeaker array at various conditions. The spatially oriented format for acoustics (SOFA), which was standardized by AES Standard 69, provides a format to store and share such data. SOFA takes into account geometric representations of many acoustic scenarios, data compression, network transfer, and a link to complex room geometries and aims at simplifying the development of interfaces for many programming languages. With the recent advancement of SOFA, the format offers a new continuous-direction representation of data by means of spherical harmonics and novel conventions representing many measurement scenarios, such as source directivity and multiple-input multiple-output spatial room impulse responses. This article reviews SOFA by first providing an introduction to SOFA and then describing examples that demonstrate the most recent features of SOFA 2.1 (AES Standard 69-2022).","2022","2023-07-12 06:46:52","2023-07-19 04:26:15","","565–584","","7/8","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R3YXRY3A","journalArticle","2022","McCormack, Leo; Politis, Archontis; McKenzie, Thomas; Hold, Christoph; Pulkki, Ville","Object-Based Six-Degrees-of-Freedom Rendering of Sound Scenes Captured with Multiple Ambisonic Receivers","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21739","This article proposes a system for object-based six-degrees-of-freedom (6DoF) rendering of spatial sound scenes that are captured using a distributed arrangement of multiple Ambisonic receivers. The approach is based on first identifying and tracking the positions of sound sources within the scene, followed by the isolation of their signals through the use of beamformers. These sound objects are subsequently spatialized over the target playback setup, with respect to both the head orientation and position of the listener. The diffuse ambience of the scene is rendered separately by first spatially subtracting the source signals from the receivers located nearest to the listener position. The resultant residual Ambisonic signals are then spatialized, decorrelated, and summed together with suitable interpolation weights. The proposed system is evaluated through an in situ listening test conducted in 6DoF virtual reality,whereby real-world sound sources are compared with the auralization achieved through the proposed rendering method. The results of 15 participants suggest that in comparison to a linear interpolation-based alternative, the proposed object-based approach is perceived as being more realistic.","2022","2023-07-12 06:46:56","2023-07-19 04:29:19","","355–372","","5","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"69FH6G69","journalArticle","1994","Olive, Sean E.; Schuck, Peter L.; Sally, Sharon L.; Bonneville, Marc E.","The Effects of Loudspeaker Placement on Listener Preference Ratings","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=6930","Through the use of an acoustically adjustable listening room and a binaural recording and reproduction system, live and binaural subjective evaluations were made of different loudspeakers placed in different room locations. The experimental results from both tests show that listener preference ratings for different loudspeakers are significantly influenced by the loudspeaker location within the room. In fact, the positional effects can be larger than the subjective differences between the loudspeakers themselves. The binaural evaluations indicate that listener preferences are influenced significantly by interactions between the loudspeaker, its location, and the type of program material auditioned. These secondary effects were less significant in the live tests, suggesting that traditional real-time listening tests may be inadequate for measuring or controlling these effects.","1994","2023-07-12 06:47:00","2023-07-19 04:35:50","","651–669","","9","42","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"72PCYJPF","journalArticle","2017","Lee, Hyunkook","Sound Source and Loudspeaker Base Angle Dependency of Phantom Image Elevation Effect","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19203","Previous research showed that when identical noise signals are presented from two loudspeakers equidistant from the listener, the resulting phantom image is perceived as being elevated in the median plane. In this study, listening tests used eleven natural sources and four noise sources with different spectral and temporal characteristics reproduced with seven loudspeaker base angles between 0° and 360°. While the degree of perceived elevation depends on the base angle of the loudspeakers, the spectral and temporal characteristics of the sound source also play a significant role in determining perceived elevation. Results generally suggest that the effect is stronger for sources that have a transient nature and a flat frequency spectrum as compared to continuous and low-frequency sources. It is proposed that the perceived degree of elevation is determined by a relative cue related to the spectral energy distribution at high frequencies and also by an absolute cue associated with the acoustic crosstalk and torso reflections at low frequencies. A novel hypothesis about the role of acoustic crosstalk and torso reflection at low frequencies is explored. At frequencies below 3 kHz, the brain might use the first notch in the ear-input spectrum, which is produced by the combination of acoustic crosstalk and torso reflection, as a cue for localizing a phantom source at an elevated position in the median plane. These results may prove useful for 3D sound panning, recording, and mixing without elevated speakers.","2017","2023-07-12 06:47:03","2023-07-19 04:18:42","","733–748","","9","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QCDQ9EA6","journalArticle","2020","Poirier-Quinot, David; Katz, Brian F.G.","Assessing the Impact of Head-Related Transfer Function Individualization on Task Performance: Case of a Virtual Reality Shooter Game","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20731","","2020","2023-07-12 06:47:07","2023-07-12 06:47:07","","248–260","","4","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SHNLQ9CS","journalArticle","2011","Uhle, Christian","Applause Sound Detection","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15926","Detecting applause in both audio recordings and real-time performances is relevant in such applications as music information retrieval and spatial audio coding. A combination of mel-frequency cepstral coefficients and low-level descriptors yielded the best classification performance in the experiments. Low-pass filtering of the feature time series leads to the concept of sigma features. Binary misclassification occurs more often when applause and nonapplause with similar amplitudes are simultaneously present.","2011","2023-07-12 06:47:12","2023-07-19 04:54:30","","213–224","","4","59","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YBE98R5Y","journalArticle","2016","Delikaris-Manias, Symeon; Bolaños, Javier Gómez; Eskelinen, Joona; Huhtakallio, Ilkka; Hæggström, Edward; Pulkki, Ville","Auralization of Source Radiation Pattern Synthesized with Laser Spark Room Responses","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18514","This research describes a method to auralize the effect of a three-dimensional directional pattern of an acoustic source in a reverberant environment using real acoustic measurements of laser sparks. Laser-induced breakdown (LIB) produces a massless and point-like acoustic source. The authors demonstrate the performance of a volumetric array of LIBs for synthesizing arbitrary radiation patterns, auralize the radiation pattern of a loudspeaker and compare the measured and synthesized impulse responses in a reverberant room, and evaluate the method using listening tests. The synthesized room response matched the target response both in room response reconstruction and in listening tests. The proposed method requires no previous knowledge about the room characteristics; auralization is performed by convolving a sound signal with the synthesized room impulse responses using the cloud of laser sparks. The quality of the synthesized version was rated to be excellent.","2016","2023-07-12 06:47:15","2023-07-19 03:51:54","","720–730","","10","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KAT5JBWG","journalArticle","2008","Herre, Jürgen; Kjörling, Kristofer; Breebaart, Jeroen; Faller, Christof; Disch, Sascha; Purnhagen, Heiko; Koppens, Jeroen; Hilpert, Johannes; Rödén, Jonas; Oomen, Werner; Linzmeier, Karsten; Chong, Kok Seng","MPEG Surround-The ISO/MPEG Standard for Efficient and Compatible Multichannel Audio Coding","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14643","","2008","2023-07-12 06:47:19","2023-07-12 06:47:19","","932–955","","11","56","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2SLY6H63","journalArticle","2021","Choi, Jeonghwan; Chang, Joon-Hyuk","Exploiting Deep Neural Networks for Two-to-Five Channel Surround Decoder","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21008","We exploited deep neural networks (DNN) for two-to-five channel surround decoding. Specifically DNNs are used to replace the primary-ambient separation and ambient-signal-rendering modules. For the training, the mean-squared error of the magnitude spectra between the decoded and five-channel target signals and the interchannel level differences between the target signals were used as the loss functions. Through this procedure the DNNs can derive the spectral weights that can be used to produce the decoded signals, similar to that for the target signals. The log spectral distance, signal-to-distortion ratio, and multiple stimuli with hidden reference and anchor tests were used for objective and subjective evaluations. The experimental results show that exploiting the DNNs can generate decoded signals that are more similar to the target signals than those obtained via previous methods.","2021","2023-07-12 06:47:34","2023-07-19 03:48:09","","938–949","","12","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6BAG2VSD","journalArticle","2003","Tao, Yufei; Tew, Anthony I.; Porter, Stuart J.","The Differential Pressure Synthesis Method for Efficient Acoustic Pressure Estimation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12212","A differential pressure synthesis (DPS) method is proposed which estimates the free-field acoustic pressure on the boundary of an object from its geometry by precalculating a database of pressure changes caused by introducing orthogonal shape deformations to a template shape. Pressures are synthesized using DPS for a two-dimensional shape and a three-dimensional KEMAR head model. The accuracy of pressure estimates compares favorably with the boundary-element method computation provided that shape deformations are moderate in relation to acoustic wavelength.","2003","2023-07-12 06:47:38","2023-07-19 04:51:50","","647–656","","7/8","51","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I3BPYNJ3","journalArticle","2021","Georg, Götz; Schlecht, Sebastian J.; Ornelas, Abraham Martinez; Pulkki, Ville","Autonomous Robot Twin System for Room Acoustic Measurements","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21033","While room acoustic measurements can accurately capture the sound field of real rooms, they are usually time consuming and tedious if many positions need to be measured. Therefore this contribution presents the Autonomous Robot Twin System for Room Acoustic Measurements (ARTSRAM) to autonomously capture large sets of room impulse responses with variable sound source and receiver positions. The proposed implementation of the system consists of two robots, one of which is equipped with a loudspeaker, while the other one is equipped with a microphone array. Each robot contains collision sensors, thus enabling it to move autonomously within the room. The robots move according to a random walk procedure to ensure a big variability between measured positions. A tracking system provides position data matching the respective measurements. After outlining the robot system, this paper presents a validation, in which anechoic responses of the robots are presented and the movement paths resulting from the random walk procedure are investigated. Additionally the quality of the obtained room impulse responses is demonstrated with a sound field visualization. In summary, the evaluation of the robot system indicates that large sets of diverse and high-quality room impulse responses can be captured with the system in an automated way. Such large sets of measurements will benefit research in the fields of room acoustics and acoustic virtual reality.","2021","2023-07-12 06:47:41","2023-07-19 04:01:58","","261–272","","4","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7CQ6PHB5","journalArticle","2002","Appel, Ronald; Beerends, John G.","On the Quality of Hearing One's Own Voice","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=11084","The way we perceive our own voice is being studied. Contrary to classical speech listening-quality experiments, where subjects judge the speech quality by listening, remaining silent themselves, in talking-quality experiments subjects judge the quality with which they perceive their own voices while actively speaking. In this way the self-listening comfort is measured. Six experiments were carried out. Five used a standard telephone setup where echo and distortions were introduced and judged by subjects on the disturbance. In one experiment subjects judged the talking quality of six different rooms. The subjective results were used to develop an objective perceptual talking-quality measure. The overall correlation between the subjectively perceived quality and the objectively measured quality was 0.97.","2002","2023-07-12 06:47:44","2023-07-19 03:37:02","","237–248","","4","50","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ET5VSLNX","journalArticle","2009","Kulesza, Maciej; Czyzewski, Andrzej","Tonality Estimation and Frequency Tracking of Modulated Tonal Components","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14815","The separation of tonelike and noiselike components within an audio signal is important for many signal processing applications, such as codecs employing a psychoacoustic criterion. Detection of tonal components allows for the creation of tonal tracks, which have specific masking properties. Conventional tonality estimation procedures are not well suited for modulated components that are typically found in music with vibrato and tremolo. In the proposed method, the analysis simultaneously uses a tonality metric based on both historic frames and spectral bins. The results for various metrics are compared.","2009","2023-07-12 06:47:48","2023-07-19 04:16:26","","221–236","","4","57","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RXI8CXXB","journalArticle","1980","Davis, Don; Davis, Chips","The LEDE- Concept for the Control of Acoustic and Psychoacoustic Parameters in Recording Control Rooms","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=3965","An approach to the standardized control room is found in the ""live end-dead end"" (LEDE-) approach. Desirable features include control of the initial time delay, psychoacoustic removal of the directional clues belonging to the control room, and control of the early reflected sound field's density, spacing in time, and acoustic level. This results in an exceptionally neutral acoustic environment and allows development of a sound field at a mixer's ears which correlates remarkably with the sound field appearing at the microphones in the studio, thereby allowing precision judgments to be made at the mixing console.","1980","2023-07-12 06:47:52","2023-07-19 03:51:45","","585–595","","9","28","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7AXLRUY9","journalArticle","2007","Majdak, Piotr; Balazs, Peter; Laback, Bernhard","Multiple Exponential Sweep Method for Fast Measurement of Head-Related Transfer Functions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14190","Presenting sounds in virtual environments requires filtering free-field signals with head-related transfer functions (HRTF). HRTFs describe the filtering effects of pinna, head, and torso measured in the ear canal of a subject. The measurement of HRTFs for many positions in space is a time-consuming procedure. To speed up the HRTF measurement, the multiple exponential sweep method (MESM) was developed. MESM speeds up the measurement by overlapping sweeps in an optimized way and retrieves the impulse responses of the measured systems. MESM and its parameter optimization are described. As an example of an application of MESM, the measurement duration of an HRTF set with 1550 positions is compared to the unoptimized method. Using MESM, the measurement duration could be reduced by a factor of four without a reduction of the signal-to-noise ratio.","2007","2023-07-12 06:47:56","2023-07-19 04:26:00","","623–637","","7/8","55","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HJYI56C8","journalArticle","2020","Lübeck, Tim; Helmholz, Hannes; Arend, Johannes M.; Pörschmann, Christoph; Ahrens, Jens","Perceptual Evaluation of Mitigation Approaches of Impairments due to Spatial Undersampling in Binaural Rendering of Spherical Microphone Array Data","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20859","Spherical microphone arrays (SMAs) are widely used to capture spatial sound fields that can then be rendered in various ways as a virtual acoustic environment (VAE) including headphone-based binaural synthesis. Several practical limitations have a significant impact on the fidelity of the rendered VAE. The finite number of microphones of SMAs leads to spatial undersampling of the captured sound field, which, on the one hand, induces spatial aliasing artifacts and, on the other hand, limits the order of the spherical harmonics (SH) representation. Several approaches have been presented in the literature that aim to mitigate the perceptual impairments due to these limitations. In this article, we present a listening experiment evaluat- ing the perceptual improvements of binaural rendering of undersampled SMA data that can be achieved using state-of-the-art mitigation approaches. We examined the Magnitude Least-Squares algorithm, the Bandwidth Extraction Algorithm for Mi- crophone Arrays, Spherical Head Filters, SH Tapering, and a newly proposed equalization filter. In the experiment, subjects rated the perceived differences between a dummy head and the corresponding SMA auralization. We found that most mitiga- tion approaches lead to significant perceptual improvements, even though audible differences to the reference remain.","2020","2023-07-12 06:47:59","2023-07-19 04:24:07","","428–440","","6","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CJC3KLQL","journalArticle","2021","Villegas, Julián; Fukasawa, Naoki; Arevalo, Camilo","The Presence of a Floor Improves Subjective Elevation Accuracy of Binaural Stimuli Created With Non-Individualized Head-Related Impulse Responses","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21534","","2021","2023-07-12 06:48:04","2023-07-12 06:48:04","","849–859","","11","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TPX2LNS6","journalArticle","2006","Biswas, Arijit; den Brinker, Albertus C.","Perceptually Biased Linear Prediction","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13894","A perceptually biased linear prediction scheme is proposed for audio coding. Using only simple modifications of the coefficients defining the normal equations for a least-squares error, the spectral masking effects are mimicked in the prediction synthesis filter without using an explicit psychoacoustic model. The main advantage of such a scheme is reduced computational complexity. The proposed approach was implemented in a Laguerre-based linear prediction scheme, and its performance was evaluated in comparison with a Laguerrebased linear prediction approach controlled by the ISO MPEG-1 Layer I–II model, as well as with one of the latest spectral integration–based psychoacoustic models. Listening tests clearly demonstrate the viability of the proposed method.","2006","2023-07-12 06:48:09","2023-07-19 03:42:32","","1179–1188","","12","54","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KMGYXZUM","journalArticle","2017","Coleman, Philip; Franck, Andreas; Jackson, Philip J. B.; Hughes, Richard J.; Remaggi, Luca; Melchior, Frank","Object-Based Reverberation for Spatial Audio","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18544","To enable future audio systems to be more immersive, interactive, and easily accessible, object-based frameworks are currently being explored as a means to that ends. In object-based audio, a scene is composed of a number of objects, each comprising audio content and metadata. The metadata is interpreted by a renderer, which creates the audio to be sent to each loudspeaker with knowledge of the speci?c target reproduction system. While recent standardization activities provide recommendations for object formats, the method for capturing and reproducing reverberation is still open. This research presents a parametric approach for capturing, representing, editing, and rendering reverberation over a 3D spatial audio system. A Reverberant Spatial Audio Object (RSAO) allows for an object to synthesize the required reverberation. An example illustrates a RSAO framework with listening tests that show how the approach correctly retains the room size and source distance. An agnostic rendering can be used to alter listener envelopment. Editing the parameters can also be used to alter the perceived room size and source distance; greater envelopment can be achieved with the appropriate reproduction system.","2017","2023-07-12 06:48:12","2023-07-19 03:49:06","","66–77","","1/2","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PKWNW5TI","journalArticle","2022","Engel, Isaac; Alon, David L.; Scheumann, Kevin; Crukley, Jeff; Mehra, Ravish","On the Differences in Preferred Headphone Response for Spatial and Stereo Content","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21564","","2022","2023-07-12 06:48:16","2023-07-12 06:48:16","","271–283","","4","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8CIXGPB8","journalArticle","2007","Breebaart, Jeroen; Hotho, Gerard; Koppens, Jeroen; Schuijers, Erik; Oomen, Werner; Van De Par, Steven","Background, Concept, and Architecture for the Recent MPEG Surround Standard on Multichannel Audio Compression","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14162","An overview of the recently finalized ISO/MPEG standard for multichannel audio compression MPEG Surround is provided. This audio compression scheme enables backwardcompatible multichannel audio coding and transmission at unsurpassed coding efficiency. This is achieved by generating a mono, stereo, or matrixed-surround compatible down mix, which can be transmitted using any existing mono or stereo service, extended with a small amount of parametric side information that describes the perceptually relevant spatial properties of the original multichannel content. The concepts behind spatial parameterization are outlined, and the architecture of the MPEG Surround system is explained. Results of subjective evaluations are included to demonstrate its efficiency.","2007","2023-07-12 06:48:19","2023-07-19 03:44:51","","331–351","","5","55","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N68GNC7U","journalArticle","2015","Shirley, Ben; Oldfield, Rob","Clean Audio for TV broadcast: An Object-Based Approach for Hearing-Impaired Viewers","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17582","As the percentage of the population with hearing loss increases, broadcasters are receiving more complaints about the difficulty in understanding dialog in the presence of background sound and music. This article explores these issues, reviews previously proposed solutions, and presents an object-based approach that can be implemented within MPEG-H to give listeners control of their audio mix. An object-based approach to clean audio, combined with methods to isolate sounds that are important to the narrative and meaning of a broadcast has the potential to enable users to have complete control of the relative levels of all aspects of audio from TV broadcast. This approach was demonstrated at the University of Salford campus in 2013.","2015","2023-07-12 06:48:23","2023-07-19 04:47:57","","245–256","","4","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LWYAHMD4","journalArticle","2023","Kirsch, Christoph; Wendt, Torben; Van De Par, Steven; Hu, Hongmei; Ewert, Stephan D.","Computationally-Efficient Simulation of Late Reverberation for Inhomogeneous Boundary Conditions and Coupled Rooms","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22040","","2023","2023-07-12 06:48:26","2023-07-12 06:48:26","","186–201","","4","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QQ83YEQT","journalArticle","2020","Moon, Soyoun; Park, Sunghwan; Park, Donggun; Yun, Myunghwan; Chang, Kyongjin; Park, Dongchul","Active Sound Design Development Based on the Harmonics of Main Order From Engine Sound","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20889","Most previous studies on active sound design (ASD) development proposed regression models based on psychoacoustic parameters for engine sound design. However, order-based parameters are required for a real ASD development, considering that an ASD system is controlled by order levels. In this paper, we propose a regression model utilizing order-level–based parameters that can be efficiently applied to ASD development. A jury test was conducted for 27 engine sound recordings using 36 participants with normal hearing ability to evaluate the level of affective adjectives. Then, acoustic parameters were measured from the engine sound recordings to identify the relationship between the adjectives and parameters. Finally, a regression model was derived through statistical analysis. The properties of the model were compared with those of models proposed in previous studies to verify its superiority. The proposed regression model can reduce the time and effort required for ASD development.","2020","2023-07-12 06:48:29","2023-07-19 04:33:21","","532–544","","7/8","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H23R59PT","journalArticle","2020","Agrawal, Sarvesh; Simon, Adèle; Bech, Søren; Bæntsen, Klaus; Forchhammer, Søren","Defining Immersion: Literature Review and Implications for Research on Audiovisual Experiences","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20857","","2020","2023-07-12 06:48:32","2023-07-12 06:48:32","","404–417","","6","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9QNENAEH","journalArticle","2012","Wankling, Matthew; Fazenda, Bruno; Davies, William J.","The Assessment of Low-Frequency Room Acoustic Parameters Using Descriptive Analysis","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16323","In small rooms, low-frequency modes have a degrading influence on the quality of the bass components of music. Using objective measures to correct these modes often fails because they do not correspond to the subjective experience of listeners. This research begins with a procedure that elicits a compact set of four verbal descriptors from subjects: articulation, resonance, strength, and depth. These are then mapped to three objective parameters: modal decay time, room volume, and source/receiver position. Results show the importance of reducing the decay time, which then provides an increase in articulation. Discussions suggest ways of extending the results.","2012","2023-07-12 06:48:36","2023-07-19 10:54:30","","325–337","","5","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ENMZMTSJ","journalArticle","2007","Møller, Henrik; Minnaar, Pauli; Olesen, S. Krarup; Christensen, Flemming; Plogsties, Jan","On the Audibility of All-Pass Phase in Electroacoustical Transfer Functions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14152","Audible effects of second-order all-pass sections with center frequencies in the range of 1-12 kHz were studied in headphone listening experiments. All-pass sections give rise to two effects. 1) A perception of “ringing” or “pitchiness,” which is related to an exponentially decaying sinusoid in the impulse response of all-pass sections with high Q factors. The ringing is especially audible for impulsive sounds, whereas it is often masked with everyday sounds such as speech and music. With an impulse signal the ringing was found to be audible when the decay time constant for the sinusoid exceeds approximately 0.8 ms (peak group delay of 1.6 ms), independent of the center frequency within the frequency range studied. 2) A lateral shift of the auditory image, which occurs when an all-pass section is inserted in the signal path to only one ear. The shift is related to the low-frequency phase and group delays of the all-pass section, and it was found to be audible whenever these exceed approximately 35 s, independent of the signal.","2007","2023-07-12 06:48:40","2023-07-19 04:32:40","","113–134","","3","55","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9L5B8HQX","journalArticle","2012","Ntalampiras, Stavros; Potamitis, Ilyas; Fakotakis, Nikos","Acoustic Detection of Human Activities in Natural Environments","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16373","Automatic recognition of sound events can be valuable for efficient analysis of audio scenes. For example, detecting human activities like trespassing and hunting in natural environments can play an important role in their preservation by alerting authorities to take action. In the proposed system, each sound class is represented by a hidden Markov model created from descriptors in the time, frequency, and wavelet domains. The system has the ability to automatically adapt to acoustic conditions of different scenes via the feedback loop that refines an unsupervised model. A reliable testing process was adopted for assessing the performance of the system under adverse conditions characterized by highly nonstationary environmental noise.","2012","2023-07-12 06:48:44","2023-07-19 04:35:24","","686–695","","9","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FCNVS4RJ","journalArticle","2008","Zwan, Pawel; Kostek, Bozena","System for Automatic Singing Voice Recognition","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14634","A neural network was trained and tested to provide automated classification of singing voices, both recognizing voice quality (amateur, semiprofessional, and professional) and voice type (bass, baritone, tenor, alto, mezzo-soprano, and soprano). Parameters related to singing were defined to form feature vectors. Single vowel samples for each singer were judged by six experts to establish a quality index. In a test based on a database of 2690 samples, 90% of the decisions were correct. These results show that it is possible to use neural networks to create an expert system to evaluate singing.","2008","2023-07-12 06:48:48","2023-07-19 11:00:03","","710–723","","9","56","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AYDK5C89","journalArticle","2014","Hendrickx, Etienne; Paquier, Mathieu; Koehl, Vincent","The Influence of Stereoscopy on the Sound Mixing of Movies: A Study on the Front/Rear Balance of Ambience","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17548","There is a range of opinions in the cinema industry about the appropriate influence of stereoscopy on the sound mixing for movies. The present study focuses on the perception of ambience. Eight sequences—in their stereoscopic and nonstereoscopic versions, with several different sound mixes—were presented to 44 subjects. For each presentation, subjects had to judge to what extent the mix sounded frontal or “surround.” The goal was to verify whether stereoscopy had an influence on the perception of the front/rear balance of ambience. Results showed that this influence was weak, which was consistent with a preliminary experiment conducted in a mixing auditorium where subjects had to mix the front/rear balance of several sequences themselves.","2014","2023-07-12 06:48:51","2023-07-19 04:04:55","","723–735","","11","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TID3H57L","journalArticle","2001","Cox, Trevor J.; Li, Francis; Darlington, Paul","Extracting Room Reverberation Time from Speech Using Artificial Neural Networks","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=10197","A novel method to extract the reverberation time from reverberated speech utterances is presented. In this study, speech utterances are restricted to pronounced digits; uncontrolled discourse is not considered. The reverberation times considered are wide band, within the frequency range of speech utterances. A multilayer feed forward neural network is trained on speech examples with known reverberation times generated by a room simulator. The speech signals are preprocessed by calculating short-term rms values. A second decision-based neural network is added to improve the reliability of the predictions. In the retrieve phase, the trained neural networks extract room reverberation times from speech signals picked up in the rooms to an accuracy of 0.1 s. This provides an alternative to traditional measurement methods and facilitates the occupied measurement of room reverberation times.","2001","2023-07-12 06:48:56","2023-07-19 03:50:20","","219–230","","4","49","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4T85MJ4C","journalArticle","2003","Esquef, Paulo A. A.; Biscainho, Luiz W. P.; Välimäki, Vesa","An Efficient Algorithm for the Restoration of Audio Signals Corrupted with Low-Frequency Pulses","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12221","","2003","2023-07-12 06:48:59","2023-07-12 06:48:59","","502–517","","6","51","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LNTF7HDD","journalArticle","2010","Menzer, Fritz; Faller, Christof","Investigations on an Early-Reflection-Free Model for BRIRs","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15517","Simulating the late reverberation of a room using a synthetically generated reverberation tail is a common practice in the design of artificial reverberators. Binaural reverberators could benefit from better knowledge of the perceptual cues that are relevant for the reverberation tail. In this study the use of filtered white Gaussian noise instead of the original tail was subjectively evaluated. Matching the interaural coherence in each frequency band produced better results than full-band matching. In some cases time-dependent matching improved quality. Results are based on subjective studies.","2010","2023-07-12 06:49:03","2023-07-19 04:30:42","","709–723","","9","58","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PQ2FG57L","journalArticle","2016","Hill, Adam J.; Hawksford, Malcolm O. J.; Newell, Philip","Enhanced Wide-Area Low-Frequency Sound Reproduction in Cinemas: Effective and Practical Alternatives to Current Calibration Strategies","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18134","The current strategies for the low-frequency calibration of cinema sound systems are based on a flawed premise of low-frequency acoustics and psychoacoustics. This research shows that there is virtually no benefit in terms of spatiotemporal variance reduction: pre- and post-calibrated systems will exhibit equally position-dependent listening experience differences. For modern cinemas, the typical focus on room-modes when designing a low frequency calibration system is not necessary because the dimensions of the space coupled with low reverberation time results in Schroeder frequencies around 35 Hz. Above this value, effects of room-modes are not perceptible. Comb-filtering between sources and low-order reflections is the primary cause of high spatial variance. Furthermore, there is no evidence that spatial averaging techniques used for measurement and equalization are subjectively beneficial. A new approach needs to be invented.","2016","2023-07-12 06:49:07","2023-07-19 04:05:58","","280–298","","5","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BX2XSCXZ","journalArticle","1996","Møller, Henrik; Sørensen, Michael Friis; Jensen, Clemen Boje; Hammershøi, Dorte","Binaural Technique: Do We Need Individual Recordings?","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=7897","The localization performance was studied when subjects listened 1) to a real sound field and 2) to binaural recordings of the same sound field, made a) in their own ears and b) in the ears of other subjects. The binaural recordings were made at the blocked ear canal entrance, and the reproduction was carried out with individually equalized headphones. Eight subjects participated in the experiments, which took place in a standard listening room. Each stimulus (female speech) was emitted from one of 19 loudspeakers, and the subjects were to indicate the perceived sound source. When compared to real life, the localization performance was preserved with individual recordings. Nonindividual recordings resulted in an increased number of errors for the sound sources in the median plane, where movements were seen not only to nearby directions but also to directions further away, such as confusion between sound sources in front and behind. The number of distance errors increased only slightly with nonindividual recordings. Earlier suggestions that individuals might localize better with recordings fro other individual's found no support.","1996","2023-07-12 06:49:10","2023-07-19 04:32:48","","451–469","","6","44","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QJWPXSP3","journalArticle","2005","Zielinski, Slawomir K.; Rumsey, Francis; Kassier, Rafael; Bech, Søren","Comparison of Basic Audio Quality and Timbral and Spatial Fidelity Changes Caused by Limitation of Bandwidth and by Down-mix Algorithms in 5.1 Surround Audio Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13407","","2005","2023-07-12 06:49:14","2023-07-12 06:49:14","","174–192","","3","53","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VYQW8WB2","journalArticle","1981","Lipshitz, Stanley P.; Vanderkooy, John","The Great Debate: Subjective Evaluation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=3899","A polarization of people has occurred regarding subjective evaluation, separating those who believe that audible differences are related to measurable differences in controlled tests, from those who believe that such differences have no direct relationship to measurements. Tests are necessary to resolve such differences of opinion, and to further the state of audio and open new areas of understanding. We argue that highly controlled tests are necessary to transform subjective evaluation to an objective plane so that preferences and bias can be eliminated, in the quest for determining the accuracy of an audio component. In order for subjective tests to be meaningful to others, the following should be observed. (1) There must be technical competence to prevent obvious and/or subtle effects from affecting the test. (2) Linear differences must be thoroughly excised before conclusions about nonlinear errors can be reached. (3) The subjective judgment required in the test must be simple, such as the ability to discriminate between two components, using an absolute reference wherever possible. (4) The test must be blind or preferably double-blind. To implement such tests we advocate the use of A/B switchboxes. The box itself can be tested for audibly intrusive effects, and several embellishments are described which allow double-blind procedures to be used in listening tests. We believe that the burden of proof must lie with those who make new hypotheses regarding subjective tests. This alone would wipe out most criticisms of the controlled tests reported in the literature. Speculation is changed to fact only by careful experimentation. Recent references are given which support out point of view. The significance of differences in audio components is discussed, and in conclusion we detail some of our tests, hypotheses and speculations.","1981","2023-07-12 06:49:18","2023-07-19 04:20:59","","482–491","","7/8","29","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TCIZRELB","journalArticle","2016","Galvez, Marcos F. Simón; Menzies, Dylan; Mason, Rusell; Fazi, Filippo Maria","Object-Based Audio Reproduction using a Listener-Position Adaptive Stereo System","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18516","Adapting a stereo reproduction system to the listener’s position allows for reproduction of 2D object-based audio with better localization accuracy when the listener is located outside of the sweet spot. The adaptation is composed of two parts: a compensation system that updates the loudspeakers feeds so that these are delivered to the listener with an intended magnitude and phase independently of the listening position, and an object-based rendering system using conventional panning algorithms. Initial localization simulations using the velocity and energy localization vectors predicted that the frequency-dependent panning can pan virtual audio sources outside of the loudspeaker region at low frequency. A perturbation analysis showed that, in practice, localization of audio objects is robust when these are panned between the stereo region, and that the localization of objects outside of the stereo region is both sensitive to errors and affected by the accuracy of the video tracking system and the homogeneity of the radiation pattern of the different loudspeakers of the system. These same results were corroborated with subjective tests.","2016","2023-07-12 06:49:22","2023-07-19 04:01:10","","740–751","","10","64","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FDQMRXI5","journalArticle","2017","Brandt, Matthias; Doclo, Simon; Gerkmann, Timo; Bitzer, Joerg","Impulsive Disturbances in Audio Archives: Signal Classification for Automatic Restoration","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19356","","2017","2023-07-12 06:49:26","2023-07-12 06:49:26","","826–840","","10","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WPIV8VZV","journalArticle","2019","Heilemann, Michael C.; Anderson, David A.; Bocko, Mark F.","Near-Field Object-Based Audio Rendering on Flat-Panel Displays","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20491","Devices such as smartphones and televisions are beginning to employ screens as both a video display and a loudspeaker. This multimodal device is well suited for object-based encoding of audio, where audio objects may be rendered at the location corresponding to the visual images. The audio object renderer must be configured to account for variations in panel behavior at different excitation frequencies. This research proposes a multiband crossover network for the audio object renderer that separates the signal for each audio object into low, midrange, and high-frequency bands. Each band is then reproduced on the panel using a different vibration rendering technique. The different rendering techniques are realized by employing a combination of actuator array processing and the natural vibration localization characteristics of point-driven panels. The cutoff frequencies for each band are determined by the physical properties of the panel. Experiments on a prototype panel employing the multiband crossover system demonstrate that the vibration response behaves as predicted in each frequency range. This system provides a platform for rendering spatial audio on devices when listeners are close to the screen, and where there are restrictions related to weight, power consumption, and form-factor.","2019","2023-07-12 06:49:29","2023-07-19 04:04:39","","531–539","","7/8","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WJA3YQJH","journalArticle","2011","Laitinen, Mikko-Ville; Kuech, Fabrian; Disch, Sascha; Pulkki, Ville","Reproducing Applause-Type Signals with Directional Audio Coding","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15774","Reproduction of surrounding applause-type signals is known to produce audible artifacts with parametric spatial audio coders. This arises from inadequate temporal resolution. Better quality results if the temporal resolution is high enough to analyze each handclap separately, which can be obtained by using different temporal windows for different frequencies. Formal listening tests confirm the improved subjective quality.","2011","2023-07-12 06:49:33","2023-07-19 04:17:11","","29–43","","1/2","59","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E9ATYGAY","journalArticle","2002","Beerends, John G.; Hekstra, Andries P.; Rix, Antony W.; Hollier, Michael P.","Perceptual Evaluation of Speech Quality (PESQ) The New ITU Standard for End-to-End Speech Quality Assessment Part II: Psychoacoustic Model","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=11062","A new model for the perceptual evaluation of speech quality (PESQ) was recently standardized by the International Telecommunications Union as Recommendation P.862. Unlike previous codec assessment models, such as PSQM and MNB (ITU-T P.861), PESQ is able to predict subjective quality with good correlation in a very wide range of conditions, which may include coding distortions, errors, noise, filtering, delay, and variable delay. The psychoacoustic model used in PESQ is introduced. Part I describes the time-delay identification technique that is used in combination with the PESQ psychoacoustic model to predict the end-to-end perceived speech quality.","2002","2023-07-12 06:49:37","2023-07-19 03:40:05","","765–778","","10","50","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5R5UJQU2","journalArticle","2020","Blanco Galindo, Miguel; Coleman, Philip; Jackson, Philip J. B.","Microphone Array Geometries for Horizontal Spatial Audio Object Capture With Beamforming","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20851","Microphone array beamforming can be used to enhance and separate sound sources, with applications in the capture of object-based audio. Many beamforming methods have been proposed and assessed against each other. However, the effects of compact microphone array design on beamforming performance have not been studied for this kind of application. This study investigates how to maximize the quality of audio objects extracted from a horizontal sound field by filter-and-sum beamforming, through appropriate choice of microphone array design. Eight uniform geometrieswith practical constraints of a limited number of microphones and maximum array size are evaluated over a range of physical metrics. Results show that baffled circular arrays outperform the other geometries in terms of perceptually relevant frequency range, spatial resolution, directivity, and robustness. Moreover, a subjective evaluation of microphone arrays and beamformers is conducted with regards to the quality of the target sound, interference suppression, and overall quality of simulated music performance re- cordings. Baffled circular arrays achieve higher target quality and interference suppression than alternative geometries with wideband signals. Furthermore, subjective scores of beamformers regarding target quality and interference suppression agree well with beamformer on-axis and off-axis responses; with wideband signals, the superdirective beamformer achieves the highest overall quality.","2020","2023-07-12 06:49:41","2023-07-19 03:42:57","","324–337","","5","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A6XNL8EU","journalArticle","2021","Werner, Kurt James; Germain, Francois G.; Goldsmith, Cory S.","Energy-Preserving Time-Varying Schroeder Allpass Filters and Multichannel Extensions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21114","We propose time-varying Schroeder allpass filters and Gerzon allpass reverberators that remain energy preserving irrespective of arbitrary variation of their allpass gains or feedback matrices over time. We propose various ways of realizing the unitary matrix involved in the Schroeder structure, based on classic ladder and lattice filters and their generalizations. We show how to construct more elaborate structures including nestings and cascade, giving various strategies for reducing their implementation cost. Extending these algorithms to the multi-input, multi-output case yields time-varying, energy-preserving generalizations of Gerzon’s reverberator, providing a link between Schroeder allpass filters and Schelcht’s recently proposed “Allpass Feedback Delay Networks.” Stability proofs are given for common uses of Schroeder allpass filters, such as inside of Feedback Delay Network reference structures. Finally we give a substantial review of the properties of time-invariant Schroeder allpass filters.","2021","2023-07-12 06:49:45","2023-07-19 10:55:17","","465–485","","7/8","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"35NQCNPN","journalArticle","2013","Tassart, Stéphan","Graphical Equalization Using Interpolated Filter Banks","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16823","While there are many topologies for implementing a graphical equalizer, this research explores a design and implementation that is based on time-varying infinite impulse response (IIR) filters. Filter blocks are obtained by interpolating between two time-invariant biquadratic filters having the same center frequency but possibly different gains and bandwidths. This corresponds to crossfading, which avoids transients during transitions. This design is suitable for digital implementation because the variable parameters are excluded from the feedback loops thus avoiding problems of stability during transitions. Moreover, the gain and the bandwidth of the band filters are jointly optimized to minimize interband interferences. Group delays are smaller than multirate or FIR implementations.","2013","2023-07-12 06:50:04","2023-07-19 04:51:59","","263–279","","5","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W79A8I2K","journalArticle","2019","Menzies, Dylan; Fazi, Filippo Maria","Multichannel Compensated Amplitude Panning, An Adaptive Object-Based Reproduction Method","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20493","Conventional approaches for surround sound panning require loudspeakers to be distributed over the regions where images are required. However in many listening situations it is not practical or desirable to place loudspeakers at some positions, such as behind or above the listener. Compensated Amplitude Panning (CAP) is an object-based reproduction method that adapts dynamically to the listener’s head orientation to provide stable images in any direction in the frequency range up to approximately 1000 Hz. This is achieved by accurately controlling the Interaural Time Difference cue. CAP can also provide images in the near-field range, by controlling the Interaural Level Difference. Using two loudspeakers and with full 6-degrees-of-freedom head tracking, it was previously shown possible to create low band images in any direction, although excessive gain is required for some listener orientations. But with 3 loudspeakers all images directions can be reproduced with moderate gain. Adding more loudspeakers to a stereo configuration does not worsen performance. For comparison, an Ambisonic approach with position tracking and 3 frontal loudspeakers can reproduce horizontal surround images, and 4 loudspeakers can reproduce full 3D.","2019","2023-07-12 06:50:09","2023-07-19 04:31:00","","549–556","","7/8","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IPYBXRAZ","journalArticle","2014","Wendt, Torben; van de Par, Steven; Ewert, Stephan D.","A Computationally-Efficient and Perceptually-Plausible Algorithm for Binaural Room Impulse Response Simulation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17550","Simulating the reverberation of rooms has numerous applications that range from evaluating acoustic scenarios in the development of signal processing algorithms to the exploration of speech intelligibility in virtual rooms with movable sources. A hybrid approach was created to simulate room acoustics, achieving high computational efficiency and perceptual plausibility: early reflections were calculated using the image source model (ISM) and the reverberant tail used a feedback delay network (FDN). The FDN approach was modified to be adaptable to various room dimensions and wall absorption coefficients. Using head-related impulse responses, the authors extended it to create spatially-distributed reverberation for arbitrary source and receiver positions. Subjective ratings of the perceived room attributes and the assessment of various common parameters showed a good correspondence between simulated and real rooms.","2014","2023-07-12 06:50:12","2023-07-19 10:55:07","","748–766","","11","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"44I27VFJ","journalArticle","2018","Vuegen, Lode; Karsmakers, Peter; Vanrumste, Bart; Hamme, Hugo Van","Acoustic Event Classification using Low-Resolution Multi-label Non-negative Matrix Deconvolution","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19567","With the increased proliferation of interconnected devices that have built-in microphones, acoustic event classification and monitoring becomes possible in a wide variety of applications, such as surveillance, healthcare, military, machine diagnostics, and wildlife tracking. The promise and success of these applications depends on robust sensing of acoustic events in the environment. Typically, sound event classes are defined by annotating training data, which is a laborious process. This work introduces an extended version of non-negative matrix deconvolution (NMD), called low-resolution multi-label non-negative matrix deconvolution (LRM-NMD), where both the observation data and the available labeling information are used during training. The proposed extension of NMD was successfully applied to the classification of acoustic events even in noisy conditions with overlapping events. Low-resolution, multi-labeling information simply indicates that the sound classes of the events take place over a longer period of time in the acoustic data without identifying beginning or endings of the individual events.","2018","2023-07-12 06:50:16","2023-07-19 10:54:04","","369–384","","5","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8785DMZU","journalArticle","2013","Beerends, John G.; Schmidmer, Christian; Berger, Jens; Obermann, Matthias; Ullmann, Raphael; Pomy, Joachim; Keyhl, Michael","Perceptual Objective Listening Quality Assessment (POLQA), The Third Generation ITU-T Standard for End-to-End Speech Quality Measurement Part I—Temporal Alignment","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16829","In this and the companion paper Part II, the authors present the Perceptual Objective Listening Quality Assessment (POLQA), the third-generation speech quality measurement algorithm, standardized by the International Telecommunication Union in 2011 as Recommendation P.863. In contrast to the previous standard (P.862 Perceptual Evaluation of Speech Quality), a more complex temporal alignment was developed allowing for the alignment of a wide variety of complex distortions for which P.862 was known to fail, such as multiple delay variations within utterances as well as temporal stretching and compression of the degraded signal. When this new algorithm is used in combination with the advanced perceptual model described in Part II, it provides a new measurement standard for predicting Mean Opinion Scores that outperforms the older PESQ standard, especially for wideband and super wideband speech signals (7 and 14 kHz audio bandwidth). Part I provides the basics of the POLQA approach and outlines the core elements of the temporal alignment.","2013","2023-07-12 06:50:20","2023-07-19 03:40:15","","366–384","","6","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8LY99GHU","journalArticle","2023","Vidal, Adrien; Herzog, Philippe; Lambourg, Christophe; Chatron, Jacques","Comparison of Transaural Configurations Inside Usual Rooms","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22041","","2023","2023-07-12 06:50:24","2023-07-12 06:50:24","","202–215","","4","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U82CPUTR","journalArticle","2017","Jukic, Ante; van Waterschoot, Toon; Gerkmann, Timo; Doclo, Simon","A General Framework for Incorporating Time–Frequency Domain Sparsity in Multichannel Speech Dereverberation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18540","Effective speech dereverberation is a prerequisite in such applications as hands-free telephony, voice-based human-machine interfaces, and hearing aids. Blind multichannel speech dereverberation methods based on multichannel linear prediction (MCLP) can estimate the dereverberated speech component without any knowledge of the room acoustics. This can be achieved by estimating and subtracting the undesired reverberant component from the reference microphone signal. This report presents a general framework that exploits sparsity in the time–frequency domain of a MCLP-based speech dereverberation. The framework combines a wideband or a narrowband signal model with either an analysis or a synthesis sparsity prior, and generalizes state-of-the-art MCLP-based speech dereverberation methods.","2017","2023-07-12 06:50:30","2023-07-19 04:09:09","","17–30","","1/2","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L5YCDTK9","journalArticle","2023","Marchan, Mick; Allen, Andrew","Multi-Layered Architecture for Efficient and Accurate HRTF Rendering","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22141","AR/VR applications commonly face difficulties binaurally spatializing many sound sources at once because of computational constraints. Existing techniques for efficient binaural rendering, such as Ambisonics, Vector-Based Amplitude Panning, or Principal Component Analysis, alleviate this issue by approximating Head-Related Transfer Function (HRTF) datasets with a linear combination of basis filters. This paper proposes a novel binaural renderer that convolves each basis filter with a layer of low-order finite impulse response filters applied in time-domain and derives both the spatial functions and filter coefficients through the minimization of a perceptually motivated error function. In a MUSHRA test, expert listeners had more difficulty differentiating the proposed method from the HRTF dataset it approximates than it did with existing methods configured with an equivalent number of Fast Fourier Transforms and identical HRTF preprocessing. This was consistent across both an internal Microsoft HRTF dataset and an individual head from the SADIE database.","2023","2023-07-12 06:50:35","2023-07-19 04:27:00","","338–348","","6","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FAI7P24Z","journalArticle","2019","Lee, Hyunkook","Capturing 360° Audio Using an Equal Segment Microphone Array (ESMA)","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19883","","2019","2023-07-12 06:50:42","2023-07-12 06:50:42","","13–26","","1/2","67","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QLR82RAU","journalArticle","2008","Kim, Sunmin; Kong, Donggeon; Jang, Seongcheol","Adaptive Virtual Surround Sound Rendering System for an Arbitrary Listening Position","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14383","","2008","2023-07-12 06:50:47","2023-07-12 06:50:47","","243–254","","4","56","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PBRCWAFZ","journalArticle","2022","Hold, Christoph; Mckenzie, Thomas; Götz, Georg; Schlecht, Sebastian J.; Pulkki, Ville","Resynthesis of Spatial Room Impulse Response Tails With Anisotropic Multi-Slope Decays","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21800","Spatial room impulse responses (SRIRs) capture room acoustics with directional information. SRIRs measured in coupled rooms and spaces with non-uniform absorption distribution may exhibit anisotropic reverberation decays and multiple decay slopes. However, noisy measurements with low signal-to-noise ratios pose issues in analysis and reproduction in practice. This paper presents a method for resynthesis of the late decay of anisotropic SRIRs, effectively removing noise from SRIR measurements. The method accounts for both multi-slope decays and directional reverberation. A spherical filter bank extracts directionally constrained signals from Ambisonic input, which are then analyzed and parameterized in terms of multiple exponential decays and a noise floor. The noisy late reverberation is then resynthesized from the estimated parameters using modal synthesis, and the restored SRIR is reconstructed as Ambisonic signals. The method is evaluated both numerically and perceptually, which shows that SRIRs can be denoised with minimal error as long as parts of the decay slope are above the noise level, with signal-to-noise ratios as low as 40 dB in the presented experiment. The method can be used to increase the perceived spatial audio quality of noise-impaired SRIRs.","2022","2023-07-12 06:50:50","2023-07-19 04:06:27","","526–538","","6","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y94VKBWB","journalArticle","2010","Faller II, Kenneth John; Barreto, Armando; Adjouadi, Malek","Augmented Hankel Total Least-Squares Decomposition of Head-Related Transfer Functions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=15238","","2010","2023-07-12 06:50:54","2023-07-12 06:50:54","","3–21","","1/2","58","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XMCDC9NZ","journalArticle","2001","Minnaar, Pauli; Olesen, S. Krarup; Christensen, Flemming; Møller, Henrik","Localization with Binaural Recordings from Artificial and Human Heads","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=10193","Previous experiments have shown that localization with binaural recordings made with artificial heads is inferior to localization in real life and also to localization with recordings made in the ears of selected humans. These results suggest that artificial heads may be improved. A new experiment was made, employing recordings from two human heads and seven artificial heads some of which had been developed recently. The listening room setup from previous experiments was used and 20 listeners participated. As in the earlier experiments, more directional errors were seen with binaural recordings than in real life. A clear learning effect was seen over five days, emphasizing the need of a balanced experimental design. The new results show that artificial heads are still not as good for recording as a well-selected human head, although some of the new heads come close. The accumulated results from the present and four earlier studies provide sufficient statistics to conclude that there are significant differences between some currently available artificial heads.","2001","2023-07-12 06:50:58","2023-07-19 04:31:43","","323–336","","5","49","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3KGTZPNK","journalArticle","1998","Gander, Mark R.","Fifty Years of Loudspeaker Developments as Viewed Through the Perspective of the Audio Engineering Society","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=12162","","1998","2023-07-12 06:51:02","2023-07-12 06:51:02","","43–58","","1/2","46","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3Q8792HB","journalArticle","2021","Tan, Yiyu; Imamura, Toshiyuki; Kondo, Masaaki","FPGA-Based Acceleration of FDTD Sound Field Rendering","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21121","Sound field rendering is computation-intensive and memory-intensive. This research investigates an FPGA-based accelerator for sound field rendering with an FDTD scheme, in which wave equations are directly implemented by reconfigurable hardware, and spatial blocking is applied to alleviate the memory bandwidth requirement. Compared to software simulation performed on a desktop machine with 128 GB DDR4 RAMs and an Intel i7-7820X processor running at 3.6 GHz, the proposed FPGA-based accelerator achieves up to 2.98 times more in computing performance in the case of different layer sizes and different numbers of nodes computed in parallel even though the FPGA system runs at about 267 MHz.","2021","2023-07-12 06:51:05","2023-07-19 04:51:42","","542–556","","7/8","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"URXKFLYF","journalArticle","2021","Heilemann, Michael C.; Anderson, David A.; Roessner, Stephen; Bocko, Mark F.","The Evolution and Design of Flat-Panel Loudspeakers for Audio Reproduction","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21014","The underlying physics and the design of loudspeakers that radiate sound through the bending vibrations of elastic panels, here referred to generically as flat-panel loudspeakers, are reviewed in this paper. The form factor, reduced weight, and aesthetic appeal of flat-panel speakers have made them a topic of interest for more than 90 years, but these advantages have been overshadowed by acoustical shortcomings, specifically the uneven frequency response and directivity in comparison to conventional cone-radiator loudspeakers. Fundamentally, the design challenges of flat-panel speakers arise from the intrinsically large number of mechanical degrees of freedom of a panel radiator. A number of methods have been explored to compensate for the acoustical shortcomings of flat-panel speakers, such as employing inverse filters, equalization, canceling mechanical resonances with actuator arrays, and modifying the panel material, shape, structure, and boundary conditions. Such methods have been used in various combinations to achieve significant audio performance improvements, and carefully designed flat-panel loudspeakers have been rated in blind listening tests as competitive with some prosumer-grade conventional loudspeakers. This review presents a brief historical account of the evolution of flat-panel loudspeakers and summarizes the essential physics and design methodologies that have been developed to optimize their fidelity and directional response.","2021","2023-07-12 06:51:09","2023-07-19 04:04:47","","27–39","","1/2","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""