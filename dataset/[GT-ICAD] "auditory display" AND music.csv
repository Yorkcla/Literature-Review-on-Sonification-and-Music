"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"4CZIETAK","journalArticle","2013","Kostek, Bożena","Auditory Display From The Music Technology Perspective","","","","","http://hdl.handle.net/1853/51655","This paper presents some applications of Auditory Displays (AD) in the domain of music technology. First, the scope of music technology and auditory display areas are shortly outlined. Then, the research trends and system solutions within the fields of music technology, music information retrieval and music recommendation are discussed. Finally, an example of an auditory display that facilities music annotation process based on gaze tracking is shown.","2013-07","2023-07-13 06:25:40","2023-07-13 06:25:40","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"888DHE2T","journalArticle","2019","Li, Grace; Walker, Bruce N.","Mixed speech and non-speech auditory displays: impacts of design, learning, and individual differences in musical engagement","","","","","http://hdl.handle.net/1853/61500","Information presented in auditory displays is often spread across multiple streams to make it easier for listeners to distinguish between different sounds and changes in multiple cues. Due to the limited resources of the auditory sense and the fact that they are often untrained compared to the visual senses, studies have tried to determine the limit to which listeners are able to monitor different auditory streams while not compromising performance in using the displays. This study investigates the difference between non-speech auditory displays, speech auditory displays, and mixed displays; and the effects of the different display designs and individual differences on performance and learnability. Results showed that practice with feedback significantly improves performance regardless of the display design and that individual differences such as active engagement in music and motivation can predict how well a listener is able to learn to use these displays. Findings of this study contribute to understanding how musical experience can be linked to usability of auditory displays, as well as the capability of humans to learn to use their auditory senses to overcome visual workload and receive important information.","2019-06","2023-07-13 06:25:40","2023-07-13 06:25:40","2023-07-12","","","","","","","Mixed speech and non-speech auditory displays","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2NEVNB9K","journalArticle","2007","Mauney, Lisa M.; Walker, Bruce N.","Individual Differences and the Field of Auditory Display: Past Research, A Present Study, and an Agenda for the Future","","","","","http://hdl.handle.net/1853/50006","There has been some interest in the study of individual differences in the field of auditory displays, but we argue that there is a much greater potential than has been realized, to date. Relevant types of individual differences that may be applicable to interpreting auditory information include perceptual abilities, cognitive abilities, musical abilities, and learning styles. There are many measures of these individual differences available; however, they have not been thoroughly utilized in the auditory display arena. We discuss several types of individual differences relevant to auditory displays. We then present some examples of past research, along with the results of a current investigation of individual differences in auditory displays. Finally, we propose an agenda as to what research and tests should be used to further study this area.","2007-06","2023-07-13 06:25:40","2023-07-13 06:25:40","2023-07-12","","","","","","","Individual Differences and the Field of Auditory Display","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SIHM39PK","journalArticle","2000","Tran, Quan T.; Mynatt, Elizabeth D.","Music monitor: Dynamic data display","","","","","http://hdl.handle.net/1853/50677","In this demo, we present an interface prototype of Music Monitor, an application targeted for home use that dynamically conducts music in real-time to reflect parallel activities in disparate locations (e.g., preparing food in the kitchen while entertaining guests in the living room) to help the user balance attention appropriately between them. Music Monitor interprets salient information from the monitored activities to map into simple music profiles (e.g., instrument selection, tempo). The prototype design focuses on providing continuous, ambient music as a peripheral auditory data display for the primary user without sacrificing aesthetic musical value for other participants and listeners.","2000-04","2023-07-13 06:25:40","2023-07-13 06:25:40","2023-07-12","","","","","","","Music monitor","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9YZLYJGJ","journalArticle","2005","Alty, James L.; Rigas, Dimitrios; Vickers, Paul","Music and speech in auditory interfaces: When is one mode more appropriate than another?","","","","","http://hdl.handle.net/1853/50170","A number of experiments, which have been carried out using non-speech auditory interfaces, are reviewed and the advantages and disadvantages of each are discussed. The possible advantages of using non-speech audio media such as music are discussed – richness of the representations possible, the aesthetic appeal, and the possibilities of such interfaces being able to handle abstraction and consistency across the interface.","2005-07","2023-07-13 06:25:40","2023-07-13 06:25:40","2023-07-12","","","","","","","Music and speech in auditory interfaces","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9XFE922I","journalArticle","2005","Won, Sook Young","Auditory display of genome data: Human chromosome 21","","","","","http://hdl.handle.net/1853/50099","The motivation for this paper is to systematically explore the efficacy of mapping data to sound parameters with the specific aim of sonifying statistical trends and hearing the `gist' [1] of the data. In this paper, we consider the task of searching through a gene sequence of the human chromosome 21 for CpG islands and type of gene evidence. Musical intervals and rhythm is proposed for detecting CpG islands and musical timber is used for representing the gene evidence. We extract human genome data from the NCBI (National Center for Biotechnology Information) [2] database and use list processing and synthesis capabilities of CLM [3].","2005-07","2023-07-13 06:25:40","2023-07-13 06:25:40","2023-07-12","","","","","","","Auditory display of genome data","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C2UJHFHF","journalArticle","2002","Neuhoff, J. G.; Knight, R.; Wayand, J.","Pitch change, sonification, and musical expertise: Which way is up?","","","","","http://hdl.handle.net/1853/51370","Frequency change is one of the most widely used acoustic dimensions in auditory display, and pitch perception is among the most widely researched topics in audition. Nonetheless, there is little research on the appropriate mapping and scaling of information to acoustic frequency in sonification. Here, we show that musical training is a contributing factor to the mapping, scaling, and conceptual relationships that exist between the information to be sonified and its acoustic representation. In Experiment 1, three groups of listeners that varied in musical expertise moved a slider to indicate the amount of pitch change that they heard in ten non-standard musical intervals. Listeners with more musical training showed greater slider movement in response to pitch change than musical novices, but not in response to brightness in a visual control condition. Novices also made significantly more errors in identifying the direction of pitch change for intervals that were well above discrimination thresholds. Experiment 2 showed that the errors by novices were due primarily to conceptual errors in labeling `rising' and `falling' pitch with a small but significant number of perceptual discrimination errors. The results suggest that musical training is an important factor in the mapping, scaling, and conceptual relationships used in sonification.","2002-07","2023-07-13 06:25:40","2023-07-13 06:25:40","2023-07-12","","","","","","","Pitch change, sonification, and musical expertise","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EIZZY7X8","journalArticle","2004","Vickers, P.","External auditory representations of programs: Past, present, and future - an aesthetic perspective","","","","","http://hdl.handle.net/1853/50854","This paper provides a summary of previous work done in the area of external auditory representations of programs (known as program auralisation). A brief historical review is given followed by a short summary of the characteristics of the main program auralisation systems that have been reported in the literature. As program auralisation systems tend to use musical representations they are necessarily affected by artistic considerations. The influence of art theory and practice on the design of computer systems and artefacts (known as aesthetic computing [1, 2]) has grown to the extent that there is now a growing field devoted to its study. Therefore, it is instructive to explore the design of program auralisation systems in the light of aesthetic computing ideas. The remainder of this paper then, discusses the main principles of aesthetic computing in relation to program auralisation, and finishes with some views on how aesthetic computing can influence the future development of program auralisation systems.","2004-07","2023-07-13 06:25:40","2023-07-13 06:25:40","2023-07-12","","","","","","","External auditory representations of programs","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ANEN77A3","journalArticle","2006","Hogg, B.; Vickers, P.","Sonification abstraite/sonification concrete: An 'aesthetic persepctive space' for classifying auditory displays in the ars musica domain","","","","","http://hdl.handle.net/1853/50641","This paper discusses æsthetic issues of sonifications and the relationships between sonification (ars informatica) and music & sound art (ars musica). It is posited that many sonifications have suffered from poor internal ecological validity which makes listening more difficult, thereby resulting in poorer data extraction and inference on the part of the listener. Lessons are drawn from the electroacoustic music and musique concrète communities as it is argued that it is not instructive to distinguish between sonifications and music/sound art. Edgard Varèse defined music as organised sound, and sonifications organise sound to reflect mimetically the thing being sonified. Therefore, an æsthetic perspective space onto which sonifications and musical compositions alike can be mapped is proposed. The resultant map allows sonifications to be compared with works in the ars musica domain with which they share characteristics. The æsthetics of those ars musica counterparts can then be interrogated revealing useful design and organisation constructs that can be used to improve the sonifications' communicative ability.","2006-06","2023-07-13 06:25:40","2023-07-13 06:25:40","2023-07-12","","","","","","","Sonification abstraite/sonification concrete","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FUBM7YWN","journalArticle","2002","Joseph, A. J.; Lodha, S. K.","Musart: Musical audio transfer function real-time toolkit","","","","","http://hdl.handle.net/1853/51366","This work describes the design and implementation of a sonification toolkit. MUSART (MUSical Audio transfer function Realtime Toolkit) is a sonification toolkit which produces musical sound maps that are played in real-time. Register, pitch, timbre, chords, duration, silence, loudness, beats, and panning are the musical concepts used to create melodic sound maps. Univariate and multivariate data sets are sonified using various sound parameter combinations and music tracks. Users have the flexibility to create personalized auditory displays by mapping each data dimension of a data set to one or more sound parameters. MUSART is designed to be flexible so that it can be used with many applications. In this work, we use musical auditory maps to explore seismic volumes used for detecting areas for drilling oil.","2002-07","2023-07-13 06:25:40","2023-07-13 06:25:40","2023-07-12","","","","","","","Musart","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NN4PMFMC","journalArticle","2007","Fagerlon, Johan","Expressive Musical Warning Signs","","","","","http://hdl.handle.net/1853/49996","Warning signals are often very simple and monotone sounds. This paper focuses on taking a more musical approach to the design of warnings and alarms than has been the case in the past. We present an experimental pilot study in which we explore the possibilities of using short musical pieces as warning signals in a vehicle cab. In the study, 18 experienced drivers experienced five different driving scenarios with different levels of urgency. Each scenario was presented together with an auditory icon, a traditional abstract warning sound, and a musical warning sound designed in collaboration with a composer. The test was carried out in an “audio-only” environment. Drivers were required to rate the perceived urgency, annoyance and appropriateness for every sound. They also had a chance to talk freely about the different warning signals. The results indicate interestingly that drivers may be able to understand the intended meaning of musical warning signals. It seems like the musical warning signals may prove useful primarily in situations of low and medium levels of urgency.","2007-06","2023-07-13 06:25:40","2023-07-13 06:25:40","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"44J9C4P9","journalArticle","2011","Winters, R. Michael","1/F Noise and Auditory Aesthetics: Sonification of a Driven Bead Pile","","","","","http://hdl.handle.net/1853/51568","“1=f noise” describes the behavior of many naturally occurring complex dynamical systems over time. Perhaps more surprising than its ubiquity in nature is its prevalence in speech and especially music. Current research suggests that aspects of the human emotional response to music can be predicted using analysis based upon 1=f distributions. Musical compositions in which the pitch and duration of notes over time correspond to 1=f distributions have been found to be more pleasant than non-1=f distributions. The current research compares the sonification of a driven bead pile to an experimentally contrived deviation. Preliminary results suggest that the findings of pitch and duration may be extended to timbre and spatialization. The benefits of continued research into the application 1=f distributions in sonification for the auditory display community are discussed.","2011-06","2023-07-13 06:25:40","2023-07-13 06:25:40","2023-07-12","","","","","","","1/F Noise and Auditory Aesthetics","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5B7XR88F","journalArticle","2000","Gandy, Maribeth; Quay, Andrew","Haptic music: Non-musicians collaboratively creating music","","","","","http://hdl.handle.net/1853/50673","The Haptic Music system allows up to three people to collaboratively create music in a jazz style. The users can have varying musical backgrounds, and even a novice can use his/her creativity to create a novel composition. The system consists of three stations: solo, rhythm, and accompaniment. The solo station allows a user to play individual notes and longer embellishments using the haptic lens interface device. At the rhythm station the user can add new beats to the composition using a midi controller. And the accompanist controls instrument levels, tempo, and the band via four proximity sensors.","2000-04","2023-07-13 06:25:40","2023-07-13 06:25:40","2023-07-12","","","","","","","Haptic music","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y7M5HHZ7","journalArticle","2005","Childs, Edward","Auditory graphs of real-time data","","","","","http://hdl.handle.net/1853/50086","The advantages of auditory display for monitoring real-time data are discussed. Parallels between the structures of real-time data and music are emphasized as potentially fruitful areas of research. The Accentus LLC design philosophy is described, followed by several examples of auditory graphs. Areas of future research are recommended.","2005-07","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S4NK5MH2","journalArticle","2018","Tislar, Kay; Duford, Zackery; Nelson, Brittany; Peabody, Madeline; Jeon, Myounghoon","Examining the learnability of auditory displays: Music, earcons, spearcons, and lyricons","","","","","http://hdl.handle.net/1853/60090","Auditory displays are a useful platform to convey information to users for a variety of reasons. The present study sought to examine the use of different types of sounds that can be used in auditory displays—music, earcons, spearcons, and lyricons—to determine which sounds have the highest learnability when presented in sequences. Participants were self-trained on sound meanings and then asked to recall meanings after listening to sequences of varying lengths. The relatedness of sounds and their attributed meanings, or the intuitiveness of the sounds, was also examined. The results show that participants were able to learn and recall lyricons and spearcons the best, and related meaning is an important contributing variable to learnability and memorability of all sound types. This should open the door for future research and experimentation of lyricons and spearcons presented in auditory streams.","2018-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","Examining the learnability of auditory displays","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QSG394HD","journalArticle","2008","Stallmann, K.; Peres, S. C.; Kortum, P.","Auditory Stimulus Design: Musically Informed","","","","","http://hdl.handle.net/1853/49885","This paper discusses an approach to auditory stimulus design that appropriates concepts and techniques commonly used in music composition. These ideas are used to create referential sound cues that orient a listener along a timeline. The functions these cues serve may include: emphasizing arrival at targeted goals; providing orientation information relative to beginning and end times; or creating a sense of imminent closure indicating a predictive end to an ongoing process. The stimulus used in the pilot study represents this team's first attempt at integrating musical ideas with stimulus design. In an effort to make this stimulus comparable with previously used experimental stimuli, extreme restrictions were placed on the design. Although the resulting stimulus is not to be confused with `music' in the proper sense of that term, it is interesting to note how an extremely restricted set of elements can be manipulated to create an aesthetically satisfactory experience that rivals responses to `real' music in untrained listeners. The application of musical techniques towards the construction of effective auditory stimuli that are, at the same time, rated aesthetically satisfactory by users, is a long-term object of study by this team.","2008-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","Auditory Stimulus Design","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3DRNR34G","journalArticle","2008","Demey, Michiel; Leman, Marc; Cornelis, Olmo","The IPEM_EME: a Wireless Music Controller for Real-time Music Interaction","","","","","http://hdl.handle.net/1853/49957","The IPEM_EME is a sound and music controller based on wireless motion sensors and concepts of embodied music cognition. This paper and demonstration aims at further testing the public acceptability of the IPEM_EME.","2008-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","The IPEM_EME","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PVNYEVFJ","journalArticle","2004","Schubert, E.","Emotionface: Prototype facial expression display of emotion in music","","","","","http://hdl.handle.net/1853/50853","EmotionFace is a software interface for visually displaying the self-reported emotion expressed by music. Taken in reverse, it can be viewed as a facial expression whose auditory connection or exemplar is the time synchronized, associated music. The present instantiation of the software uses a simple schematic face with eyes and mouth moving according to a parabolic model: Smiling and frowning of mouth represents valence (happiness and sadness) and amount of opening of eyes represents arousal. Continuous emotional responses to music collected in previous research have been used to test and calibrate EmotionFace. The interface provides an alternative to the presentation of data on a two-dimensional emotion-space, the same space used for the collection of emotional data in response to music. These synthesized facial expressions make the observation of the emotion data expressed by music easier for the human observer to process and may be a more natural interface between the human and computer. Future research will include optimization of EmotionFace, using more sophisticated algorithms and facial expression databases, and the examination of the lag structure between facial expression and musical structure. Eventually, with more elaborate systems, automation and greater knowledge of emotion and associated musical structure, it may be possible to compose music meaningfully from synthesized and real facial expressions.","2004-07","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","Emotionface","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LZWD2RYV","journalArticle","2012","Winters, R. Michael; Wanderley, Marcelo M.","New Directions for Sonification of Expressive Movement in Music","","","2168-5126","","http://hdl.handle.net/1853/44450","Expert musical performance is rich with visual cues that facilitate expressive communication. Previous sonifications in this domain have evaluated the saliency of mapping based upon a sound's direct visual correspondence. Although this approach may be important for motion sonification more generally, a little flexibility might be necessary for expressive gesture. A review of literature on gesture in music performance suggests that for evaluation, it matters less what is moving than how and how much. New efforts focus on the potential use of sonification with the underlying performance audio. A series of videos and sonifications are presented as examples of this point.","2012-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A4P3SKZL","book","2010","Fritz, Thomas","The Anchor Model of Musical Culture","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/50063","In a recent cross-cultural study with participants from an autochthonous African population (Mafa) and Western participants, it was shown that the recognition of several emotional expressions (happy, sad, fearful) in music are likely to be music universals [1]. The Mafa listeners (who were naïve to the Western music) were quite successful at recognizing the emotional expressions in the Western music, although their own music seems not to emphasize, or even comprise this musical feature. Here I propose a model, which is aimed at illustrating how different human musical cultures intersect and “anchor” in a set of musical features that are universally perceived, while also displaying culturally acquired specifics (see Figure 2), that accounts for the Mafa results. It explains also why musical universals cannot simply be determined by specifying the common denominator between the musical features of all cultures, which may actually not exist","2010-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V798KSTP","journalArticle","2005","Gossmann, Joachim","Towards an auditory representation of complexity","","","","","http://hdl.handle.net/1853/50189","In applications of sonification, the information inferred by the sonification strategy applied often supersedes the amount of information which can be retrieved by the ear about the object of sonification. This paper focuses on the representation of complex geometric formation through sound, drawing on the development of an interactive installation sonifying escape time fractals as an example. The terms “auditory emergence and formation” are introduced and an attempt is made to interpret them for music composition, data display and information theory. The example application,. “Audio Fraktal”, is a public installation in the permanent exhibition of the Museum for Media Art at ZKM, Karlsruhe. The design of the audiovisual display system that allows the shared experience of interactive spatial auditory formation is described. The work was produced by the author at the Institute for Music and Acoustics at ZKM, Karlsruhe.","2005-07","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3XFMM4BM","journalArticle","1994","Mayer-Kress, Gottfried","Dynamical resonances and synchronization of auditory stimuli and evoked reponses in multi-channel EEG","","","","","http://hdl.handle.net/1853/50841","Perceptions of complex structures or other cognitive events are associated with synchronous oscillation of neural cell assemblies which take place in the 40 Hz domain and last for about 100 ms. We use EEG signals from 31 scalp electrodes to identify these short-term synchronization events. The challenge for audification applications is to make these relatively rare events perceptible in the background of incoherent signals from all electrodes. In a &st approach we use an ""Orchestra Paradigm"" by asigning a different standard musical instrument (piano, flute, violin, glocken) to four of these electrodes. We can clearly discriminate transitions between uncorrelated activities of these areas and the events of short-term synchronization. We expect a general resonance principle to apply also in the inverse problem. The nonlinear resonance hypothesis of music perception was tested in an experiment comparing a group of musically sophisticated and a group of less sophisticated subjects. The prediction that weakly chaotic music entrains less complex brain wave (EEG) oscillations at the prefrontal cortex was confirmed by using a correlational dimension algorithm. Strongly chaotic (stochastic) and periodic music both stimulated higher brain wave complexity. More sophisticated subjects who prefer classical music showed higher EEG dimensions while less sophisticated subjects responded with a drop in brain wave complexity to rhythmical weakly chaotic music. Subjects' ratings of perceived complexity of the musical pieces followed mathematical (objective) structure of the music and did not reflect the changes in brain wave complexity. The results are interpreted in the context of an associated (Hebbian) network theory of nonlinear brain dynamics.","1994-11","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G2KVPAJU","journalArticle","2011","Preti, Constanza; Schubert, Emery","Sonification of Emotions II: Live music in a pediatric hospital","","","","","http://hdl.handle.net/1853/51749","In this paper, we argue that music can be used to sonify emotions. Further, we propose that the ‘sonification of emotion’ conceptualization can explain some aspects of the practice of playing music in hospitals. Music can be constantly adjusted to reflect (but more accurately, we argue, sonify) various aspects of the emotional situations unfolding in a hospital, namely the interaction between the child/patient, their caregivers and the hospital staff. The case study of a musical interaction between a musician and a child/patient is presented and led to the development of a Cycle of Sonification model, where the musician collects complex environmental cues and then portrays them through the music to adjust the emotional ‘temperature’. That is, the emotion of the environment is reflected (sonified) by the music, but additionally the music is also calibrated so as to allow the regulation of the emotional mood (including distraction) in the otherwise stressful environment. As well as adjusting the mood, the music provides a ‘reading’ or measure of emotion through the auditory, non-speech mode, consistent with some pertinent definitions of sonification","2011-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","Sonification of Emotions II","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3F927I2H","journalArticle","2001","Bresin, Roberto; Friberg, Anders","Expressive musical icons","","","","","http://hdl.handle.net/1853/50615","Recent research on the analysis and synthesis of music performance has resulted in tools for the control of the expressive content in automatic music performance [1]. These results can be relevant for applications other than performance of music by a computer. In this work it is presented how the techniques for enhancing the expressive character in music performance can be used also in the design of sound logos, in the control of synthesis algorithms, and for achieving better ringing tones in mobile phones.","2001-07","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L52RUVEI","journalArticle","2013","MacDonald, Doon; Stockman, Tony","The Development Of A Method For Designing Auditory Displays Based On Soundtrack Composition","","","","","http://hdl.handle.net/1853/51666","This paper details work toward the design of a method for creating auditory displays for the human-computer interface, based on soundtrack composition. We begin with the benefits to this approach before discussing methods for auditory display design and the need for a unification of different design techniques. We then outline our on-going investigation into the tools and techniques employed within the working practices of sound designers and soundtrack composers. Following this we report our observations of the main priorities that influence how composers create soundtracks and propose ways in which our method may support these. We argue that basing the first steps of the method on a ‘cue sheet could enable designers to identify actions, objects and events within an HCI scenario whilst taking into account the user and the context of use. This is followed by some initial observations of a preliminary study into whether a participant can successfully use this cue sheet methodology. We conclude by identifying that certain elements of the methodology need to be changed: Further investigation and subsequent design needs to be carried out into ways participants can successfully comprehend and systematically use the cue sheet to identify seen and unseen events, actions and objects within the human-computer interface. Additionally we need to investigate how best categorize and map these elements to sound. We conclude our paper with our plans for future work","2013-07","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VL9P4PSK","journalArticle","1997","Lodha, Suresh K.; Beahan, John; Heppe, Travis; Joseph, Abigail; Zane-Ulman, Brett","MUSE: A musical data sonification toolkit","","","","","http://hdl.handle.net/1853/50750","Data sonification is the representation of data using sound. Last year we presented a flexible, interactive and portable data sonification toolkit called LISTEN, that allows mapping of data to several sound parameters such as pitch, volume, timbre and duration [20]. One of the potential drawbacks of LISTEN is that since the sounds generated are non-musical, they can be fatiguing when exploring large data sets over extended periods of time. A primary goal in the design of MUSE – a MUsical Sonification Environment – is to map scientific data to musical sounds. The challenge is to ensure that the data meanings are preserved and brought out by these mappings. MUSE provides flexible data mappings to musical sounds using parameters such as pitch (melody), rhythm, tempo, volume, timbre and harmony. MUSE is written in C++ for the SGI platform and works with the freely available sound specification software CSound developed at MIT. We have applied MUSE to map uncertainty in some scientific data sets to musical sounds.","1997-11","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","MUSE","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YB8CA6HY","book","2015","Zeller, Bernhard; Vogt, Katharina","Auditory graph evolution by the example of spurious correlations","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54205","Auditory graphs can be seen as an alternative to visual graphs or as an additional element to display data. This paper will offer an approach on how a simple sound mapping can be improved into a mental model. The ironic nature of the chosen dataset is reflected in the sound enhancing the data visualization.","2015-07","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WLPD6EMZ","book","2010","Straebel, Volker","The Sonification Metaphor in Instrumental Music and Sonification's Romantic Implications","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/50066","The sonification metaphor is not limited to electronic sound synthesis and computer music, but can be applied to instrumental music as well. The relation of sonification to program and experimental music is discussed and works by Iannis Xenakis, Karlheinz Stockhausen, John Cage and Alvin Lucier are briefly introduced. The paper leads to a discussion of the connection between sonification and romanticism, where the desire is to directly evoke an understanding of natural phenomena.","2010-06","2023-07-13 06:25:41","2023-07-21 08:28:52","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T5Y6AEGT","journalArticle","1996","Vickers, Paul; Alty, James L.","CAITLIN: A musical problem auralisation tool to assist novice programmers with debugging","","","","","http://hdl.handle.net/1853/50803","In the field of auditory display relatively little work has focused on the use of sound to aid program debugging. In this paper, we describe CAITLIN , a pre-processor for Turbo Pascal programs that musically auralises programs with a view to assisting novice programmers with locating errors in their code. A discussion follows of an experiment which showed that programmers could use the musical feedback to visualise and describe program structure. We then present conclusions and a discussion of future work.","1996-11","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","CAITLIN","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SNAFDZFB","journalArticle","2012","Bethancourt, Matt","The Sounds of the Discussion of Sounds","","","2168-5126","","http://hdl.handle.net/1853/44403","The Sound of the Discussion of Sounds is a real-time composition and performance computer program built in PHP, MySQL, Max/MSP and Ableton Live that allows listeners to musically hear the subtle changes that occur over time for trending and popular musical artists on Twitter.","2012-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U5PKL9E7","journalArticle","2012","Ash, Kingsley","Affected States: Analysis and Sonification of Twitter Music Trends","","","2168-5126","","http://hdl.handle.net/1853/44400","This paper describes an approach to the sonification of real- time twitter music trend data realized for the ICAD 2012 Sonification Competition: Listening to the World Listening. The paper will discuss the techniques used to create the sonification and the motivations behind them, including details of the data analysis, mapping strategies, visual display and sonification output. The system analyses the Twitter Music Trends data feed, which aggregates music listening data from Twitter by artist, as well as the Echo Nest REST API to determine the perceived emotional affect and prevailing descriptions of a selection of the latest trending artists. The resulting data is visualized and sonified in real-time to facilitate analysis and generate an appealing visual and auditory display of the resulting data. Experience with the system suggests that it is successful in allowing users to determine perceived emotional affect and quality for a number of artists simultaneously, and could allow further investigation into the correlation between these factors. The system also generates appealing visual music that reaches beyond the practice of scientific investigation to reach out to a wider audience.","2012-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","Affected States","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ABWE23U3","journalArticle","2003","Sturm, Bob L.","Ocean buoy spectral data sonification: Research update","","","","","http://hdl.handle.net/1853/50477","Since first presenting this work at the 2002 International Conference on Auditory Display the author has professionally produced a multimedia CD that explores the sonification of ocean wave data. “Music from the Ocean” is a multimedia CD that explores the sonification of ocean wave data for oceanography, science pedagogy, and music and sound synthesis. Turning this data into sound provides new ways of experiencing and comprehending the phenomena involved; the processes come alive and are more comprehensible, memorable and exciting than graphs of the data. This CD contains over 55 minutes of sound examples as well as an interactive Flash presentation and research paper explaining the methods in more detail. The accompanying 16-page booklet features graphics and information about the data, phenomena, and music. The CD appeals to a wide variety of consumers, from oceanographers, science teachers, experimental computer music enthusiasts, and music therapists.","2003-07","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","Ocean buoy spectral data sonification","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EESN4R55","book","2015","Mittmannsgruber, Rainer; Vogt, Katharina","Auditory assistance for timing presentations","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54196","Every presentation has to end at some point. Signaling the approaching end of the presentation time in conferences is often accomplished by showing signs with the remaining time written on it, i.e., by visual contact. The idea of this project is to investigate if it is possible to present this information acoustically and if the lecturer profits from this representation compared to the usual one.","2015-07","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4T2TPRQC","journalArticle","2012","Oswald, David","Non-Speech Audio-Semiotics: A Review and Revision of Auditory Icon and Earcon Theory","","","2168-5126","","http://hdl.handle.net/1853/44434","The aim of this paper is to develop a theory and taxonomy of auditory signs, based on semiotics. For more than two decades, the discourse on non-speech audio interfaces has been dominated by a dichotomy between auditory icons, which are based on everyday hearing, and earcons, which are based on musical hearing. The corresponding theory behind these concepts has to be revised for several reasons. First, the authors of these theories partly use semiotic concepts and terminology, but not always in a correct way. Second, the classification of auditory icons as ""iconic"", and earcons as ""abstract"" is too simple and based on the questionable premise that everyday sounds are per se iconic and musical motives are per se abstract and symbolic. Third, this widespread idea ignores the crucial role of the user in the process of perception. In addition, the users' perception of visual and auditory signs in computer interfaces is fundamentally different today, from how it was in the early years of graphical user interfaces — the time when the first auditory interfaces and the corresponding theories were developed.","2012-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","Non-Speech Audio-Semiotics","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YSPFFVVT","journalArticle","2011","Schubert, Emery; Ferguson, Sam; Farrar, Natasha; McPherson, Gary E.","Sonification of Emotion I: Film Music","","","","","http://hdl.handle.net/1853/51748","This paper discusses the uses of sound to provide information about emotion. The review of the literature suggests that music is able to communicate and express a wide variety of emotions. The novel aspect of the present study is a reconceptualisation of this literature by considering music as having the capacity to sonify emotions. A study was conducted in which excerpts of non-vocal film music were selected to sonify six putative emotions. Participants were then invited to identify which emotions each excerpt sonified. The results demonstrate a good specificity of emotion sonification, with errors attributable to selection of emotions close in meaning to the target (excited confused with happy, but not with sad, for example). While ‘sonification’ of emotions has been applied in opera and film for some time, the present study allows a new way of conceptualizing the ability of sound to communicate affect through music. Philosophical and psychological implications are considered.","2011-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","Sonification of Emotion I","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GM4VNNJK","journalArticle","2006","Margounakis, D.; Politis, D.","Converting images to music using their colour properties","","","","","http://hdl.handle.net/1853/50491","Music is associated to colors since ancient years. Different mappings between attributes of sound and images allow the efficient conversion between the two types of media. The proposed method for converting images to music using the concept of chromaticism provides the area of computer music with a parameterized environment for audio-visual presentations. The auditory display of colour images may bring the different ways that a listener perceives a musical piece (because of colour transitions) to light. A design template for chromatic synthesis is described. A short example, based on a graphical digital icon, demonstrates the preliminary results.","2006-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MLZ5M6SF","journalArticle","2013","Worrall, David","Understanding The Need For Micro-Gestural Inflections In Parameter-Mapping Sonification","","","","","http://hdl.handle.net/1853/51668","Most of the software tools used for data sonification have been adopted or adapted from those designed to compose computer music, which in turn, adopted them from abstractly notated scores. Such adoptions are not value-free; by using them, the cultural paradigms underlying the music for which the tools were made have influenced the conceptualization and, it is argued, the effectiveness of data sonifications. Recent research in cognition supports studies in empirical musicology that suggest that listening is not a passive ingestion of organised sounds but is an embodied activity that invisibly enacts gestures of what is heard. This paper outlines an argument for why sonifiers using parametric-mapping sonification should consider incorporating micro-gestural inflections if they are to mitigate The Mapping Problem in enhancing the intelligibility of sonified data.","2013-07","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B3YZVQS2","journalArticle","2008","Jung, Ralf","Ambience for Auditory Displays: Embedded Musical Instruments As Peripheral Audio Cues","","","","","http://hdl.handle.net/1853/49874","From alarm signals and data sonification to multimodal interfaces, auditory displays are omnipresent in our everyday life and they become more and more popular. But there are some challenges we have to meet because of the differentness of the auditory sense compared to the visual sense. Usually, audio notification signals are limited to simple warning cues and system feedback that are in most cases intrusive be- cause they differ from the environmental noise. That has the effect that people present in the room could be distracted from their current tasks because they cannot “close their ears.” To prevent the disturbing effect of traditional notification signals we developed the novel concept of non-speech audio notification embedded in ambient soundscapes to provide multi-user notification in a more discreet and non-disturbing way. Instead of using well-known non-speech cues like auditory icons and earcons, we decided to compose and record peripheral soundscapes and notification instruments by ourselves towards a more aesthetic approach. In this paper, we give an overview of our location-aware system with two applications (PAAN, AeMN) and sketch a real life scenario in a wine department of a supermarket. We will also present findings from a user study and provide a small collection of notification instruments and soundscapes as audio samples.","2008-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","Ambience for Auditory Displays","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5S3AJBYD","journalArticle","2003","Hiipakka, Jarmo; Gaetan, Lorho","A spatial audio user interface for generating music playlists","","","","","http://hdl.handle.net/1853/50433","This paper presents a user interface (UI) designed for non-visual interaction with a music collection. The UI system can be utilized to navigate a large list of musical items organized in a hierarchical structure, and to generate personal playlists. The interface only relies on stereo audio output and tactile input using a limited small of keys. This interaction scheme enables the use of the UI in situations where visual feedback is limited on the device, or when visual attention from the user is not possible or desired. However, the interface was also designed to be compatible with and complementary to visual UI concepts found in existing audio players. The design and implementation of this spatial audio UI is described in the paper. Preliminary subjective tests are also reported regarding the immediate usability of this interface, when in `eyesfree' mode on a handheld computer.","2003-07","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DHD9J5FZ","journalArticle","2012","Schmele, Timothy; Gomez, Imanol","Exploring 3D audio for brain sonification","","","2168-5126","","http://hdl.handle.net/1853/44442","Brain activity data, measured by functional Magnetic Resonance Imaging (fMRI), produces extremely high dimensional, sparse and noisy signals which are difficult to visualize, monitor and analyze. The use of spatial music can be particularly appropriate to represent its contained patterns. The literature describes several research done on sonifying neuroimaging data as well as different techniques to use spatialization as a musical language. In this paper, we discuss an artistic approach to fMRI sonification exploiting new compositional paradigms in spatial music. There fore, we have consider the brain activity as audio base material of a the spatial musical composition. Our approach attempts to explore the aesthetic potential of brain sonification not by transforming the data beyond the recognizable, and presenting the data as direct as possible.","2012-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XB6PMZMM","book","2010","Spondike, David","A Strategy for Composing Music using the Sonification of ""Snapshot"" type Data Collections ""Schnappschuss von der Erde""","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49861","Transforming the sonification of data into a musical experience that is satisfying on scientific as well as aesthetic grounds requires balancing similar and competing objectives and sensibilities. One possible solution, described here, is used to create the digital music composition Schnappschuss von der Erde in response to the call for compositions by the International Community for Auditory Display (ICAD) Conference.","2010-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WCDKRJRW","journalArticle","2018","Rönnberg, Niklas; Löwgren, Jonas","Photone: Exploring modal synergy in photographic images and music","","","","","http://hdl.handle.net/1853/60084","We present Photone, an interactive installation combining photographic images and musical sonification. An image is displayed, and a dynamic musical score is generated based on the overall color properties of the image and the color value of the pixel under the cursor. Hence, the music changes as the user moves the cursor. This simple approach turns out to have interesting experiential qualities in use. The composition of images and music invites the user to explore the combination of hues and textures, and musical sounds. We characterize the resulting experience in Photone as one of modal synergy where visual and auditory output combine holistically with the chosen interaction technique. This tentative finding is potentially relevant to further research in auditory displays and multimodal interaction.","2018-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","Photone","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7DG9HTPT","journalArticle","2012","Gossmann, Joachim","A perspective on the limited potential for simultaneity in auditory display","","","2168-5126","","http://hdl.handle.net/1853/44421","The auditory environment is frequently described as a juxtaposition between an array of pre-disclosed auditory streams and a process of attentional selection. The orientation of attentional selection toward environmental streams is characterized by a differentiation in the types of stream: Speech, music and sound effects are only three examples in an open polymorphism of what could be described as perceptual strategies through which we access the sounding world. The differentiable-simultaneous manifold of environmental streams allows perceptual participation only in a certain number of streams at the same time - only one speaking voice, one sense of ""harmony"", a single ""rhythm"", and so forth: We propose a re-basing of sonfication strategies not on the definition of external mechanisms, but on the definition and application of new modal strategies that are circumscribed and accessible through 'what is not possible to perceive at the same time'.","2012-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WCBYF5VI","journalArticle","2012","Vigani, Andrea","Sonic Window #1 [2011] — A Real Time Sonification","","","2168-5126","","http://hdl.handle.net/1853/44446","This is a real time audio installation in Max/MSP. It is a sonification of an abstract process: the writing on Twitter about music listening experiences on the web by people around the world. My purpose is not to sonify the effects of this process on a musical structure (the musical structure of the listened songs) like a real-time-echo-web-mix or a new version of J.Cage “Imaginary landscape n°4”, but to sonify the structure of the process itself, with its languages transducers, its media, its rules. For this purpose I created a musical instrument played by the data, like a windchimes but here all the sounds are created by the web data itself, like if the material of a windchimes is the wind itself.It’s like an open windows on the web listeners, you observe the action of listening and talking about music, but you don’t hear the music listened and you search for connections, reactions, interactions among the listeners, among them, the transmission media and the code language.","2012-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WRCFQ7VD","book","2015","Stahl, Benjamin; Vogt, Katharina","The effect of audiovisual congruency on short-term memory of serial spatial stimuli: A pilot test","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54200","This paper is motivated from the question if the use of spatial sounds enhances learning in multi-modal teaching aids. In a basic pilot study the consolidation of serial stimuli was tested for unimodal conditions (visual; auditory) and a bimodal condition (spatially congruent, audiovisual). In contrary to our hypothesis, the audiovisual condition did not show better results than the visual one. In this particular test the auditory display was clearly inferior to the two other ones.","2015-07","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","The effect of audiovisual congruency on short-term memory of serial spatial stimuli","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KZTDFSEC","journalArticle","2005","Droumeva, Milena","Understanding immersive audio: A historical and socio-cultural exploration of auditory displays","","","","","http://hdl.handle.net/1853/50196","This paper examines the historical and socio-cultural underpinnings of immersive audio seen from the paradigms of Acoustic Ecology and Acoustic Communication. The paper offers the view that in order to understand the many implications of immersive sound, both from a design perspective and from a cultural studies perspective, we need to first examine its social and technological histories. The paper explores examples and concepts of immersive sound from the natural, electroacoustic, digital and interactive domains, and presents a case study of Ec(h)o - a real audioaugmented immersive museum installation.","2005-07","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","Understanding immersive audio","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M3DIKVYF","book","2009","Schoon, Andi; Dombois, Florian","Sonification in Music","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51415","Seen from the vantage point of cultural history, contemporary sonification is essentially characterised by two aspects that to date have seldom been considered in combination: the first aspect is sonification as the transformation of the inaudible into the sphere of the audible, and the second its use as an instrument for gaining knowledge via the concrete listening experience. One of the aims of the research project “Denkgeräusche” (""Sounds of Thought""), conducted at Bern University of the Arts, was to make a contribution to a (yet unwritten) cultural history of sonification. With this target in mind, a database was compiled of historical and contemporary musical compositions in which procedures associated with sonification were employed.","2009-05","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MRSLEK4B","journalArticle","2008","Fassbender, Eric; Richards, Debbie; Thompson, Bill; Bilgin, Ayse; Taylor, Alan","The effect of Music on Learning in Virtual Environments - Initial Results","","","","","http://hdl.handle.net/1853/49959","In this paper we discuss the effect of music on learning in virtual-immersive environments. Auditory stimuli were presented within a specialized display system in which seventy- two undergraduate students watched a 3-D computer animated video. This video included narrated historical information and background music. Background music was experimentally manipulated to create six versions of the video: No Music; original tempo/pitch; fast-tempo music; slow-tempo music; low-pitched music; and high-pitched music. After watching the video, participants were tested on the historical material presented, and answered a number of other questions about the video and background music. Data analyses revealed that people under the influence of one particular computer game soundtrack remembered information better than their peers at statistically significant level. Also, those participants who were more immersed into the virtual environment performed significantly better. Further encouraging results are reported in this paper, however, follow-up experiments are needed.","2008-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5SNSFM7M","journalArticle","2013","Jeon, Myounghoon; Walker, Bruce N.; Bruce, Carrie M.","Science or art? “Sonification in the age of biocybernetics reproduction”: A case study of the Accessible Aquarium Project","","","","","http://hdl.handle.net/1853/51566","With digital art being pervasive, art, technology, and science seem to be no longer separable and have been re-integrated. In fact, art history shows that when combined with them, art could give birth to a ground-breaking masterpiece. Based on that, we pose a simple question, “Can we analyze sonification works from the viewpoint of digital art aesthetics?” As a case study, we try to place the Accessible Aquarium Project (AAP) at the intersection of scientific research and art. Relying on term, “biocybernetics”, we discuss aesthetic meanings of the AAP in terms of new temporality (dynamicity), transformed relationships (combined gazes), dialectic improvement of the original (interactivity), and enacted collective art-work (embodied cognition). We hope this review will help illuminate the artistic contribution of interactive sonification and explore future directions. Further, this work is expected to contribute to facilitating discussions of aesthetics about the sonification works in the auditory display community.","2013-07","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","Science or art?","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MXMYRBPR","journalArticle","1996","Porcaro, Nick; Putnam, William; Scandalis, Pat; Stilson, Tim; Jaffe, David; Smith, Julius; Van Duyne, Scott","SynthBuilder and Frankenstein: Tools for the creation of musical physical models","","","","","http://hdl.handle.net/1853/50815","SynthBuilder is a user-extensible, object-oriented, NEXTSTEP Music Kit application for interactive real-time design and performance of synthesizer patches (especially physical models) (Jaffe & Smith, 1983; Smith, 1987). Patches are represented by networks consisting of digital signal processing elements called unit generators and MIDI event elements called note filters and note generators. The Frankenstein box is a multi-DSP compute engine that was developed as a research platform (Putnam, 1996). The box communicates with an x86 host via an ISA interface card that resides in the host computer. Frankenstein currently contains 8 Motorola 56002 Evaluation Modules (EVMs), and it can be scaled to an additional 8 EVMs for a 16 EVM total. The outputs of EVMs can also be sent to an external mixer.","1996-11","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","SynthBuilder and Frankenstein","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RCQK43QM","book","2010","Alexander, Robert; Umbaugh, John; Turley, Patrick","An Overview of Algorithmic Music Composition in the Noteworks Software Platform","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49752","Noteworks is music composition software that re-imagines the way music is created, played, and shared. Users create musical compositions by building networks and interacting with them in real time. Noteworks reduces the learning curve for algorithmic-music composition, such that most individuals with a basic knowledge of computer interaction can create original compositions with limited instruction. Dynamic networks have the potential to play back for hours without repeating. This document will provide a brief summary overview of the GUI.","2010-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ET2LB5JN","book","2009","Groux, Sylvain Le; Verschure, Paul","Neuromuse: training your brain through musical interaction","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51404","Human aural system is arguably one of the most refined sensor we posess. It is sensitive to such highly complex stimuli as conversa- tions or musical pieces. Be it a speaking voice or a band playing live, we are able to easily perceive relaxed or agitated states in an auditory stream. In turn, our own state of agitation can now be detected via electroencephalography technologies. In this pa- per we propose to explore both ideas in the form of a framework for conscious learning of relaxation through sonic feedback. Af- ter presenting the general paradigm of neurofeedback, we describe a set of tools to analyze electroencephalogram (EEG) data in real- time and we introduce a carefully designed, perceptually-grounded interactive music feedback system that helps the listener keeping track of and modulate her agitation state as measured by EEG.","2009-05","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","Neuromuse","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B7UQL45M","journalArticle","2005","Pape, Daniel","Is pitch perception and discrimination of vowels language-dependent and influencd by the vowels spectral properties?","","","","","http://hdl.handle.net/1853/50165","Pitch discrimination and accuracy has been found to depend on different factors. However, little work has been done (1) on the cross-linguistic influence of the listeners' native language and (2) on the influence of the spectral structure on the pitch perception of vowels as well as (3) cross-linguistic differences regarding different levels of muscial education. If differences in pitch discrimination between different language families exist this would be a crucial knowledge in the design and failure-safe application of auditory displays driven by pitch differences in speech control. Therefore the current study examines pitch discrimination of German vowels with a similar vowel height differing in rounding and tenseness for (1) native German listeners and (2) native Catalan listeners. Significant differences in the sensitivity of pitch perception between these two languages were found. Catalan listeners, independent of their musical education, were mostly insensitive to even large pitch differences in the vowels to be judged. The accuracy of pitch judgements for German listeners were significantly different for musically educated listeners in comparison to musically uneducated listeners. Further, both languages show a significant pitch difference for rounded vowels compared to the unrounded vowels. The current study provides evidence that pitch discrimination is language-dependent, at least partially.","2005-07","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I4AXXATD","journalArticle","2017","Andreopoulou, Areti; Goudarzi, Visda","Reflections on the Representation of Women in the International Conferences on Auditory Displays (ICAD)","","","","","http://hdl.handle.net/1853/58362","This paper investigates the representation of women researchers and artists in the conferences of the International Community for Auditory Display (ICAD). In the absence of an organized membership mechanism and / or publicly available records of conference attendees, this topic was approached through the study of publication and authorship patterns of female researchers in ICAD conferences. Temporal analysis showed that, even though there has been an increase in the number of publications co-authored by female researchers, the annual percentage of female authors remained in relatively unchanged levels (mean = 17.9%) throughout the history of ICAD conferences. This level, even though low, remains within the reported percentages of female representation in other communities with related disciplines, such as the International Computer Music Association (ICMA) and the Conferences of the International Society for Music Information Retrieval (ISMIR), and significantly higher than in more audio engineering-related communities, such as the Audio Engineering Society (AES).","2017-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9E3AKP7F","journalArticle","2008","Baumann, Alex; Grinstein, Georges","Haptic Sound: Expressive Control of Song Playback Using Haptics","","","","","http://hdl.handle.net/1853/49935","Haptics devices are an excellent tool to add user feel and expression to applications. For music software they can provide realistic simulations of real world instruments, or heighten the experience of interacting with musical objects. This paper focuses on an application that uses haptics as a means of interacting with pre-recorded pieces of music allowing the user to navigate and add personal expression. Simplicity and ease of use were keys to the design so that people without any prior musical knowledge were able to create and interact with music right away.","2008-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","Haptic Sound","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V59TCJBP","journalArticle","2014","Jeon, Myounghoon; Sun, Yuanjing","Design and Evaluation of Lyricons (Lyrics + Earcons) for Semantic and Aesthetic Improvements of Auditory Cues","","","","","http://hdl.handle.net/1853/52086","Auditory researchers have developed various non-speech cues in designing auditory user interfaces. A preliminary study of “Lyricons” (lyrics + earcons) has provided a novel approach to devising auditory cues in electronic products, by combining the concurrent two layers of musical speech and earcons (short musical motives). The purpose of the present study is to introduce iterative design processes and to validate the effectiveness of lyricons compared to earcons, whether people can more intuitively grasp functions that lyricons imply than those of earcons. Results favor lyricons over earcons. Future work and practical application directions are also discussed.","2014-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AMS7KCQT","journalArticle","2011","Worrall, David","A method for developing an improved mapping model for data sonification","","","","","http://hdl.handle.net/1853/51570","The unreliable detection of information in sonifications of multivariate data that employ parameter mapping is generally thought to be the result of the co-dependency of psychoacoustic dimensions. The method described here is aimed at discovering whether the perceptual accuracy of such information can be improved by rendering the sonification of the data with a mapping model influenced by the gestural metrics of performing musicians playing notated versions of the data. Conceptually, the Gesture-Encoded Sound Model (GESM) is a means of transducing multivariate datasets to sound synthesis and control parameters in such as way as to make the information in those datasets available to general listeners in a more perceptually coherent and stable way than is currently the case. The approach renders to sound a datastream not only using observable quantities (inverse transforms of known psychoacoustic principles), but latent variables of a Dynamic Bayesian Network trained with gestures of the physical body movements of performing musicians and hypotheses concerning other observable quantities of their coincident acoustic spectra. If successful, such a model should significantly broaden the applicability of data sonification as a perceptualisation technique.","2011-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HG87SEZ8","journalArticle","2007","Payling, Dave; Mills, Stella; Howle, Tim","Hue Music - Creating Timbral Soundscapes from Coloured Pictures","","","","","http://hdl.handle.net/1853/50000","This paper discusses a technique to convert 2 dimensional images into music by associating hue values with timbres. The colour information in 2D still images was used to drive an 8 channel timbral audio mixer. 8 musical timbres were recorded to represent 8 hue values and these timbres were changed in amplitude dependent on the quantity of each hue in the image. A maximum of 64 blue 'pixel-blocks' for example resulted in a sound that was exclusively the timbre associated with blue. The technique is successful for enhancing some images but not others but after some familiarisation it is possible to distinguish the timbres and therefore to identify the colour composition of the picture through audio alone.","2007-06","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TWRLWRSE","journalArticle","1998","Cook, Perry R.; Essl, Georg; Tzanetakis, Georgos; Trueman, Dan","N\textgreater\textgreater2: Multi-speaker display systems for virtual reality and spatil audio projection","","","","","http://hdl.handle.net/1853/50726","This paper describes multi-speaker display systems for immersive auditory environments, collaborative projects, realistic acoustic modeling, and live musical performance. Two projects are described. The sound sub-system of the Princeton Display Wall project, and the NBody musical instrument body radiation response project. The Display Wall is an 18' x 8' rear-projection screen, illuminated by 8 high-resolution video projectors. Each projector is driven by a 4-way symmetric-multi-processor PC. The audio sub-system of this project involves 26 loudspeakers and server PCs to drive the speakers in real time from soundfile playback, audio effects applied to incoming audio streams, and parametric sound synthesis. The NBody project involves collecting and using directional impulse responses from a variety of stringed musical instruments. Various signal processing techniques were used to investigate, factor, store, and implement the collected impulse responses. A software workbench was created which allows virtual microphones to be placed around a virtual instrument, and then allows signals to be processed through the resulting derived transfer functions. Multi-speaker display devices and software programs were constructed which allow real-time application of of the filter functions to arbitrary sound sources. This paper also discusses the relation of spherical display systems to conventional systems in terms of spatial audio and sound-field reconstruction, with the conclusion that most conventional techniques can be used for spherical display systems as well.","1998-11","2023-07-13 06:25:41","2023-07-13 06:25:41","2023-07-12","","","","","","","N\textgreater\textgreater2","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EISEHB5M","journalArticle","2016","Khan, Ridwan A.; Avvari, Ram K.; Wiykovics, Katherine; Ranay, Pooja; Jeon, Myounghoon","LifeMusic: Reflection Of Life Memories By Data Sonification","","","","","http://hdl.handle.net/1853/56590","Memorable life events are important to form the present selfimage. Looking back on these memories provides an opportunity to ruminate meaning of life and envision future. Integrating the life-log concept and auditory graphs, we have implemented a mobile application, ""LifeMusic"", which helps people reflect their memories by listening to their life event sonifcation that is synchronous to these memories. Reflecting the life events through LifeMusic can relieve users of the present and have them journey to the past moments and thus, they can keep balance of emotions in the present life. In the current paper, we describe the implementation and workflow of LifeMusic and briefly discuss focus group results, improvements, and future works.","2016-07","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","LifeMusic","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LPJ7BIZ9","journalArticle","2018","Middleton, Jonathan; Hakulinen, Jaakko; Tiitinen, Katariina; Hella, Juhu; Keskinen, Tuuli; Huuskonen, Pertti; Linna, Juhani; Turunen, Markku; Ziat, Mounia; Raisamo, Roope","Sonification with musical characteristics: a path guided by user engagement","","","","","http://hdl.handle.net/1853/60071","Sonification with musical characteristics can engage users, and this dynamic carries value as a mediator between data and human perception, analysis, and interpretation. A user engagement study has been designed to measure engagement levels from conditions within primarily melodic, rhythmic, and chordal contexts. This paper reports findings from the melodic portion of the study, and states the challenges of using musical characteristics in sonifications via the perspective of form and function – a long standing debate in Human-Computer Interaction. These results can guide the design of more complex sonifications of multivariable data suitable for real life use.","2018-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","Sonification with musical characteristics","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AA5KHKU4","journalArticle","2011","Grimm, Giso; Bracher, Claire; Frey, Laura; Veto, Julia; Harders, Claas","Harmony of the Spheres: Cosmology and Number Aesthetics in 16th and 20th Century Music for Viola da Gamba","","","","","http://hdl.handle.net/1853/51699","Harmony of the Spheres is a concert programme based on cosmology during the renaissance. Historic philosophical and astronomical concepts from ancient Greece until the modern age are accessed through the music of the 16th and 20th century. The five musicians of the ensemble OR- LANDOviols play on violas da gamba of several sizes, but also employ the potential of electronics and digital signal processing to craft musical concepts in space. Low-delay real-time spatial processing is applied to the instruments and presented via a multi-channel speaker setup.","2011-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","Harmony of the Spheres","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U8X34HK5","journalArticle","2013","Avissar, Daniel; Leider, Colby; Bennett, Hristopher; Gailey, Robert","An audio game app using interactive Movement sonification for Targeted posture control","","","","","http://hdl.handle.net/1853/51640","Interactive movement sonification has been gaining validity as a technique for biofeedback and auditory data mining in research and development for gaming, sports, and physiotherapy. Naturally, the harvesting of kinematic data over recent years has been a function of an increased availability of more portable, high-precision sensory technologies, such as smart phones, and dynamic real time programming environments, such as Max/MSP. Whereas the overlap of motor skill coordination and acoustic events has been a staple to musical pedagogy, musicians and music engineers have been surprisingly less involved than biomechanical, electrical, and computer engineers in research efforts in these fields. Thus, this paper proposes a prototype for an accessible virtual gaming interface that uses music and pitch training as positive reinforcement in the accomplishment of target postures.","2013-07","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UWZVHYM8","book","2015","Walus, Bartlomiej P.; Pauletto, Sandra; Mason-Jones, Amanda","Evaluating the use of sonification and music to support the communication of alcohol health risk to young people: Initial results","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54151","The interdisciplinary research project, Using Sonification to COmmunicate public health Risk data (SCORe), aims to experimentally test how sonification, interactivity in combination with music, could increase the communicative potential of a visual presentation directed to young people and focused on health risk data of alcohol consumption. Specifically, we are studying how this type of presentation can support engagement with health information, and the effective interpretation and recall of data. In order to explore the possible influence of sound in understanding important health risk messages, a 3-arm pilot randomised control (participant-blinded) trial was designed. We compared a visual presentation augmented by sonification, music and interaction with a simple visual presentation and a visual presentation augmented by simple user interaction. This paper describes the most complex of the three health presentations (the audio-visual and interactive presentation) and presents initial findings that relate to this presentation only.","2015-07","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","Evaluating the use of sonification and music to support the communication of alcohol health risk to young people","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5DJDB5YL","journalArticle","2008","Polotti, Pietro; Benzi, Carlo","Rhetorical Schemes for Audio Communication","","","","","http://hdl.handle.net/1853/49944","The application of rhetorical techniques to the use of non-verbal sound in the interaction between humans and technologies is the core idea of this paper. We present our ideas at a general level and illustrate an exploratory case based on the application of rhetorical schemes to the sonification of computer operating system events. Both cases of musical sounds and everyday sounds are investigated. This work is intended as a preliminary study aiming at motivating a larger scale and more rigorous research about the potentiality of the use of rhetoric in the domain of Auditory Display (AD) and Sonic Interaction Design (SID).","2008-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"828B39BD","journalArticle","2003","Quinn, Marty; Quinn, Wendy; Hatcher, Ben","For those who died: A 9/11 tribute","","","","","http://hdl.handle.net/1853/50465","For Those Who Died is a 6-minute dance and music tribute to those who died on 9/11. It premiered 9/11/2002 as part of a larger event entitled “Reflections, A Gift to the Community” that occurred at the Music Hall in Portsmouth NH. It features extensive textual sonification, dance, and two layers of visual presentation of nine textual datasets containing the names of those who died on September 11, 2001, the US Constitution and some DNA from chromosomes 1 and 9. The music was produced using five Yamaha QY70 and QY100 synthesizers and presented in surround sound.","2003-07","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","For those who died","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CYF7R7W8","journalArticle","2008","Larkin, Oliver; Koerselman, Thijs; Ong, Bee; Ng, Kia","Sonification of Bowing Features for String Instrument Training","","","","","http://hdl.handle.net/1853/49953","This paper presents work on an auditory display for use in string instrument training, based on 3D motion analysis. We describe several sonifications that are intended to provide both real-time and non real-time feedback about bowing technique. Tests were conducted with string players to assess the effectiveness of the sonifications. We discuss our findings as well as ideas for further work in this area.","2008-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z9DM59PK","journalArticle","2019","Neuhoff, John G.","Is sonification doomed to fail?","","","","","http://hdl.handle.net/1853/61531","Despite persistent research and design efforts over the last twenty years, widespread adoption of sonification to display complex data has largely failed to materialize, and many of the challenges to successful sonification identified in the past persist. Major impediments to the widespread adoption sonification include fundamental perceptual differences between vision and audition, large individual differences in auditory perception, musical biases of sonification researchers, and the interdisciplinary nature of sonification research and design. The historical and often indiscriminate mingling of art and science in sonification design may be a root cause of some of these challenges. Future sonification design efforts that explicitly strive to meet either artistic or scientific goals may lead to greater clarity and success in the field and more widespread adoption of useful sonification techniques.","2019-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T7Z249U5","journalArticle","2005","Mauney, Lisa M.","Individual differences in interpreting auditory graphs","","","","","http://hdl.handle.net/1853/50164","Very little research has been done on the role of individual differences in the interpretation of auditory graphs. Research with the visually impaired, musicians, and college students point to interesting differences in the way sound is interpreted. However, in order for auditory graphs to be successful, a more thorough understanding of individual differences is needed. This paper proposes a series of experiments that look at cognitive abilities, musical abilities, and other demographics in college students and the visually impaired. The author, however, stresses the importance of collaborating with other researchers to obtain data on other groups of people.","2005-07","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BBX9J76N","book","2015","Winters, R. Michael; Weinberg, Gil","Sonification of the Tohoku earthquake: Music, popularization & the auditory sublime","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54149","The past century has witnessed the emergence of expressive musical forms that originate in appropriated technologies and practices. In most cases, this appropriation is performed without protest— but not always. Carefully negotiating a space for sound as an objective, scientific medium, the field of sonification has cautiously guarded the term from subjective and affective endeavors. This paper explores the tensions arising in sonification popularization through a formal analysis of Sonification of the Tohoku Earthquake, a two-minute YouTube video that combined audification with a time-aligned seismograph, text and heatmap. Although the many views the video has received speak to a high public impact, the features contributing to this popularity have not been formalized, nor the extent to which these characteristics further sonifications’ scientific mission. For this purpose, a theory of popularization based upon “sublime listening experiences” is applied. The paper concludes by drawing attention to broader themes in the history of music and technology and presents guidelines for designing effective public-facing examples.","2015-07","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","Sonification of the Tohoku earthquake","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KZW2H4R3","journalArticle","1996","Tkaczevski, Alejandro","Auditory interface problems and solutions for commercial multimedia products","","","","","http://hdl.handle.net/1853/50801","This paper will explore the various aesthetic, technical, and musical issues that sound designers face when creating audio for commercial products. I will draw from my experience at and use materials from Broderbund Software. In the first section, I will discuss issues concerning interface sonification. In the second section, I will briefly illustrate a quick musical solution to a potentially large logistical problem. In the last section, I will show a solution for balancing digital audio and MIDI on a variety of devices, drivers, and operation systems.","1996-11","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6KLGE9GK","journalArticle","2011","Grohn, Matti; Ahonen, Lauri; Huotilainen, Minna","Some Effects of Continous Tempo and Pitch Transformations in Perceived Pleasantness of Listening to a Musical Sound File","","","","","http://hdl.handle.net/1853/51745","Physiological changes, i.e., changes in the heart rate and its variability, respiratory patterns etc., are induced by listening to mu- sic. Both the low-level acoustic features of the sound and the per- ceived pleasantness of the music contribute to the type and strength of the physiological changes. Thus, for studying the effects of the pleasantness, it would be important to keep the low-level acoustic features as similar as possible in the sound samples. In this project we explored the effects of continuous tempo and pitch transformations in perceived pleasantness of listening to a musical sound file. Subjects evaluated pleasantness accord- ing to seven step Likert scale in which one is the most unpleasant and seven is the most pleasant. According to subjects’ judgment the changes in tempo affects less to perceived pleasantness than changes in pitch.","2011-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BXTEQLDY","journalArticle","2019","Kleinberger, Rebecca; George, Stefanakis; Franjou, Sebastian","Speech companions: Evaluating the effects of musically modulated auditory feedback on the voice","","","","","http://hdl.handle.net/1853/61511","Changing the way one hears one's own voice, for instance by adding delay or shifting the pitch in real-time, can alter vocal qualities such as speed, pitch contour, or articulation. We created new types of auditory feedback called Speech Companions that generate live musical accompaniment to the spoken voice. Our system generates harmonized chorus effects layered on top of the speakerﾒs voice that change chord at each pseudo-beat detected in the spoken voice. The harmonization variations follow predetermined chord progressions. For the purpose of this study we generated two versions: one following a major chord progression and the other one following a minor chord progression. We conducted an evaluation of the effects of the feedback on speakers and we present initial findings assessing how different musical modulations might potentially affect the emotions and mental state of the speaker as well as semantic content of speech, and musical vocal parameters.","2019-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","Speech companions","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UVVFVTM3","journalArticle","1998","Leplatre, Gregory; Brewster, Stephen A.","An investigation of using music to provide navigation cues","","","","","http://hdl.handle.net/1853/50713","This paper describes an experiment that investigates new principles for representing hierarchical menus such as telephone-based interface menus, with non-speech audio. A hierarchy of 25 nodes with a sound for each node was used. The sounds were designed to test the efficiency of using specific features of a musical language to provide navigation cues. Participants (half musicians and half non-musicians) were asked to identify the position of the sounds in the hierarchy. The overall recall rate of 86% suggests that syntactic features of a musical language of representation can be used as meaningful navigation cues. More generally, these results show that the specific meaning of musical motives can be used to provide ways to navigate in a hierarchical structure such as telephone-based interfaces menus.","1998-11","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YKJTCZGG","book","2010","Diniz, Nuno; Deweppe, Alexander; Demey, Michiel; Leman, Marc","A Framework for Music-based Interactive Sonification","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49768","In this paper, a framework for interactive sonification is introduced. It is argued that electroacoustic composition techniques can provide a methodology for structuring and presenting multivariable data through sound. Furthermore, an embodied music cognition driven interface is applied to provide an interactive exploration of the generated music-based output. The motivation and theoretical foundation for this work are presented as well as the framework’s implementation and an exploratory use case.","2010-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QY4HBBXT","journalArticle","2013","Henry, Ashley G.; Bruce, Carrie M.; Winton, Riley J.; Walker, Bruce N.","Sonification Mapping Configurations: Pairings Of Real-Time Exhibits And Sound","","","","","http://hdl.handle.net/1853/51679","Visitors to aquariums typically rely on their vision to interact with live exhibits that convey rich descriptive and aesthetic visual information. However, some visitors may prefer or need to have an alternative interpretation of the exhibitÕs visual scene to improve their experience. Musical sonification has been explored as an interpretive strategy for this purpose and related work provides some guidance for sonification design, yet more empirical work on developing and validating the music-to-visual scene mappings needs to be completed. This paper discusses work to validate mappings that were developed through an investigation of musician performances for two specific live animal exhibits at the Georgia Aquarium. In this proposed study, participants will provide feedback on musical mapping examples which will help inform design of a real-time sonification system for aquarium exhibits. Here, we describe our motivation, methods, and expected contributions.","2013-07","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","Sonification Mapping Configurations","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"84ZINU55","journalArticle","1996","Metois, Eric; Back, Maribeth","BROWeb: An interactive collaborative auditory environment on the world wide web","","","","","http://hdl.handle.net/1853/50802","We describe an infrastructure for a real-time shared auditory environment on the World Wide Web (WWW). The system consists of the BROWeb ""star-shaped"" Web server and the StarClient Java class and interface. The BROWeb server is designed to facilitate implementation of Java applets wherein users see and hear each other's activity. Developed as an interactive music system for public performance, BROWeb is robust enough to support hundreds of users in a demanding application. We also discuss design issues regarding the actual experiences that a Java applet overlying this infrastructure can deliver. We created, for instance, a privileged user–a ""director""–who can make decisions affecting the overall system's performance and also the data streams from other users. As for the sound source of the actual experience, we present two alternatives that we have tested: local sounds that download when the experience begins and streamed audio (RealAudio, Xing, or other network broadcasting tool).","1996-11","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","BROWeb","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TLY9XYBN","journalArticle","1998","Vickers, Paul; Alty, James L.","Towards some organising principles for musical progrm auralisations","","","","","http://hdl.handle.net/1853/50735","Early studies have shown that musical program auralisations can convey structural and run-time information about Turbo Pascal programs to listeners [3, 4, 10]. Auralisations were effected by mapping program events and structures to musical signature tunes, known as motifs. The design of the motifs was based around the taxonomical nature of the Turbo Pascal language constructs [3]. However, it became clear that as the musical complexity and grammatical rigour of the motifs increased, their discernability by the average user decreased. Therefore, from the lessons learnt from our work we propose a set of organising principles for the design and construction of musically-based program auralisations. These organising principles are aimed towards providing accessible auralisations to the average programmer who has no formal musical training.","1998-11","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DK355DJJ","journalArticle","2021","Roddy, Stephen; Bridges, Brian","The design of a smart city sonification system using a conceptual blending and musical framework, web audio and deep learning techniques","","","","","http://hdl.handle.net/1853/66352","This paper describes an auditory display system for smart city data for Dublin City, Ireland. It introduces and describes the different layers of the system and outlines how they operate individually and interact with one another. The system uses a deep learning model called a variational autoencoder to generate musical content to represent data points. Further data-to-sound mappings are introduced via parameter mapping sonification techniques during sound synthesis and post-processing. Conceptual blending and music theory provide frameworks, which govern the design of the system. The paper ends with a discussion of the design process that contextualizes the contribution, highlighting the interdisciplinary nature of the project, which spans data analytics, music composition and human-computer interaction.","2021-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N2MUES54","journalArticle","2005","Beilharz, Kirsty","Wireless gesture controllers to affect information sonification","","","","","http://hdl.handle.net/1853/50198","This paper proposes a framework for gestural interaction with information sonification in order to both monitor data aurally and, in addition, to interact with it, transform and even modify the source data in a two-way communication model (Figure 1). Typical data sonification uses automatically generated computational modelling of information, represented in parameters of auditory display, to convey data in an informative representation. It is essentially a one-way data to display process and interpretation by users is usually a passive experience. In contrast, gesture controllers, spatial interaction, gesture recognition hardware and software, are used by musicians and in augmented reality systems to affect, manipulate and perform with sounds. Numerous installation and artistic works arise from motion-generated audio. The framework developed in this paper aims to conflate those technologies into a single environment in which gestural controllers allow interactive participation with the data that is generating the sonification, making use of the parallel between spatial audio and spatial (gestural) interaction. Converging representation and interaction processes bridge a significant gap in current sonification models. A bi-modal generative sonification and visualisation example from the author's sensate laboratory illustrates mappings between socio-spatial human activity and display. The sensor cow project, using wireless gesture controllers fixed to a calf, exemplifies some real time computation and representation issues to convey spatial motion in an easily recognised sonification, suitable for ambient display or intuitive interaction.","2005-07","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YYZ3ECSJ","book","2010","Leonard, Neil","Sonification: Celestial Data and Poetic Inquiry","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/50058","This paper describes a composition/sonification project to be realized by faculty and students from the Electronic Production and Design department (EP/D) at Berklee College of Music in Boston. The goal of the project is compose music for a 30- minute interdisciplinary-networked performance to be premiered in Boston, Lyon and Havana involving artists from each city. In the process, artists are examining new modes of expression and the construction of knowledge and artistic dialog. Kelly Snook, Ph.D. Astrophysicist, Division of Solar System Exploration, NASA Goddard Spaceflight Center is working with the group to choose scientific data for sonification including compelling new planetary science and solar system data.","2010-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","Sonification","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7GYZAGKY","journalArticle","2017","Marins, Paulo R. A.","Challenges and Constraints of Using Audio in Online Music Education","","","","","http://hdl.handle.net/1853/58366","Several online music courses have been developed lately by educational companies. In addition, many universities have been offering music online degree programs. Since these courses and programs are taught through distance education, many ICTs are used such as: recorded video, online software, social networks, and audio. Although audio is widely used in the online courses and degree programs that aim to teach applied music, only a few research reports have been published recently about this subject. This paper intends to clarify – through a literature review - some questions concerning this use and also aims to provide a discussion regarding the challenges and constraints of using audio in online applied music lessons. It is also hoped that the discussions made in this paper may lead to the development of research in the area of online music education as well as in the specific field of sound in learning.","2017-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5MRLM3LS","journalArticle","2017","Fox, K. Michael; Stewart, Jeremy; Hamilton, Rob","madBPM: Musical and Auditory Display for Biological Predictive Modeling","","","","","http://hdl.handle.net/1853/58367","The modeling of biological data can be carried out using structured sound and musical process in conjunction with integrated visualizations. With a future goal of improving the speed and accuracy of techniques currently in use for the production of synthetic high value chemicals through the greater understanding of data sets, the madBPM project couples real-time audio synthesis and visual rendering with a highly flexible data-ingestion engine. Each component of the madBPM system is modular, allowing for customization of audio, visual and data-based processing.","2017-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","madBPM","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V3QGVXD3","book","2015","Chechile, Alex","Creating spatial depth using distortion product optoacoustic emissions in music composition","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54100","Distortion product otoacoustic emissions (also known as combination tones) are sounds generated within the listener’s ears upon physical and physiological interactions between spectral components in a given auditory input. The relationship between sounds generated by loudspeakers and sounds generated in the listener's ears offers fertile ground for the exploration of spatial depth in music. The author’s On the Sensations of Tone is a series of compositions that provoke ear tones to create an additional spatial dimension in each piece. Following a brief overview of the history and mechanisms behind otoacoustic emissions, this paper discusses techniques for composing with the phenomenon as used in On the Sensations of Tone VII (2015).","2015-07","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WMXTFTF9","journalArticle","2016","Landry, Steven; Sun, Yuangjing; Slade, Darnishia; Jeon, Myounghoon","Tempo-Fit Heart Rate App: Using Heart Rate Sonification As Exercise Performance Feedback","","","","","http://hdl.handle.net/1853/56567","Physical inactivity is a worldwide issue causing a variety of health problems. Exploring novel ways to encourage people to engage in physical activity is a topic at the forefront of research for countless stakeholders. Based upon a review of the literature, a pilot study, and exit interviews, we propose an app prototype that utilizes music tempo manipulation to guide users into a target heart rate zone during an exercise session. A study was conducted with 26 participants in a fifteen-minute cycling session using different sonification mappings and combinations of audiovisual feedback based on the user's current heart rate. Results suggest manipulating the playback speed of music in real time based on heart rate zone departures can be an effective motivational tool for increasing or decreasing activity levels of the listener. Participants vastly preferred prescriptive sonifications mappings over descriptive mappings, due to people's natural inclination to follow the tempo of music.","2016-07","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","Tempo-Fit Heart Rate App","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I9A66E97","journalArticle","2011","Mealla, Sebastian; Bosi, Mathieu; Jorda, Sergi; Valjamae, Aleksander","Sonification of Brain and Body Signals in Collaborative Tasks Using a Tabletop Musical Interface","","","","","http://hdl.handle.net/1853/51747","Physiological Computing has been applied in different disciplines, and is becoming popular and widespread in Human-Computer In- teraction, due to device miniaturization and improvements in real- time processing. However, most of the studies on physiology- based interfaces focus on single-user systems, while their use in Computer-Supported Collaborative Work (CSCW) is still emerg- ing. The present work explores how sonification of human brain and body signals can enhance user experience in collaborative mu- sic composition. For this task, a novel multimodal interactive sys- tem is built using a musical tabletop interface (Reactable) and a hybrid Brain-Computer Interface (BCI). The described system al- lows performers to generate and control sounds using their own or their fellow team member’s physiology. Recently, we assessed this physiology-based collaboration system in a pilot experiment. Dis- cussion on the results and future work on new sonifications will be accompanied by practical demonstration during the conference.","2011-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WNXL7PX5","book","2009","Allali, Julien; Hanna, Pierre; Robine, Matthias; Ferraro, Pascal","Extending alignment algorithm for polyphonic comparison","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51300","Existing symbolic music comparison systems generally consider monophonic music or monophonic reduction of polyphonic music. Adaptation of alignment algorithms to the music leads to accurate systems, but extensions to polyphonic music arise new problems. Indeed, a chord may match several notes, or the difference be- tween two similar motifs may be a few swapped notes. Moreover, it is difficult to set up the substitution scores between chords. In this paper, we propose a general framework for polyphonic mu- sic which permits to directly apply the substitution score scheme set for monophonic music, and which allows new operations by extending the operations proposed by Mongeau and Sankoff [1]. From a practical point of view, the limitations of the size of chords and the number of notes that can be merged lead to a complexity that remains quadratic.","2009-05","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VJQC4BB7","journalArticle","2006","Liljedahl, M.; Lefford, N.; Lindberg, S.","Digiwall - an audio mostly game","","","","","http://hdl.handle.net/1853/50589","DigiWall is a hybrid between a climbing wall and a computer game. The climbing grips are equipped with touch sensors and lights. The interface has no computer screen. Instead sound and music are principle drivers of DigiWall interaction models. The gaming experience combines sound and music with physical movement and the sparse visuals of the climbing grips. The DigiWall soundscape carries both verbal and nonverbal information. Verbal information includes instructions on how to play a game, scores, level numbers etc. Non-verbal information is about speed, position, direction, events etc. Many different types of interaction models are possible: competitions, collaboration exercises and aesthetic experiences.","2006-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GDMX8KLG","journalArticle","2011","Tissberger, Johann P.; Wersenyi, Gyorgy","Sonification Solutions for Body Movements in Rehabilitation of Locomotor Disorders","","","","","http://hdl.handle.net/1853/51751","One of the recent fields of sonification focuses on the sonification of body movements in sports or rehabilitation. This is usually some kind of monitoring of real-time measurement data and auditory feedback for the patient. This paper presents two sonification approaches in medicine: a balancing coordination system and a robot for moving the legs after serious injuries of the lower body parts. These two systems are evaluated and compared based on the method of sonification, and transmission and analysis of the auditory information. Finally, a supposed method for using musical notes and measures is presented, and a selection method for the length of sonification based on the initial time interval is suggested.","2011-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MNZNBPLM","journalArticle","2013","Lukasik, Ewa; Materski, Michal","Sonification Of A Virtual Model Of The Old Rare Musical Instrument","","","","","http://hdl.handle.net/1853/51683","The paper describes a project whose goal was to enable users realistically interact with a 3D virtual model of a historical musical instrument – the clavichord attributed to the famous 18th century maker. A challenge of enabling the user to play the virtual copy of the instrument was resolved by using the dynamic MIDI keyboard. The prerecorded clavichord sound samples are controlled by the software using the MIDI commands. Playing the real keyboard is synchronized with the movement of virtual keys and other parts of sound generating mechanism. The sound effects, characteristic for the clavichord Tragen der Tonne and Bebung may be imitated, which gives the user the impression of playing the real instrument.","2013-07","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5AJX6683","journalArticle","2014","Adeli, Mohammad; Rouat, Jean; Molotchnikoff, Stéphane","On the Importance of Correspondence Between Shapes and Timbre","","","","","http://hdl.handle.net/1853/52093","The results of a preliminary study of the audio-visual correspondence between musical timbre and visual shapes are reported. 22 participants had to play 20 musical sounds and choose a shape for each. An association between timbre and visual shapes emerged. Soft timbres seem to match with rounded shapes, harsh timbres with sharp angular shapes and timbres having elements of softness and harshness together with a mixture of the two previous shapes. The correspondence between timbres and shapes should lend itself to the development of perceptually supported musical interfaces and substitution systems. A larger scale experiment with more sounds and participants is underway and confirms the preliminary results reported in this paper.","2014-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9D7NMVEZ","journalArticle","2017","Landry, Steven; Jeon, Myounghoon","Participatory Design Research Methodologies: A Case Study in Dancer Sonification","","","","","http://hdl.handle.net/1853/58379","Given that embodied interaction is widespread in Human-Computer Interaction, interests on the importance of body movements and emotions are gradually increasing. The present paper describes our process of designing and testing a dancer sonification system using a participatory design research methodology. The end goal of the dancer sonification project is to have dancers generate aesthetically pleasing music in real-time based on their dance gestures, instead of dancing to pre-recorded music. The generated music should reflect both the kinetic activities and affective contents of the dancer’s movement. To accomplish these goals, expert dancers and musicians were recruited as domain experts in affective gesture and auditory communication. Much of the dancer sonification literature focuses exclusively on describing the final performance piece or the techniques used to process motion data into auditory control parameters. This paper focuses on the methods we used to identify, select, and test the most appropriate motion to sound mappings for a dancer sonification system.","2017-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","Participatory Design Research Methodologies","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NTCIAZDB","journalArticle","2000","Lemmens, Paul M. C.; Bussemakers, Myra P.; de Haan, Abraham","The effect of earcons on reaction times and error-rates in a dual-task vs. a single-task experiment","","","","","http://hdl.handle.net/1853/50685","An experiment with two picture categorization tasks with auditory distracters containing redundant information was carried out to investigate the effects distracters, in this case earcons, have on categorization. In the first task participants had to carry out an extra mental addition task. The secondary task consisted of just the categorization. The dual-task situation was expected to lead to longer reaction times and more errors than the single-task situation, possibly providing new insights when analyzed. The results confirmed previous experimental results and indicated that significantly fewer errors were made in conditions in which the earcons contained relevant redundant information, compared to conditions with no relevant redundant information.","2000-04","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VPSMDM5N","journalArticle","2001","Chafe, Chris; Leistikow, Randal","Levels of temporal resolution in sonification of network performance","","","","","http://hdl.handle.net/1853/50623","The standard “ping” utility provides a momentary measurement of round trip time. Sequences of ping events are used to gather longer-term statistics about jitter and packet loss in order to describe the quality of service of a network path. A more finegrained tool is needed to evaluate paths which carry interactive media streams for collaborative environments. Natural interaction depends on obtaining consistent low-latency, low-jitter service, something which normally requires several ping “takes” to assess and even then only provides an averaged picture of quality of service. We have designed a stream-based method which directly displays the critical qualities to the ear by continuously driving a bidirectional connection to create sound waves. The network path itself becomes the acoustic medium which our probe sets into vibration. The granularity of this display better matches the time-scales of variance that are important in interactive applications (for example, bidirectional audio streams for long-distance musical collaboration or high-quality teleconference applications). The ear's acuity for pitch fluctuation and timbral constancy make this an unforgiving test. A related sonification technique is discussed which is a sonarlike mapping of momentary ping data to musical tones. Temporal levels of musical foreground, middleground and background can be heard in the melodies derived from the data and correspond to structures that are of importance in the analysis of network performance.","2001-07","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LV6GJ3GU","journalArticle","2014","Winters, R. Michael; Cumming, Julie E.","Sonification of Symbolic Music in the Elvis Project","","","","","http://hdl.handle.net/1853/52072","This paper presents the development of sonification in the ELVIS project, a collaboration in interdisciplinary musicology targeting large databases of symbolic music and tools for their systematic analysis. An sonification interface was created to rapidly explore and analyze collections of musical intervals originating from various composers, genres, and styles. The interface visually displays imported musical data as a sound-file, and maps data events to individual short, discrete pitches or intervals. The user can interact with the data by visually zoom in, making selections, playing through the data at various speeds, and adjusting the transposition and frequency spread of the pitches to maximize acoustic comfort and clarity. A study is presented in which rapid pitchmapping is applied to compare differences between similar corpora. A group of 11 participants were able to correctly order collections of sonifications for three composers (Monteverdi, Bach, and Beethoven) and three presentation speeds (10², 10³, and 10⁴ notes/second). Benefits of sonification are discussed including the ability to quickly differentiate composers, find non-obvious patterns in the data, and ‘direct mapping’. The interface is made available as a MacOSX standalone application written in Super- Collider.","2014-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HYZXABDY","journalArticle","2007","Oliveros, Pauline","Improvising with Spaces","","","","","http://hdl.handle.net/1853/50007","This paper explores qualitative changes that occur in voices and instruments in relationships with changing spaces ordinarily held in a stationary paradigm of performance practice, spatial transformations and the effect on sounds in multi-channel speaker systems. Digital technology allows one to compose and improvise with acoustical characteristics and change the apparent space during a musical performance. Sounds can move in space and space can morph and change affecting the sounds. Space is an integral part of sound. One cannot exist without the other. Varieties of sounds and spaces combine in symbiotic relationships that range from very limited to very powerful for the interweaving expressions of the music, architectures and audiences.","2007-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QR55MICR","book","2015","Jeon, Myounghoon; Lee, Ju-Hwan; Sterkenburg, Jason; Plummer, Christopher","Cultural differences in preference of auditory emoticons: USA and South Korea","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54115","For the last two decades, research on auditory displays and sonification has continuously increased. However, most research has focused on cognitive and functional mapping rather than emotional mapping. Moreover, there has not been much research on cultural differences on auditory displays. The present study compared user preference of auditory emoticons in two countries: USA and South Korea. Seventy students evaluated 112 auditory icons and 115 earcons regarding 30 emotional adjectives. Results indicated that they showed similar preference in the same category (auditory icons or earcons), but they showed different patterns when they were asked to select the best sound between the two categorical sounds. Implications for cultural differences in preference and directions for future design and research of auditory emoticons are discussed.","2015-07","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","Cultural differences in preference of auditory emoticons","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5MC9VVP3","journalArticle","2005","Bower, John E.; Lindroth, Scott; Burchett, John; Feller, Steven D.; Brady, David J.; Brady, Rachael","Soundsense: Sonifying pryroelectric sensor data for an interactive media event","","","","","http://hdl.handle.net/1853/50178","Collaborations between artists, engineers, and scientists often occur when creating new media works. These interdisciplinary efforts must overcome the ideals and practical-limitations inherent in both artistic and research pursuits. In turn, successful projects may truly be greater than the sum of their parts, enabling each collaborator to gain insight into their own work. soundSense, a cooperative effort between engineers, composers, and other specialists, sonifies pyroelectic sensor data to create a novel interactive-media event. Signals generated by multiplexing pyroelectric detectors inform datadriven audio and visual displays articulating – in real-time – the presence and motion of individuals within the sensed space.","2005-07","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","Soundsense","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TB6I5CHA","journalArticle","2001","Barra, Maria; Cillo, Tania; de Santis, Antonio; Petrillo, Umberto Ferraro; Negro, Alberto; Scarano, Vittoro; Matlock, Teenie; Maglio, Paul P.","Personal webmelody: Customized sonification of web servers","","","","","http://hdl.handle.net/1853/50626","This paper presents Personal WebMelody, a sonified web server that informs its administrator of both normal and abnormal operation through background music. It allows customization and full integration of system-generated music representing web server activity with external music sources (audio CD, MP3, etc) selected by the administrator. Our sonification technique works by associating MIDI or WAV sound tracks with web server events. In an attempt to enable the webmaster to listen to such system-generated music for a long period without becoming fatigued, we introduce the opportunity of mixing an external music source with systemgenerated music. In this way, the administrator can hear the status of the web server while listening to his or her preferred music. We present an empirical study that shows how our web server sonification can convey useful information efficiently.","2001-07","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","Personal webmelody","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"38BUZ325","journalArticle","2016","Coop, Allan D.","Sonification, Musification, and Synthesis of Absolute Program Music","","","","","http://hdl.handle.net/1853/56577","When understood as a communication system, a musical work can be interpreted as data existing within three domains. In this interpretation an absolute domain is interposed as a communication channel between two programatic domains that act respectively as source and receiver. As a source, a programatic domain creates, evolves, organizes, and represents a musical work. When acting as a receiver it re-constitutes acoustic signals into unique auditory experience. The absolute domain transmits physical vibrations ranging from the stochastic structures of noise to the periodic waveforms of organized sound. Analysis of acoustic signals suggest recognition as a musical work requires signal periodicity to exceed some minimum. A methodological framework that satisfies recent definitions of sonification is outlined. This framework is proposed to extend to musification through incorporation of data features that represent more traditional elements of a musical work such as melody, harmony, and rhythm.","2016-07","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JFZF4RG8","journalArticle","2004","Ciardi, F. C.","sMax: A multimodal toolkit for stock market data sonification","","","","","http://hdl.handle.net/1853/50909","In this work, we present sMax a multimodal toolkit for stock market data sonification. Unlike most research focusing their effort primarily on the sonification of single stock information, sMax provides an auditory display for the user to monitor parallel distributed data. sMax uses a set of Java and Max modules to map real time stock market information into recognizable musical patterns. One of the main design goals of the toolkit is to allow low-latency controls over real-time data sonification. Because of its object-oriented architecture, sMax can be easily extended by the user when additional functionality is required. The project outcomes range from the creation of art installations to auditory display for mobile computing devices. We present the theoretical background, and the structure of the program.","2004-07","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","sMax","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"725SAP5Z","journalArticle","2018","Worrall, David","Sonification: A Prehistory","","","","","http://hdl.handle.net/1853/60081","The idea that sound can convey information predates the modern era, and certainly the computational present. Data sonification can be broadly described as the creation, study and use of the non-speech aural representation of information to convey information. As a field of contemporary enquiry and practice, data sonification is young, interdisciplinary and evolving; existing in parallel to the field of data visualization. Drawing on older practices such as auditing, and the use of information messaging in music, this paper provides an historical understanding of how sound and its representational deployment in communicating information has changed. In doing so, it aims to encourage a critical awareness of some of the socio- cultural as well as technical assumptions often adopted in sonifying data, especially those that have been developed in the context of Western music of the last half-century or so.","2018-06","2023-07-13 06:25:42","2023-07-21 08:29:23","2023-07-12","","","","","","","Sonification","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VFLV34BM","journalArticle","2005","Cabrera, Daniel; Ferguson, Sam; Tilley, Steven; Morimoto, Masayuki","Recent studies on the effect of signal frequency on auditory vertical localization","","","","","http://hdl.handle.net/1853/50177","The authors have been involved in a series of studies showing that the fundamental frequency of a complex tone, the center frequency of a noise band, or the cut-off frequencies of simultaneously presented high- and low-passed noise bands, systematically affect the elevation of auditory images – such that high frequencies are associated with high elevations, and low frequencies with low elevations. These studies show this effect not just for median plane localization, but in some circumstances even for sources on the aural axis. This paper reviews the findings of these studies, and considers their implications and applications, including for auditory display.","2005-07","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3A4KYK6H","book","2009","Beilharz, Kirsty; Ferguson, Sam","An interface and framework design for interactive aesthetic sonification","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51293","This paper describes the interface design of our AeSon (Aesthetic Sonification) Toolkit motivated by user-centred customisation of the aesthetic representation and scope of the data. The interface design is developed from 3 premises that distinguish our approach from more ubiquitous sonification methodologies. Firstly, we prioritise interaction both from the perspective of changing scale, scope and presentation of the data and the user's ability to reconfigure spatial panning, modality, pitch distribution, critical thresholds and granularity of data examined. The user, for the majority of parameters, determines their own listening experience for real-time data sonification, even to the extent that the interface can be used for live data-driven performance, as well as traditional information analysis and examination. Secondly, we have explored the theories of Tufte, Fry and other visualization and information design experts to find ways in which principles that are successful in the field of information visualization may be translated to the domain of sonification. Thirdly, we prioritise aesthetic variables and controls in the interface, derived from musical practice, aesthetics in information design and responses to experimental user evaluations to inform the design of the sounds and display. In addition to using notions of meter, beat, key or modality and emphasis drawn from music, we draw on our experiments that evaluated the effects of spatial separation in multivariate data presentations.","2009-05","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2L62UABQ","journalArticle","2019","Seica, Mariana; Martins, Pedro; Roque, Licinio; Cardoso, F. Amilcar","A sonification experience to portray the sounds of portuguese consumption habits","","","","","http://hdl.handle.net/1853/61524","The stimuli for consumption is present in everyday life, where major retail companies play a role in providing a large range of products every single day. Using sonification techniques, we present a listening experiment of Portuguese consumption habits in the course of ten days, gathered from a Portuguese retail company. We focused on how to represent this time-series data as a musical piece that would engage the listenerﾒs attention and promote an active listening attitude, exploring the influence of aesthetics in the perception of auditory displays. Through a phenomenological approach, ten participants were interviewed to gather perceptions evoked by the piece, and how the consumption variations were understood. The tested composition revealed relevant associations about the data, with the consumption context indirectly present throughout the emerging themes: from the idea of everyday life, routine and consumption peaks to aesthetic aspects as the passage of time, frenzy and consumerism. Documentary, movie imagery and soundtrack were also perceived. Several musical aspects were also mentioned, as the constant, steady rhythm and the repetitive nature of the composition, and sensations such as pleasantness, satisfaction, annoyance, boredom and anxiety. These collected topics convey the incessant feeling and consumption needs which portray our present society, offering new paths for comprehending musical sound perception and consequent exploration.","2019-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KHPJJ5U2","journalArticle","2012","Vogt, Katharina; Goudarzi, Visda; Höldrich, Robert","Chirping Stars","","","2168-5126","","http://hdl.handle.net/1853/44448","Chirping Stars is a concept for a tape piece of a sonification of Twitter data. A snapshot of the popularity of musicians, randomly drawn in March 2012, yielded eight of the most popular stars at that time. Data of their Twitter followers shows the involvement of rapidly evolving fans of the artists on social media. The sonic interpretation of this development is created by mapping the data to parameters that modulate and re-synthesize the sound tracks of the artists.","2012-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PMBJZWGF","journalArticle","2011","Nguyen, Vinh Xuan","Tonal DisCo: Dissonance and Consonance in a Gaming Engine","","","","","http://hdl.handle.net/1853/51921","Whilst there are several existing toolkits specifically designed for sonification, there has been little investigation into the utilization of computer game engines for sonification. This paper will demonstrate the implementation of a real time game engine for the purpose of sonification and discuss the opportunities and limitations. An important aspect which is lacking in existing sonification toolkits is the ability to sonify streaming data in real-time. Gaming engines not only offer the potential to do this but also offer the ability to visualize data in 3D and in real-time. The sound design of an art exhibition is used as a case study to demonstrate the potential of a computer game editor/sandbox used for visualization and sonification. For the exhibition real-world objects were tracked inside a gallery space and represented in the virtual environment of a computer game, which was displayed on a projector screen. Their movement was sonified into musical form to convey their steady/consistent movement as consonant and their agitated/inconsistent movement as dissonant. Tonal “DisCo” is used to describe their dissonance and consonance rating in both musical tone and visual color. Although the sonification of data into musical structure distorts the accuracy of absolute data values, it does maintain the relationship between data values. This loss of resolution is counteracted by an increase in clarity of data relationships. This case study appropriates the single ratio scale of pitch into both an interval scale of tone and an ordinal scale of octaves in order to express interrelationships.","2011-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","Tonal DisCo","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TNUAWQAI","journalArticle","2017","Tsuchiya, Takahiko; Freeman, Jason","Spectral Parameter Encoding: Towards a Framework for Functional-Aesthetic Sonification","","","","","http://hdl.handle.net/1853/58370","Auditory-display research has had a largely unsolved challenge of balancing functional and aesthetic considerations. While functional designs tend to reduce musical expressivity for the fidelity of data, aesthetic or musical sound organization arguably has a potential for representing multi-dimensional or hierarchical data structure with enhanced perceptibility. Existing musical designs, however, generally employ nonlinear or interpretive mappings that hinder the assessment of functionality. The authors propose a framework for designing expressive and complex sonification using small timescale musical hierarchies, such as the harmony and timbral structures, while maintaining data integrity by ensuring a close-to-the-original recovery of the encoded data utilizing descriptive analysis by a machine listener.","2017-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","Spectral Parameter Encoding","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R9QWQLB9","journalArticle","1997","Alty, James L.; Vickers, Paul","The CAITLIN auralization system: Hierarchical leitmotif design as a clue to program comprehension","","","","","http://hdl.handle.net/1853/50753","Early experiments have suggested that program auralization can convey information about program structure [8]. Languages like Pascal contain classes of construct that are similar in nature allowing hierarchical classification of their features. This taxonomy can be reflected in the design of musical signatures which are used within the CAITLIN program auralization system. Experiments using these hierarchical leitmotifs indicate whether or not their similarities can be put to good use in communicating information about program structure and state. (Note, at time of going to press experimental results could not be included. These will be presented at the conference and included later.)","1997-11","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","The CAITLIN auralization system","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NFHTVHWZ","journalArticle","2005","Berger, Jonathan; Seung Yeo, Woon","Framework for designing image sonification methods","","","","","http://hdl.handle.net/1853/50158","Time is not just a parameter of auditory disply, but is rather the principle dimension within which all other auditory parameters are placed. While this is a feature of time ordered information, it poses a particular challenge to effective sonification of time-independent data, such as images. In this paper we present two concepts of time mapping, scanning and probing to provide a framework for con- ceptualizing mappings of static data to the time domain. We then consider the geometric characteristics of images to define mean- ingful references in time. Finally, we proceed to suggest a new approach of modeling human image perception for image sonifi- cation, which can be designed upon our framework.","2005-07","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HD4Z3H8D","journalArticle","2008","Bologna, Guido; Deville, Benoit; Pun, Thierry","Pairing Colored Socks and Following a Red Serpentine With Sounds of Musical Instruments","","","","","http://hdl.handle.net/1853/49864","The See ColOr interface transforms a small portion of a colored video image into sound sources represented by spatialized musical instruments. This interface aims at providing visually impaired people with a capability of perception of the environment. As a first step of this on-going project, the purpose is to verify the hypothesis that it is possible to use sounds from musical instruments to replace color. Compared to state of the art devices, a quality of the See ColOr interface is that it allows the user to receive a feed-back auditory signal from the environment and its colors, promptly. Two experiments based on a head mounted camera have been performed. The first experiment pertaining to object manipulation is based on the pairing of colored socks, while the second experiment is related to outdoor navigation with the goal of following a colored serpentine. The “socks” experiment demonstrated that seven blindfolded individuals were able to accurately match pairs of colored socks. The same participants successfully followed a red serpentine for more than 80 meters.","2008-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"88CNS9LA","journalArticle","2008","Brungart, Douglas S.; Simpson, Brian D.","Design, Validation, and In-Flight Evaluation of an Auditory Attitude Indicator Based on Pilot-Selected Music","","","","","http://hdl.handle.net/1853/49897","Although all cockpits are currently equipped with visual displays that provide accurate information about the attitude of the aircraft, spatial disorientation continues to be one of the leading causes of aviation accidents. In this paper, we describe the design of an audio display that modifies the acoustic properties of an arbtrary audio input signal (i.e., pilot-selected music) to provide the pilot with supplementary information about the current attitude of the aircraft. Details are provided about how and why the cues were selected, and how they were implemented in a real-time audio system in the aircraft. Results are also provided from laboratory and flight tests that were used to evaluate the performance of the system.","2008-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5VED4BUE","journalArticle","1994","Cohen, Michael","Using audio windows to analyze music","","","","","http://hdl.handle.net/1853/50882","Alternative nonimmersive perspectives enable new paradigms of perception, especially in the context of frames-of-reference for musical audition and groupware. ""maw,"" acronymic for multidimensional audio windows, is an application for manipulating sound sources and sinks in virtual rooms, featuring an exocentric graphical interface driving an egocentric audio backend. Listening to sound presented in such a spatial fashion is as different from conventional stereo mixes as sculpture is from painting. A schizophrenic existence suggests sonic (analytic) cubism, presenting multiple acoustic perspectives simultaneously. Clusters can be used to hierarchically group related mixels together. New interaction modalities are enabled by this sort of perceptual aggression and liquid perspective. In particular, virtual concerts may be ""broken down"" by individuals or groups.","1994-11","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A7E9LHY7","journalArticle","2018","MacDonald, Doon; Stockman, Tony","SoundTrAD, a method and tool for prototyping auditory displays: Can we apply it to an autonomous driving scenario?","","","","","http://hdl.handle.net/1853/60074","This paper presents SoundTrAD, a method and tool for designing auditory displays for the user interface. SoundTrAD brings together ideas from user interface design and soundtrack composition and supports novice auditory display designers in building an auditory user interface. The paper argues for the need for such a method before going on to describe the fundamental structure of the method and construction of the supporting tools. The second half of the paper applies SoundTrAD to an autonomous driving scenario and demonstrates its use in prototyping ADs for a wide range of scenarios.","2018-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","SoundTrAD, a method and tool for prototyping auditory displays","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S6ZUVD3K","journalArticle","2021","Drymonitis, Alexandros; Chatzopoulou, Nicoleta","Data mining / Live scoring: A live acoustic algorithmic composition based on Twitter","","","","","http://hdl.handle.net/1853/66338","Data Mining / Live Scoring is a live algorithmic composition for a six member acoustic ensemble. The algorithm receives feed from the popular social platform Twitter, sonifying the tweets it receives by writing a score for each of the six musicians. The concept behind this performance is to create a musical composition which can serve as a soundtrack for Twitter, depending on what is written by the users. Based on the sentiment of the tweets and a musical library written especially for this project, the algorithm attempts to create a score which reflects the mood on Twitter in an abstract way. This project was commissioned by the Onassis Cultural Centre in Athens, and was presented there twice in April 2019. It was realized for the ARTéfacts Ensemble.","2021-06","2023-07-13 06:25:42","2023-07-13 06:25:42","2023-07-12","","","","","","","Data mining / Live scoring","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M2WCUC2V","journalArticle","2002","Upson, R.","Educational sonification exercises: Pathways for mathematics and musical achievement","","","","","http://hdl.handle.net/1853/51358","This paper reports developments in the use of sonifications and sonification software for educational purposes. Adolescent subjects received training in Cartesian graphing over several sessions with sonification software and a sonification-enhanced curriculum. The project attracted students with low linguistic and logical-mathematical capabilities. Students were engaged by musical composition activities, but they remained anxious about traditional mathematics activities. Though students' mathematical abilities improved only slightly according to a traditional mathematical assessment, this project demonstrated the students' increased comfort level with the subject of mathematics and an increased understanding of the concepts within their own set of linguistics.","2002-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","Educational sonification exercises","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G76Z3LHI","book","2009","Vogt, Katharina; Pirro, David; Kobenz, Ingo; Höldrich, Robert; Eckel, Gerhard","Physiosonic - movement sonification as auditory feedback","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51409","We detect human body movement interactively via a tracking sys- tem. This data is used to synthesize sound and transform sound files (music or text). A subject triggers and controls sound param- eters with his or her movement within a pre-set range of motion. The resulting acoustic feedback enhances new modalities of per- ception and the awareness of the body movements. It is ideal for application in physiotherapy and other training contexts. The sounds we use depend on the context and aesthetic pref- erences of the subject. On the one hand, metaphorical sounds are used to indicate the leaving of the range of motion or to make un- intended movements aware. On the other hand, sound material like music or speech is played as intuitive means and motivating feedback to address humans. The sound material is transformed in order to indicate deviations from the target movement. With this sonification approach, subjects perceive the sounds they have cho- sen themselves in undistorted playback as long as they perform the training task appropriately. Our main premises are a simple map- ping of movement to sound and common sense metaphors, that both enhance the understanding for the subject.","2009-05","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U2AWF9KK","book","2015","Camier, Cédric; Féron, François-Xavier; Boissinot, Julien; Guastavino, Catherine","Tracking moving sounds: Perception of spatial figures","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54207","With the emergence of electroacoustic music, spatial figures have become part of the musical vocabulary of many composers. But the perception of auditory trajectories has received scant attention in the scientific literature. This study aims at determining under which conditions simple common spatial figures (such as circles, squares and triangle) can be perceived by a listener positioned in the center of a loudspeaker arrays. In a series of listening tests, we investigate the effect of rendering techniques (VPAB vs. WFS), reverberation (dry vs. modeled reflections) and sound velocity on spatial figure identification performance.","2015-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","Tracking moving sounds","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WM9CALXP","journalArticle","2013","Vogt, Katharina; Goudarzi, Visda; Parncutt, Richard","Empirical Aesthetic Evaluation Of Sonifications","","","","","http://hdl.handle.net/1853/51663","This paper discusses three experiments on the aesthetic evaluation of different sonifications. The effects of training and understanding of the auditory display on its aesthetic appealing were tested. Results showed no significant effect, but a trend towards less acceptance due to longer exposure to the sounds in general. Furthermore, there might be effects of musical ability and gender that should be further explored.","2013-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KXDGGKG4","journalArticle","2006","Hoffman, M.; Cook, P. R.","Feature-based synthesis for sonification and psychoacoustic research","","","","","http://hdl.handle.net/1853/50592","We present a general framework for synthesizing audio manifesting arbitrary sets of perceptually motivated, quantifiable acoustic features. Much work has been done recently on finding acoustic features that describe perceptually relevant aspects of sound. The ability to synthesize sounds defined by arbitrary feature values would allow perception researchers to more directly generate stimuli “to order,” as well as providing an opportunity to directly test the perceptual relevance and characteristics of such features. The methods we describe also provide a straightforward way of approaching the problem of mapping from data to synthesis control parameters for sonification.","2006-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7THP7CCQ","journalArticle","2008","Cahen, Roland","Sound Design for Navigation in Topophonies","","","","","http://hdl.handle.net/1853/49947","This paper attempts to develop concepts of multimodal objects made out of the interaction between space and sound though motion. The present paper is mostly descriptive and tries to gather experiences in order to delimit a new field of applications, concepts and method. My purpose is to describe some specificities of sound navigation design, starting from everyday life experience and cultural backgrounds towards virtual reality. My point of view will be that of a sound designer. I will examine different methods of implementation related to several sound navigation concepts and metaphors related to this particular approach. For this purpose, I will take different examples mainly in my works to show how some kind of scenarios propose different ways to link motion to sound, what kind of sound behaviours they produce, how they can be created and controlled, what kind of human and artistic experience they offer.","2008-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DYZK3768","journalArticle","2005","Ma, Minhua; McKevitt, Paul","Lexical semantics and auditory presentation in virtual storytelling","","","","","http://hdl.handle.net/1853/50167","Audio presentation is an important modality in virtual storytelling. In this paper we present our work on audio presentation in our intelligent multimodal storytelling system, CONFUCIUS, which automatically generates 3D animation speech, and non-speech audio from natural language sentences. We provide an overview of the system and describe speech and non-speech audio in virtual storytelling by using linguistic approaches. We discuss several issues in auditory display, such as its relation to verb and adjective ontology, concepts and modalities, and media allocation. Finally we conclude that introducing linguistic knowledge provides more intelligent virtual storytelling, especially in audio presentation.","2005-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LQMKT68E","journalArticle","2012","Gröhn, Matti; Ahonen, Lauri; Huotilainen, Minna","Effects of pleasant and unpleasant auditory mood induction on the performance and in brain activity in cognitive tasks","","","2168-5126","","http://hdl.handle.net/1853/44422","Mood induction with pleasant and unpleasant auditory stimuli during the break. Our test includes subjective evaluation (Nasa-TLX, KSS, POMS), cognitive tests and brain responses (MEG and EEG). We aim studying the effect affective state has on work-like tasks. Hypothesis: pleasantness of auditory mood induction affects cognitive performance and brain responses.","2012-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8HQ4YK9M","journalArticle","2017","Taylor, Stephen","From Program Music to Sonification: Representation and the Evolution of Music and Language","","","","","http://hdl.handle.net/1853/58373","Research into the origins of music and language can shed new light on musical representation, including program music and more recent incarnations such as data sonification. Although sonification and program music have different aims—one scientific explication, the other artistic expression—similar techniques, relying on human and animal biology, cognition, and culture, underlie both. Examples include Western composers such as Beethoven and Berlioz, to more recent figures like Messiaen, Stockhausen and Tom Johnson, as well as music theory, semiotics, biology, and data sonifications by myself and others. The common thread connecting these diverse examples is the use of human musicality, in the biomusicological sense, for representation. Links between musicality and representation—dimensions like high/low, long/short, near/far, etc., bridging the real and abstract—can prove useful for researchers, sound designers, and composers.","2017-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","From Program Music to Sonification","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GW295I3Z","journalArticle","2019","Rönnberg, Niklas; Lowgren, Jonas","Traces of modal synergy: studying interactive musical sonification of images in general-audience use","","","","","http://hdl.handle.net/1853/61543","Photone is an interactive installation combining color images with musical sonification. The musical expression is generated based on the syntactic (as opposed to semantic) features of an image as it is explored by the userﾒs pointing device, intending to catalyze a holistic user experience we refer to as modal synergy where visual and auditory modalities multiply rather than add. We collected and analyzed two months' worth of data from visitorsﾒ interactions with Photone in a public exhibition at a science center. Our results show that a small proportion of visitors engaged in sustained interaction with Photone, as indicated by session times. Among the most deeply engaged visitors, a majority of the interaction was devoted to visually salient objects, i.e., semantic features of the images. However, the data also contains instances of interactive behavior that are best explained by exploration of the syntactic features of an image, and thus may suggest the emergence of modal synergy.","2019-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","Traces of modal synergy","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LN5NSHMZ","journalArticle","2012","Jeon, Myounghoon; Winton, Riley J.; Yim, Jung-Bin; Bruce, Carrie M.; Walker, Bruce N.","Aquarium fugue: interactive sonification for children and visually impaired audience in informal learning environments","","","2168-5126","","http://hdl.handle.net/1853/44427","In response to the need for more accessible Informal Learning Environments (ILEs), the Georgia Tech Accessible Aquarium Project has been studying sonification for the use in live exhibit interpretation in aquariums. The present work attempts to add more interactivity [1] to the project’s existing sonification work, which is expected to lead to more accessible learning opportunities for visitors, particularly people with vision impairments as well as children. In this interactive sonification phase, visitors can actively experience an exhibit by using tangible objects to mimic the movement of animals. Sonifications corresponding to the moving tangible objects can be paired with real-time interpretive sonifications produced by the existing Accessible Aquarium system to generate a cooperative fugue. Here, we describe the system configuration, pilot test results, and future works. Implications are discussed in terms of embodied interaction and interactive learning.","2012-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","Aquarium fugue","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QF62NYK8","journalArticle","2004","Walker, Bruce N.; Mauney, Lisa M.","Individual differences, cognitive abilities, and the interpretation of auditory graphs","","","","","http://hdl.handle.net/1853/50862","Auditory graphs exploit pattern recognition in the auditory system, but questions remain about the relationship between cognitive abilities, demographics, and sonification interpretation. Subjects completed a magnitude estimation task relating sound dimensions to data dimensions. Subjects also completed a working memory task (2-back task) and a spatial reasoning task (Raven's Progressive Matrices) to assess cognitive abilities. Demographics, such as gender, age, handedness, and musical experience, were also reported and included in the analysis. A stepwise multiple regression analysis was performed to determine the relationship between the independent (cognitive abilities and demographics) and dependent (individual slopes and R-squared values) variables. The regression analysis indicates some support for most of the predictor variables, especially predicting R-squared values. The 2-back task does not seem to contribute significantly to the interpretation of sonifications and auditory graphs. However, Raven's and many of the demographic variables do show predictive value for interpretation of auditory graphs.","2004-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JM6IM3YE","journalArticle","2019","Falk, Courtney; Dykstra, Josiah","Sonification with music for cybersecurity situational awareness","","","","","http://hdl.handle.net/1853/61496","Cyber defenders work in stressful, information-rich, and highstakes environments. While other researchers have considered sonification for security operations centers (SOCs), the mappings of network events to sound parameters have produced aesthetically unpleasing results. This paper proposes a novel sonification process for transforming data about computer network traffic into music. The musical cues relate to notable network events in such a way as to minimize the amount of training time a human listener would need in order to make sense of the cues. We demonstrate our technique on a dataset of 708 million authentication events over nine continuous months from an enterprise network. We illustrate a volume-centric approach in relation to the amplitude of the input data, and also a volumetric approach mapping the input data signal into the number of notes played. The resulting music prioritizes aesthetics over bandwidth to balance performance with adoption.","2019-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D2DE9G48","journalArticle","2001","Fernstrom, Mikael; Griffith, Niall; Taylor, Sean","Bliain le baisteach - sonifying a year with rain","","","","","http://hdl.handle.net/1853/50611","In this paper the development of software for the creation of Bliain Le Baisteach is described. Over 77,000 datapoints were received from the Irish meteorological service. A neural network was designed and trained with 1,000 traditional Irish melodies. The data was then partitioned according to the four geographical provinces of Ireland and made to stimulate the network, generating the different parts of a score for the Irish Chamber Orchestra.","2001-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PMQ24PGM","journalArticle","2022","Kalonaris, Stefano","Tōkyō kion-on: Query-based generative sonification of atmospheric data","","","","","http://hdl.handle.net/1853/67390","Amid growing environmental concerns, interactive displays of data constitute an important tool for exploring and understanding the impact of climate change on the planet’s ecosystemic integrity. This paper presents Tokyo kion-on, a query-based sonification model of Tokyo’s air temperature from 1876 to 2021. The system uses a recurrent neural network architecture known as LSTM with attention trained on a small dataset of Japanese melodies and conditioned upon said atmospheric data. After describing the model’s implementation, a brief comparative illustration of the musical results is presented, along with a discussion on how the exposed hyper-parameters can promote active and non-linear exploration of the data.","2022-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","Tōkyō kion-on","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RGHQJMLG","journalArticle","2008","Vogt, Katharina; Bovermann, Till; Huber, Philipp; de Campo, Alberto","Exploration of 4D-Data Spaces. Sonification in Lattice QCD","","","","","http://hdl.handle.net/1853/49933","We describe a pilot study on the sonification of data from lattice Quantum Chromodynamics, a branch of computational physics. This data is basically 4-dimensional and discretized on a lattice. The implementation allows interactive navigation through the data via different interfaces. Two different sonification schemes have been applied, giving information on small regions of the lattice. In real data sets we searched for structures that are hidden by quantum fluctuations. First results have been achieved with simplified data sets.","2008-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q4N9Q9SQ","book","2015","Hauer, Wolfgang; Vogt, Katharina","Sonification of a streaming-server logfile","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54180","This paper presents the sonification of a multimedia streaming server based on its log file. The server is used at a technical university to provide lecture recordings. Due to the large number of datapoints and various data categories it’s a promising approach to use sonification as a display method instead of the traditional visual representation. The SuperCollider script allows to simulate a realtime scenario as well as to monitor a past period of time.","2015-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3Z8R6NZG","book","2010","Worrall, David","Parameter Mapping Sonic Articulation and the Perceiving Body","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49899","In data sonification research, there is a well-known perceptual problem that arises when abstract multivariate datasets of a certain size and complexity are parametrically mapped into sound. In listening to such sonifications, when a feature appears, it is sometimes difficult to ascertain whether that feature is actually a feature of the dataset or just a resultant of the psychoacoustic interaction between co-dependent parametric dimensions. A similar effect occurs in visualisation, such as when parallel lines can appear more or less curved on different backgrounds. Couched in psycho-philosophical terms, we can ask whether this failure is related to classical phenomenology's inability to produce an eidetic science of essential invariant forms that involve no assertion of actual material existence, or to there not yet having been found some generalisably acceptable limits from heuristically tested mappings. This paper discusses the nature of this problem and introduces a sonification research project based on embodied, non-representational phenomenal models of perception.","2010-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BHPTFSFL","journalArticle","2021","Kantan, Prithvi Ravi; Spaich, Erika G.; Dahl, Sofia","A metaphor-based technical framework for musical sonification in movement rehabilitation","","","","","http://hdl.handle.net/1853/66350","Interactive sonification has increasingly shown potential as a means of biofeedback to aid motor learning in movement rehabilitation. However, this application domain faces challenges related to the design of meaningful, task-relevant mappings as well as aesthetic qualities of the sonic feedback. A recent mapping design approach is that of using conceptual metaphors based on image schemata and embodied music cognition. In this work, we developed a framework to facilitate the design and real-time exploration of rehabilitation-tailored mappings rooted in a specific set of music-based conceptual metaphors. The outcome was a prototype system integrating wireless inertial measurement, flexible real-time mapping control and physical modelling-based musical sonification. We focus on the technical details of the system, and demonstrate mappings that we created through it for two exercises. These will be iteratively honed and evaluated in upcoming usercentered studies. We believe our framework can be a useful tool in musical sonification design for motor learning applications.","2021-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GSGQEFWR","journalArticle","2001","Lemmens, Paul M. C.; Bussemakers, Myra P.; de Haan, Abraham","Effects of auditory icons and earcons on visual categorization: The bigger picture","","","","","http://hdl.handle.net/1853/50617","This paper presents an overview of work on the effects of earcons and auditory icons on picture categorization and the results of 2 new experiments. The general finding of the experiments is that earcons have an inhibitory effect on picture categorization whereas auditory icons, in general, have a facilitating effect. These findings will be discussed and related to the theoretical framework of perceptual versus conceptual categorization.","2001-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","Effects of auditory icons and earcons on visual categorization","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C52BT3ID","journalArticle","2012","Giot, Rudi; Courbe, Yohan","InteNtion – Interactive Network Sonification","","","2168-5126","","http://hdl.handle.net/1853/44419","This paper presents an innovative approach in monitoring network traffic by adding a new dimension: the sound. InteNtion (Interactive Network Sonification) is a project aimed at mapping network activity to musical aesthetic. The network traffic analysis is made with the SharpPCap library (a port of WinPCap to C# environment). From this analysis, the collected data are converted into MIDI (Musical Instrument Digital Interface) messages and sent to dedicated synthesizers, which generate sounds dynamically mixed together. The whole process results in an interactive soundscape. This novel approach will initiate two opportunities for technological development. It allows users to actively take part in an interactive exhibition system through simple actions involving network access, including streaming radio over the Internet, sharing music on Twitter, downloading mp3 files and others. This project initiates also a new dimension in monitoring the network by helping the administrator in detecting efficiently the hacking and abuse of the infrastructure.","2012-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UEYVP7Y8","journalArticle","2017","Worrall, David","Computational Designing for Auditory Environments","","","","","http://hdl.handle.net/1853/58416","This paper is a call for sonification designers to adapt their representational practices from that of designing objects for auditory engagement to the construction of systems of formally described relationships that define the ‘state space’ from which streams of such objects can be drawn. This shift from the crafting individual sonic objects and streams to defining dynamical space of design possibilities we call ‘computational designing’. Such sonification model spaces are inaudible, heard only through its instances, or the manifestations of particular trajectories through the space. Approaching the design of auditory displays as computational tasks poses both considerable challenges and opportunities. These challenges are often understood to be technical, requiring scripting or programming skills, however the main challenge lies in computational design thinking which is not best understood as the extension of established designing processes. The intellectual foundations of computational designing rest at the confluence of multiple fields ranging from mathematics, computer science and systems science to biology, psychophysical and cognitive perception, social science, music theory and philosophy. This paper outlines the fundamental concepts of computational design thinking based on seminal ideas from these fields and explores how they it might be applied to the construction of models for synthesized auditory environments.","2017-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GSYFSUTC","journalArticle","2021","Yang, Jing; Roth, Andreas","Musical features modification for less intrusive delivery of popular notification sounds","","","","","http://hdl.handle.net/1853/66322","Less intrusive information delivery has been a popular research topic for auditory displays. While most research has addressed this issue by creating new notification cues such as rendering ambient soundscapes or modifying background music, we present a novel method to gently deliver artificial notification sounds that have been commonly used in digital devices and for popular applications. We propose to play a notification sound by embedding it into the music that a user is listening to, after changing the musical timbre, amplitude, tempo, and octave of the notification to match these features of the music. To implement this concept, we extend a melody extraction algorithm for notification timbre transfer, and we present a pipeline that algorithmically selects a proper time spot and harmoniously embeds the notification into music. To validate our design concept, we present a user study comparing our method with the standard method of playing notification sounds on digital devices. Through an extensive analysis of 96 tasks performed by 32 participants, we demonstrate that our method can deliver notification sounds in a less intrusive but adequately noticeable manner and is preferred by most participants.","2021-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6GMEG4YE","book","2015","Schmele, Timothy; Romero, Juan Alzate; Troge, Thomas A.; Ruiter, Nicole; Zapf, Michael","Sonifying multichannel ultrasound data for periphonic loudspeaker array","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54139","The following paper describes several transformations of ultrasonic data into musical material for hemispheric loudspeaker setup. This data comes from a medical prototype for the purpose of early breast cancer detection via 3D multimodal imaging. The data is acquired in a semi-ellipsoid mesh of thousands of ultrasonic emitters and receivers surrounding the measurement object with water as medium. A hemispheric loudspeaker array can reflect this aperture design and offers the possibility of projecting this data in a direct fashion for auditory display. The ultrasounds in the megahertz range are inaudible and need to be transformed into the audible range. We here describe our investigations and methods to transfer medical ultrasound data in an artistic context and report on our insights using different sonification strategies to gain audible, musical material.","2015-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9LS43KWN","journalArticle","2018","Snook, Kelly; Barri, Tarik; Goßmann, Joachim; Potts, Jason; Schedel, Margaret; Warm, Hartmut","Kepler Concordia: Designing an immersive modular musical and scientific instrument using novel blockchain and sonification technologies in XR","","","","","http://hdl.handle.net/1853/60094","This paper describes the first steps in the creation of a new scientific and musical instrument to be released in 2019 for the 400th anniversary of Johannes Kepler's Harmonies of the World, which laid out his three laws of planetary motion and launched the field of modern astronomy. Concordia is a musical instrument that is modularly extensible, with its first software and hardware modules and underlying framework under construction now. The instrument is being designed in an immersive extended-reality (XR) environment with scientifically accurate visualizations and datatransparent sonifications of planetary movements rooted in the musical and mathematical concepts of Johannes Kepler [1], extrapolated into visualizations by Hartmut Warm [2], and sonified. Principles of game design, data sonification/visualization optimization, and digital and analog music synthesis are used in the 3D presentation of information, the user interfaces (UX), and the controls of the instrument, with an optional DIY hardware “cockpit” interface. The instrument hardware and software are both designed to be modular and open source; Concordia can be played virtually without the DIY cockpit on a mobile platform, or users can build or customize their own interfaces, such as traditional keyboards, button grids, or gestural controllers with haptic feedback to interact with the system. It is designed to enable and reward practice and virtuosity through learning levels borrowed from game design, gradually building listening skills for decoding sonified information. The frameworks for uploading, verifying, and accessing the data; programming and verifying hardware and software module builds; tracking of instrument usage; and managing the instrument's economic ecosystem are being built using a combination of distributed computational technologies and peer-to-peer networks, including blockchain and the Interplanetary Filesystem (IPFS). Participants in Concordia fall into three general categories, listed here in decreasing degrees of agency: 1) Contributors; 2) Players; and 3) Observers. This paper lays out the broad structure of Concordia, describes progress on the first software module, and explores the creative, social, economic, and educational potential of Concordia as a new type of creative ecosystem.","2018-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","Kepler Concordia","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YZFCAY7J","journalArticle","2014","Speth, Florina; Wahl, Michael","Specifying Rhythmic Auditory Stimulation for Robot-assisted Hand Function Training in Stroke Therapy","","","","","http://hdl.handle.net/1853/52046","Following a stroke 90% of all patients suffer from a loss of arm and hand function, whereby 30-40% of them never regain full functionality ever again. Robot-assisted hand function training (RT) intensifies and complements common ergo-therapeutic treatment effectively. Most of robotic rehabilitation devices are connected to multimedia-environments offering playful training to promote motivation. “Rhythmic Acoustic Stimulation” (RAS), an effective therapeutic technique for post-stroke-treatment, was never specified, applied nor evaluated for RT. This paper suggests specified sound designs with rhythmic stimuli for RT that aim to enhance function and motivation. Four pilot experiments are described evaluating, if specified rhythmic stimulation designs applied during a fine motor task influence motivation and function in comparison to no stimulation. As results of these experiments indicate that rhythmic stimulation designs may enhance function and motivation, they are discussed for further observations applied in RT with stroke-patients.","2014-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5XSGGGJY","journalArticle","2006","Orzessek, B.; Falkner, M.","Sonification of autonomic rhythms in the frequency spectrum of heart rate variability","","","","","http://hdl.handle.net/1853/50639","This poster presents some of the work currently being done at the Paracelsus Clinic in Switzerland on heart rate variability biofeedback with a real time auditory display. Heart rate variability biofeedback is an important diagnostic and therapeutic tool in the work with a wide variety of chronic disorders. We use a proprietary building-block type laboratory computer program that is linked via MIDI to a software sequencer with a VST virtual instrument library. Beyond the sonification of RR intervals as discrete numbers, the development of new techniques became necessary in order to be able to sonify the dynamic, wave-like structure of autonomic rhythms in the frequency spectrum of HRV, what we call ”heartmusic”. The fact that patients can hear their inner autonomic activity as music in real time and so work with elements of their own autonomous rhythmic oscillations, may also add an important new dimension to this field in the future.","2006-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MAXGUG6R","journalArticle","2008","Grand, Florian; Dall Antonia, Fabio","Sumo. A Sonification Utility for Molecules","","","","","http://hdl.handle.net/1853/49952","In this paper we present SUMO, an open source software environment, which is designed to facilitate the open development of molecular sonifications for everyday research in chemistry and structural biology. Sonifications of chemical data are developed since more than 25 years but surprisingly auditory display is not yet a scientifically established mode to interact and explore molecular data. Before presenting SUMO we introduce the implications of presenting molecular data to the sonification community. For chemists and structural biologists, we briefly review different sonification approaches made so far and discus their potential. Within this broader scope we situate SUMO, the lab proof sonification framework. We describe the software environment in detail and present two implementations of methods for sonifying conformations of amino acids and B factors.","2008-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FYVSRJ8P","journalArticle","2008","Martens, William L.; Sakamoto, Shuichi; Suzuki, Yoiti","Perceived Self Motion in Virtual Acoustic Space Facilitated by Passive Whole-Body Movement","","","","","http://hdl.handle.net/1853/49866","When moving sound sources are displayed for a listener in a manner that is consistent with the motion of a listener through an environment populated by stationary sound sources, listeners may perceive that the sources are moving relative to a fixed listening position, rather than experiencing their own self motion (i.e., a change in their listening position). Here, the likelihood of auditory cues producing such self motion (aka auditory-induced vection) can be greatly facilitated by coordinated passive movement of a listener's whole body, which can be achieved when listeners are positioned upon a multi-axis motion platform that is controlled in synchrony with a spatial auditory display. In this study, the temporal synchrony between passive whole-body motion and auditory spatial information was investigated via a multimodal time-order judgment task. For the spatial trajectories taken by sound sources presented here, the observed interaction between passive whole-body motion and sound source motion clearly depended upon the peak velocity reached by the moving sound sources. The results suggest that sensory integration of auditory motion cues with whole-body movement cues can occur over an increasing range of intermodal delays as virtual sound sources are moved increasingly slowly through the space near a listener's position. Furthermore, for the coordinated motion presented in the current study, asynchrony was relatively easy for listeners to tolerate when the peak in whole-body motion occurred earlier in time than the peak in virtual sound source velocity, but quickly grew to be intolerable when the peak in whole-body motion occurred after sound sources reached their peak velocities.","2008-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6QRLJGEA","book","2010","Sanchez, Javier","Identifying and Communicating 2D Shapes using Auditory Feedback","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49891","This research project shows a technique for allowing a user to ""see"" a 2D shape without any visual feedback. The user gestures with any universal pointing tool, as a mouse, a pen tablet, or the touch screen of a mobile device, and receives auditory feedback. This allows the user to experiment and eventually learn enough of the shape to effectively trace it out in 2D. The proposed system is based on the idea of relating spatial representations to sound, which allows the user to have a sound perception of a 2D shape. The shapes are predefined and the user has no access to any visual information. While exploring the space using the pointer device, sound is generated, which pitch and intensity varies according to some given strategies. 2D shapes can be identified and easily followed with the pointer tool, using the sound as only reference.","2010-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7UJXSA8S","journalArticle","2008","Hermann, Thomas","Taxonomy and Definitions for Sonification and Auditory Display","","","","","http://hdl.handle.net/1853/49960","Sonification is still a relatively young research field and many terms such as sonification, auditory display, auralization, audification have been used without a precise definition. Recent developments such as the introduction of Model-Based Sonification, the establishment of interactive sonification and the increased interest in sonification from arts have raised the need to revisit the definitions in order to move towards a clearer terminology. This paper introduces a new definition for sonification and auditory display that emphasizes the necessary and sufficient conditions for organized sound to be called sonification. It furthermore suggests a taxonomy, and discusses the relation between visualization and sonification. A hierarchy of closed-loop interactions is furthermore introduced. This paper aims to initiate vivid discussion towards the establishment of a deeper theory of sonification and auditory display.","2008-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YC5WBJMI","journalArticle","2019","King, Rob","‘Music of the people': Music from data as social commentary","","","","","http://hdl.handle.net/1853/61539","Data-music reflects the ubiquity of data in modern society. Composers have not engaged widely with the opportunities opened up by this, despite the chance to overcome a gulf between academic art music and social engagement. Their reluctance might be traced to the challenge of reconciling abstract data and concrete sound, in political implications, and in technological barriers in computer music. The present paper argues that socially relevant music composition for the 21st century can adopt a programme of sonification grounded in politically acute data. As examples of such practice, two compositions are discussed founded upon US and UK social data sets, and realised via the SuperCollider programming language. The consequences for the composer of new music are further discussed from political and musicological angles, with the ﾑpurposeﾒ of writing such music analysed from the perspective of various commentators.","2019-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","‘Music of the people'","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RC6RN3WQ","journalArticle","2019","Nadri, Chihab; Anaya, Chairunisa; Yuan, Shan; Jeon, Myounghoon","Preliminary guidelines on the sonification of visual artworks: Linking music, sonification & visual arts","","","","","http://hdl.handle.net/1853/61536","Sonification and data processing algorithms have advanced over the years to reach practical applications in our everyday life. Similarly, image processing techniques have improved over time. While a number of image sonification methods have already been developed, few have delved into potential synergies through the combined use of multiple data and image processing techniques. Additionally, little has been done on the use of image sonification for artworks, as most research has been focused on the transcription of visual data for people with visual impairments. Our goal is to sonify paintings reflecting their art style and genre to improve the experience of both sighted and visually impaired individuals. To this end, we have designed initial sonifications for paintings of abstractionism and realism, and conducted interviews with visual and auditory experts to improve our mappings. We believe the recommendations and design directions we have received will help develop a multidimensional sonification algorithm that can better transcribe visual art into appropriate music.","2019-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","Preliminary guidelines on the sonification of visual artworks","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"69Q5FBIA","journalArticle","2002","Childs, E.","Achorripsis: A sonification of probability distributions","","","","","http://hdl.handle.net/1853/51332","The 1957 musical composition Achorripsis by Iannis Xenakis was composed using four different probability distributions, applied over three different organizational domains, during the course of the 7 minute piece. While Xenakis did not have sonification in mind, his artistic choices in rendering mathematical formulations into musical events (time, space, timbre, glissando speed) provide useful contributions to the “mapping problem” in three significant ways: 1. He pushes the limit of loading the ear with multiple formulations simultaneously. 2. His mapping of “velocity” to string glissando speed provides a useful method of working with a vector quantity with magnitude and direction. 3. His artistic renderings, ie. “musifications” of these distributions, invite the question, in general, as to whether musical/ artistic sonifications are more intelligible to the human ear than sonifications prepared without any musical “filtering” or constraints (e.g. that they could be notated and performed by musicians).","2002-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","Achorripsis","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WNKFRUZX","journalArticle","2013","Rogińska, Agnieszka; Friedman, Kent; Mohanraj, Hariharan","Exploring sonification for augmenting Brain scan data","","","","","http://hdl.handle.net/1853/51653","Medical image data has traditionally been analyzed using visual displays and statistical analysis methods. Visual representations are limited due to the nature of the display and the number of dimensions that can be represented visually. This paper describes the use of sonification to represent medical image data of brain scans of patients with AlzheimerÕs dementia. The use of sonification is described as an approach to augment traditional diagnosis methods used to diagnose AlzheimerÕs dementia.","2013-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X93WMZMR","journalArticle","2003","England, David; Salces, Fausto J. Sainz; Vickers, Paul","Household appliances control device for the elderly","","","","","http://hdl.handle.net/1853/50466","An evaluation of musical earcons was carried out to see whether they are an effective and efficient method of delivering information about household appliances to elderly people. A test was carried out to explore the ability of the elderly subjects in remembering and learning the musical earcons. This test indicated a poor rate of recognition of the earcons. A second test that included the presentation of information in three modes (audio, visual and multimodal) was performed to determine which modality was preferred to deliver certain types of information among this group. We hypothesized that the multimodal interface would be the best in terms of speed and accuracy of response, and this was supported by the data. The results showed the need for a redesign of the earcons.","2003-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6KNL6FM3","journalArticle","2004","Parthy, A.; Jin, C.; van Schaik, A.","Reverberation for ambient data communication","","","","","http://hdl.handle.net/1853/50906","We propose an ambient communication system that modulates the reverberance applied to music with a single variable in order to communicate non-musical information to the listener. In order to assess the effectiveness of such a system, psychoacoustic tests of four subjects' ability to discriminate changes in reverberant decay time (RDT) were conducted. Results indicate that human listeners are able to accurately detect changes in RDT from a reference value of two, five, and ten seconds when the RDT increases by more than 60% or decreases by more than 30%. The rather large change in RDT required for accurate perception limits the dynamic range of the information that can be communicated when the variable is used to linearly change the RDT.","2004-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UP2IL94Z","journalArticle","2002","Nagashima, Y.","Interactive multimedia performance with bio-sensing and bio-feedback","","","","","http://hdl.handle.net/1853/51360","This is a report of research and some experimental applications of human-computer interaction in multi-media performing arts. The human performer and the computer systems perform computer graphic and computer music interactively in realtime. In general, many sensors are used for the interactive communication as interfaces, and the performer receives the output of the system via graphics, sounds and physical reactions of interfaces like musical instruments. I have produced many types of interfaces, not only with physical/electrical sensors but also with biological/physiological sensors. This paper is intended as an investigation of some special approaches: (1) sensing/reacting with “breathing” in performing arts, (2) 16-channel electromyogram sensor and its application of “muscle performing music”, (3) 8-channel electricfeedback system and its experiments of “bodyhearing sounds” and “body-listening to music”.","2002-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SK9U9V8S","journalArticle","2000","Sturm, Bob L.","Sonification of particle systems via de Broglie's Hypothesis","","","","","http://hdl.handle.net/1853/50683","Quantum mechanics states a particle can behave as either a particle or a wave. Thus systems of particles might be likened to a complex superposition of dynamic waves. Motivated by this, the author develops methods for the sonification of particle systems in a logical manner. Many systems and physical phenomena have thus far been simulated, producing a wide range of unique sonic events. The applications that have been explored are for algorithmic sound synthesis and music composition. Of critical importance is addressing the issue of latencies, caused by large complex numerical operations at audio sampling rates. This becomes painfully clear when particles interact with each other. Further applications of this system include scientific sonification, with an appropriate integration of psychoacoustic principles; creating an application for physics and music students to extend and enrich their comprehension of both topics; and inspiring philosophical dialogue regarding the similarities, intersections, and interdependence of Art and Science. Future work aims to produce a real-time application for simulated and real systems, and a deeper integration of quantum mechanics into these techniques.","2000-04","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AC3APS4G","journalArticle","2013","Kikukawa, Yuya; Kato, Megumi; Baba, Tetsuaki; Kushiyama, Kumiko","Hakoniwa: A sonification art installation consists of sand and woodblocks","","","","","http://hdl.handle.net/1853/51681","In this research we present an interactive tabletop installation ÒHakoniwaÓ. It consists of a wooden box, white sand and painted woodblocks. In this system, corresponding to the arrangement of woodblocks, a ceiling-mounted projector shows visual effects on the sand surface. At the same time, generative music is composed in a computer corresponding to the arrangement of woodblocks and modeling of the sand. This is an attempt of sonification of miniature garden. We studied Sandtray therapy, one of a famous form of art therapy, as a motif of the installation. We directed our attention to tactile sensation of sand and woodblocks, and tried to extend sandtray using computer vision processing and multi-media output. In this paper we describe details of the interactive system and discuss the possibility of supporting primitive play using such interactive systems.","2013-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","Hakoniwa","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JGIL3YT5","book","2015","FakhrHosseini, Maryam; Kirby, Paul; Jeon, Myounghoon","Regulating Drivers’ Aggressiveness by Sonifying Emotional Data","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54179","There have been efforts within the area of cognitive and behavioral sciences to mitigate drivers’ emotion to decrease the associated traffic accidents, injuries, fatalities, and property damage. In this study, we targeted aggressive drivers and try to regulate their emotion through sonifying their emotional data. Results are discussed with an affect regulation model and future research.","2015-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ITWA4GUI","book","2015","Höldrich, Robert; Vogt, Katharina","Augmented audification","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54113","We present a sonification method that blends between audification and auditory graphs which we call ”Augmented Audification”. It is based on a combination of single-side-band modulation and a pitch modulation of the original data stream. Benefits include the flexible adjustment of the sonification’s frequency range to the human hearing range and the possibility to interactively zoom into the data set at any scale. The paper introduces the method by three examples: deterministic harmonic complexes, random signal analysis, and seismology.","2015-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PQVAX8VU","journalArticle","2021","Fiedler, Brett L.; Walker, Bruce N.; Moore, Emily B.","To sonify or not to sonify? Educator perceptions of auditory display in interactive simulations","","","","","http://hdl.handle.net/1853/66347","With the growing presence of auditory display in popular learning tools, it is beneficial to researchers to consider not only the perceptions of the students who use the tools, but the educators who include the tools in their curriculum. We surveyed over 4000 educators to investigate educator perceptions and preferences across four interactive physics simulations for the presence and qualities of non-speech auditory display, as well as surveying users' selfrated musical sophistication as potentially predictive of auditory display preference. We find that the majority of teachers preferred the simulations with auditory display and consistently rated aspects of the experience using simulations with sound positively over the without-sound variants. We also identify simulation design features that align with trends in educator ratings. We did not find the measured musical sophistication to be a predictor of auditory display preference.","2021-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","To sonify or not to sonify?","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JK9ZF7E8","journalArticle","2000","Ghez, Claude; Dubois, R. Luke; Rikakis, Thanassis; Cook, Perry R.","An auditory display system for aiding interjoint coordination","","","","","http://hdl.handle.net/1853/50663","Patients with lack of proprioception are unable to build and maintain `internal models' of their limbs and monitor their limb movements because these patients do not receive the appropriate information from muscles and joints. This project was undertaken to determine if auditory signals can provide proprioceptive information normally obtained through muscle and joint receptors. Sonification of spatial location and sonification of joint motion, for monitoring arm/hand motions, was attempted in two pilot experiments with a patient. Sonification of joint motion though strong time/synchronization cues was the most successful approach. These results are encouraging and suggest that auditory feedback of joint motions may be substitute for proprioceptive input. However, additional data will have to be collected and control experiments will have to be done.","2000-04","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q9IAZ67U","journalArticle","2014","Zareei, Mo H.; McKinnon, Dugal; Kapur, Ajay; Carnegie, Dale A.","Complex: Physical Re-sonification of Urban Noise","","","","","http://hdl.handle.net/1853/52076","This paper explores the aesthetic and social values of the noises of modern urban soundscapes and discusses some strategies for boosting the accessibility and appreciation of works of sound art and experimental music that employ them. A proposed audiovisual installation––entitled complex––is outlined as a practical application of techniques designed to reveal the sonic aesthetics of urban technological noise, primarily through resonification and visualization. This will be achieved sonically and physically, by mapping sonic data collected from New York City soundscape (using the Citygram project) onto custom-designed mechatronic soundsculptures.","2014-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","Complex","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LUUSIQB4","journalArticle","2013","Väljamäe, A.; Steffert, T.; Holland, S.; Marimon, X.; Benitez, R.; Mealla, S.; Oliveira, A.; Jorda, S.","A review of real-time eeg sonification research","","","","","http://hdl.handle.net/1853/51645","Over the last few decades there has been steady growth in research that addresses the real-time sonification of electroencephalographic (EEG) data. Diverse application areas include medical data screening, Brain Computer Interfaces (BCI), neurofeedback, affective computing and applications in the arts. The present paper presents an overview and critical review of the principal research to date in EEG data sonification. Firstly, we identify several sub-domains of real-time EEG sonification and discuss their diverse approaches and goals. Secondly, we describe our search and inclusion criteria, and then present a synoptic summary table spanning over fifty different research projects or published research findings. Thirdly, we analyze sonification approaches to the various EEG data dimensions such as time-frequency filtering, signal level, location, before going on to consider higher order EEG features. Finally, we discuss future application domains which may benefit from new capabilities in the real-time sonification of EEG data. We believe that the present critical review may help to reduce research fragmentation and may aid future collaboration in this emerging multidisciplinary area.","2013-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4UK4I79L","book","2009","Rindel, J. H.; Christensen, C. L.","An audio mixer based on 3D acoustical simulation of architectural models","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51290","The acoustic illusion of music being performed in any room, being virtual or real, is possible with today’s advanced room acoustic software, Odeon. Recently, a music CD has been released with the simulated acoustics of the famous Hagia Sophia in Istanbul, although the recordings were originally made in an anechoic room. The quality and realism of the 3D sound is believed to be unparalleled. The acoustic simulation technique has also been applied for the simulation of a complete symphony orchestra using multi-source-auralisation and an integrated mixing of the sound from all the instruments. The room acoustic software Odeon was originally developed at the Technical University of Denmark with the purpose of making acoustic simulations as a tool for the design of concert halls and other acoustic venues. The simulation technique is based on theoretical models and approximations of the physical behaviour of sound in rooms such as reflection, absorption, scattering, and diffraction. The directional characteristic of the sound sources is also taken into account. The presentation will include music examples combined with the acoustics of a reconstructed Roman theatre, a Byzantine church, and a new concert hall.","2009-05","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VE8PQGMS","journalArticle","2003","Beamish, Timothy; van de Doel, Kees; MacLean, Karon; Fels, Sidney","D'Groove: A haptic turntable for digital audio control","","","","","http://hdl.handle.net/1853/50452","In this paper, we discuss the design and implementation of D'Groove, an intelligent Disc Jockey (DJ) system that uses a haptic turntable for controlling the playback of digital audio. We begin by describing the tasks of a DJ and defining some of the challenges associated with the traditional DJ process. We then introduce our new system, discussing how it improves auditory navigation for DJs and introduces new performance possibilities. We also discuss the role of haptics in an auditory display.","2003-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","D'Groove","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3RV3SXE8","journalArticle","2002","Johannsen, G.","Auditory display of directions and states for mobile systems","","","","","http://hdl.handle.net/1853/51337","Auditory displays for mobile systems, such as service robots, have been developed. The design of directional sounds and of additional sounds for robot states (e.g., Heavy Load), as well as the design of more complicated robot sound tracks are explained. Basic musical elements and robot movement sounds have been combined. Two experimental studies, on the understandability of the directional sounds and the robot state sounds as well as on the auditory perception of intended robot trajectories in a simulated supermarket scenario, are described. Subjective evaluations of sound characteristics such as urgency, expressiveness, and annoyance have been performed by non-musicians and musicians. These experimental results are compared with the diagrams which have been computed with two wavelet techniques for time-frequency analyses.","2002-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SX5LR7L3","journalArticle","2018","Khan, Ridwan Ahmed; Jeon, Myounghoon; Yoon, Tejin","“Musical Exercise” for people with visual impairments: A preliminary study with the blindfolded","","","","","http://hdl.handle.net/1853/60091","Performing independent physical exercise is critical to maintain one's good health, but it is specifically hard for people with visual impairments. To address this problem, we have developed a Musical Exercise platform for people with visual impairments so that they can perform exercise in a good form consistently. We designed six different conditions, including blindfolded or visual without audio conditions, and blindfolded or visual with two different types of audio feedback (continuous vs. discrete) conditions. Eighteen sighted participants participated in the experiment, by doing two exercises - squat and wall sit with all six conditions. The results show that Musical Exercise is a usable exercise assistance system without any adverse effect on exercise completion time or perceived workload. Also, the results show that with a specific sound design (i.e., discrete), participants in the blindfolded condition can do exercise as consistently as participants in the non-blindfolded condition. This implies that not all sounds equally work and thus, care is required to refine auditory displays. Potentials and limitations of Musical Exercise and future works are discussed with the results.","2018-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","“Musical Exercise” for people with visual impairments","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IU6E5ADJ","book","2015","Rutz, Hanns Holger; Vogt, Katharina; Höldrich, Robert","The SysSon platform: A computer music perspective of sonification","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54126","We introduce SysSon, a platform for the development and application of sonification. SysSon aims to be an integrative system that serves different types of users, from domain scientists to sonification researchers to composers and sound artists. It therefore has an open nature capable of addressing different usage scenarios. We have used SysSon both in workshops with climatologists and sonification researchers and as the engine to run a real-time sound installation based on climate data. The paper outlines the architecture and design decisions made, showing how a sonification system can be conceived as a collection of specialised abstractions that sit atop a general computer music environment. We report on our experience with SysSon so far and make suggestions about future improvements.","2015-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","The SysSon platform","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6MXKFBWV","book","2015","Terasawa, Hiroko; Morimoto, Yota; Matsubara, Masaki; Sato, Akira; Ohara, Makoto; Kawarasaki, Masatoshi","Guiding auditory attention toward the subtle components in electrocardiography sonification","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54144","ECG (electrocardiography) consists of a few components and features that indicate the details of electrical conduction in each part of the heart. The simple pitch-mapping sonification of ECG cannot indicate these components to listeners, limiting its potential for diagnostic usages. We present an improved method to emphasize such key features in ECG sonification, using the principle that “we are attentive to loud, high-pitched, and rapidly fluctuating sounds.” With this principle, we can guide the listener’s attention to subtle yet critical elements of the ECG waveform by controlling the degree of auditory saliency for each component. In this report, we describe ECG sonification step by step and discuss the cognitive and attentive issues as well as the emotional responses of the listeners.","2015-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RKS2F42F","journalArticle","2003","Gossmann, Joachim; Dombois, Florian","The spatial sound lab at Fraunhofer IMK.VE","","","","","http://hdl.handle.net/1853/50503","Sound in VR, generally a sidekick to the visual presentation, i s a central issue in the ongoing VR opera production within the project “Digitales Beethoven-Haus” at Fraunhofer IMK. To be able to answer the technological and artistic questions brought up by such an endeavour, a lab room was established allowing for high quality immersive VR for both sound and image at the same time. Strategies were developed to successfully stage a piece of classical music within a virtual environment, allowing the audience to interact with the virtual world in an interesting way while being faithful to the musical performance that forms the basis of what is audible.","2003-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L6R39ULK","journalArticle","2016","Zhang, Ruimin; Barnes, Jaclyn; Ryan, Joseph; Jeon, Myounghoon; Park, Chung Hyuk; Howard, Ayanna M.","Musical Robots For Children With ASD Using A Client-Server Architecture","","","","","http://hdl.handle.net/1853/56589","People with Autistic Spectrum Disorders (ASD) are known to have difficulty recognizing and expressing emotions, which affects their social integration. Leveraging the recent advances in interactive robot and music therapy approaches, and integrating both, we have designed musical robots that can facilitate social and emotional interactions of children with ASD. Robots communicate with children with ASD while detecting their emotional states and physical activities and then, make real-time sonification based on the interaction data. Given that we envision the use of multiple robots with children, we have adopted a client-server architecture. Each robot and sensing device plays a role as a terminal, while the sonification server processes all the data and generates harmonized sonification. After describing our goals for the use of sonification, we detail the system architecture and on-going research scenarios. We believe that the present paper offers a new perspective on the sonification application for assistive technologies.","2016-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H9HUDFJR","journalArticle","2005","Zouhar, Vit; Lorenz, Rainer; Musil, Thomas; Zmolnig, Johannes; Höldrich, Robert","Hearing Varese's Poeme Electronique inside a virtual philips pavilion","","","","","http://hdl.handle.net/1853/50082","Topic of this paper is an interactive sound reproduction system to create virtual sonic environments to visual spaces, called Virtual Audio Reproduction Engine for Spatial Environments (VARESE). Using VARESE, Edgard Varèse's Poème électronique was recre- ated within a simulated Philips Pavilion (a construction originally designed by Le Corbusier and Iannis Xenakis for the 1958 World Fair in Brussels, then dismantled after its closing). The system draws on binaural sound reproduction principles including spatial- ization techniques based on the Ambisonics theory. Using head- phones and a head-tracker, listeners can enjoy a preset reproduc- tion of the Poème électronique from their individual standpoint as they freely move through the virtual architectural space. While VARESE was developed specifically for its use in reconstructing Poemè électronique, it is flexible enough to function as a standard interpreter of sounds and visual objects, enabling users to design their own spatializations. The system runs on a standard laptop PC with a modern graphics card, using Miller Puckette's computer music software Pure Data (pd), plus an extended graphic interface (running equally on Linux and MS Windows operating systems and featuring a maximum of five sound sources using Ambison- ics).","2005-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZL53P9RD","journalArticle","2014","Perkins, Rhy","An Enhanced Data Transformation Framework for the Sonification of Simulated Rigid Bodies","","","","","http://hdl.handle.net/1853/52047","While simulated rigid bodies hold a wealth of information that can be understood through sound (Table 1), current interpretive methods will often overlook important features of their data. This proves to be detrimental when placing the same data in the context of an auditory display where the user might wish to analyse or express specific dimensions under a range of circumstances. The following investigation describes a framework for a model-induced parameter mapping technique which allows for an explicit level of control over the flow of information, supported by a number of key conditions in both the auditory and visual channels. Given that the formative decisions behind this design tailors the data to meet the purposes of sonification, the user is presented with a viable alternative that overcomes a number of limitations inherent to employing a more conventional physical modelling approach.","2014-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6V5HK77Y","journalArticle","2017","Newbold, Joseph W.; Bianchi-Berthouze, Nadia; Gold, Nicolas E.","Musical Expectancy in Squat Sonification For People Who Struggle With Physical Activity","","","","","http://hdl.handle.net/1853/58350","Physical activity is important for a healthy lifestyle. However, it can be hard to stay engaged with exercise and this can often lead to avoidance. Sonification has been used to support physical activity through the optimisation/correction of movement. Though previous work has shown how sonification can improve movement execution and motivation, the specific mechanisms of motivation have yet to be investigated in the context of challenging exercises. We investigate the role of music expectancy as a way to leverage people’s implicit and embodied understanding of music within movement sonification to provide information on technique while also motivating continuation of movement and rewarding its completion. The paper presents two studies showing how this musically informed sonification can be used to support the squat movement. The results show how musical expectancy impacted people’s perception of their own movement, in terms of reward, motivation and movement behaviour and the way in which they moved.","2017-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K6J9JAZ2","book","2015","Sjuve, Eva","Metopia: experiencing complex environmental data through sound","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54199","This extended abstract describes Metopia, a research project in the early stages of progress, a wireless sensor network for urban spaces, to aquire a complex set of data from the environment for the purpose of making a sound composition. The programming language Pure Data is used to create a sound composition from the aquired data. This research project is using a real-world problem such as air-pollution as a way to explore a responsive environment, to communicate the state of the toxic level into an immediate auditory response. Atmospheric pollutants is a major health issue and Metopia is one way of examining this problem through aesthetic and conceptual choices and at the same time making sense of complex data through generative principles and through algorithms. The composition is using Pure Data on embedded Raspberry Pi 2 equipped with ARM processors for real-time processing, coupled with an array of sensors using Arduino for data acquisition.","2015-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","Metopia","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8VWRV9FL","book","2009","Laurson, Mikael; Kuuskankare, Mika; Kuitunen, Kimmo; Sandred, Orjan","Statistical rules in constraint-based programming","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51416","In this paper we introduce a system that first generates statistical analysis data from a musical score. The results are then translated automatically to constraint rules that in turn can be used in com- bination with ordinary rules to generate scores that have similar statistical distributions than the original. Statistical analysis rules are formalized using our special rule syntax where our focus will be in the pattern-matching part of the rules. The pattern-matching part has two important tasks in our paper: first, it is used to extract various musical entities from the score, such as melodic, harmonic and voice-leading formations; second, it is used also to generate statistical rules which will be used in the re-synthesis part of our system. We first introduce the rule syntax. After this we discuss a practical case study where we analyze a melodic line. Finally we generate out of this material statistical rules which are used to produce new scores.","2009-05","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EVXBZZJM","book","2009","Kuuskankare, Mika; Laurson, Mikael","Extending Vivo as a MIR System","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51299","In this paper we extend the VIVO system with MIR function- ality that allows us to extract the VIVO rule database from a sym- bolic score. As a reference material we use a collection of se- lected Chorale harmonizations by J.S.Bach. Our extended system takes symbolic scores as input and generates a harmonic progres- sion database as output. The VIVO and the new extended sys- tem are realized inside the PWGL environment. Complete work- ing patches are given as examples. As a proof of concept the database-extracted from the repertoire-is then used in the VIVO to re-harmonize a short melody.","2009-05","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XB9QREKS","journalArticle","2005","Frauenberger, C.; Putz, V.; Höldrich, Robert; Stockman, T.","Interaction patterns for auditory user interfaces","","","","","http://hdl.handle.net/1853/50161","This paper proposes the use of interaction patterns in the design process of auditory displays in human-computer interaction. To avoid introducing visual concepts in auditory design, a common ground for developing user interfaces without determining their means of representation is proposed. This meta domain allows for the design of user interfaces which can be equally realised in different interaction modalities or multi-modal settings. Although this work focuses on the auditory domain the concept shown is developed keeping in mind that it should be equally applicable in other modalities. A set of mode independent interaction patterns for design in the meta domain are introduced along with their transformation into the auditory domain. A real world application was chosen to evaluate the approach. MS Explorer was analysed, described through the mode independent interaction patterns and transformed into the auditory domain making extensive use of 3D audio rendering techniques. The result, a virtual audio reality version of a file manager, was evaluated with normally sighted persons as well as visually impaired and blind participants showing the feasibility and usability of the approach.","2005-07","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BBVD2VIS","journalArticle","2008","Meier, Manuela; Saranti, Anna","Sonic Explorations With Earthquake Data","","","","","http://hdl.handle.net/1853/49954","The composition “underground sounds” - an interdisciplinary project including a concert piece as its artistic element - deals with the phenomenon of the constantly moving, therefore resonating earth and is based on data taken from an earthquake which reached 7.8 on the Richter scale and triggered a tsunami on April 1st, 2007 close to the Solomon Islands in the Southwestern Pacific. The data from several related seismic events was provided via a real-time data server belonging to the GEOFON network of seismic stations and converted to audio data using programs specifically developed for that purpose “underground sounds” is not an audification; the seismometers records were used as raw material for several applications of signal processing effects. The four parts of the composition concentrate on different characteristics of seismic events including sounds of the same seismic event recorded by different stations, the filtered harmonic sounds of the measuring instruments and the output of the separation of the earthquake's impulse-like components from the earth's constant movements, each used as separate instruments in the composition.","2008-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6Q2PMK4H","journalArticle","2007","Braasch, Jonas","Cranial Transitions for Soprano Saxophone and Electronic Processing","","","","","http://hdl.handle.net/1853/49984","Cranial Transitions was written for soprano saxophone and electronic processing to explore the relationship between internally and externally perceived auditory events. The piece, which was recorded for reproduction with headphones, is based on the personal observation that too often our internal representation of musical ideas seems to be more ambiguous and abstract than we share with others. Thus, the external auditory events which symbolize the outside representation of our musical life follow more traditional techniques of arpeggios and melodies, while the internal representation follows more abstract ideas. The internal world is manipulated with electronic processing using a specially designed program, the Intra-Cranial Spatializer. In contrast, the outside world captures the acoustic side of the soprano saxophone. In this case, electronic processing is only used to create a virtual acoustic environment.","2007-06","2023-07-13 06:25:43","2023-07-13 06:25:43","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HEB4RNDV","journalArticle","2003","Sandor, Aniko; Lane, David M.","Sonification of absolute values with single and multiple dimensions","","","","","http://hdl.handle.net/1853/50487","Although auditory displays are effective for the representation of patterns in data, they are generally thought to be less effective for the communication of absolute values [1, 2]. Nonetheless, there are times when it is desirable to represent absolute values with sound. Two experiments were conducted to investigate the limits of representing absolute values with sound and to compare several ways of representing the values. Temporal representations of data led to better performance than the representation with pitch or the redundant use of temporal and pitch representations. We introduce the term “Mappable Difference” to refer to the smallest difference in a dimension that can be consistently mapped to a numeric value. Knowing the “Mappable Difference” in an auditory dimension can potentially aid display designers in determining the number of absolute values that can be represented by sound.","2003-07","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BYEDDYHN","journalArticle","2018","Tsuchiya, Takahiko; Freeman, Jason","A study of exploratory analysis in melodic sonification with structural and durational time scales","","","","","http://hdl.handle.net/1853/60092","Melodic sonification is one of the most common methods of sonification: data modulates the pitch of an audio synthesizer over time. This simple sonification, however, still raises questions about how we listen to a melody and perceive the motions and patterns characterized by the underlying data. We argue that analytical listening to such melodies may focus on different ranges of the melody at different times and discover the pitch (and data) relationships gradually over time and after repeated listening. To examine such behaviors in real-time listening to a melodic sonification, we conducted a user study employing interactive time and pitch resolution controls for the user. The study also examines the relationships of these changing time and pitch resolutions to perceived musicality. The results indicate a stronger general relationship between the time progression and the use of time-resolution control to analyze data characteristics, while the pitch resolution controls tend to have more correlation with subjective perceptions of musicality.","2018-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CTJBRTMP","journalArticle","2001","Tzanetakis, George; Cook, Perry","MARSYAS3D: A prototype audio browser-editor using a large scale immersive visual and audio display","","","","","http://hdl.handle.net/1853/50625","Most audio editing tools offer limited capabilities for browsing and editing large collections of files. Moreover working with many audio files tends to clutter the limited screen space of a desktop monitor. In this paper we describe MARSYAS3D, a prototype audio browser and editor for large audio collections. A variety of 2D and 3D graphics interfaces for working with collections and/or individual files have been developed. Many of these interfaces are informed by automatic content-based audio analysis tools. Although MARSYAS3D can be used with a desktop monitor it has been specifically designed as an application for the Princeton Scalable Display Wall project. The current Display Wall system has an 8 x 18-foot rear projection screen with a resolution of 4096 x 1536 pixels. For sound a custom-made 16-speaker surround system is used. The users interact with the Wall using a variety of input methods. This immersive display allows for visual and aural presentation of detailed information for browsing and editing large audio collections and supports natural interactive collaborations among multiple simultaneous users.","2001-07","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","MARSYAS3D","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7Q5LLB56","journalArticle","2004","Roeber, N.; Masuch, M.","Interacting with sound: An interaction paradigm for virtual auditory worlds","","","","","http://hdl.handle.net/1853/50861","The visual and the auditory field of perception respond on different input signals from our environment. Thus, interacting with worlds solely trough sound is a very challenging task. This paper discusses methods and techniques for sonification and interaction in virtual auditory worlds. In particular, it describes auditory elements such as speech, sound and music and discusses their application in diverse auditory situations, as well as interaction techniques for assisted sonification. The work is motivated by the development of a framework for the interactive exploration of auditory environments which will be used to evaluate the later discussed techniques. The main focus for the design of this framework is the use in narrative environments for auditory games, but also for general purpose auditory user interfaces and communication processes.","2004-07","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","Interacting with sound","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"48EBDKYM","journalArticle","2001","Quinn, Marty","Research set to music: The climate symphony and other sonifications of ice core, radar, DNA, seismic and solar wind data","","","","","http://hdl.handle.net/1853/50634","The Climate Symphony and Other Sonifications of Ice Core, Radar, DNA, Seismic and Solar Wind Data is a one-hour performance/presentation of sonification research by Marty Quinn of Design Rhythmics Sonification Research Lab and BAE Systems. It was presented in November 2000 at the National Science Foundation at the invitation of the Director's Office of Public Affairs and the Office of Polar Programs and was warmly received. This paper describes the Climate Symphony portion of the presentation in detail.","2001-07","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","Research set to music","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y8W5KPD8","book","2010","Jeon, Myounghoon","Two or Three Things you need to know about AUI Design or Designers","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/50064","This paper presents an overview of the work on Auditory User Interface (AUI) design at the LG Electronics Corporate Design Center, where the author was responsible for all of the AUI designs from 2005 to 2008. The definition and strategy of AUI design is covered, as well as the role of AUI designers at the global company. Details on process, methodology, and design solutions of four big projects are provided. The paper also discusses how a practitioner’s perspective is related to theoretical framework of auditory display. The review of this practical AUI design aims to inspire other practitioners and researchers on auditory display and sound design, and to facilitate communication in the ICAD community.","2010-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5NSPDJ5S","book","2009","Baldan, Stefano; Ludovico, Luca; Mauro, Davide A.","Puremx: automatic transcription of midi live music performances into xml format","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51410","This paper addresses the problem of the real-time automatic tran- scription of a live music performance into a symbolic format based on XML. The source data are given by any music instrument or other device able to communicate with Pure Data by MIDI. Pure Data is a free, multi-platform, real-time programming environment for graphical, audio, and video processing. During a performance, music events are parsed and their parameters are evaluated thanks to rhythm and pitch detection algorithms. The final step is the creation of a well-formed XML document, validated against the new international standard known as IEEE 1599. This work will shortly describe both the software environment and the XML format, but the main analysis will involve the real- time recognition of music events. Finally, a case study will be presented: PureMX, an applica- tion able to perform such an automatic transcription.","2009-05","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","Puremx","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AQY244EE","journalArticle","2004","Coyle, Eugene; Cullen, C.","Orchestration within the sonification of basic data sets","","","","","http://hdl.handle.net/1853/50901","The use of sonification as a means of representing and analysing data has become a growing field of research in recent years and as such has become a far more accepted means of working with data. Existing work carried out as part of this research has focused primarily on the sonification of DNA/RNA sequences and their subsequent protein structures for the purposes of analysis. This sonification work raised many questions as regards the need for sequences to be set to music in a standard manner so that different strands could be analysed by comparison, and hence the orchestration and instrumentation used became of great importance. The basic principles of sonification can be rapidly extended to include many different data elements within a single rendering, and thus the importance of orchestration grows accordingly. Existing work on the use of rhythmic parsing within a sonification had suggested that far more information could be represented when orchestrated in a rhythmic manner than when simply reconstituted in single musical block. The principle was further extended to include the allocation specific instruments and pitches within rhythmic patterns so that each sonic event would convey the data it was intended to represent. To this end a fictional database of employees in a company was created as a means of developing the principles required for more effective sonification through orchestration. The employee database was intended as a means of using a straightforward data set to analyse the effect of basic changes in instrumentation and orchestration rather than the data itself. The allocation of chord intervals or melodies to different data elements allowed the data to be represented in different ways at output in order that these differences would eventually highlight some form of framework for effective sonification of data sets with multiple elements.","2004-07","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VQ67H8WL","book","2015","Lindborg, PerMagnus; Yuning, David Liu","Locust wrath: An iOS audience participatory auditory display","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54117","Mobile devices have been used in soundscape installations and performances over the past decade or longer, often to emphasize social interaction. Multichannel sonification has been found to successfully represent data describing kinematic phenomena. However, there are few if any examples where these two approaches are combined. The Locust Wrath project has evolved in stages: first, as surround sonifications of climate data for a multimedia dance performance; then, as a frontal display sound installation and as material in a live performance of ‘musical’ interactive sonification; and recently, as an audience participator work. We developed a system for spatialized sonification of data using a server-client model with iOS devices. In two multimedia performances, the audience members’ iPhones were employed ad hoc to constitute a large auditory display. This paper describes the artistic background to the project, outlines the stages, and focuses on the design and implementation of the Locust Wrath client app.","2015-07","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","Locust wrath","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ENNA83NG","journalArticle","2021","Ciardi, Fabio Cifariello","Strategies and tools for the sonification of prosodic data: A composer's perspective","","","","","http://hdl.handle.net/1853/66344","Does it make sense to sonify information that refers to an already audible phenomenon such as prosodic data? In order to be useful, a sonification of prosody should contribute to the comprehension of paralinguistic features that may not otherwise attract the attention of the listener. Within this context, this paper illustrates a modular and flexible framework for the reduction and processing of prosodic data to be used for enhancing the perception of a speaker's intention, attitude and emotions. The model uses speech audio as input and provides MIDI and MusicXML data as output so allowing samplers and notation software to auralize and display the information. The described architecture has been subjectively tested by the author over many years in compositions for solo instruments, ensembles and orchestra. Two outcomes of the research are discussed: the advantages of an adaptive strategy for data reduction, and the auditory display of the deep pitch and temporal structures underlying prosodic processing.","2021-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","Strategies and tools for the sonification of prosodic data","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZRHYD47W","book","2015","Tsuchiya, Takahiko; Freeman, Jason; Lerner, Lee W.","Data-to-music API: Real-time data-agnostic sonification with musical structure models","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54146","In sonification methodologies that aim to represent the underlying data accurately, musical or artistic approaches are often dismissed as being not transparent, likely to distort the data, not generalizable, or not reusable for different data types. Scientific applications for sonification have been, therefore, hesitant to use approaches guided by artistic aesthetics and musical expressivity. All sonifications, however, may have musical effects on listeners, as our trained ears with daily exposure to music tend to naturally distinguish musical and non-musical sound relationships, such as harmony, rhythmic stability, or timbral balance. This study proposes to take advantage of the musical effects of sonification in a systematic manner. Data may be mapped to high-level musical parameters rather than to one-to-one low-level audio parameters. An approach to create models that encapsulate modulatable musical structures is proposed in the context of the new DataTo- Music JavaScript API. The API provides an environment for rapid development of data-agnostic sonification applications in a web browser, with a model-based modular musical structure system. The proposed model system is compared to existing sonification frameworks as well as music theory and composition models. Also, issues regarding the distortion of original data, transparency, and reusability of musical models are discussed.","2015-07","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","Data-to-music API","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ELVFFVTD","journalArticle","2007","Jung, Ralf; Schwartz, Tim","Peripheral Notification with Customized Embedded Audio Cues","","","","","http://hdl.handle.net/1853/50022","Peripheral notification services allow users to monitor information with less distraction of attendees in their surrounding. In the majority of cases, the information is provided by visual displays that often have several disadvantages, e.g. the lack of privacy or the user is locally bounded to the surrounding of the display. In this paper, we introduce an approach for a discreet notification of persons in multi-user environments. In particular, we use the current user position to provide a personalized and locationaware notification service with non-speech audio cues embedded in aesthetic background music. Thereby we enriched the music, and especially the notification audio cues, with functionality to influence the perception and to control the attention of the listeners. These functional pieces of music should stay in the peripheral background to avoid too much attention. The used ambient soundscapes and the set of corresponding notification instruments were composed and recorded by ourselves. The development process including compositional constraints with respect to auditive perception and emotional effects raised by music will also be introduced in this paper as well as the nomadic event notification service which includes our indoor positioning system for mobile devices.","2007-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LE68IP4X","journalArticle","2017","Nees, Michael A.; Harris, Joanna; Leong, Peri","How Do People Think They Remember Melodies and Timbres? Phenomenological Reports of Memory for Nonverbal Sounds","","","","","http://hdl.handle.net/1853/58378","Memory for nonverbal sounds such as those used in sonifications has been recognized as a priority for cognitiveperceptual research in the field of auditory display. Yet memory processes for nonverbal sounds are not well understood, and existing theory and research have not provided a consensus on a mechanism of memory for nonverbal sounds. We report a new analysis of a qualitative question that asked participants to report the strategy they used to retain nonverbal sounds—both melodies and sounds discriminable primarily by timbre. The question was originally posed as part of the debriefing procedure for three separate memory experiments whose primary findings are reported elsewhere. Results of this new analysis suggested that auditory memory strategies— remembering acoustic properties of sounds—were common across both types of sounds but were more commonly reported for remembering melodies. Motor strategies were also more frequently reported for remembering melodies. Both verbal labeling of sounds and associative strategies—linking the sounds to existing information in memory—were more commonly reported as strategies for remembering sounds discriminable primarily by timbre. Implications for theory and future research are discussed.","2017-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","How Do People Think They Remember Melodies and Timbres?","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QR7NLG83","journalArticle","2019","Morawitz, Falk","Multilayered narration in electroacoustic music composition using nuclear magnetic resonance data sonification and acousmatic storytelling","","","","","http://hdl.handle.net/1853/61526","Nuclear magnetic resonance (NMR) spectroscopy is an analytical tool to determine the structure of chemical compounds. Unlike other spectroscopic methods, signals recorded using NMR spectrometers are frequently in a range of zero to 20000 Hz, making direct playback possible. As each type of molecule has, based on its structural features, distinct and predictable features in its NMR spectra, NMR data sonification can be used to create auditory ﾑfingerprintsﾒ of molecules. This paper describes the methodology of NMR data sonification of the nuclei nitrogen, phosphorous, and oxygen and analyses the sonification products of DNA and protein NMR data. The paper introduces On the Extinction of a Species, an acousmatic music composition combining NMR data sonification and voice narration. Ideas developed in electroacoustic composition, such as acousmatic storytelling and sound-based narration are presented and investigated for their use in sonification-based creative works.","2019-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VQ8Q7285","journalArticle","2021","Spence, Heather Ruth; Ballora, Mark","Layers of meaning: The ocean's natural acoustics and the music of its datasets","","","","","http://hdl.handle.net/1853/66323","The transdisciplinary National Academies Keck Futures Initiative (NAKFI) conference on the Deep Blue Sea sparked a collaboration between sonification expert Mark Ballora and marine biologist and sound artist Heather Spence. Research involving long-term Marine Passive Acoustic Monitoring (MPAM) of the MesoAmerican Reef system forms the basis for a gradient of audio products: 1) layering a tour guide acoustic instrument over raw and manipulated soundscape recordings; 2) layering of multiple acoustic instruments over duty cycle interpretation sampling; and 3) layering of data sonification over the original data, with additional acoustic instrument layers. The audio products are designed to promote data exploration and understanding by researchers and students, as well as an emotional impact musically with conservation themes. Presentations have included live and virtual performances and workshops. Next steps include sonification of other correlated environmental data with the original sound data in raw, manipulated, and sonified forms.","2021-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","Layers of meaning","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UF45AI8W","journalArticle","2005","Lee, Kyogu; Sell, Gregory; Berger, Jonathan","Sonification using digital waveguides and 2- and 3-dimensional digital waveguide mesh","","","","","http://hdl.handle.net/1853/50181","We describe a method of auditory display of complex data, object identification, and classification using digital waveguides and waveguide mesh. Our overall goal is to distinguish highly dimensional data sets from one another in such a way that reveals meaningful differences in a particular context. In this paper we provide a summary of the application of waveguide and waveguide mesh architectures to sonification, and demonstrate the digital waveguide, 2- and 3-dimensional mesh in a variety of sonification tasks.","2005-07","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YA42WQCD","journalArticle","2019","Mitchell, Thomas J.; Thom, Jess; Pountney, Matthew; Hyde, Joseph","The alchemy of chaos: A sound art sonification of a year of Tourette’s episodes","","","","","http://hdl.handle.net/1853/61514","Touretteshero is the name of a organisation that aims to raise awareness of Touette's syndrome by sharing and celebrating the creativity and humour of the involuntary vocal and movement tics that characterise the condition. This paper documents the development of a Touretteshero project called The Alchemy of Chaos, a sound art piece that translates a year of intensive ticcing episodes (or 'ticcing fits') into a six minute sonification. The work emphasises both the faithful representation of data and the aesthetic sound quality, drawing techniques and ideas from sound design for film, which is often used to convey information about a visual scene in ways that can be used for sonfication. Specifically, the work uses Chion's elements of auditory setting: short punctual sounds that can express locations with minimal sonic references. Sound parameters are also classified into groups that have data significance and those that do not, with aesthetic interventions limited to those parameters that do not impact on data transparency. The resulting piece was included within a keynote talk at the Royal Albert Hall in the UK and the paper includes a qualitative reflection on the work and the potential value that sound design techniques for film can bring to the auditory display community.","2019-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","The alchemy of chaos","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GS4XYWAM","journalArticle","2005","Terasawa, Hiroko; Slaney, Malcolm; Berger, Jonathan","Perceptual distance in timbre space","","","","","http://hdl.handle.net/1853/50176","This paper describes a perceptual space for timbre, defines an ob- jective metric that takes into account perceptual orthogonality, and measures the quality of timbre interpolation applicable to percep- tually valid timbral sonification. We discuss two timbre represen- tations and measure perceptual judgment. We determined that a timbre space based on Mel-frequency cepstral coefficients (MFCC) is a good model for perceptual timbre space.","2005-07","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IN7GMTS4","journalArticle","2007","Lennox, Peter; Myatt, Tony","Concepts of Perceptual Significance for Composition and Reproduction of Explorable Surround Sound Fields","","","","","http://hdl.handle.net/1853/49987","Recent work in audio and visual perception suggests that, over and above sensory acuities, exploration of an environment is a most powerful perceptual strategy. For some uses, the plausibility of artificial sound environments might be dramatically improved if exploratory perception is accommodated. The composition and reproduction of spatially explorable sound fields involves a different set of problems from the conventional surround sound paradigm, developed to display music and sound effects to an essentially passive audience. This paper is based upon contemporary models of perception and presents proposals for additional spatial characteristics beyond classical concepts of three-dimensional positioning of virtual objects.","2007-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZK2X66RQ","journalArticle","2014","Musick, Michael; Turner, Johnathan; Park, Tae Hong","Interactive Auditory Display of Urban Spatio-Acoustics","","","","","http://hdl.handle.net/1853/52088","This paper presents an interactive exploration platform and toolset for spatial, big-data auditory display. The exploration platform is part of the Citygram project, which focuses on geospatial research through a cyber-physical system that automatically streams, analyzes, and maps urban environmental acoustic energies. Citygram currently concentrates on dynamically capturing geo-tagged, low-level audio feature vectors from urban soundscapes. These various feature vectors are measured and computed via Android-based hardware, traditional personal computers, and mobile computing devices that are equipped with a microphone and Internet connection. The low-level acoustic data streams are then transmitted to, and stored in, the Citygram database. This data can then be used for auditory display, sonification, and visualization by external clients interfacing with the Citygram server. Client users can stream data bi-directionally via custom software that runs on Cycling ‘74’s Max and SuperCollider, allowing for participatory citizen-science engagement in auditory display.","2014-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IMLWDY5W","journalArticle","2003","Frauenberger, Christopher; Noistering, Markus","3D audio interfaces for the blind","","","","","http://hdl.handle.net/1853/50428","Our work is dealing with alternative interaction modes for visually impaired and blind people to use computers. The aim of the proposed approach is to exploit the human hearing capabilities to a better degree than this is done by customary screen-readers. A surrounding, three-dimensional audio interface is potentially increasing the information flow between a computer and the user. This paper presents a virtual audio reality (VAR) system which allows computer users to explore a virtual environment only by their sense of hearing. The used binaural audio rendering implements directional hearing and room acoustics via headphones to provide an authentic simulation of a real room. Users can freely move around using a joystick. The proposed application programming interface (API) is intended to ease the development of user applications for this VAR system. It provides an easy to use C++ interface to the audio rendering layer. The signal processing is performed by a digital signal processor (DSP). Besides the details of the technical realisation, this paper also investigates the user requirements for the target group.","2003-07","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FRHH7ARS","journalArticle","2014","Nees, Michael A.; Best, Kathryn","A Verification Task with Lateralized Tones and Accelerated Speech","","","","","http://hdl.handle.net/1853/52102","Research has suggested that the left hemisphere of the brain may be specialized for processing auditory speech, whereas the right hemisphere may be specialized for processing nonspeech auditory stimuli. Due to contralaterality in auditory pathways, this functional specialization has been reflected in behavioral advantages for speech stimuli presented to the right ear and for nonspeech stimuli presented to the left ear. We used a verification task with lateralized presentations of brief tonal stimuli (sonifications) and accelerated speech stimuli (spearcons) to examine performance as a function of the presentation ear and the type of auditory display. The general pattern of results showed that reaction time and accuracy were facilitated when two accelerated speech stimuli were compared to each other. Based on the results of this study, reported effects of left and right ear advantages do not seem to be robust enough to warrant general ergonomic recommendations (i.e., left ear presentation of nonspeech sounds and right ear presentation of speech sounds) for auditory display design.","2014-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QTMXJHX7","book","2009","Allali, J.; Antoniou, P.; Ferraro, P.; Iliopoulos, C. S.; Michalakopoulos, S.","Overlay problems for music and combinatorics","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51407","Motivated by the identification of the musical structure of pop songs, we introduce combinatorial problems involving overlays (non-overlapping substrings) and the covering of a text t by them. We present 4 problems and suggest solu- tions based on string pattern matching techniques. We show that decision problems of this type can be solved using an Aho-Corasick keyword automaton. We conjecture that one general optimization problem of the type, is NP-complete and introduce a simpler, more pragmatic optimization prob- lem. We solve the latter using suffix trees and finally, we suggest other open problems for further investigation.","2009-05","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KCI7URKW","journalArticle","2003","Brazil, Eoin; Fernstrom, Mikael","Where's that sound? Exploring arbitrary user classifications of sounds for audio collection management","","","","","http://hdl.handle.net/1853/50508","Collections of sound and music of increasing size and diversity are used both by general personal computer users and multimedia designers. Browsing audio collections poses several challenges to the design of effective user interfaces. In this paper, we report results from a new version of the Sonic Browser for managing general sound resources on personal computers. In particular, we have evaluated browsing of everyday sounds. The investigation was directed at comparing browsing of audio resources with arbitrary classifications. The problem of sound resource browsing for multimedia designers is the specific area of focus for our experiment. Finally, we conclude with current trends of our approach for further improvement of the system.","2003-07","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","Where's that sound?","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U2BB832Z","journalArticle","2008","Lagrange, Mathieu; Scavone, Gary; Depalle, Philippe","Time-Domain Analysis / Synthesis of the Excitation Signal in a Source / Filter Model of Contact Sounds","","","","","http://hdl.handle.net/1853/49962","Contact sounds represent an important subset of environmental sounds that are useful for enhancing the interaction of a user with a computer-simulated virtual reality or augmented environment. The real-time synthesis of these sounds has received much attention in the auditory display community and some convincing results have been achieved[1, 2]. This paper focuses on the modeling, analysis, and synthesis of more complex contact interactions, such as sliding, rolling and bouncing. We assume the widely adopted source-filter approach and use a modal representation of the filter component. In this paper, an explicit time-domain model of the excitation is proposed that produces promising results. The main advantages of the proposed model is that it allows a compact and versatile representation of the system, as well as an efficient synthesis scheme.","2008-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WJYEN9VD","journalArticle","2001","Childs, Edward","The sonification of numerical fluid flow simulations","","","","","http://hdl.handle.net/1853/50655","Computational Fluid Dynamics (CFD) software simulates fluid, air flow and heat transfer by solving the Navier-Stokes (N-S) equations numerically. Realistic 3-D engineering simulations typically yield the values of 7 or more variables (e.g. fluid component velocities and temperatures) at hundreds of thousands of points in space, all as a function of time. It has been noted that solutions of the N-S equations sometimes yield highly complex, non-linear flow fields which can be aesthetically interesting from a purely visual standpoint. The analysis of CFD results may benefit substantially from sonification, to depict convergence behavior, scan large amounts of data with low activity, or codify global events in the flow field. As a corollary to this interest in developing CFD sonification techniques, we can explore its unusual potential as a tool for algorithmic musical composition. This paper will report the results of an initial implementation of the author's port of the two-dimensional, steady, laminar CFD code TEACH-L on a JAVA platform, in which the numerical output is linked in real time to the JSyn digital audio synthesis package. The sonification of steady, laminar, developing flow in a two dimensional duct will be described in detail.","2001-07","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DY5E6ST4","journalArticle","2003","Zmolnig, Johannes; Ritsch, Winfried; Sontacchi, Alois","The IEM-cube","","","","","http://hdl.handle.net/1853/50505","Traditional multichannel-reproduction systems are mainly used for recreation of pantophonic sound elds. Fully periphonic reproduction has been limited by computational power to manipulate large numbers of audio-channels as well as needed speaker-layouts. Since in the last years digital hardware has become fast enough to meet the computational requirements, a medium-sized concert-hall for reproduction of periphonic electro-acoustic music, the so called IEM-Cube, has been installed at the the IEM. The room is equipped with a hemisphere consisting of 24 loudspeakers, that allows reproduction of three-dimensional sound elds following ambisonic principles of at least 3rd order. To make use of this, a linear 3Dmixing system on PC-basis has been developed. The system may be used as a production-tool for periphonic mixing into a set of ambisonic-channels, as a reproduction-environment for recreating a 3D-sound eld out of such set of ambisonic-encoded channels, and as a live-instrument that allows free positioning and movement of a number of virtual sources in real-time.","2003-07","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HDQ4J7V9","book","2015","Goudarzi, Visda; Vogt, Katharina; Höldrich, Robert","Observations on an interdisciplinary design process using a sonification framework","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54108","In sonification of scientific data, designers know very little about the domain science and domain scientists are not familiar with the sonification methodology. The knowledge about the domain science is not given, but evolved during the problem-solving process. We discuss design challenges in auditory display design regarding user-centerdness and introduce an approach to involve domain scientists throughout a sonification design. We explore this within a workshop in which sonification experts, domain experts, and programmers worked together to better understand and solve problems collaboratively. The sonification framework that is used during the workshops is briefly described and the workshop process and how each group worked together during the workshop sessions is examined. Participants worked on pre-defined and exploratory tasks to sonify climate data. Resulting sonification prototypes and workshop sessions are documented on a wiki and could be used as a starting point for future sonification procedures. Furthermore, the participants grasped each others’ domains; climate scientists especially became more open to use auditory display and sonification as a tool in their data mining tasks.","2015-07","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3TZ4RZ2M","journalArticle","2011","Gurevich, Michael; Donohoe, Donal; Bertet, Stephanie","Ambisonic Spatialization for Networked Music Performance","","","","","http://hdl.handle.net/1853/51573","In spite of recent widespread interest in network technologies for real-time musical collaboration between distant locations, there has been little focus on spatial audio in such applications. We discuss the potential for dynamic spatialization in the context of network music collaboration, in particular through the use of Higher Order Ambisonics. We describe a platform for real-time encoding, streaming and decoding of spatial audio using Ambisonics, and provide details of two case studies of creative applications built on top of this platform. We demonstrate that Higher Order Ambisonics is a viable and effective means for real-time, simultaneous spatialization in multiple locations, and that it enables a range of creative uses that explore the nature of space, distance and location in networked performance.","2011-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3DXZVG2T","journalArticle","2007","Song, Hong Jun; Beilharz, Kirsty; Cabrera, Densil","Evaluation of Spatial Presentation in Sonification for Identifying Concurrent Audio Streams","","","","","http://hdl.handle.net/1853/49998","The ultimate goal of sonification is to transfer information effectively to listeners. While there is a large amount of multidisciplinary investigation in the field of psychoacoustic, psychology, cognition and human computer interaction, sonification design still lacks empirical evidence on which to base design decisions [1]. This paper presents an empirical investigation of spatialization, which can provide one or more dimensions for auditory display. It focuses, in particular, on evaluating spatial presentation in sonification so as to enhance pattern identification when two audio streams are played simultaneously. Hence it aims to develop design decisions that benefit from effective information representation. The sounds were created for binaural reproduction using nonindividual head-related transfer functions. The results reported are based on the listeners' performance within two display modes: (i) two co-located streams and (ii) two streams spatially separated at static locations. It concludes with ideas for future improvements and developments for this type of sonification.","2007-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ULWH53P8","journalArticle","2004","Frauenberger, C.; Hoeldrich, R.; De Campo, A.","A generic semantically based design approach for spatial auditory computer displays","","","","","http://hdl.handle.net/1853/50778","This paper describes a design approach for creating generic computer user interfaces with spatial auditory displays. It proposes a structured depiction process from formulating mode independent descriptions of user interfaces (UIs), to audio rendering methods for virtual environments. As the key step in the process a semantic taxonomy of user interface content is proposed. Finding semantic classi cations of UI entities corresponding to properties of auditory objects is the ultimate goal. We beleive that this abstract approach detaches the process from visual paradigms and will reveal valuable insights into the representation of user interfaces in the auditory domain.Possible ways of accessing operating systems for UI information are discussed along with an overview over common accessibility interfaces. Critical aspects are highlighted for the composition of auditory UI entities in spatial environments and state-of-the-art techniques are presented for the creation of 3D audio. Besides some possible elds of application, relevant utility and usability engineering aspects are discussed.","2004-07","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NMUQ6ZSM","book","2015","Huotilainen, Minna; Gröhn, Matti; Yli-Kyyny, Iikka; Virkkala, Jussi; Paunio, Tiina","Sleep Enhancement by Sound Stimulation","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54210","Recently research groups have reported that the depth and/or duration of Slow Wave Sleep (SWS) can be increased and memory consolidation can be improved by playing short sounds with approximately 1-2 second intervals during or prior to SWS. These studies have used sounds with neutral or negative valence: sinusoidal 1-kHz tones or short pink noise bursts. We have confirmed memory improvement with own experiments using short pink noise bursts. Since music therapy research shows beneficial effects of pleasant, natural sounds and music, the sounds in the experiments may have been suboptimal, and we are currently extending these finding using optimized sound stimulus. In this work in progress experiment setup is described.","2015-07","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZYSXYLXD","journalArticle","2021","Dedousis, Georgos; Katsantonis, Konstantinos; Georgaki, Anastasia; Andreopoulou, Areti","Designing historically informed soundscapes for the augmentation of modern travel-guides: Challenges and compromises","","","","","http://hdl.handle.net/1853/66340","The design of immersive soundscape experiences, both for artistic and informative purposes, is an established field in Auditory Display. This paper describes the process of designing historically informed soundscapes to be incorporated in modern travel-guide applications. The work stems from the research project TRACCE (TRavelogue with Augmented Cultural & Contemporary Experience), which focuses on the design and development of a platform for augmented cultural routes. Using this platform, hikers can follow the journey of 18th and 19th century travelers, having access to the original travelogues, tracked routes, and a wide variety of modern information. User experience is augmented by means of visual and auditory reconstructions of the original surrounding environments in several identified points of interest in each path. Apart from the creative process and technical details, the paper discusses the design challenges, which mainly stem from a) the limited data available which would allow an accurate and convincing reconstruction of the acoustic environments, b) the need for diverse auditory displays which would grasp the users' attention, and c) the difficulty in designing soundscapes which would be interesting, appropriate, and informative for a wide audience of various age groups, educational backgrounds, and sensory abilities.","2021-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","Designing historically informed soundscapes for the augmentation of modern travel-guides","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KB552Y5Z","journalArticle","2011","Gomez, Imanol; Ramirez, Rafael","A DATA SONIFICATION APPROACH TO COGNITIVE STATE IDENTIFICATION","","","","","http://hdl.handle.net/1853/51569","The study of human brain functions has dramatically increased greatly due to the advent of Functional Magnetic Resonance Imaging (fMRI), arguably the best technique for observing human brain activity that is currently available. However, fMRI techniques produce extremely high dimensional, sparse and noisy data which is difficult to visualize, monitor and analyze. In this paper, we propose two different sonification approaches to monitor fMRI data. The goal of the resulting fMRI data sonification system is to allow the auditory identification of cognitive states produced by different stimuli. The system consists of a feature selection component and a sonification engine. We explore different feature selection methods and sonification strategies. As a case study, we apply our system to the identification of cognitive states produced by volume accented and duration accented rhythmic stimuli.","2011-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WMU6M3KC","journalArticle","2012","Perkins, Rhys","Sonification of a real-time physics simulation within a virtual environment","","","2168-5126","","http://hdl.handle.net/1853/44436","There has been an increasing amount of research in utilising 3D virtual environments as a core component of interactive sonifications. While showing considerable potential for their ability in producing both real-time visualisation and sound, they often come with constraints as a result of their design decision processes. This paper presents developments of a prototype that have arisen out of my attempts to address some of the issues involved in bringing sonification to a wider audience through a universal metaphor. These new additions allow for an intuitive, elementary introduction into to the world of auditory display, while providing a more flexible and immersive environment for composition and sound design.","2012-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8XSFXK87","journalArticle","2014","Fan, Jianyu; Topel, Spencer","SonicTaiji: A Mobile Instrument for Taiji Performance","","","","","http://hdl.handle.net/1853/52096","SonicTaiji is a mobile instrument designed for the Android Platform. It utilizes accelerometer detection, sound synthesis, and data communication techniques to achieve real-time Taiji sonification. Taiji is an inner-strength martial art aimed at inducing meditative states. In this mobile music application, Taiji movements are sonified via gesture detection, connecting listening and movement. This instrument is a tool for practitioners to enhance the meditative experience of performing Taiji. We describe the implementation of gesture position selection, real-time synthesis, and data mapping. We describe outcomes of subjective tests of the user experience.","2014-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","SonicTaiji","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W3JXQX96","journalArticle","2007","Nees, Michael A.; Walker, Bruce N.","Listener, Task, and Auditory Graph: Toward a Conceptual Model of Auditory Graph Comprehension","","","","","http://hdl.handle.net/1853/50010","Auditory graph design and implementation often has been subject to criticisms of arbitrary or atheoretical decision-making processes in both research and application. Despite increasing interest in auditory displays coupled with more than two decades of auditory graph research, no theoretical models of how a listener processes an auditory graph have been proposed. The current paper seeks to present a conceptual level account of the factors relevant to the comprehension of auditory graphs by human listeners. We attempt to make links to the relevant literature on basic auditory perception, and we offer explicit justification for, or discussion of, a number of common design practices that are often justified only implicitly or by intuition in the auditory graph literature. Finally, we take initial steps toward a qualitative, conceptual level model of auditory graph comprehension that will help to organize the available data on auditory graph comprehension and make predictions for future research and applications with auditory graphs","2007-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","Listener, Task, and Auditory Graph","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z63PH2AS","journalArticle","2012","Gillard, Jessica; Schutz, Michael","Impoving the efficacy of auditory alarms in medical devices by exploring the effect of amplitude envelope on learning and retention","","","2168-5126","","http://hdl.handle.net/1853/44417","Despite strong interest in designing auditory alarms in medical devices, learning and retention of these alarms remains problematic. Based on our previous work exploring learning and retention of associations between sounds and objects, we suspect that some of the problems might in fact stem from the types of sounds used in medical auditory alarms. Several of our previous studies demonstrate improvements in memory associations when using sounds with “percussive” (i.e. decaying) envelops vs. those with “flat” (i.e. artificial sounding) envelopes – the standard structure generally used in many current alarms. Here, we attempt to extend our previous findings on the effects of temporal structure on the learning and memory. Unfortunately, we did not find evidence of any such benefit in the current study. However, several interesting patterns are emerging with respect to “confusions” – the times when one alarm was confused with another. We believe this paradigm and way of thinking about alarms (i.e. attention to temporal structure) could provide insight on ways to improve auditory alarms, thereby prevent injuries and saving lives in hospitals. We welcome the chance to gather feedback on our approaches and thoughts as to why our current attempts (which we believe are based on a solid theoretical basis) have not yet led to our hoped-for improvements.","2012-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4SPRJY3U","journalArticle","2005","Lemmens, Paul M. C.","Using the major and minor mode to create affectively-charged earcons","","","","","http://hdl.handle.net/1853/50197","The importance of the structured fabrication of auditory (feedback) signals like earcons is common knowledge in the ICAD community. To create such structured families of earcons musical transformations like rhythm or pitch (and many others) are usually employed. However, one important transformation in Western tonal music, that of the distinction between major and minor mode, to our knowledge, has not been exploited, despite the fact that the affective connotation of the major and minor mode might be useful for research into auditory signals for affective human– computer interfaces. The present study investigated whether the transformation to major or minor mode can be used to create affectively–charged earcons for use in affective– computing research [1]. The affective–congruency effect that we obtained provides evidence that the processing of affective information can interfere with making rational, cognitive decisions. We argue that the transformation to the major or minor mode is suitable to create affectively–charged earcons and that it is important to ensure affective correspondence in computer interfaces to be able to realize optimal performance levels.","2005-07","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I3P2A8V2","journalArticle","2022","Seiça, Mariana; Roque, Licínio; Martins, Pedro; Cardoso, F. Amílcar","Artefact, Participant and Interaction in Auditory Experiences","","","","","http://hdl.handle.net/1853/67386","The act of sound perception and its subjective dimensions, from physical to psychoacoustics, from semantic to affective, carry an inherent challenge for the conception and evaluation of every audio-based artefact. Starting from a previous framework of evaluation approaches, we seek to deconstruct the configuring elements of these processes, searching for theoretical foundations informing Sound Design and possible applications for Auditory Displays. This work is a first step into identifying a body of knowledge on the listener’s experience, how the act of listening takes place and how the sequence of listening actions can evolve as forms of dialogue, creating dialogical spaces for making sense of auditory information. With this work, practitioners can gain new insights into how existing techniques for creating auditory artefacts can be configured and transformed into new, alternative approaches.","2022-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GLLEZ4QI","journalArticle","2006","Palomaki, H.","Meanings conveyed by simple auditory rhythms","","","","","http://hdl.handle.net/1853/50599","In this article we concentrate on perception of non-musical rhythm. The purpose of this study has been to find possible meanings related to simple auditory rhythms. Meanings were examined using semantic scales. 26 subjects rated nine different rhythm samples according to adjective pair scales. We also identify some preliminary design suggestions as to how rhythm can be used in sonification and discuss duration limitation when composing earcons.","2006-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KUCHLN7H","book","2015","Ferguson, Sam","Using audio feature extraction for interactive feature-based sonification of sound","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54106","Feature extraction from an audio stream is usually used for visual analysis and measurement of sound. This paper seeks to describe a set of methods for using feature extraction to manipulate concatenative synthesis, and develops experiments with reconfigurations of the feature-based concatenative synthesis systems within a live, interactive context. The aim is to explore sound creation and manipulation within an interactive, creative, feedback loop.","2015-07","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9WHUQ8PH","journalArticle","2014","Ballora, Mark","Sonification Strategies for the Film Rhythms of the Universe","","","","","http://hdl.handle.net/1853/52075","Design strategies are discussed for sonifications that were created for the short film Rhythms of the Universe, which was conceived by a cosmologist and a musician, with multi-media contributions from a number of artists and scientists. Sonification functions as an engagement factor in this scientific outreach project, along with narration, music, and visualization. This paper describes how the sonifications were created from datasets describing pulsars, the planetary orbits, gravitational waves, nodal patterns in the sun’s surface, solar winds, extragalactic background light, and cosmic microwave background radiation.","2014-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7KRAIG7A","journalArticle","2003","Noisternig, Markus; Musil, Thomas; Sontacchi, Alois; Höldrich, Robert","A 3D real time rendering engine for binaural sound reproduction","","","","","http://hdl.handle.net/1853/50440","A method of computationally efficient 3D sound reproduction via headphones is presented using a virtual Ambisonic approach. Previous studies have shown that incorporating head tracking as well as room simulation is important to improve sound source localization capabilities. The simulation of virtual acoustic space requires to filter the stimuli with head related transfer functions (HRTFs). In time-varying systems this yields the problem of high quality interpolation between different HRTFs. The proposed model states that encoding signals into Ambisonic domain results in time-invariant HRTF filters. The proposed system is implemented on a usual notebook using Pure Data (PD), a graphically based open source real time computer music software.","2003-07","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HVI9FZAA","journalArticle","2012","Nambiar, Ajayan; Jacobson, Jeffrey","Spatialized audio for mixed reality theater: the Egyptian oracle","","","2168-5126","","http://hdl.handle.net/1853/44431","In the Egyptian Oracle, we project a simulation of an ancient temple onto a large projection screen. (See http://publicvr.org/egypt/oracle/shortvid.html). We create the illusion of a contiguous space by matching the scale of virtual and physical objects. In the live performance, actors in front of the screen interact with human-operated avatar actors in the virtual space. As with any dramatic production, music, sound, and dialogue are a large part of the experience. Our goal is to create a unified aural space that extends from the physical through the virtual to encompass the entire performance. We use commodity electronics to produce an elegant affordable solution, which produces an impressive dramatic effect. We also confront fundamental issues in performances of this type, pointing the way to more advanced auditory solutions for interactive mixed reality spaces. This project was funded by the National Endowment for the Humanities, and everything is free to the public as open source.","2012-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","Spatialized audio for mixed reality theater","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4ZM6WEB6","journalArticle","2019","Savery, Richard; Ayyagari, Madhukesh; May, Keenan; Walker, Bruce N.","Soccer sonification: Enhancing viewer experience","","","","","http://hdl.handle.net/1853/61512","We present multiple approaches to soccer sonification, focusing on enhancing the experience for a general audience. For this work, we developed our own soccer data set through computer vision analysis of footage from a tactical overhead camera. This data-set included X, Y, coordinates for the ball and players throughout, as well as passes, steals and goals. After a divergent creation process, we developed four main methods of sports sonification for entertainment. For the Tempo Variation and Pitch Variation methods, tempo or pitch is operationalized to demonstrate ball and player movement data. The Key Moments method features only pass, steal and goal data, while the Musical Moments method takes existing music and attempts to align the track with important data points. Evaluation was done using a combination of qualitative focus groups and quantitative surveys, with 36 participants completing hour long sessions. Results indicated an overall preference for the Pitch Variation and Musical Moments methods, and revealed a robust trade-off between usability and enjoyability.","2019-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","Soccer sonification","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YXGP3TAL","journalArticle","1998","Humon, Naut; Thibault, Bill; Galloway, Vance; Willis, Garnet; Wing, Jessica Grace","Sound traffic control: An interactive 3-D audio system for live musical performance","","","","","http://hdl.handle.net/1853/50730","Sound Traffic Control (STC) is a system for interactively controlled 3-D audio, displayed using a loudspeaker array. The intended application is live musical performance. Goals of the system include flexibility, ease of use, fault tolerance, audio quality, and synchronization with external media sources such as MIDI, audio feeds from musicians, and video. It uses a collection of both commercial and custom components. The development and design of the current system is described, embodying ideas developed during over a decade of experimentation, and is evaluated based on the experiences of users and developers.","1998-11","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","Sound traffic control","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L4S3PBWR","journalArticle","2002","Modegi, T.","Development of MIDI encoder tool “auto-F” for general time-based electric signals","","","","","http://hdl.handle.net/1853/51351","The MIDI interface is originally designed for electronic musical instruments but we consider this music-note based coding concept can be extended for general acoustic signal description. We proposed applying the MIDI technology to coding of bio-medical auscultation sound signals such as heart sounds for retrieving medical records and performing telemedicine. Then we have tried to extend our encoding targets including vocal sounds, natural sounds and electronic bio-signals such as ECG, using Generalized Harmonic Analysis method. Currently, we are trying to separate vocal sounds included in popular songs and encode both vocal sounds and background instrumental sounds into separate MIDI channels. And also, we are trying to extract articulation parameters such as MIDI pitch-bend parameters in order to reproduce natural acoustic sounds using a GM-standard MIDI tone generator. In this paper, we present an abstract algorithm of our developed MIDI software encoder tool, which can convert any kind of electronic signals to MIDI-controllable interactive audio contents.","2002-07","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8GKVC54G","journalArticle","2022","Fick, Jason; Syed, Ibrahim; Zhang, Brian J.; Fitter, Naomi T.","Toward generative sound cues for robots using emotive musification","","","","","http://hdl.handle.net/1853/67388","Sound is an essential yet under-studied way for robots to communicate with humans. When designed well, robot sound can improve aspects of human-robot interaction from social perception to team fluency, but at its worst, robot sound can discourage use of robots altogether. Thus, sound cues must be carefully and intentionally designed. To address this need, we present a system that uses the inherent emotional connotations of musical qualities to dynamically generate emotive sound for robots . Our application utilizes real-time modification of tempo, pitch, scale, and sound brightness to algorithmically generate melodic phrases intended to evoke specific moods or feelings in human listeners. An in-the-wild exploratory study with N = 26 participants demonstrated that our generative sounds caused human listeners to perceive the robot as happier and warmer. This effort is a first step toward a planned full system that will democratize the design of music-based emotional communication in human-robot interaction.","2022-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EXC7F6X3","book","2015","Perez-Lopez, Andres","3DJ: A supercollider framework for real-time sound spatialization","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54123","The field of real time sound spatizalization is recently receiving much attention, as suggested by the large number of proposals appeared in last years - both from software spatialization frameworks and from hardware spatialization interfaces. However, most of the proposed works do not take into account the existing knowledge in Human Computer Interaction Design, which causes them to remain in a simplified approach. We propose a theoretical basis for real-time spatialization design from a holistic perspective, based on the Digital Musical Instruments theory, and use it to provide a comparative review of recent proposals. Furthermore, we develop our own state-of-the-art software spatialization system, 3Dj, which may help in the task of design and evaluation of new proposals for real-time sound spatialization in the fields of interactive performance, data sonification or virtual environments.","2015-07","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","3DJ","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E54E6ZTI","journalArticle","2019","García Riber, Adrian","Sonifigrapher: Sonified light curve synthesizer","","","","","http://hdl.handle.net/1853/61497","In an attempt to contribute to the constant feedback existing between science and music, this work describes the design strategies used in the development of the virtual synthesizer prototype called Sonifigrapher. Trying to achieve new ways of creating experimental music through the exploration of exoplanet data sonifications, this software provides an easy-touse graph-to-sound quadraphonic converter, designed for the sonification of the light curves from NASAﾒs publiclyavailable exoplanet archive. Based on some features of the first analog tape recorder samplers, the prototype allows end-users to load a light curve from the archive and create controlled audio spectra making use of additive synthesis sonification. It is expected to be useful in creative, educational and informational contexts as part of an experimental and interdisciplinary development project for sonification tools, oriented to both non-specialized and specialized audiences.","2019-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","Sonifigrapher","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VJCHGI9N","journalArticle","2006","Walker, Bruce N.; Godfrey, Mark T.; Orlosky, Jason E.; Bruce, Carrie; Sanford, Jon","Aquarium sonification: Soundscapes for accessible dynamic informal learning environments","","","","","http://hdl.handle.net/1853/50426","Museums, science centers, zoos and aquaria are faced with educating and entertaining an increasingly diverse visitor population with varying physical and sensory needs. There are very few guidelines to help these facilities develop non-visual exhibit information, especially for dynamic exhibits. In an effort to make such informal learning environments (ILEs) more accessible to visually impaired visitors, the Georgia Tech Accessible Aquarium Project is studying auditory display and sonification methods for use in exhibit interpretation. The work presented here represents the initial tool building stage. We discuss the sonification system we are developing, and present some examples of the soundscape implementations that have been produced so far.","2006-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","Aquarium sonification","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R3XQBYK3","journalArticle","2021","Lindetorp, Hans; Falkenberg, Kjetil","Sonification for everyone everywhere: Evaluating the WebAudioXML sonification toolkit for browsers","","","","","http://hdl.handle.net/1853/66351","Creating an effective sonification is a challenging task that requires skills and knowledge on an expertise level in several disciplines. This study contributes with WebAudioXML Sonification Toolkit (WAST) that aims at reaching new groups who have not yet considered themselves to be part of the ICAD community. We have designed, built, and evaluated the toolkit by analysing ten student projects using it and conclude that WAST did meet our expectations and that it led to students taking a deep approach to learning and successfully contributed to reaching the learning outcomes. The result indicates that WAST is both easy-to-use, highly accessible, extensively flexible and offers possibilities to share the sonification in any device's web browser simply through a web link, and without installations. We also suggest that a sonification toolkit would become an even more creative environment with virtual instruments and mixing features typically found in Digital Audio Workstations.","2021-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","Sonification for everyone everywhere","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ESN6DHNL","book","2010","Baker, Cooper","Signal Painting","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/50050","Proceedings of the 16th International Conference on Auditory Display (ICAD2010)","2010-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QMVA5N85","journalArticle","2018","Riber, Adrián García","Planethesizer: Approaching exoplanet sonification","","","","","http://hdl.handle.net/1853/60073","The creation of simulations, sounds and images based on information related to an object of investigation is currently a real tool used in multiple areas to bring the non-specialized public closer to scientific achievements and discoveries. Under this context of multimodal representations and simulations developed for educational and informational purposes, this work intends to build a bridge between virtual musical instruments’ development and physical models, using the gravitation laws of the seven planets orbiting around the Trappist-1 star. The following is a case study of an interdisciplinary conversion algorithm design that relates musical software synthesis to exoplanets’ astronomical data - measured from the observed flux variations in the light curves of their star- and that tries to suggest a systematic and reproducible method, useful for any other planetary system or model-based virtual instrument design. As a result, the Virtual Interactive Synthesizer prototype Planethesizer is presented, whose default configurations display a multimodal Trappist-1, Kepler-444 and K2-72 planetary systems simulation.","2018-06","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","Planethesizer","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4CJ6QTIB","journalArticle","2003","O'Sullivan, Conor; Fernstrom, Mikael","Instrument timbre models with noisy partial inclusion","","","","","http://hdl.handle.net/1853/50469","This paper presents the results of work performed in the area of analysis, manipulation and re-synthesis of musical instrument sounds. The goal is an efficient method of musical instrument sound modelling. Building on work previously carried out on analysis/resynthesis methods, in addition to phase vocoding methods, the work here presented proffers an alternative method of harmonic partial analysis and filtration. A new technique for sound model generation is presented and results from preliminary testing are discussed.","2003-07","2023-07-13 06:25:44","2023-07-13 06:25:44","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3YMAEHVQ","journalArticle","2004","de Campo, A.; Frauenberger, C.; Hoeldrich, R.","Designing a generalized sonification environment","","","","","http://hdl.handle.net/1853/50758","This paper proposes a design for a generalized sonification environment (“SonEnvir”) that aims to be flexible, easily extensible, and practically useful for researchers from many scientific domains (target fields) that deal with data analysis and exploration. SonEnvir is intended to allow for very quick development of new sonification designs, based on a library of existing sound processes that have multiple perceptually independent control parameters. By using modular software architecture which decouples components like basic data handling objects, data processing, sound synthesis processes, mappings used, playing approaches, and real-time interaction possibilities, all the individual aspects of one sonification design can easily be reused as starting points for new designs. Prototypes for this sonification environment are being developed on the SuperCollider3 (SC3) and PureData (pd) platforms","2004-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"22FUJD6H","book","2009","Feron, Francois-Xavier; Frissen, Ilja; Guastavino, Catherine","Upper limits of auditory motion perception: the case of rotating sounds","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51424","We report two experiments investigating rotating sounds presented on a circular array of 12 speakers. Velocity thresholds were mea- sured for three different types of stimuli (broadband noises, white noise, harmonic sounds). In the first experiment, we gradually in- creased or decreased the velocity and asked participants to indicate the point at which they stopped or started (respectively) perceiving a rotating sound. The thresholds ranged between 1.95-2.80 rot/s for noises and 1.65-2.75 rot/s for harmonic sounds. We observed significant effects of the direction of velocity change (accelera- tion or deceleration), stimulus type and fundamental frequencies for harmonic sounds, but no effect of centre frequency was ob- served for broadband noises. In the second experiment, stimuli were presented at constant velocities in a single-interval forced- choice paradigm: listeners were asked to indicate if the sound was rotating or not. The thresholds obtained were within the range of those of the first experiment. The effect of frequency for harmonic sounds was confirmed.","2009-05","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","Upper limits of auditory motion perception","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AJN2IX6X","book","2015","Yu, Bin; Feijs, Loe; Funk, Mathias; Hu, Jun","Designing auditory display of heart rate variability in biofeedback context","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54153","This paper presents a set of real-time sonifications of heart rate variability in the context of biofeedback. The objective of the study is to explore new ways in providing biofeedback information rather than the typical graphic displays in medical products. Four different auditory displays were created by mapping heart rate variability to timing variations of the sound. In the experiment, ten subjects completed five tests of biofeedback training with four auditory displays and one graphic display. During all tests, the heart rate variability and respiration data were recorded for evaluation of the effectiveness of biofeedback training. Subjects were also asked to rate their subjective experience after each test. The results suggest that most subjects could achieve a similar training effect with auditory feedback compared to graphic feedback. Although the user experience of auditory feedback did not meet our expectations, some subjects were enthusiastic about the direct auditory feedback. We discuss these results and provide a description of what is learnt from our design explorations.","2015-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NZBVX9A7","journalArticle","2011","Morrell, Martin J.; Reiss, Joshua D.; Stockman, Tony","Auditory Cues for Gestural Control of Multi-Track Audio","","","","","http://hdl.handle.net/1853/51575","This paper presents a study undertaken to evaluate user ratings on auditory feedback of sound source selection within a multi-track auditory environment where sound placement is controlled by a gesture control system. Selection confirmation is presented to the participants via changes to the audio mixture over the stereo loudspeakers or feedback over a single ear bluetooth headset. Overall five different methods are compared and results of our study are presented. A second task in the study was given to evaluate a preselection method to help find sound sources before selection, the participant altered a width control of the pre-selection that was heard in the bluetooth headset. Results indicate a specific value irrespective of genre that the pre-selection should be set to whilst the selection confirmation can be perceived to be dependant on genre and instrumentation.","2011-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5DEKCQRB","journalArticle","2018","Boren, Braxton; Genovese, Andrea","Acoustics of virtually coupled performance spaces","","","","","http://hdl.handle.net/1853/60079","Many different musical applications, including remote sonification, sound installation, augmented reality, and distributed/ telematic music performance, make use of high speed Internet connections between different performance spaces. Most of the technical literature on this subject focuses on system latency, but there are also significant contributions from the acoustics of all rooms connected: specifically, smaller auxiliary rooms will tend to introduce spectral coloration, and the “main” larger volume will send more reverberation to the off-site performers. Measurements taken in two linked networked sites used in telematic performance show that both of these issues are present. Some improvements are suggested, including physical room alterations and equalization methods using signal processing.","2018-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"USG4LUHM","book","2015","Landry, Steven; Croschere, Jayde","Subjective Assessment of In-Vehicle Auditory Warnings for Rail Grade Crossings","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54181","Human factors research has played an important role in reducing the incidents of vehicle-train collisions at rail grade crossings over the past 30 years. With the growing popularity of in-vehicle infotainment systems and GPS devices, new opportunities arise to cost-efficiently and effectively alert drivers of railroad crossings and to promote safer driving habits. To best utilize this in-vehicle technology, 32 auditory warnings (16 verbal, 7 train-related auditory icons, and 9 generic earcons) were generated and presented to 31 participants after a brief low-fidelity driving simulation. Participants rated each sound on eight dimensions deemed important in previous auditory warning literature. Preliminary results and possible interpretations are discussed.","2015-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JVQQWV2V","journalArticle","2005","Fleck, Christian; Daye, Christian; de Campo, Alberto; de Campo, Marianne Egger; Edelmayer, Georg; Mayer, Peter; Panek, Paul","Sonification as a tool to reconstruct user's actions in unobservabe areas","","","","","http://hdl.handle.net/1853/50084","This paper deals with the sonification of log data gathered during a field test of an innovative toilet system. These tests have been carried out in a day care center for multiple sclerosis patients in winter 2004/05 in the framework of the EU funded Friendly Rest Room project (FRR). For ethical reasons, no direct observational data are at hand to validate the design concept. We will show a way to solve this dilemma by sonification of the log data. This sonification must be designed in a way that enables the researcher to reconstruct the user's actions s/he could not directly observe.","2005-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TDSZ4TY7","journalArticle","2012","Choi, Hongchan","An alternative implementation of VBAP with graphical interface for sound motion design","","","2168-5126","","http://hdl.handle.net/1853/44413","An implementation of vector-based amplitude panning (VBAP) for spatial display of sonified data is presented. The proposed method offers an implicit conversion from Spherical to Cartesian coordinates thus being particularly well suited for auditory display. Two techniques from computer graphics are adapted in order to predefine an optimum set of speaker triplets and perform the amplitude panning in real-time. Furthermore, the consideration of time delay from a virtual sound source to actual speakers is incorporated. Due to the geometrical nature of this procedure, the resulting system can be easily visualized by the graphic library OpenGL. Using this library I provide users with an intuitive control interface. A prototype is demonstrated that enables a user to compose a trajectory of sound in three dimensional space.","2012-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RQP7E6RQ","book","2010","Vogt, Katharina; Höldrich, Robert; Pirrös, David; Gattringer, Christof","Spin Quartets. Sonification of the XY Model","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/50062","We present an intuitive sonification of data from a statistical physics model, the XY-spin model. Topological structures (anti-/vortices) are hidden to the eye by random spin movement. The behavior of the vortices changes by crossing a phase transition as a function of the temperature. Our sonification builds on basic acoustic properties of phase modulation. Only interesting structures like anti-/vortices remain heard, while everything else falls silent, without additional computational effort. The researcher interacts with the data by a graphical user interface. The sonification can be extended to any lattice model where locally turbulent structures are embedded in rather laminar fields.","2010-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VLKALKIS","journalArticle","2002","Ben-Tal, O.; Berger, J.; Cook, B.; Daniels, M.; Scavone, Gary","Sonart: The sonification application research toolbox","","","","","http://hdl.handle.net/1853/51376","The Sonification Application and Research Toolbox (SonART) is an open-source effort to develop a platform-independent collection of methods to map data to sonification parameters. A set of graphical user interface tools that will provide practical and intuitive utilities for experimentation and auditory display. SonART aims to provide publicly available, well-documented code that will be easily adapted to address a broad range of sonification needs. The effort will build upon the Synthesis ToolKit (STK) [1]. In this paper we describe SonART's parameter engine framework, its interface to STK, and relevant recent developments in STK. We present an example of sonified stock market data to illustrate the principles of SonART.","2002-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","Sonart","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MFVAELZS","journalArticle","2019","Hansen, Brian; Baltaxe-Admony, Leya Breanna; Kurniawan, Sri; Forbes, Angus G.","Exploring sonic parameter mapping for network data structures","","","","","http://hdl.handle.net/1853/61527","In this paper, we explore how sonic features can be used to represent network data structures that define relationships between elements. Representations of networks are pervasive in contemporary life (social networks, route planning, etc), and network analysis is an increasingly important aspect of data science (data mining, biological modeling, deep learning, etc). We present our initial findings on the ability of users to understand, decipher, and recreate sound representations to support primary network tasks, such as counting the number of elements in a network, identifying connections between nodes, determining the relative weight of connections between nodes, and recognizing which category an element belongs to. The results of an initial exploratory study (n=6) indicate that users are able to conceptualize mappings between sounds and visual network features, but that when asked to produce a visual representation of sounds users tend to generate outputs that closely resemble familiar musical notation. A more in-depth pilot study (n=26) more specifically examined which sonic parameters (melody, harmony, timbre, rhythm, dynamics) map most effectively to network features (node count, node classification, connectivity, edge weight). Our results indicate that users can conceptualize relationships between sound features and network features, and can create or use mappings between the aural and visual domains.","2019-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WJGCMFYR","journalArticle","2016","Bramwell-Dicks, Anna; Petrie, Helen; Edwards, Alistair","Can Listening to Music Make You Type Better? The Effect of Music Style, Vocals and Volume on Typing Performance","","","","","http://hdl.handle.net/1853/56576","Music psychologists have frequently shown that music affects people's behaviour. Applying this concept to work-related computing tasks has the potential to lead to improvements in a person's productivity, efficiency and effectiveness. This paper presents two quantitative experiments exploring whether transcription typing performance is affected when hearing a music accompaniment that includes vocals. The first experiment showed that classifying the typists as either slow or fast ability is important as there were significant interaction effects once this between group factor was included, with the accuracy of fast typists reduced when the music contained vocals. In the second experiment, a Dutch transcription typing task was added to manipulate task difficulty and the volume of playback was included as a between groups independent variable. When typing in Dutch the fast typists' speed was reduced with louder music. When typing in English the volume of music had little effect on typing speed for either the fast or slow typists. The fast typists achieved lower speeds when the loud volume music contained vocals, but with low volume music the inclusion of vocals in the background music did not have a noticeable affect on typing speed. The presence of vocals in the music reduced the accuracy of the text entry across the whole sample. Overall, these experiments show that the presence of vocals in background music reduces typing performance, but that we might be able to exploit instrumental music to improve performance in tasks involving typing with either low or high volume music.","2016-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","Can Listening to Music Make You Type Better?","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HY6ZGFA6","journalArticle","2002","Somers, E.","Simultaneity and polyphony in speech based audio art","","","","","http://hdl.handle.net/1853/51383","The author discusses the use of simultaneous speech in the design of radiophonic sound compositions. He begins with a discussion of speech in pre-literate societies and shows how simultaneity is a characteristic of speech prior to the influence in writing and print, and discusses the how the linearity of printed communications has tended to eliminate aspects of spoken words that cannot be duplicated in print. The author then discusses the design of radiophonic sound projects, by himself and others, which utilize the ability of the ear to keep track of multiple simultaneous spoken narratives and which may more closely simulate thought patterns of eye entered cultures. Finally, he relates how the differences between eye-culture perception and ear-culture perception might impact the design projects related to the sonification of data.","2002-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J682H2L9","book","2015","Steffert, Tony; Holland, Simon; Mulholland, Paul; Dalton, Sheep; Väljamäe, Alexander","Prototyping a method for the assessment of real-time EEG sonifications","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54142","This paper presents a first step in the development of a methodology to compare the ability of different sonifications to convey the fine temporal detail of the Electroencephalography (EEG) brainwave signal in real time. In EEG neurofeedback a person‟s EEG activity is monitored and presented back to them, to help them to learn how to modify their brain activity. Learning theory suggests that the more rapidly and accurately the feedback follows behaviour the more efficient the learning will be. Therefore a critical issue is how to assess the ability of a sonification to convey rapid and temporally complex EEG data for neurofeedback. To allow for replication, this study used sonifications of pre-recorded EEG data and asked participants to try and track aspects of the signal in real time using a mouse. This study showed that, although imperfect, this approach is a practical way to compare the suitability of EEG sonifications for tracking detailed EEG signals in real time and that the combination of quantitative and qualitative data helped characterise the relative efficacy of different sonifications.","2015-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HMBSRZZK","book","2010","Vogt, Katharina; Höldrich, Robert; Pirró, David; Rumori, Martin; Rossegger, Stefan; Riegler, Werner; Tadel, Matevz","A Sonic Time Projection Chamger: Sonified Particle Detection at CERN","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49862","In a short-term research project at CERN, an auditory display of elementary particle tracks has been developed. Data stems from simulations of the Time Projection Chamber (TPC) in ALICE experiment. Particle detection there is based on pattern recognition algorithms, but is still today double checked with visualization tools. The sonification works with cluster data of the TPC and was designed in analogy to the physics behind the measurement device. Thus it is possible to listen directly to the otherwise silent detector.","2010-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","A Sonic Time Projection Chamger","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NU4ZPGXC","book","2010","Höldrich, Robert; Vogt, Katharina","A Metaphoric Sonification Method - Towards the Acoustic Standard Model of Particle Physics","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49863","The sound of a sonification has, like any sound, a metaphoric content. Ideally, the sound is designed in a way that it fits the metaphors of the final users. This paper suggests a metaphoric sonification method in order to explore the most intuitive mapping choices with the right polarities. The method is based on recorded interviews, asking experts in a field what they expect data properties to sound like. Language metaphors and sounds of the recordings are then interpreted by the sonification designer. The method has been used for developing an ‘Acoustic Standard Model of particle physics’ with physicists at CERN.","2010-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y8BY7ZHB","journalArticle","2002","Dorin, A.","Liquiprism: Generating polyrhythms with cellular automata","","","","","http://hdl.handle.net/1853/51361","This paper describes a MIDI instrument for the generative composition of polyrhythmic patterns. Patterns are determined by the activation of cells in a non-homogeneous set of inter- connected cellular automata grids which form the faces of a cube in virtual three-dimensional space. This paper discusses the selection of the rules of the cellular automata which enable rhythmic outcomes to be generated by the system, and the motivation for organizing the cells in their current configuration. The fluid sonification and visualization of the cellular automata arrays was a specific aim of this work for gallery installation. Hence the means by which the rigidly-defined and deterministic automata behave was intended to be visualized and sonified in such a way as to convey a sense of emerging liquidity from the mechanism. A brief discussion of these issues appears in this paper.","2002-07","2023-07-13 06:25:45","2023-07-21 08:29:52","2023-07-12","","","","","","","Liquiprism","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N7TTG2XH","journalArticle","2004","Gray, P.; Brewster, S.; Nicol, C.","Designing sound: Towards a system for designing audio interfaces using timbre spaces","","","","","http://hdl.handle.net/1853/50852","The creation of audio interfaces is currently hampered by the difficulty of designing sounds for them. This paper presents a novel system for generating and manipulating non-speech sounds. The system is designed to generate Auditory Icons and Earcons through a common interface. Using a timbre space representation of the sound, it generates output via an FM synthesiser. The timbre space has been compiled in both Fourier and Constant Q Transform versions using Principal Components Analysis (PCA). The design of the system and initial evaluations of these two versions are discussed, showing that the Fourier analysis appears to produce higher quality results, contrary to initial expectations.","2004-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","Designing sound","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GV8LEP5Z","journalArticle","2014","Leminen, Miika; Ahonen, Lauri; Gröhn, Matti; Huotilainen, Minna; Paunio, Tiina; Virkkala, Jussi","Comparing Auditory Stimuli for Sleep Enhancement: Mimicking a Sleeping Situation","","","","","http://hdl.handle.net/1853/52132","Recently, two research groups have reported that the depth and/or duration of Slow Wave Sleep (SWS) can be increased by playing short sounds with approximately 1 second intervals during or prior to SWS. These studies have used sounds with neutral or negative valence: sinusoidal I -kHz tones or short pink noise bursts. Since music therapy research shows beneficial effects of pleasant, natural sounds and music, the sounds in the experiments may have been suboptimal. Thus, we aimed at choosing optimal sounds such that they could be used in increasing the depth or duration of SWS taking into account both the need of fast rise times, short duration, and pleasantness. Here we report results of a listening test mimicking a sleeping situation in which the subjects compared how pleasant, relaxing, and image-evoking they found 3 natural, short instrument sounds with fast rise times compared to a short pink noise burst used in the previous experiments. The natural sounds were selected from our previous listening test as the most pleasant ones. The results will be used as the basis for choosing the optimal sounds for the sleep studies.","2014-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","Comparing Auditory Stimuli for Sleep Enhancement","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FNM9TUBP","journalArticle","2006","Kleiman-Weiner, M.; Berger, J.","The sound of one arm swinging: A model for multidimensional auditory display of physical motion","","","","","http://hdl.handle.net/1853/50691","We present a novel approach to the sonification of complex human movement. As a specific model we examine the golf swing, of which two components, the velocity of the club head and the relative rotation of shoulders with respect to the hips (the X-factor [1]) are considered as two critical factors [2]. We map these dimensions to independent resonant filters to simulate vowel like formants. We consider other methods of sonification and illustrate the advantages of using vowel sounds. This sonification creates an auditory mapping that may prove useful in mastering and improving a golfer's swing and can be generalized for sonification of complex temporal data.","2006-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","The sound of one arm swinging","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y23CMISE","book","2015","Barrett, Natasha; Nymoen, Kristian","Investigations in coarticulated performance gestures using interactive parameter-mapping 3D sonification","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54174","Spatial imagery is one focus of electroacoustic music, more recently advanced by 3D audio furnishing new avenues for exploring spatio-musical structures and addressing what can be called a tangible acousmatic experience. In this paper we present new insights into spatial, temporal and sounding coarticulated (contextually smeared) gestures by applying interactive parameter-mapping sonification in three-dimensional highorder ambisonics, numerical analysis and spatial composition. 3D motion gestures and audio performance data are captured and then explored in sonification. Spatial motion combined with spatial sound is then numerically analyzed to isolate gestural objects and smaller coarticulated atoms in time, space and sound. The results are then used to explore the acousmatic coarticulated image and as building blocks for a composed dataset embodying the original gestural performance. This new data is then interactively sonified in 3D to create acousmatic compositions embodying tangible gestural imagery.","2015-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IU5VNK5C","journalArticle","2000","Somers, Eric","Abstract sound objects to expand the vocabulary of sound design for visual and theatrical media","","","","","http://hdl.handle.net/1853/50667","In this design paper the author explores the kind of sound objects which are typically used in designing sound for theatre and media then proposes to expand the “vocabulary” of traditional sound design through the use of abstract sonic objects to be associated with specific visual events and visual environments. The use of sonic icons and other sound design for interactive electronic devices is seen as a basis for linking abstract sounds with visual expression. Several experimental design experiments using abstract sound objects to accompany visual communication are described. Also discussed is the role of sound in making communication memorable.","2000-04","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U2KADXH7","journalArticle","2014","Nagel, Frederik; Stöter, Fabian-Robert; Degara, Norberto; Balke, Stefan; Worrall, David","Fast and Accurate Guidance - Response Times to Navigational Sounds","","","","","http://hdl.handle.net/1853/52058","Route guidance systems are used every day by both, sighted and visually impaired people. Systems, such as those built into cars and smart phones, usually using speech to direct the user towards their desired location. Sounds other than functional and speech sounds can, however, be used for directing people in distinct directions. The present paper compares response times with different stimuli and error rates in the detection. Functional sounds are chosen with and without intrinsic meanings, musical connotations, and stereo locations. Panned sine tones are identified as the fastest and most correctly identified stimuli in the test while speech is not identified faster than arbitrary sounds that have no particular meaning.","2014-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IIBWQ5YI","journalArticle","1998","Fernstrom, Mikael; McNamara, Caolan","After direct manipulation - direct sonification","","","","","http://hdl.handle.net/1853/50711","The effectiveness of providing multiple-stream audio to support browsing on a computer was investigated through the iterative development and evaluation of a series of sonic browser prototypes. The data set used was a database containing music. Interactive sonification1 was provided in conjunction with simplified human-computer interaction sequences. It was investigated to what extent interactive sonification with multiple-stream audio could enhance browsing tasks, compared to interactive sonification with single-stream audio support. With ten users it was found that with interactive multiple-stream audio the users could accurately complete the browsing tasks significantly faster than those who had single-stream audio support.","1998-11","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5LZA76KH","journalArticle","2000","Tzanetakis, George; Cook, Perry R.","Experiments in computer-assisted annotation of audio","","","","","http://hdl.handle.net/1853/50671","Advances in digital storage technology and the wide use of digital audio compression standards like MPEG have made possible the creation of large archives of audio material. In order to work efficiently with these large archives much more structure than what is currently available is needed. The creation of the necessary text indices is difficult to fully automate. However, significant amounts of user time can be saved by having the computer assist the user during the annotation process. In this paper, we describe a prototype audio browsing tool that was used to perform user experiments in semi-automatic audio segmentation and annotation. In addition to the typical sound-editor functionality the system can automatically suggest time lines that the user can edit and annotate. We examine the effect that this automatically suggested segmentation has on the user decisions as well as timing information about segmentation and annotation. Finally we discuss thumbnailing and semantic labeling of annotated audio.","2000-04","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YWE5EFPM","journalArticle","2003","Keller, John M.; Prather, Edward E.; Boynton, William V.; Enos, Heather L.; Jones, Lauren V.; Pompea, Stephen M.; Slater, Timothy F.; Quinn, Marty","Educational testing of an auditory display regarding seasonal variation of martian polar ice caps","","","","","http://hdl.handle.net/1853/50459","During Fall 2002, planetary scientists and astronomy education researchers from the University of Arizona and the National Optical Astronomy Observatory collaborated with composer Marty Quinn of Design Rhythmics Sonification Research Lab in New Hampshire to create both a visual and auditory display of recent gamma ray data from Mars. This product will be used both to highlight the value of data from the current Mars 2001 Odyssey mission and to serve as a testbed for research into the use and effectiveness of auditory displays in science education. This paper provides background on the Mars data presented, an overview of the animation/sonification product, preliminary results from educational testing of the product, and future research plans. The authors hope to present both the sonification and preliminary results of educational research at the ICAD conference this summer.","2003-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UHP9S2GE","journalArticle","2018","Groß-Vogt, Katharina; Weger, Marian; Höldrich, Robert; Hermann, Thomas; Bovermann, Till; Reichmann, Stefan","Augmentation of an institute’s kitchen: An ambient auditory display of electric power consumption","","","","","http://hdl.handle.net/1853/60088","Efficient feedback on energy consumption is regarded as one step towards a more sustainable lifestyle. Sonification is very apt to convey such information continuously in an ambient and effective way. This paper presents a pilot system for sonifying the electric power consumption of an institute’s kitchen. The reverberation of the kitchen is changed depending on the actual consumption and its difference to a weekly baseline. If the actual consumption is low, it is mapped to a plausible kitchen reverberation. If it is high compared to the baseline, the reverberation becomes unnatural. Evaluating the system gave insights on perceptibility and acceptance of auditory augmentation in a semi-home context.","2018-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","Augmentation of an institute’s kitchen","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N6CXP8B4","journalArticle","2011","Dubus, Gael; Bresin, Roberto","Sonification of Physical Quantities Throughout History: A Meta-Study of Previous Mapping Strategies","","","","","http://hdl.handle.net/1853/51750","We introduce a meta-study of previous sonification designs taking physical quantities as input data. The aim is to build a solid foun- dation for future sonification works so that auditory display re- searchers would be able to take benefit from former studies, avoid- ing to start from scratch when beginning new sonification projects. This work is at an early stage and the objective of this paper is rather to introduce the methodology than to come to definitive conclusions. After a historical introduction, we explain how to collect a large amount of articles and extract useful information about mapping strategies. Then, we present the physical quantities grouped according to conceptual dimensions, as well as the sound parameters used in sonification designs and we summarize the cur- rent state of the study by listing the couplings extracted from the article database. A total of 54 articles have been examined for the present article. Finally, a preliminary analysis of the results is performed.","2011-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","Sonification of Physical Quantities Throughout History","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G4NK8CID","journalArticle","2004","Barbour, J.","Analytic listening: A case study of radio production","","","","","http://hdl.handle.net/1853/50759","Radio is the ultimate auditory display: all information is conveyed aurally to a listener. Sound from a radio invokes visual images in the imagination of a listener, the theatre of the mind. Listening to a radio voice creates images of a person talking from a studio live to their audience, providing information and entertainment. However, much radio is pre-recorded, produced in a radio studio at a different time and replayed on cue, often controlled by sophisticated computer systems. The art of radio production relies on developing excellent listening skills. The craft of radio production now relies extensively on digital audio technology. How has the process of radio production changed with the introduction of new technology, particularly the visualization of audio on a computer screen? Have these changes affected the listening skills of radio producers and has this affected the quality of radio production? This paper explores these issues based on individual interviews with experienced radio producers and the author's experience in training new producers.","2004-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","Analytic listening","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A79AQDAG","book","2009","Garavaglia, Javier Alejandro","Full automation in live-electronics: advantages and disadvantages","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51301","The paper’s aim is to show a personal approach to the programming of live-electronics, which relies on the full automation of the real- time processing. After defining the term ‘live-electronics’, the article gives a summary of the main periods of development of what is generally understood today by this term; it further describes in a general way the tools and methods/processes employed in each period. Finally, the last few sections explain the way automation is implemented in some works of my own since 2002 (all utilizing full automation of the electronic part), showing the most important features and techniques involved, added to the advantages and disadvantages involved in the automation of most (or all) of the real- time processes in those works. Automation is not a new technique in the live-electronics scene, but full automation as described here needs some special attention, due to its impact on the performance and its perception. This paper aims to fully explain these main issues in the last sections in the light of my own experience as a performer and composer/programmer in the past 15 years.","2009-05","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","Full automation in live-electronics","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QSEUNX97","journalArticle","2012","McLachlan, Ross; McGee-Lennon, Marilyn; Brewster, Stephen","The sound of musicons: investigating the design of musically derived audio cues","","","2168-5126","","http://hdl.handle.net/1853/44429","Musicons (brief samples of well-known music used in auditory interface design) have been shown to be memorable and easy to learn. However, little is known about what actually makes a good Musicon and how they can be created. This paper reports on an empirical user study (N=15) to explore the recognition rate and preference ratings for a set of Musicons that were created by allowing users to self-select 5 second sections from (a) a selection of their own music and (b) a set of control tracks. It was observed that sampling a 0.5 second Musicon from a 5-second musical section resulted in easily identifiable and well liked Musicons. Qualitative analysis highlighted some of the underlying properties of the musical sections that resulted in ‘good’ Musicons. A preliminary set of guidelines is presented that provides a greater understanding of how to create effective and identifiable Musicons for future auditory interfaces.","2012-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","The sound of musicons","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9XLRL9HJ","book","2015","Ballora, Mark","Two examples of sonification for viewer engagement: Hurricanes and squirrel hibernation cycles","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54172","This extended abstract describes two sets of sonifications that were commissioned by researchers from the fields of meteorology and animal ecology. The sonifications were created with the software synthesis program SuperCollider [1]. The motivation for creating them was to pursue additional levels of engagement and immersion, supplementing the effects of visual plots. The goal is for audiences, in particular students and laypeople, to readily understand (and hopefully find compelling) the phenomena being described. The approach is parameter-based, creating “sonic scatter plots” [2] in the same manner as work described in earlier publications [3-4].","2015-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","Two examples of sonification for viewer engagement","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QPRHXG7R","journalArticle","2011","Adhitya, Sara; Kuuskankare, Mika","The Sonified Urban Masterplan (Sum) Tool: Sonification for Urban Planning and Design","","","","","http://hdl.handle.net/1853/51918","This paper describes the progress of an interdisciplinary project that explores the potential for sonification in urban planning and design. The project involves the translation of visual urban mapping techniques used in urban planning and design, into sound, through the development of the Sonified Urban Masterplan (SUM) tool. We will describe our sonification approach and outline the implementation of the SUM tool within the computer-aided composition environment PWGL. The tool will be applied to a selected urban data set to demonstrate its potential. The paper concludes with the advantages of such an approach in urban analysis, as well as introduces the possibility, within such CAC environments as PWGL and OpenMusic, to ‘compose’ urban plans and design using sound.","2011-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","The Sonified Urban Masterplan (Sum) Tool","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JJWJYVMY","journalArticle","2002","Sturm, B. L.","Surf music: Sonification of ocean buoy spectral data","","","","","http://hdl.handle.net/1853/51384","The Coastal Data Information Program (CDIP) has been collecting data on ocean wave conditions since late 1975, first using arrays of pressure sensors, and more recently directional buoys. Fourier analysis of the data reveals the spectral and directional content of the wave-driven motions measured by the buoy. Shifting the spectrum to an audible range and synthesizing a time-domain signal creates an aurally interesting and illuminating sonification of ocean wave dynamics. The work done so far has been guided by artistic curiosity; but input from a senior oceanographer has given guidance toward interpretation and elaboration of the methodology. Examples of ocean buoy spectral data sonification are presented, each illustrating important aspects of physical oceanography. Three forms of the data are sonified, from the least detailed to the most. The obvious sonic events are the effects of energy from storms, both local and far away. From the sonification one can estimate the energy of the storm, and the distance it originated. Entire years of data have been sonified in one to thirty minute durations for buoys in different regions, which demonstrate dramatic seasonal and regional differences. Also displayed are the time-lags of South moving storm energies at three distantly separated points on the West Coast of the United States.","2002-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","Surf music","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LJNYFN43","journalArticle","2017","Mathew, Marlene; Cetinkaya, Mert; Roginska, Agnieszka","BSONIQ: A 3-D EEG Sound Installation","","","","","http://hdl.handle.net/1853/58349","Brain Computer Interface (BCI) methods have received a lot of attention in the past several decades, owing to the exciting possibility of computer-aided communication with the outside world. Most BCIs allow users to control an external entity such as games, prosthetics, musical output etc. or are used for offline medical diagnosis processing. Most BCIs that provide neurofeedback, usually categorize the brainwaves into mental states for the user to interact with. Raw brainwave interaction by the user is not usually a feature that is readily available for a lot of popular BCIs. If there is, the user has to pay for or go through an additional process for raw brain wave data access and interaction. BSoniq is a multi-channel interactive neurofeedback installation which, allows for real-time sonification and visualization of electroencephalogram (EEG) data. This EEG data provides multivariate information about human brain activity. Here, a multivariate event-based sonification is proposed using 3D spatial location to provide cues about these particular events. With BSoniq, users can listen to the various sounds (raw brain waves) emitted from their brain or parts of their brain and perceive their own brainwave activities in a 3D spatialized surrounding giving them a sense that they are inside their own heads.","2017-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","BSONIQ","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BGARDBPC","journalArticle","2007","Cabrera, Densil; Ferguson, Sam; Schubert, Emery","'Psysound3': Software for Acoustical and Psychoacoustical Analysis of Sound Recordings","","","","","http://hdl.handle.net/1853/49969","This paper describes a software project called PsySound3. This software provides an accessible platform for the analysis of sound recordings using procedures applied in acoustics and psychoacoustics. Acoustical analysis methods include a sound level meter module, as well as processes such as Fourier transform, cepstrum, Hilbert transform and auto-correlation. Psychoacoustical models include dynamic loudness, sharpness, roughness, loudness fluctuation, pitch height and pitch strength. Results are presented as numbers, auditory graphs and visual graphs. The software is modular, allowing additional analysis methods to be contributed. Several additional analysis modules are planned. The software is distributed freely via www.psysound.org. This paper illustrates some of the analysis possibilities by using auditory alarms as examples.","2007-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","'Psysound3'","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RH423SDI","journalArticle","2018","Cowden, Patrick; Dosiek, Luke","Auditory displays of electric power grids","","","","","http://hdl.handle.net/1853/60076","This paper presents auditory displays of power grid voltage. Due to the constantly changing energy demands experienced by a power system, the voltage varies slightly about nominal, e.g., 120±2 V at 60±0.04 Hz. These variations are small enough that any audible effects, such as transformer hum, appear to have constant volume and pitch. Here, an audification technique is derived that amplifies the voltage variations and shifts the nominal frequency from 60 Hz to a common musical note. Sonification techniques are presented that map the voltage magnitude and frequency to MIDI velocity and pitch, and create a sampler trigger from frequency deviation. Several examples, including audio samples, are given under a variety of power system conditions. These results culminate in a multi-instrument track generated from the sonification of time-synchronized geographically widespread power grid measurements. In addition, an inexpensive Arduino-based device is detailed that allows for real-time sonification of wall outlet voltage.","2018-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BEADWUYY","journalArticle","2019","Joo, Woohun","Sonifyd: A graphical approach for sound synthesis and synesthetic visual expression","","","","","http://hdl.handle.net/1853/61519","This paper describes Sonifyd, a sonification driven multimedia and audiovisual environment based on color-sound conversion for real-time manipulation. Sonifyd scans graphics horizontally or vertically from a scan line, generates sound and determines timbre according to its own additive synthesis based color-to-sound mapping. Color and sound relationships are fixed as default, but they can be organic for more tonal flexibility. Within this ecosystem, flexible timbre changes will be discovered by Sonifyd. The scan line is invisible, but Sonifyd provides another display that represents the scanning process in the form of dynamic imagery representation. The primary goal of this project is to be a functioning tool for a new kind of visual music, graphic sonification research and to further provide a synesthetic metaphor for audiences/users in the context of an art installation and audiovisual performance. The later section is a discussion about limitations that I have encountered: using an additive synthesis and frequency modulation technique with the line scanning method. In addition, it discusses potential possibilities for the future direction of development in relation to graphic expression and sound design context.","2019-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","Sonifyd","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NJDP474C","book","2010","Berger, Jonathan; Wang, Ge; Chang, Mindy","Sonification and Visualization of Neural Data","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/50053","This paper describes a method for integrating audio and visual displays to explore the activity of neurons in the brain. The motivation is twofold: to help understand how populations of neurons respond during cognitive tasks and in turn explore how signals from the brain might be used to create musical sounds. Experimental data was drawn from electrophysiological recordings of individual neurons in awake behaving monkeys, and an interface was designed to allow the user to step through a visual task as seen by the monkey along with concurrent sonification and visualization of activity from a population of recorded neurons. Data from two experimental paradigms illustrating different functional properties of neurons in the prefrontal cortex during attention and decisionmaking tasks are presented. The current system provides an accessible way to learn about how neural activity underlies cognitive functions and serves as a preliminary framework to explore both analytical and aesthetic dimensions of audiovisual representations of the data.","2010-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7Z8HQTEI","journalArticle","2002","Kaltenbrunner, M.","Y-windows: Proposal for a standard AUI environment","","","","","http://hdl.handle.net/1853/51396","This paper introduces a draft framework for a shared auditory user interface (AUI) environment. Y-Windows, similar to the approach of the X-Windows GUI framework [1] in the Unix world, aims to provide common functionality for the easier development and design of AUIs. This initial publication of these ideas, originally roughly developed within a diploma thesis [2], should encourage researchers and developers from the auditory interfaces community to contribute to the further development and possible future implementation of this concept.","2002-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","Y-windows","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y5WY7622","journalArticle","2013","Roddy, Stephen; Furlong, Dermot","Embodied Cognition In Auditory Display","","","","","http://hdl.handle.net/1853/51657","This paper makes a case for the use of an embodied cognition framework, based on embodied schemata and cross-domain mappings, in the design of auditory display. An overview of research that relates auditory display with embodied cognition is provided to support such a framework. It then describes research efforts towards the development this framework. By designing to support human cognitive competencies that are bound up with meaning making, it is hoped to open the door to the creation of more meaningful and intuitive auditory displays.","2013-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CXPCXIGY","journalArticle","2016","McGee, Ryan; Rogers, David E.","Musification of Seismic Data","","","","","http://hdl.handle.net/1853/56569","Seismic events are physical vibrations induced in the earth's crust which follow the general wave equation, making seismic data naturally conducive to audification. Simply increasing the playback rates of seismic recordings and rescaling the amplitude values to match those of digital audio samples (straight audification) can produce eerily realistic door slamming and explosion sounds. While others have produced a plethora of such audifications for international seismic events (i.e. earthquakes), the resulting sounds, while distinct to the trained auditory scientist, often lack enough variety to produce multiple instrumental timbres for the creation of engaging music for the public. This paper discusses approaches of sonification processing towards eventual musification of seismic data, beginning with straight audification and resulting in several musical compositions and new-media installations containing a variety of seismically derived timbres.","2016-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YLBYIG4S","journalArticle","2007","Hug, Daniel; Franinovic, Karmen; Visell, Yon","Sound Embodied: Explorations of Sonic Interaction Design for Everyday Objects in a Workshop Setting","","","","","http://hdl.handle.net/1853/50031","We describe an emergent field of considerable relevance to the auditory display community – that of sonic interaction design for everyday artifacts. It is positioned at the intersection of auditory display, product interaction design, and ubiquitous computing. We describe an exploration of this field that we have undertaken in a workshop setting, with an international mix of designers, students and researchers, aimed at investigating new roles for auditory display in everyday products, and possible methodologies for designing them. In this paper, we define sonic interaction design, describe the outcome of this workshop, which has been planned as the first in a series, and indicate future directions. We point to new research initiatives, including the European project CLOSED (Closing the Loop of Sound Evaluation and Design), which aims at providing new tools that are needed by designers working in this emerging field.","2007-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","Sound Embodied","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GJWZGBCU","journalArticle","2007","de Campo, Alberto","Toward a data sonification design space map","","","","","http://hdl.handle.net/1853/50042","We propose a systematic approach for reasoning about experimental sonification designs for a given type of dataset. Starting from general data properties, the approach recommends initial strategies, and lists possible refinements to consider in the design process. An overview of the strategies included is presented as a mental (and visual) map, and the refinement steps to consider correspond to movements on the map. The main purpose of this approach is to extract 'theory' from 'observation' (in our case, of design practice), similar to grounded theory in sociology [1]: to make implicit knowledge (often expressed in 'natural' ad hoc decisions by sonification experts) explicit and thus available for reflection, discussion, learning, and application in design work. This approach is the result of analysing design sessions which took place in an interdisciplinary sonification workshop 'Science By Ear' [2], held in March 2006. In order to explain the concept in practice as well, a set of workshop sessions on one dataset is analysed here in the terms proposed.","2007-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HFRZA4BM","journalArticle","2005","Finlayson, J. Louise; Mellish, Chris","The 'audioview' - providing a glance at Java source code","","","","","http://hdl.handle.net/1853/50187","Having an overview of the structure of information has been shown to be necessary to effectively approach the reading of it. This paper describes how programming constructs can be represented using speech and non-speech audio to provide an important ‘glance’ at program source code prior to reading it. Three methods of representing program code are investigated, using pure speech, non-speech and a combination of speech and non-speech to determine the most effective method to convey this type of information. On the basis of these results, this paper concludes that non- speech sounds are able to successfully convey information about program structure. However, significantly better results are achieved when using speech output, either alone or in combination with the non-speech audio, with a significantly lower mental workload. These results suggest that earcons and non-speech sounds be used as a supplement to speech representations, rather than as an alternative.","2005-07","2023-07-13 06:25:45","2023-07-21 08:30:16","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UKCF8AJT","journalArticle","2007","Fontana, Simone; Farina, Angelo; Grenier, Yves","Binaural for Popular Music: A Case of Study","","","","","http://hdl.handle.net/1853/49982","The goal of this study is to retrieve useful information about the reactions of listeners to different recording techniques, namely binaural and stereo. This has been done comparing different mixes of the same song. Each mix is obtained from stereophonic and binaural recordings, or processing proximity recordings with stereo panning, binaural synthesis or a hybrid approach. The comparison is made through listening tests with headphones and an analysis of subjects' reactions and meaningful subjective parameters ratings.","2007-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","Binaural for Popular Music","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VCIBXMYV","journalArticle","2012","Schedel, Margaret; Yager, Kevin","Hearing Nano-Structures: A Case Study in Timbral Sonification","","","2168-5126","","http://hdl.handle.net/1853/44441","We explore the sonification of x-ray scattering data, which are two-dimensional arrays of intensity whose meaning is obscure and non-intuitive. Direct mapping of the experimental data into sound is found to produce timbral sonifications that, while sacrificing conventional aesthetic appeal, provide a rich auditory landscape for exploration. We discuss the optimization of sonification variables, and speculate on potential real-world applications.","2012-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","Hearing Nano-Structures","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5C8W7PQS","journalArticle","1996","Back, Maribeth; Des, D.","Micro-narratives in sound design: Context, character, and caricature in waveform manipulation","","","","","http://hdl.handle.net/1853/50810","This paper reviews sound design techniques used in professional audio for music and theater and proposes a conceptual approach to the construction of audio based in narrative structure. The sound designer does not attempt to replicate ""real"" sounds; the task is rather to create the impression of a real sound in a listener's mind. In this attempt to create a sound in the listener's mind, the sound designer is aided by user expectations based upon cultural experience as well as physical experience. Practical sound manipulation techniques are discussed in view of their usefulness in matching a listener's mental model of a sound. Narrative aspects of audio design in computational environments are also delineated. Some keywords involved in this paper are sound design, auditory display, multimodal interaction, interface design, narrative, sonic narrative, micro-narrative, and audio.","1996-11","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","Micro-narratives in sound design","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6Z9E7KPW","journalArticle","2004","Cassidy, R. J.; Berger, J.; Lee, K.; Maggioni, M.; Coifman, R. R.","Auditory display of hyperspectral colon tissue images using vocal synthesis models","","","","","http://hdl.handle.net/1853/50777","The human ability to recognize, identify and compare sounds based on their approximation of particular vowels provides an intuitive, easily learned representation for complex data. We describe implementations of vocal tract models speci cally designed for sonification purposes. The models described are based on classical models including Klatt[1] and Cook[2]. Implementation of these models in MatLab, STK[3], and PD[4] is presented. Various soni cation methods were tested and evaluated using data sets of hyperspectral images of colon cells","2004-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PN9M9KER","journalArticle","2004","Polli, A.","Atmospherics/Weather works: A multi-channel storm sonification project","","","","","http://hdl.handle.net/1853/50828","Atmospherics/Weather Works is an interdisciplinary project in the sonification of storms and other meteorological events generated directly from data produced by a highly detailed and physically accurate model of weather systems used for research and forecasting. This paper discusses the background, conception, and execution of a series of sonifications of a historical hurricane and winter snowstorm that resulted in several performances, stereo recordings, a public multi-channel spatialized sound installation, and an online interactive sound listening environment.","2004-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","Atmospherics/Weather works","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F2Z4S2Q6","journalArticle","2018","Weger, Marian; Herrmann, Thomas; Höldrich, Robert","Plausible auditory augmentation of physical interaction","","","","","http://hdl.handle.net/1853/60085","Interactions with physical objects usually evoke sounds, i.e., auditory feedback that depends on the interacting objects (e.g., table, hand, or pencil) and interaction type (e.g., tapping or scratching). The continuous real-time adaptation of sound during interaction enables the manipulation/refinement of perceived characteristics (size, material) of physical objects. Furthermore, when controlled by unrelated external data, the resulting ambient sonifications can keep users aware of changing data. This article introduces the concept of plausibility to the topic of auditory augmentations of physical interactions, aiming at providing an experimentation platform for investigating surface-based physical interactions, understanding relevant acoustic cues, redefining these via auditory augmentation / blended sonification and particularly to empirically measure the plausibility limits of such auditory augmentations. Besides conceptual contributions along the trade-off between plausibility and usability, a practical experimentation system is introduced, together with a very first qualitative pilot study.","2018-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UNUTNHBL","journalArticle","2016","Schiemer, Greg","Satellite Gamelan: Microtonal Sonification Using a Large Consort of Mobile Phones","","","","","http://hdl.handle.net/1853/56588","This paper describes an approach to sonification based on an iPhone app created for multiple users to explore a microtonal scale generated from harmonics using the combination product set method devised by tuning theorist Erv Wilson. The app is intended for performance by a large consort of hand-held mobile phones where phones are played collaboratively in a shared listening space. Audio consisting of handbells and sine tones is synthesised independently on each phone. Sound projection from each phone relies entirely on venue acoustics unaided by mains-powered amplification. It was designed to perform a microtonal composition called Transposed Dekany which takes the form of a chamber concerto in which a consort of players explore the properties of an microtonal scale. The consort subdivides into families of instruments that play in different pitch registers assisted by processes that are enabled and disabled at various stages throughout the performance. The paper outlines Wilson's method, describes its current implementation and considers hypothetical sonification scenarios for implementation using different data with potential applications in the physical world.","2016-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","Satellite Gamelan","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q2SF7ST8","book","2010","Hall, David L.; Gourley, Matthew; Panulla, Brian; Ballora, Mark","Preliminary Steps in Sonifying Web Log Data","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49896","Detection of intrusions is a continuing problem in network security. Due to the large volumes of data recorded in Web server logs, analysis is typically forensic, taking place only after a problem has occurred. We are exploring the detection of intrusion signatures and patterns via an auditory display. Web log data is parsed and formatted using Python, then read as a data array by the synthesis language SuperCollider, which renders it as a sonification. This can be done either for the study of pre-existing data sets or in monitoring Web traffic in real time. Components rendered aurally include IP address, geographical information, and server Return Codes. Users can interact with the data, speeding or slowing the speed of representation (for pre-existing data sets) or “mixing” sound components to optimize intelligibility for tracking suspicious activity.","2010-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9MCV6IAC","journalArticle","2007","Frauenberger, Christopher; de Campo, Alberto; Eckel, Gerhard","Analysing Time Series Data","","","","","http://hdl.handle.net/1853/49974","This paper investigates the use of auditory perceptualisation for analysing the statistical properties of time series data. We introduce the problem domain and provide basic background on higher order statistics like skewness and kurtosis. The chosen approach was direct audification because of the inherent time line and the high number of data points usually available for time series. We present the tools we developed to investigate this problem domain and elaborate on a listening test we conducted to find perceptual dimensions that would correlate with the statistical properties. The results indicate that there is evidence that kurtosis correlates with roughness or sharpness and that participants were able to distinguish signals with increasing difference of the kurtosis. For the setting in the experiment the just noticeable difference was found to be 5. The collected data did not show any similar evidence for skewness and it remains unclear whether this is perceivable in direct audification at all.","2007-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2Y2ELPZV","book","2009","Barri, Tarik","Versum: audiovisual composing in 3d","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51425","This paper introduces the new audiovisual sequencing system “Versum” that allows users to compose in three dimensions. First, the conceptual soil from which this system has sprung is discussed. Secondly, the basic concepts with which Versum operates are explained, providing a general idea of what is meant by sequencing in three dimensions and explaining what compositions made in Versum can look and sound like. Thirdly, the practical ways in which a composer can use Versum to make his own audiovisual compositions are presented by means of a more detailed description of the different graphical user interface elements. Fourthly the consequences of Versum’s properties with regard to the composing process are discussed. Then a short description is given of the modular structure of the software underlying Versum. Finally, several foresights regarding the directions in which Versum will continue to develop in the near future are presented.","2009-05","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","Versum","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CSAMNQDC","journalArticle","2006","Droumeva, M.; Wakkary, R.","The role of participatory workshops in investigating narrative and sound ecologies in the design of an ambient intelligence audio display","","","","","http://hdl.handle.net/1853/50661","We describe two participatory workshops conducted to support design decisions in the making of the audio display for an ambient intelligent game platform. The workshops discussed here explore specific issues of players' interactions with sound and auditory display design. The workshops helped move our design process forward by specifying the role of narrative and sound ecologies in our design. They clarified the role of sound in creating narrative coherence, guiding player actions, and supporting group interaction. We describe the workshops, the auditory display issues we addressed, discuss how the workshops helped inform our subsequent design, and extend recommendations on how participatory workshops can be used by other designers of auditory displays.","2006-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K9K2FKVM","journalArticle","2004","Paine, G.","Reeds - a responsive sound installation","","","","","http://hdl.handle.net/1853/50905","This paper discusses the responsive sound installation Reeds. The Melbourne International Festival of the Arts commissioned the Reeds project in 2000, for exhibition on the Ornamental Lake at the Royal Botanic Gardens, Melbourne. It consists of twenty-one large floating sculptures1, modeled to represent clusters of river reeds in immaculate man-made plantings. Each reed pod2 contained a collection of electronics for either the gathering of weather information or the reception and dispersion of sound. The sound installation gathered data from two realtime weather stations, and produced eight channels of musical output by interpreting the machine unit pulses of the weather data as pulse inputs to Inverse Fast Fourier Transform (IFFT) algorithms. The Reeds project focused on a consideration of multiple streams of chaotic and constantly varying sound. I was interested in exploring whether the sonic environment would remain homogenous even though, unlike a musical ensemble, the control inputs varied randomly and independently of each other. The sound installation was site specific, reflecting directly upon the environment it inhabited, both in terms of its visual quality, and aesthetic of the sound.","2004-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MBPS4YX4","book","2009","Barrass, Stephen; Frauenberger, Chris","A communal map of design in auditory display","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51285","The workshop on Recycling Auditory Displays at ICAD 2008 aimed to capture knowledge about the design of auditory displays from the participants in a manner that would be easy to understand and reuse. The participants introduced themselves by providing examples of a good and a bad sound design. These examples raised issues of culture, identity, aesthetics and context that are more usually associated with product sound design than auditory display. Based on these discussions the themes Users, Applications, Techniques, and Environments were chosen to focus the further development of ideas. A mindmapping session was used to collect over 150 entries under these themes, and more than 30 references. An additional Others theme was needed for ideas that did not fit neatly into the existing categories. The information that has been collected shows that most research in auditory display falls under the themes of Applications and Techniques. The information under the themes of Users and Others shows the overlap with related disciplines such as auditory neuroscience, product design, sound arts, semiotics, and interface design. The Environment theme raised the need for future research to include contextual issues. The outcome of the workshop has been to produce a collaborative understanding of the current state of design knowledge in the Auditory Display community, and to identify future directions for research into the design of AudiDisplays. more detail. We conclude by discussing the major outcomes and their relevance for future work in this field.","2009-05","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"79NK692C","journalArticle","2003","Wakefield, Gregory H.","Vocal pedagogy and pedagogical voices","","","","","http://hdl.handle.net/1853/50507","Singers learn from their teachers lessons that to the outsider are not transparently obvious. Some of these lessons are discussed in the paper, and their application to problems in sound quality, music information retrieval, and the modeling of the singing voice are presented.","2003-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E8HJ6L9Y","book","2015","Bukvic, Ivica Ico; Matthews, Michael","Aegis audio engine: integrating a real-time analog signal processing, pattern recognition, and a procedural soundtrack in a live twelve-perfomer spectacle with crowd participation","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54098","In the following paper we present Aegis: a procedural networked soundtrack engine driven by real-time analog signal analysis and pattern recognition. Aegis was originally conceived as part of Drummer Game, a game-performance-spectacle hybrid research project focusing on the depiction of a battle portrayed using terracotta soldiers. In it, each of the twelve cohorts—divided into two armies of six—are led by a drummer-performer who issues commands by accurately drumming precomposed rhythmic patterns on an original Chinese war drum. The ensuing spectacle is envisioned to also accommodate large audience participation whose input determines the morale of the two armies. An analog signal analyzer utilizes efficient pattern recognition to decipher the desired action and feed it both into the game and the soundtrack engine. The soundtrack engine then uses this action, as well as messages from the gaming simulation, to determine the most appropriate soundtrack parameters while ensuring minimal repetition and seamless transitions between various clips that account for tempo, meter, and key changes. The ensuing simulation offers a comprehensive system for pattern-driven input, holistic situation assessment, and a soundtrack engine that aims to generate a seamless musical experience without having to resort to cross-fades and other simplistic transitions that tend to disrupt a soundtrack’s continuity.","2015-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","Aegis audio engine","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T9LELB4N","book","2015","Stahl, Benjamin; Thoshkahna, Balaji","Real-time heart rate sonification for athletes","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54140","In this paper, we describe a sonification system that delivers instantaneous heart rate information to an athlete. The sonification system uses a textitPolar H7 heart rate sensor to monitor the heart rate of the user. We describe the techniques used for the processing of the captured heart rate data, its sonification and the relevance of the feedback methods via both a subjective and objective analysis. Our tests prove the strength of having a method for the sonification of instantaneous heart rate. Our testing procedure also serves as a pointer for evaluation of the efficacy of sonification methods in general.","2015-07","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JQK6L68V","journalArticle","2021","Drymonitis, Alexandros; Charitos, Dimitrios","ATHsENSe: A multisensory outdoor installation","","","","","http://hdl.handle.net/1853/66339","The ATHsENSe installation is a four part AV installation based on environmental data from the city of Athens. It comprises four different installations: a visualization projection of the data, a sound installation where the data are being sonified, an installation comprising eight LED displays projecting texts from users of the web app of ATHsENSe, and a light installation reflecting the sound levels of a central point of Athens. The second of the four installations is presented here where data concerning CO, CO2, humidity and others, together with texts provided by the web app users, are being sonified in a six-channel surround installation. In addition to the electronic sounds of the sonification, four kinetic sculptures are being triggered by the system which create acoustic sounds by hitting found objects. This installation is located at the Serafeio complex in Athens, and was commissioned to the Spatial Research Media Group by the municipality of Athens after their successful participation in the Interventions in the City competition.","2021-06","2023-07-13 06:25:45","2023-07-13 06:25:45","2023-07-12","","","","","","","ATHsENSe","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PDMKVTDS","journalArticle","2003","Childs, Edward; Pulkki, Ville","Using multi-channel spatialization in sonification: A case study with meteorological data","","","","","http://hdl.handle.net/1853/50506","Spatialization can be used in sonification as an additional mapping dimension. Immersive spatialization also transmits more auditory data to a subject, compared to monophonic or stereophonic presentation. In this study the VBAP spatialization method is applied to the sonification of meteorological data gathered from a large geographical area in two different listening conditions. Parameter-driven FM synthesis, using JSyn (a Java-based sound API) [1] was used to reinforce the spatialization with timbral coloring. The sonification, designed to display a year of data in about twenty minutes, afforded a sense of storm movement, North-South seasonal variation, and comparison between years.","2003-07","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","Using multi-channel spatialization in sonification","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K3VU6ZJ6","journalArticle","2021","Frid, Emma; Orini, Michele; Martinelli, Giampaolo; Chew, Elaine","Mapping inter-cardiovascular time-frequency coherence to harmonic tension in sonification of ensemble interaction between a COVID-19 patient and the medical team","","","","","http://hdl.handle.net/1853/66326","This paper presents exploratory work on sonic and visual representations of heartbeats of a COVID-19 patient and a medical team. The aim of this work is to sonify heart signals to reflect how a medical team comes together during a COVID-19 treatment, i.e. to highlight other aspects of the COVID-19 pandemic than those usually portrayed through sonification, which often focuses on the number of cases. The proposed framework highlights synergies between sound and heart signals through mapping between timefrequency coherence (TFC) of heart signals and harmonic tension and dissonance in music. Results from a listening experiment suggested that the proposed mapping between TFC and harmonic tension was successful in terms of communicating low versus high coherence between heart signals, with an overall accuracy of 69%, which was significantly higher than chance. In the light of the performed work, we discuss how links between heart- and sound signals can be further explored through sonification to promote understanding of aspects related to cardiovascular health.","2021-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JVEK7HIU","journalArticle","2005","Cullen, Charlie; Coyle, Eugene","TrioSon: A graphical user interface for pattern sonification","","","","","http://hdl.handle.net/1853/50186","The TrioSon software allows users to map musical patterns to input data variables via a graphical user interface (GUI). The application is a Java routine designed to take input files of standard Comma Separated Values (CSV) format and output Standard Midi Files (SMF) using the internal Java Sound API. TrioSon renders output Sonifications from input data files for up to 3 user-defined parameters, allocated as bass, chord and melody instruments for the purposes of arrangement. In this manner each parameter concerned is distinguished by its individual instrumental timbre, with the option of rendering any combination of 1 to 3 parameters as required. The software parses indexed input data relating to individual variables for each user-defined parameter, and provides the means to allocate musical patterns to each variable for Sonification using drag and drop functionality. Control over the Rhythmic Parsing of the Sonification is provided, alongside individual control of the volume, panning, muting and timbre of each instrument in the trio. Sonifications can be rendered as full output files of the entire data, or can also be auditioned by index as required. This feature is designed to allow the user complete control over the data they are sonifying- either on an individual or collective basis. Context for each output Sonification is provided by Midi events defined by the index of the input data, which are mapped to percussive timbres in the final SMF (via track 10). Java development provides the added advantage of portability, with the final application being small enough (200kb) to attach in an email document. It is hoped that the compact and intuitive nature of the application will make it a straightforward means of investigating the Sonification of data sets.","2005-07","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","TrioSon","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PUJP8GY4","journalArticle","2012","Winton, Riley J.; Gable, Thomas M.; Schuett, Jonathan; Walker, Bruce N.","A sonification of Kepler space telescope star data","","","2168-5126","","http://hdl.handle.net/1853/44451","A performing artist group interested in including a sonification of star data from NASA’s Kepler space telescope in their next album release approached the Georgia Tech Sonification Lab for assistance in the process. The artists had few constraints for the authors other than wanting the end product to be true to the data, and a musically appealing “heavenly” sound. Several sonifications of the data were created using various techniques, each resulting in a different sounding representation of the Kepler data. The details of this process are discussed in this poster. Ultimately, the researchers were able to produce the desired sounds via sound synthesis, and the artists plan to incorporate them into their next album release.","2012-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6PX24JC9","journalArticle","2006","Lock, D.; Schiemer, G.; Ong, L.","Orbophone: a new interface for radiating sound and image","","","","","http://hdl.handle.net/1853/50603","The Orbophone is a new interface that radiates rather than projects sound and image. It provides a cohesive platform for audio and visual presentation in situations where both media are transmitted from the same location and localization in both media is perceptually correlated. This paper discusses the advantages of radiation over conventional sound and image projection for certain kinds of interactive public multimedia exhibits and describes the artistic motivation for its development against a historical backdrop of sound systems used in public spaces. An account of an exhibit using the Orbophone is given together with description and critique of the prototype, discussing aspects of its design and construction. The paper concludes with an outline of the Orbophone version 2.","2006-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","Orbophone","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KWP6JSBH","journalArticle","2004","Woszczyk, W.; Martens, W. L.","Perceived synchrony in a bimodal display: Optimal intermodal delay for coordinated auditory and haptic reproduction","","","","","http://hdl.handle.net/1853/50902","The purpose of this study was to determine the range of optimum intermodal delay values for coordinated auditory and haptic reproduction of brief impact events. Indirect psychophysical methods were used to find the intermodal delay that would be most likely to generate the response of perceived synchrony between acoustic and structural vibration components of those events. A recording of a representative impact sound was processed to create bimodal stimuli with varying amounts of intermodal delay between the bimodally reproduced components. The haptic component of the bimodal stimulus was whole-body vibration presented via a platform on which the observer was seated. Using four actuators moving together, users could be displaced linearly upwards or downwards, with a very quick response and with considerable force (the feedback-corrected linear system frequency response was flat to 50 Hz). The auditory component of the bimodal stimulus was presented in an immersive virtual acoustic environment via a multichannel reproduction of simulated indirect sound. The direct sound component matched to the haptic stimulus was reproduced via a frontally-located pair of loudspeakers that included a low-frequency driver capable of reproducing sound with a linear frequency response ranging from 25 to 300 Hz and a high-frequency driver extending well above 20kHz. The intermodal delay was adaptively varied using a two-alternative, forced-choice (2AFC) procedure to track the point of subjective simultaneity (PSS) based upon temporal order judgments with the following response options: 1) haptic sensation seemed to precede auditory sensation; and 2) haptic sensation seemed to follow auditory sensation. Then, in order to avoid sequential response biases in the tracking procedure, a constant stimulus method was used to determine directly the optimal range of intermodal delay values for producing observer responses of intermodal synchrony, with two response options: haptic sensation either seemed to precede or to follow auditory sensation.","2004-07","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","Perceived synchrony in a bimodal display","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K7B7WYWU","journalArticle","2005","Gallo, Emmanuel; Lemaitre, Guillaume; Tsingos, Nicolas","Prioritizing signals for selective real-time audio processing","","","","","http://hdl.handle.net/1853/50175","This paper studies various priority metrics that can be used to progressively select sub-parts of a number of audio signals for realtime processing. In particular, five level-related metrics were examined: RMS level, A-weighted level, Zwicker and Moore loudness models and a masking threshold-based model. We conducted a pilot subjective evaluation study aimed at evaluating which metric would perform best at reconstructing mixtures of various types (speech, ambient and music) using only a budget amount of original audio data. Our results suggest that A-weighting performs the worst while results obtained with loudness metrics appear to depend on the type of signals. RMS level offers a good compromise for all cases. Our results also show that significant sub-parts of the original audio data can be omitted in most cases, without noticeable degradation in the generated mixtures, which validates the usability of our selective processing approach for real-time applications. In this context, we successfully implemented a prototype 3D audio rendering pipeline using our selective approach.","2005-07","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"88NUK6VX","journalArticle","2012","Brent, William","Physical navigation of visual timbre spaces with timbreID and DILIB","","","2168-5126","","http://hdl.handle.net/1853/44409","This paper summarizes recent development of two open source software libraries that enable auditory display in Pure Data (Pd), and describes developing projects that were achieved using the two packages in tandem. The timbreID feature extraction and classification library enables real- and non-real-time audio analysis via high-level modules that can be programmed for a variety of purposes. DILib (the Digital Instrument Library) provides software tools for accessing and managing gestural control streams as captured by inexpensive, widely available sensor hardware. Realized at the intersection of these software packages, three applications are discussed from technological and performative viewpoints: a system for navigating visual timbre spaces with gestures drawn from full body tracking, a similar system based on open-air infrared fingertip tracking, and the Gesturally Extended Piano–an augmented instrument controller that uses piano performance gestures to create visually explicit action-sound relationships.","2012-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BXVFAAYS","journalArticle","2011","Roginska, Agnieszka; Wakefield, Gregory H.; McCullen, Kyla","Searching for Sources from a Fixed Point in a Virtual Auditory Environment","","","","","http://hdl.handle.net/1853/51743","Interaction between the listener and their environment in a spatial auditory display plays an important role in creating better situational awareness, resolving front/back and up/down confusions, and improving localization. Prior studies with 6DOF interaction suggest that using either a head tracker or a mouse-driven interface yields similar performance during a navigation and search task in a virtual auditory environment. In this paper, we present a study that compares listener performance in a virtual auditory environment under a static mode condition, and two dynamic conditions (head tracker and mouse) using orientation-only interaction. Results reveal tradeoffs among the conditions and interfaces. While the fastest response time was observed in the static mode, both dynamic conditions resulted in significantly reduced front/back confusions and improved localization accuracy. Training effects and search strategies are discussed.","2011-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8N39I83P","journalArticle","2006","de Campo, A.; Frauenberger, C.; Vogt, K.; Wallisch, A.; Daye, C.","Sonification as an interdisciplinary working process","","","","","http://hdl.handle.net/1853/50636","This paper describes the progress of an interdisciplinary project that aims to develop a general sonification software environment. The approach taken is interdisciplinary; a number of target sciences form an integral part of the work process. Within the first year, we have made good progress in some areas, while others have turned out to be more difficult. We describe the project background, the work done so far in the target sciences, the general software framework, and what we have learned about the interdisciplinary work process.","2006-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZJUBG7PE","journalArticle","2008","Nees, Michael A.; Walker, Bruce N.","Encoding and Representation of Information in Auditory Graphs: Descriptive Reports of Listener Strategies for Understanding Data","","","","","http://hdl.handle.net/1853/49907","While a growing wealth of data have offered insights into the best practices for auditory display design and application, little is known about how listeners internally represent and use the information presented in auditory displays. At the conclusion of three separate studies, participants responded to an open-ended question about the strategies they used to perform auditory graphing tasks. We report a descriptive analysis of these qualitative responses. Participants' comments were coded by two raters along a number of dimensions that were chosen to represent a comprehensive set of encoding and task strategy possibilities. These descriptive analyses suggest that auditory graph listeners use a variety of strategies to cognitively represent the data in the display. Furthermore, these qualitative data offer a number of insights and questions for future research on information representation for auditory displays.","2008-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","Encoding and Representation of Information in Auditory Graphs","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZSPWUJFQ","book","2010","Gossmann, Joachim","From Metaphor to Medium: Sonification as Extension of Our Body","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49887","Following Marshal McLuhan’s perspective on media as extensions of man, sonification for the generation of knowledge can be regarded as an extension of our auditory sense toward previously imperceptible properties of our environment. Investigating our own involvement from an ontological perspective allows us to generate conceptual handles for the research, development and use of tools for sonification and the implied extension of our physical body through technology. Based on the nature of our bodies as mediators between the shared exterior and the individual interior, a model of three problematic areas of our extended bodies is presented: the cognitive, the physical and the extended. When we research, design and develop new applications and methods in sonification, we investigate the models and metaphors used in each of these areas, but it is only when we use the developed applications that we actually understand what potentials of perception and exploration we are provided with. It is therefore not sufficient to only build an exterior apparatus: The extended body is each of our own—each researcher and user of sonification develops an individual relationship to all affordances provided.","2010-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","From Metaphor to Medium","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PTT3UAYG","journalArticle","2004","Vickers, P.","Sonification and visualization of narrative: Getting computing students to think aurally and visually rather thn audio-visually","","","","","http://hdl.handle.net/1853/50917","In this paper we describe a final-year undergraduate honours course that requires multimedia computing students to create narrative auditory and visual displays. The aim was to use discontinuity as a catalyst for creativity by getting students to tell a story in non-speech sound and in vision in which each medium conveyed a different mood.","2004-07","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","Sonification and visualization of narrative","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EBHZKDYT","journalArticle","2021","Falkenberg, Kjetil; Frid, Emma; Eriksson, Martin Ljungdahl; Otterbring, Tobias; Daunfeldt, Sven-Olov","Auditory notification of customer actions in a virtual retail environment: Sound design, awareness and attention","","","","","http://hdl.handle.net/1853/66330","In this paper, we introduce sonification as a less intrusive method for preventing shoplifting. Music and audible alerts are common in retail, and auditory monitoring of a store can aid clerks and reduce losses. Despite these potential advantages, sonification of interaction with goods in retail is an undeveloped field. We conducted an experiment focusing on peripheral auditory notifications in a virtual retail environment, evaluating aspects such as awareness and attention, sound design and noticeability, and localization of event sounds. Results highlighted behavioral differences depending on whether users were informed about the presence of auditory notification sounds or not. The alerts did not cause distraction or annoyance and we suggest that the findings give a promising starting point for future studies and investigations focused on improving the auditory environments in retail.","2021-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","Auditory notification of customer actions in a virtual retail environment","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6PJFX3PB","journalArticle","2008","Mustonen, Manne-Sakari","A review-based conceptual analysis of auditory signs and their design","","","","","http://hdl.handle.net/1853/49882","The research frames of auditory display have traditionally mainly focused on the evaluation of different applications and devices, whereas the theoretical development has had a minor role. In order to reach the goal of functional and intuitive auditory signs, the theoretical basis must be on a robust basis. User interface sound types have been traditionally divided into two exclusionary sound types: earcons and auditory icons. However, when approaching the issues from the viewpoints of for example human communication or semiotics, one can see that the current definitions and practices in auditory display as a scientific discipline are not pragmatic. It is recommended to define auditory signs to include different levels of meaning, as was originally proposed. Following current theoretical concepts leaves the full potential of auditory signs unexposed. In this paper, I introduce important viewpoints and approaches for more practical theoretical approaches for the design of auditory signs in order to develop a theoretical basis for usable syntax.","2008-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5G5ZT8I3","journalArticle","2019","Moore, Carl; Brent, William","Interactive real-time concatenative synthesis in virtual reality","","","","","http://hdl.handle.net/1853/61530","This paper presents a new platform for interactive concatenative synthesis designed for virtual reality and proposes further applications for immersive audio tools and instruments. TimbreSpace VR is an extension ofWilliam Brentﾒs TimbreSpace software using the timbreID library for Pure Data. Design and implementation of the application are discussed, as well as its live performance aspects. Finally, future work is laid out for the project, proposing versatile audio manipulation software specifically for XR platforms.","2019-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7ZRMV8CH","book","2010","Herrera, Perfecto; Schirosa, Mattia; Kersten, Stefan; Janer, Jordi; Roma, Gerard","Content-based Retreival from Unstructured Audio Databases using an Ecological Acoustics Taxonomy","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49868","In this paper we describe a method to search for environmental sounds in unstructured databases with user-submitted material. The goal of the project is to facilitate the design of soundscapes in virtual environments. We analyze the use of a Support Vector Machine (SVM) as a learning algorithm to classify sounds according to a general sound events taxonomy based on ecological acoustics. In our experiments, we obtain accuracies above 80% using crossvalidation. Finally, we present a web prototype that integrates the classifier to rank sounds according to their relation to the taxonomy concepts.","2010-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2LSR8P7A","journalArticle","2006","Verfaille, V.; Quek, O.; Wanderley, M.","Sonification of musicians' ancillary gestures","","","","","http://hdl.handle.net/1853/50640","This paper describes the sonification of movements of three clarinetists'. Rather than quantifying the different kinds of movements and presenting such information using visual methods such as graphs or tables, sonification of such gestures provides a complementary way of analysing movements which is possibly more informative than visualisation of data. This paper describes the design methodologies, mappings, and synthesis techniques used in transforming a set of data markers each with x, y and z cartesian coordinates into informative and intelligible sonifications.","2006-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7WFAC8AN","journalArticle","2005","Susini, Patrick; McAdams, Stephen; Smith, Bennett K.","Loudness asymmetries for tones with increasing and decreasing levels","","","","","http://hdl.handle.net/1853/50169","Studies of loudness change for tones with linearly varying levels using different loudness rating methods, such as direct estimation or indirect estimation based on the start and end levels, have revealed an asymmetry depending on the direction of change (increasing vs decreasing). The present study examines loudness asymmetry between increasing and decreasing levels for 1-kHz tones over the range 60-80 dB SPL and over four ramp durations (2, 5, 10 and 20 s) using direct global and continuous loudness ratings made by subjects. Three measures extracted from continuous ratings (loudness duration, loudness change, loudness slope), on the one hand, and the global loudness rating, on the other hand are examined and analyzed separately. Measures extracted from continuous ratings do not reveal any significant perceptual asymmetry between an increasing and a decreasing ramp. However, direct estimation of the global loudness is higher for an increasing ramp than for a decreasing ramp. This result can be explained by a short-term auditory memory effect called the “recency effect”.","2005-07","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I8YLCUYX","journalArticle","2021","Halac, Federico Nicolás Cámara; Matiás, Delgadino","DreamSound: Deep activation layer sonification","","","","","http://hdl.handle.net/1853/66336","Deep learning (DL) in audio signal processing has received much attention in the last four years, and it is still a growing field. Some attempts were made to translate deep networks from image to audio applications, and we discuss some of the issues that arise. In the present paper, we present DreamSound, a creative adaptation of Deep Dream to sound addressed from two approaches: input manipulation, and sonification design. Our chosen model is YAMNet, a pre-trained deep network for sound classification. We present the original Deep Dream activation maximization function in relation to three filter-based creative variations that were implemented. We find that more interesting results are achieved with this filter-based approach, but that there is still room for experimentation.","2021-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","DreamSound","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HN44LCBG","journalArticle","2019","Collins, Nick","Sonification of the Riemann zeta function","","","","","http://hdl.handle.net/1853/61506","The Riemann zeta function is one of the great wonders of mathematics, with a deep and still not fully solved connection to the prime numbers. It is defined via an infinite sum analogous to Fourier additive synthesis, and can be calculated in various ways. It was Riemann who extended the consideration of the series to complex number arguments, and the famous Riemann hypothesis states that the non-trivial zeroes of the function all occur on the critical line 0.5 + ti, and what is more, hold a deep correspondence with the prime numbers. For the purposes of sonification, the rich set of mathematical ideas to analyse the zeta function provide strong resources for sonic experimentation. The positions of the zeroes on the critical line can be directly sonified, as can values of the zeta function in the complex plane, approximations to the prime spectrum of prime powers and the Riemann spectrum of the zeroes rendered; more abstract ideas concerning the function also provide interesting scope.","2019-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VD7LBMH6","journalArticle","2004","Nesbitt, K. V.","Comparing and reusing visualisation and sonification designs using the ms-taxonomy","","","","","http://hdl.handle.net/1853/50848","Comparing designs of sonifications is difficult enough but comparing a visual display with a sound display is much harder. Yet the designer of multi-sensory displays would like to make sensible decisions about when to use each modality. This paper describes a classification of abstract data displays that is general for all senses. This allows the same terminology to be used for describing both visualisations and sonifications. The classification of displays is hierarchical and describes multiple levels of abstraction. In software engineering terms the taxonomy allows a designer to consider reuse at both an abstract architectural level and also a more detailed component level. Thus design mappings can be discussed independently of the sensory modality to be used. This allows for exactly the same design to be implemented for each sense and subsequently compared.","2004-07","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7XPB58KD","journalArticle","2006","Pirhonen, A.; Murphy, E.; McAllister, G.; Yu, W.","Non-speech sounds as elements of a use scenario: A semiotic perspective","","","","","http://hdl.handle.net/1853/50602","At present most sound design methods for non-speech sounds in auditory interfaces are based on empirical knowledge, often resulting in sounds derived from random selection or the personal preferences of the designer. A more theoretical design background is required which will create a framework that can be integrated with a practical approach to create the required results. The design framework selected and presented in this paper is based on a semiotic approach to the design of nonspeech sounds. In this approach, the design process is conceptualised by referring to structural semiotics, taking into account the unique qualities of non-speech sounds, as a mode of conveying information. The central question is how individual non-speech sounds in an auditory interface can be integrated within their overall use context. A sound design method is presented as a synthesis of the theoretical points. This method is based on a rich use scenario presented to a design panel. Finally, a case study where the design method has been applied is presented and evaluated.","2006-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","Non-speech sounds as elements of a use scenario","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CG4ARGUK","journalArticle","2014","Rouat, Jean; Lescal, Damien; Wood, Sean","Handheld Device for Substitution From Vision to Audition","","","","","http://hdl.handle.net/1853/52092","Sensorial substitution has great potential in rehabilitation, education, games, and in the creation of music and art. Current technologies allow us to develop sensorial substitution and sonification systems that would not have been imaginable two decades ago. It is desirable to let a large audience use and test sonification systems to provide feedback and improve their design. Handheld devices like smartphones or tablets include network connectivity (WIFI and/or Cellular radio) that can be used to transmit anonymous information about the configuration and strategies adopted by users. It is now feasible to obtain feedback from any user of substitution and sonification technology and not only from a limited number of subjects in the laboratory. Testing in the field with a large number of users is now possible thanks to telecommunication networks and machine learning tools to analyze big data. This work presents a handheld implementation of a simple video sonification system designed to test the acceptability of vision to audition substitution systems and in the near future to provide feedback from users. A first beta version was publicly released in November 2013 as an iOS application for large scale testing. The extended abstract introduces the interface and the underlying technology.","2014-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GAAEV7U9","book","2009","Droumeva, Milena; Antle, Alissa; Corness, Greg; Bevans, Allen","Springboard: exploring embodied metaphor in the design of sound feedback for physical responsive environments","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51414","In this paper we propose a role for suing embodied metaphor in the design of sound feedback for interactive physical environments. We describe the application of a balance metaphor in the design of the interaction model for a prototype interactive environment called Springboard. We focus specifically on the auditory feedback, and conclude with a discussion of design choices and future research directions based on our prototype.","2009-05","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","Springboard","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HHYJ6C4B","journalArticle","2022","García Riber, Adrian; Serradilla Garcia, Francisco","Sonification of TESS data validation timeseries files","","","","","http://hdl.handle.net/1853/67385","In August 2022, the Transiting Exoplanet Survey Satellite (TESS) will start its fifth year of exploration having observed and analyze more than a million stars in the search of extrasolar planets around both celestial hemispheres. At this time, with 5,767 declared TESS objects of interest (TOI), the total number of confirmed planets that appear in peer-reviewed journals exceeds 220. This article describes the prototype design and domain conversion strategies used for the sonifcation of Data Validation Timeseries (DVT) files from TESS mission. Focused on 132 examples of publicly available lightcurves from stars hosting at least one planet confirmed during the first four years of TESS observations, the resulting proposal allows the multi-modal and multi-channel automatic interactive aural exploration of astronomical variables, generating sequences of complete soundscapes from target variables stored in each DVT file, which can be controlled in terms of complexity and balance. Aesthetics and inspiring aspects of Sonifcation are also open for discussion.","2022-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P47WDBM2","book","2009","Julieta Lopez, Mariana; Pauletto, Sandra","The design of an audio film for the visually impaired","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51421","Nowadays, Audio Description is used to enable visually impaired people to access films. However, it presents an important limitation, which consists in the need of the visually impaired audiences to rely on a describer, not being able to access the work directly. The aim of this project was to design a format of sonic art called audio film that eliminates the need of visual elements and of a describer, by providing information solely through sound, sound processing and spatialization, and which might be considered as an alternative to Audio Description. In order to explore the viability of this format an example has been designed based on Roald Dahl’s Lamb to the Slaughter (1954) using a 6.1 surround sound configuration. Through the design of this example it could be noticed that this format can successfully convey a story without the need of either visual elements or of a narrator.","2009-05","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N4S653HE","journalArticle","2004","Hermann, T.; Hunt, A.","The importance of interaction in sonification","","","","","http://hdl.handle.net/1853/50923","This paper argues for a special focus on the use of dynamic human interaction to explore datasets while they are being transformed into sound. We describe why this is a special case of both human computer interaction (HCI) techniques and sonification methods. Humans are adapted for interacting with their physical environment and making continuous use of all their senses. When this exploratory interaction is applied to a dataset (by continuously controlling its transformation into sound) new insights are gained into the data's macro and micro-structure, which are not obvious in a visual rendering. This paper reviews the importance of interaction in sonification, describes how a certain quality of interaction is required, provides examples of the techniques being applied interactively, and outlines a plan of future work to develop interaction techniques to aid sonification.","2004-07","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"75JHNQ7P","journalArticle","2005","Fox, Jesse; Carlile, Jennifer; Berger, Jonathan","Sonimime: Sonification of fine motor skills","","","","","http://hdl.handle.net/1853/50180","This paper describes the design of SoniMime, a system for the soni cation of hand motion. Among SoniMime's applications is the use of auditory feedback to re ne ne motor skills in a wide variety of tasks. Our primary soni cation method involves mapping movement to timbre parameters. Speci cally, we explore the application of the tristimulus timbre model for the soni cation of gestural data, working toward the goals of assisting a user to learn a particular motion or gesture with minimal deviation. We also explore real-time timbre shaping and its possible use as an intuitive soni cation tool related to formant synthesis and human vowel recognition. SoniMime uses two 3-D accelerometers connected to an Atmel microprocessor which outputs OSC control messages. Data ltering, parameter mapping, and sound synthesis take place in Pd running on a Linux computer.","2005-07","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","Sonimime","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AEBBWB24","journalArticle","2016","Gifford, Toby","Tuning Into The Task: Sonic Environmental Cues And Mental Task Switching","","","","","http://hdl.handle.net/1853/56585","This position paper suggests a novel approach to enhancing productivity for professionals whose core business is deep thinking, by manipulation of the sonic environment. Approaching the issue from the perspective of sound-design, it proposes the composition and algorithmic generation of background soundscapes that promote a psychological state of flow [1], and can become mentally associated with particular tasks through exposure, so as to facilitate task switching by switching soundscapes. These background soundscapes are intended to mask distracting clatter, oppressive quiet, and other suboptimal sonic environments frequently encountered in office workplaces. Consequently, I call them active-silences: soundscapes designed to be not heard, although they may be relatively loud. The most commonly used active-silence is white noise, though there are surprisingly diverse other approaches to crafting active-silence. This variety suggests the possibility of training associations that pair distinct active-silences with distinct mental tasks.","2016-07","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","Tuning Into The Task","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N4A7ZRAM","book","2015","Roddy, Stephen; Furlong, Dermot","Sonification listening: An empirical embodied approach","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54125","This paper presents a sonification listening model built from models of embodied cognitive meaning-making faculties. The aim of such a model is to aid in understanding how meaning is applied to auditory stimuli at the cognitive level.this in trun can aid auditory display designers in creating more effective auditory displays. The concept of ‘scale’ in sonification is considered in relation to the faculties described in the model. An experiment that explores how embodied auditory cognition, as described by the model, understands and interprets sonifications is then presented. This examining two speciffic kinds of ‘scale models’ listeners employ to interpret a sonification. The results obtained from this experiment are particularly convincing showing that a listeners knowledge of the data-set being sonified will determine how they interpret changes in the auditory stimuli in a sonification. The existence of these scale models, the impact of a listeners knowledge on their perception of a sonification and the implications imposed by the embodied nature of auditory cognition suggest a new avenue for auditory display researchers interested in devloping meaningful sonifications that explit the embodied nature of auditory cognition.","2015-07","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","Sonification listening","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZFUY6XPV","book","2010","Roginska, Agnieszka; Wakefield, Gregory H.; Santoro, Thomas S.; McMullen, Kyla","Effects of Interface Type on Navigation in a Virtual Spatial Auditory Environment","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49886","In the design of spatial auditory displays, listener interactivity can promote greater immersion, better situational awareness, reduced front/back confusion, improved localization, and greater externalization. Interactivity between the listener and their environment has traditionally been achieved using a head tracker interface. However, trackers are expensive, sensitive to calibration, and may not be appropriate for use in all physical environments. Interactivity can be achieved using a number of alternative interfaces. This study compares learning rates and performance in a single-source auditory search task for a headtracker and a mouse/keyboard interface within a single source and multi-source context.","2010-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NMZZTCJT","journalArticle","2013","Andreopoulou, Areti; Rogińska, Agnieszka; Mohanraj, Hariharan","A Database Of Repeated Head-Related Transfer Function Measurements","","","","","http://hdl.handle.net/1853/51682","This paper describes a new HRTF collection, measured at the Music and Audio Research Laboratory (MARL), at NYU. This collection of 40 datasets, consists of repeated HRTF measurements on 4 subjects (10 HRTF sets per subject). Analysis of the data offers an understanding of the expected degree of variation in HRTF sets, which, when supported by subjective evaluation, can provide deeper insight for perceptually detected differences between binaural filters. Such knowledge has applications in various research fields and related signal processing tasks, such as: binaural auditory displays, HRTF modeling, and HRTF interpolation.","2013-07","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U7S7FMAI","journalArticle","2022","Morabito, Robin; Armitage, Jack; Magnusson, Thor","Ritualistic approach to sonic interaction design: A poetic framework for participatory sonification","","","","","http://hdl.handle.net/1853/67382","While sonification is often adopted as an analytical tool to understand data, it can also be an efficient basis for the construction of an interaction model for an aesthetic sound piece. Mirroring the performative arts, a ritualistic approach in participatory sonification can take place whenever a work relies more on the outer form of the piece, than on the meaning attributed to the information communicated, to connect with an audience, losing some degrees of readability and intelligibility in the process while maintaining a reliable data-to-sound relationship. We will present a few examples that anticipate or expand the use of sonification as analytical tool to propose aesthetic approaches, accessing more complex layers of meaning in interactive design. By proposing topological, semantic, and technical perspectives, we demonstrate the functional aspects of the multimedia artwork “The Only Object They Could Retrieve From Earth’s Lost Civilisation” (“The Only Object” from now on). Outcomes will be considered under the multidisciplinary framework here proposed, to conclude with possibilities and implications of a ritualistic approach to interaction design.","2022-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","Ritualistic approach to sonic interaction design","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"743A2W75","journalArticle","2004","Martens, W. L.","Decoupled loudness and range control for a source located within a small virtual acoustic environment","","","","","http://hdl.handle.net/1853/50851","For headphone-based spatial auditory display systems, binaural synthesis of sound localization cues typically use source reproduction level as the primary control for source range. This approach can be quite effective when indirect sound is simulated in order to externalize virtual sources within a small virtual acoustic environment. A computationally efficient simulation solution is described here that does not rely solely upon the sound reproduction level of the source to control source range (i.e., perceived egocentric distance), and provides an extremely economical synthesis of the indirect sound component that is effective in creating externalized spatial auditory images. The performance of the solution has been psychophysically validated using indirect scaling methods that required experimental listeners to compare two displayed sound stimuli and report which of the two was the louder or the closer. In particular, it was shown that the simulation allows for decoupled loudness and range control for a source located near the listener's head, so that equally loud sources can be positioned at varying source range. Likewise, within certain limits, source loudness may be varied while holding source range constant. This performance feature has benefits for auditory display applications for which selective attention should be supported for a spatially distributed set of virtual sound sources.","2004-07","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WA3L39X8","journalArticle","2011","Vogt, Katharina","A QUANTITATIVE EVALUATION APPROACH TO SONIFICATIONS","","","","","http://hdl.handle.net/1853/51571","Different evaluation approaches have been taken within the field of sonification. This paper introduces methods of the multi-criteria decision aid (MCDA) to the field. Different stakeholders are taken into account. In the area of explorative data analysis, the domain scientists of the field are included in the process in addition to sonification experts. The method allows to compare different sonification designs of the same or different data sets quantitatively. This enables the sonification designer to evaluate the sonification design objectively and draw conclusions on which kind of sonification is appropriate for the end user.","2011-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V68EXLEN","book","2010","R. Ness, Steven; Reimer, Paul; Krell, Norman; Odowichuck, Gabrielle; Schloss, W. Andrew; Tzanetakis, George","Sonophenology: A Tangible Interface for Sonification of Geo-Spatial Phenological Data at Multiple Time-scales","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/50061","Phenology is the study of periodic biological processes, such as when plants flower and birds arrive in the spring. In this paper we sonify phenology data and control the sonification process through a tangible interface consisting of a physical paper map and tracking of fiducial markers. The designed interface enables one or more users to concurrently specify point and range queries in both time and space and receive immediate sonic feedback. This system can be used to study and explore the effects of climate change, both as tool to be used by scientists, and as a way to educate members of the general public.","2010-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","Sonophenology","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UPPKDPYT","journalArticle","2019","Hermann, Thomas; Weger, Marian","Data-driven auditory contrast enhancement for everyday sounds and sonifications","","","","","http://hdl.handle.net/1853/61528","We introduce Auditory Contrast Enhancement (ACE) as a technique to enhance sounds at hand of a given collection of sound or sonification examples that belong to different classes, such as sounds of machines with and without a certain malfunction, or medical data sonifications for different pathologies/conditions. A frequent use case in inductive data mining is the discovery of patterns in which such groups can be discerned, to guide subsequent paths for modelling and feature extraction. ACE provides researchers with a set of methods to render focussed auditory perspectives that accentuate inter-group differences and in turn also enhance the intra-group similarity, i.e. it warps sounds so that our human built-in metrics for assessing differences between sounds is better aligned to systematic differences between sounds belonging to different classes. We unfold and detail the concept along three different lines: temporal, spectral and spectrotemporal auditory contrast enhancement and we demonstrate their performance at hand of given sound and sonification collections.","2019-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BZDX66VX","journalArticle","2004","Wilson, R. S.; Berger, J.; Yeo, W. S.","A flexible framework for real-time sonification with sonart","","","","","http://hdl.handle.net/1853/50830","We describe significant developments towards a real-time implementation of SonArt, the parameter mapping framework first presented in [1] 1. Enhancements include the incorporation of Open Sound Control (OSC) [2] which facilitates network communications, direct access to a variety of real-time synthesis software platforms, and distributed synthesis. The original goals for opensource, platform independence, and modularity are further discussed with an example implementation using Java and OSC ([4]).","2004-07","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G9XZBCTL","book","2009","Fagerlonn, Johan; Liljedahl, Mats","Awesome sound design tool: a web based utility that invites end users into the audio design process","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51287","Previous auditory display research has shown how fundamental aspects of an auditory signal may influence perception and impact the emotional state of the listener. However, a challenge for designers is how to find signals that correspond to user situations and make sense within a user context. In this article we present a web based application called AWESOME Sound Design Tool; a tool that invites users to take part in the design process of auditory signals. The basic idea is to give users control over some aspects of the auditory stimuli and encourage them to manipulate the sound with a specific user scenario in mind. The software may help developers working with applied design to find more appropriate sounds for user situations. It might also help researchers to better understand correlations between the properties of a sound and characteristics of a user situation. A pilot study has been conducted in which car drivers designed warning signals for critical traffic situations. The pilot study illustrated how the tool could be useful for applied audio design.","2009-05","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","Awesome sound design tool","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RX3C2EQV","book","2009","Aramaki, Mitsuko; Gondre, Charles; Kronland-Martinet, Richard; Voinier, Thierry; Ystad, Solvi","Thinking the sounds: an intuitive control of an impact sound synthesizer","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51419","In this paper we describe a synthesizer to be used both for virtual reality and musical purposes, based on an additive synthesis model, and that offers an intuitive control of impact sounds. A three layer control strategy is proposed for this purpose, where the top layer gives access to a control through verbal descriptions, the middle layer to a control of perceptually relevant signal descriptors, while the bottom layer is directly linked to the parameters of the synthesis model. The mapping strategies between the parame- ters of the different layers are described. The synthesizer has been implemented using Max/MSP, offering the possibility to manip- ulate the sounds in real-time through the control of the different parameters.","2009-05","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","Thinking the sounds","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6KAKHWU4","journalArticle","2016","Murphy, Jim; McKinnon, Dugal; Zareei, Mo H.","Lost Oscillations: Exploring a City’s Space and Time With an Interactive Auditory Art Installation","","","","","http://hdl.handle.net/1853/56568","Lost Oscillations is a spatio-temporal sound art installation that allows users to explore the past and present of a city's soundscape. Participants are positioned in the center of an octophonic speaker array; situated in the middle of the array is a touch-sensitive user interface. The user interface is a stylized representation of a map of Christchurch, New Zealand, with electrodes placed throughout the map. Upon touching an electrode, one of many sound recordings made at the electrode's real-world location is chosen and played; users must stay in contact with the electrodes in order for the sounds to continue playing, requiring commitment from users in order to explore the soundscape. The sound recordings have been chosen to represent Christchurch's development throughout its history, allowing participants to explore the evolution of the city from the early 20th Century through to its post-earthquake reconstruction. This paper discusses the motivations for Lost Oscillations before presenting the installation's design, development, and presentation.","2016-07","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","Lost Oscillations","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4MALA56F","journalArticle","2019","Vickers, Paul; Höldrich, Robert","Direct segmented sonification of characteristic features of the data domain","","","","","http://hdl.handle.net/1853/61516","Like audification, auditory graphs maintain the temporal relationships of data while using parameter mappings to represent the ordinate values. Such direct approaches have the advantage of presenting the data stream 'as is' without the imposed interpretations or accentuation of particular features found in indirect approaches. However, datasets can often be subdivided into short non-overlapping variable length segments that each encapsulate a discrete unit of domain-specific significant information and current direct approaches cannot represent these. We present Direct Segmented Sonification (DSSon) for highlighting the segments' data distributions as individual sonic events. Using domain knowledge DSSon presents segments as discrete auditory gestalts while retaining the overall temporal regime and relationships of the dataset. The method's structural decoupling from the sound stream's formation means playback speed is independent of the individual sonic event durations, thereby offering highly flexible time compression/stretching to allow zooming into or out of the data. DSSon displays high directness, letting the data 'speak' for themselves.","2019-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7RUN8NRA","journalArticle","2012","Alexander, Robert Lewis; O'Modhrain, Sile; Gilbert, Jason; Zurbuchen, Thomas; Simoni, Mary","Recognition of Audified Data in Untrained Listeners","","","2168-5126","","http://hdl.handle.net/1853/44398","The effective navigation and analysis of large data sets is a persistent challenge within the scientific community. The objective of this experiment was to determine whether participants who received no training were able to identify audified data sets at a rate above chance in a forced-choice listening task. Nineteen participants with various levels of musical and scientific expertise were asked to place audio examples into one of the five following categories: Digitally Generated Sound - White Noise, Solar Wind Data, Neuron Firing Data from a Human Brain, Seismic Data (Earthquake Activity), and Digitally Generated Sound - Sinusoidal Waveform. At no time were participants made aware of the accuracy of their responses during the experiment. Participants who had never been exposed to audified data sets were able to recognize audification examples at a rate that was 23 percentage points above chance performance; however, the sample size of individuals with no previous exposure to audified data was not large enough to determine statistical significance. When controlling for previous exposure to any of the provided listening examples, all participants performed well above the statistical likelihood of chance responses in the recognition of digitally generated white noise and sinusoidal waveforms. This indicates that participants with no previous exposure to audified data were able to discriminate between audified data and digitally generated sounds.","2012-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CKV4JJUR","book","2015","Maculewicz, Justyna; Erkut, Cumhur; Serafin, Stefania","An investigation on the influence of soundscapes and footstep sounds in affecting preferred walking pace","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54118","In this paper we describe an experiment whose goal is to investigate the role of footstep sounds and soundscapes to affect the pace of a person who is walking in place, e.g., mimicking the act of walking without leaving the current position. The results of a preliminary experiment with nine subjects show that people change their walking pace when exposed to different soundscapes.","2015-07","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NZEPK7BM","journalArticle","2008","Marentakis, Georgios; Malloch, Joe; Peters, Nils; Mark, Marshall; Wanderlay, Marcelo; McAdams, Stephen","Influence of performance gestures on the identification of spatial sound trajectories in a concert hall","","","","","http://hdl.handle.net/1853/49937","An experimental study was performed on the effects of the visibility of a performer's gestures on the identification of virtual sound trajectories in the concert hall. We found that when working in synchrony, the performer's gestures integrate with the audio cues to significantly increase identification performance, normalize for the effects of off-centre listening in the hall and overcome problems related to the complexity of the soundscape. In the absence of visual cues, identification performance depends on the listening seat, the sound trajectory and the complexity of the soundscape.","2008-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EUUF6QKX","journalArticle","2003","Skantze, Daniel; Dahlback, Nils","Auditory icon support for navigation in speech-only interfaces for room-based design metaphors","","","","","http://hdl.handle.net/1853/50443","In this paper, a navigation support approach for speech-only interaction based on auditory icons for room-based designs is presented, i.e. naturally occurring sounds that have a natural mapping to the system's underlying design metaphor. In contrast to many recent investigations that have focused on multi-modal or augmented reality systems, this paper concerns a unimodal speech and sound-only system. An auditory icon based prototype system for buildings maintenance support using a room-based metaphor was developed. The design was evaluated in a comparison with earcons and no-sound designs. The users' subjective attitudes toward auditory icons were significantly more positive than to earcons.","2003-07","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZZGETMZH","journalArticle","2021","Andreopoulou, Areti; Goudarzi, Visda","Sonification first: The role of ICAD in the advancement of sonification-related research","","","","","http://hdl.handle.net/1853/66335","Sonification as a means for exploring and analysing data is an established research domain in the field of Auditory Displays. Since 1992, the International Community for Auditory Displays (ICAD) fosters the development of this field through the organization of annual conferences, special journal issues, scientific publications, and research advancements. Sonification has been a focal point in these activities. This paper reflects on nearly 3 decades of sonification research, analysing the proceedings of the past 24 ICAD conferences in a semi-automated manner. The balance between artistic and scientific data exploration as well as changes in the focus of sonification-related work published are monitored and discussed. Word frequency analysis on the abstracts and titles of selected papers shines further light into the evolution of sonification as reflected in the ICAD conference proceedings.","2021-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","Sonification first","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IRQSZ8NP","journalArticle","2017","Sterkenburg, Jason; Landry, Steven; Jeon, Myounghoon","Influences of Visual and Auditory Displays on Aimed Movements Using Air Gesture Controls","","","","","http://hdl.handle.net/1853/58376","With the proliferation of technologies operated via in-air hand movements, e.g. virtual/augmented reality, in-vehicle infotainment systems, and large public information displays, there remains an open question about if/how auditory displays can be used effectively to facilitate eyes-free aimed movements. We conducted a within-subjects study, similar to a Fitts paradigm study, in which 24 participants completed simple aimed movements to acquire targets of varying sizes and distances. Participants completed these aimed movements for six conditions – each presenting a unique combination of visual and auditory displays. Results showed participants were generally faster to make selections when using visual displays compared to displays without visuals. However, selection accuracy was similar for auditory-only displays when compared to displays with visual components. These results highlight the potential for auditory displays to aid aimed movements using air gestures in conditions where visual displays are impractical, impossible, or unhelpful.","2017-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PN62K4WY","journalArticle","2007","McGregor, Ian; Crerar, Alison; Benyon, David; Leplatre, Gregory","Establishing Key Dimensions for Reifying Soundfields and Soundscapes from Auditory Professionals","","","","","http://hdl.handle.net/1853/49997","This paper presents a unique insight into the way acousticians, computing specialists and sound designers describe the dimensions of sound they use. Seventy-five audio professionals completed a detailed questionnaire created to elicit common definitions of the words noise and soundscape, and to establish common methods of reifying sound, architectural acoustics and hearing abilities. The responses in have contributed to a better understanding of sound from a practitioner's perspective, the impact of the physical environment on sound perception and also effects experienced by those with hearing difficulties. We report a method of data analysis and that is appropriate for use by diverse groups of professionals engaged in the design and evaluation of auditory displays for shared environments. This research suggests that a far simpler approach to the measurement and evaluation of sounds and soundscapes is practiced than might be assumed from studying the exhaustive lists of measures and methods detailed in current textbooks and published standards.","2007-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"25BENTMP","journalArticle","2011","Tiuraniemi, Maria-Karoliina","Guiding Divers with Sound","","","","","http://hdl.handle.net/1853/51698","Dive computers can be used to instruct the diver during the dive. The alarms in this study consisted of a long, repetitive Alerting sound of High or Low priority followed by a shorter Informative sound representing the functions Ascend, Descend and Change gas. An internet questionnaire was sent to 30 experienced divers, who were divided in three groups. Each group listened to the alarms in a different order and was asked to recognize the priorities and functions. They were also asked to choose the most informative, pleasant and annoying sound from three different versions of each alarm. The subjects’ musical backgrounds did not affect the recognition of the sounds. The High and Low alarms were well recognized. The Ascend and Change gas alarms were quite well recognized, but the Descend alarm was perceived as very similar to the other sounds. After the validation the sound was changed to a more informative one. The A versions of all sounds received the best ratings and were chosen for further development. It may have affected the outcome that all subjects listened to these versions first in the recognition test.","2011-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LQWVM83G","journalArticle","1997","Weinstein, Jessica R.; Cook, Perry R.","FAUST: A framework for algorithm understanding and sonification testing","","","","","http://hdl.handle.net/1853/50746","One of the main obstacles to experimentation with algorithm sonification is the lack of simple tools for auditory display. There is considerable overhead both in creating sounds and in deciding which sounds are best suited to a particular algorithm. The goal of FAUST is to provide a framework that programmers can use to easily sonify their programs. FAUST is a modular tool which allows simple mapping of algorithm events to sound parameters and provides the actual structure needed to create the sound. Programmers can thus test sonification options without having to understand the underlying sound synthesis mechanisms. However, the underlying sound synthesis framework does allow interested programmers to easily change sound synthesis algorithms, or to add features and parameters to the existing algorithms.","1997-11","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","FAUST","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TXYZHP3L","journalArticle","2006","Beilharz, K.; Ferguson, S.; Song, H. J.; Cabrera, D.","An interactive approach for teaching information sonification","","","","","http://hdl.handle.net/1853/50421","Teaching sonification is interdisciplinary and multifaceted. It includes areas such as information graphing, auditory parameters for representation, psychoacoustics affected by the context and combination of parameters, auditory cognition, programming and foundational synthesis or sound production. The interactive pedagogical method presented here fuses these elements in a real-time interactive environment for learning and experimentation in order to familiarise students with basic concepts and develop an understanding of the interdependencies. It allows the student to listen and interact with instructive examples, quickly evaluate the efficacy of different display possibilities, move through the material at their own pace, investigate further online reading lists, and eventually helps them build their own sonifications. This paper describes pedagogical software that integrates these disciplines.","2006-06","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D2YJNZW2","book","2009","Saranti, Anna; Eckel, Gerhard; Pirro, David","Quantum harmonic oscillator sonification","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51411","This work deals with the sonification of a quantum mechanical system and the processes that occur as a result of its quantum me- chanical nature and interactions with other systems. The quantum harmonic oscillator is not only regarded as a system with sonifi- able characteristics but also as a storage medium for quantum in- formation. By representing sound information quantum mechan- ically and storing it in the system, every process that unfolds on this level is inherited and reﬂected by the sound. The main profit of this approach is that the sonification can be used as a first in- sight for two models: a quantum mechanical system model and a quantum computation model.","2009-05","2023-07-13 06:25:46","2023-07-13 06:25:46","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2T2Q6FCC","book","2010","Pun, Thierry; Deville, Benoit; Bologna, Guido","Sonification of Colour and Depth in a Mobility Aid for Blind People","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/50052","The See Color interface transforms a small portion of a colored video image into sound sources represented by spatialized musical instruments. Basically, the conversion of colors into sounds is achieved by quantization of the HSL color system. Our purpose is to provide visually impaired individuals with a capability of perception of the environment in real time. In this work the novelty is the simultaneous sonification of color and depth, depth being coded by sound rhythm. Our sonification model is illustrated by several experiments, such as: (1) detecting an open door in order to go out from the office; (2) walking in a hallway and looking for a blue cabinet; (3) walking in a hallway and looking for a red tee shirt; (4) moving outside and avoiding a parked car. Videos with sounds of experiments are available on http://www.youtube.com/guidobologna.","2010-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8Z8H4Z6S","journalArticle","2003","Malandrino, Delfina; Mea, Daniela; Negro, Alberto; Palmieri, Giuseppina; Scarano, Vittoro","NeMoS: Network monitoring with sound","","","","","http://hdl.handle.net/1853/50475","In this paper we present NeMoS, a program written in Java that allows monitoring of a distributed system with sound. The architecture is client/server: the server collects (by polling via SNMP [16]) data from the monitored Network Components and the client plays accordingly. The sonification technique associates events (as defined by the user) to MIDI tracks. Our system is versatile (several channels of events can be created and used), easily configurable (personalization of events and tracks is offered to users), standard (it fits within the framework described in RFC 2570 [5]), distributed (multiple clients can be anywhere in the system) and portable (using Java as Programming Language).","2003-07","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","NeMoS","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L64LXZGG","journalArticle","2014","Roginska, Agnieszka; Mohanraj, Hariharan; Keary, James; Friedman, Kent","Sonification Method to Enhance the Diagnosis of Dementia","","","","","http://hdl.handle.net/1853/52066","Positron emission tomography (PET) scans of brains result in large datasets that are traditionally analyzed using visual displays and statistical analyses. Due the complexity and multidimensionality of the data, there exist many challenges in the interpretation of the scans. This paper describes the use of a sonification method to assist in improving the diagnosis of patients with different levels of Alzheimer’s dementia. A tripletone method is introduced, and the audible beating patterns resulting from the interaction of the three tones is explored as a metric to interpret the data. The sonification method is presented and evaluated using subjective listening tests. Results show the triple-tone sonification method is effective at evaluting PET scan brain data, even for listeners with no medical background.","2014-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"25R7Y9C6","journalArticle","1998","Eckel, Gerhard","A spatial auditory display for the cyberstage","","","","","http://hdl.handle.net/1853/50710","The CyberStage is GMD's CAVE-like audio-visual projection system integrating a 4-side visual stereo display and an 8-channel spatial auditory display. A software-based sound server for the generation of auditory cues for interactive virtual environments has been developed for this display system in the context of a research project on integrated simulation of image and sound (ISIS). Hardware and software components of the auditory display and their integration in the CyberStage application development process are described. Four applications from different areas are discussed as examples.","1998-11","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LRDST383","journalArticle","2022","Ngo, Angela; Sardana, Disha; Bukvic, Ivica Ico","Sonifying 2D Cellular Behavior Using Cellular Stehoscope","","","","","http://hdl.handle.net/1853/67380","This paper presents an approach to sonifying 2D cellular data. Its primary goal is attaining listener comprehension parity between original visual data and its sonified counterpart for the purpose of understanding cell behavior, including movement, mitosis (or division), and cell death. Here, we present the initial findings of the automated sonification prototype named “Cellular Stethoscope” that was assessed through a 19-subject pilot study to assess its ability to accurately reflect the cell behavior captured in the video footage. The resulting system is envisioned to serve as a foundation for a complementing and potentially more efficient approach to studying cell behavior when subjected to various pharmaceutical interventions.","2022-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KPG4RTVY","book","2015","Camier, Cédric; Boissinot, Julien; Guastavino, Catherine","Does reverberation affect upper limits for auditory motion perception?","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54099","We report three experiments measuring the upper limits, defined as auditory velocity thresholds beyond which listeners are no longer able to perceptually resolve a smooth circular trajectory in various reverberate conditions. These thresholds were measured for white noise, band-limited white noise and band-limited white noise mixed with a pure tone, in different reverberation conditions: acoustically dry room, two simulated source-image-based reverberations and natural reverberation with different configurations of loudspeaker arrays. Experiment 1 took place in a dry room and thresholds were measured with and without a reverberation simulation of an actual reverberant room. In Experiment 2, various simulated reverberation parameters were tested in the same dry room, and two different loudspeaker configurations were tested in a reverberant room. Experiment 3 investigated the effect of audio source type in simulated reverberation condition and for high velocities. No significant effects were observed among reverberation conditions, suggesting that the upper limit is robust against reverberation.","2015-07","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DN3YB3WY","book","2009","Rubisch, Julian; Husinsky, Matthias; Raffaseder, Hannes","Allthatsounds: associative semantic categorization of audio data","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51291","Finding appropriate and high-quality audio files for the creation of a sound track nowadays presents a serious hurdle to many media producers. As most digital sound archives restrict the categoriza- tion of audio data to verbal taxonomies, this process of retrieving suitable sounds often becomes a tedious and time-consuming part of their work. The research project AllThatSounds tries to en- hance the search procedure by supplying additional, associative and semantic classifications of the audio files. This is achieved by annotating these files with suitable metadata according to a cus- tomized systematic categorization scheme. Moreover, additional data is collected by the evaluation of user profiles and by analyzing the sounds with signal processing methods. Using artificial intel- ligence techniques, similarity distances are calculated between all the audio files in the database, so as to devise a different, highly efficient search algorithm by browsing across similar sounds. The project’s result is a tool for structuring sound databases with an ef- ficient search component, which means to guide users to suitable sounds for their sound track of media","2009-05","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","Allthatsounds","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2J8AV342","journalArticle","2007","de Campo, Alberto; Hoeldrich, Robert; Eckel, Gerhard; Wallisch, Annette","New Sonification Tools for EEG Data Screening and Monitoring","","","","","http://hdl.handle.net/1853/50017","This paper describes two software implementations for EEG data screening and realtime monitoring by means of sonification. Both have been designed in close collaboration with our partner institutions. Both tools were tested in depth with volunteers, and then tested with the expert users they are intended for, i.e. neurologists working with EEG data. In the course of these tests, a number of improvements to the designs were realised; tests and the final versions of the tools are described in detail. The scope of the paper is intended to provide an integrated description and analysis of all aspects of the design process from sonification design issues to interaction choices to user acceptance.","2007-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IPPWYND9","journalArticle","2019","Weger, Marian; Hermann, Thomas; Höldrich, Robert","Real-time auditory contrast enhancement","","","","","http://hdl.handle.net/1853/61505","Every day, we rely on the information that is encoded in the auditory feedback of our physical interactions. With the goal to perceptually enhance those sound characteristics that are relevant to us - especially within professional practices such as percussion and auscultation - we introduce the method of real-time Auditory Contrast Enhancement (ACE). It is derived from algorithms for speech enhancement as well as from the remarkable sound processing mechanisms of our ears. ACE is achieved by individual sharpening of spectral and temporal structures contained in a sound while maintaining its natural gestalt. With regard to the targeted real-time applications, the proposed method is designed for low latency. As the discussed examples illustrate, it is able to significantly enhance spectral and temporal contrast.","2019-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SBLY2DW2","journalArticle","2019","Malikova, E.; Adzhiev, V.; Fryazinov, O.; Pasko, A.","Visual-auditory volume rendering of scalar fields","","","","","http://hdl.handle.net/1853/61517","This paper describes a novel approach to visual-auditory volume rendering of continuous scalar fields. The proposed method uses well-established similarities in light transfer and sound propagation modelling to extend the visual scalar field data analysis with auditory attributes. We address the visual perception limitations of existing volume rendering techniques and show that they can be handled by auditory analysis. In particular, we describe a practical application to demonstrate how the proposed approach may keep the researcher aware of the visual perception issues in colour mapping and help track and detect geometrical features and symmetry break, issues that are important in the context of interpretation of the physical phenomena.","2019-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BCSLEYPR","journalArticle","2007","Roginska, Agnieszka","The Influence of Presentation Speed and Spatial Location on Reaction Time to Auditory Displays","","","","","http://hdl.handle.net/1853/50044","To gain a better understanding of what parameters influence the redirection of attention to auditory stimuli, principal spatiotemporal factors and their affect on subjects' focus were studied during a categorization task. Factors studied include presentation speed, stimulus location on the horizontal plane, for sounds perceived to be internalized and externalized. Statistically significant results indicate that 1) stimuli perceived inside the head result in a faster response than externalized stimuli, 2) response time does not change linearly with presentation speed, rather, there is an optimal presentation rate at which the response time if fastest, 3) stimuli presented in the frontal hemisphere are attended to faster than those in the back hemisphere. These findings indicate the existence of key factors influencing subjects' performance in attending to auditory stimuli.","2007-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QL3I6N3Y","journalArticle","2013","Wersényi, György; Répás, József","Practical Recommendations For Using Sound Transducers With Glass Memberane As Auditory Display Based On Measurements And Simulations","","","","","http://hdl.handle.net/1853/51660","Newly designed vibrating transducers can be used coupled directly to solid surfaces such as wood or glass as plane wave short-distance loudspeakers. Our former analysis evaluated the SolidDrive transducer glued on to glass surfaces of different sizes and forms. Further investigations concluded that these parameters do not influence the transmission and quality of the coupled system significantly. The second part of this investigation included only one size and geometry for further numerical simulations using COMSOL FEM, as well as for acoustic measurements of the transfer function at selected frequencies. Examination of different wall-fixing and mounting methods were also included. Concluding results show that the overall sound quality is inferior to regular loudspeakers, however, using a supplementary subwoofer results in enjoyable music transmission for a short distance. Furthermore, placement of the transducer in the middle of the membrane and having mounting points at the corners are recommended in practical applications","2013-07","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"88LQVQ2K","book","2015","Melchior, Frank; Lennox, Peter; Fiebrink, Rebecca; Barrass, Stephen","Spatial Sound - Interactive Sound (Keynotes - the 21st International Conference on Auditory Display (ICAD 2015)","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54127","This is 4 Keynote speeches from ICAD2015. Titles include: ""Let the sound interact and not the user—responsive and immersive experiences for the next generation broadcasting"" by Frank Melchoir; ""The philosophy of perception and stupidity"" by Peter Lennox; ""Data as design tool. How understanding data as a user interface can make end-user design more accessible, efficient, effective, and embodied, while challenging machine learning conventions"" by Rebecca Fiebrink; ""Sonic information design"" by Stephen Barrass.","2015-07","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5EVDIMM9","journalArticle","2013","Lech, Michał; Kostek, Bożena","Gesture-Controlled Sound Mixing System With A Sonified Interface","","","","","http://hdl.handle.net/1853/51658","In this paper the Authors present a novel approach to sound mixing. It is materialized in a system that enables to mix sound with hand gestures recognized in a video stream. The system has been developed in such a way that mixing operations can be performed both with or without visual support. To check the hypothesis that the mixing process needs only an auditory display, the influence of audio information visualization on sound mixing and the ergonomics of the system usage in comparison to a mouse and keyboard interface are tested and the results of this study are presented.","2013-07","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GVU9WIZA","journalArticle","2013","Pietrowicz, Mary; Karahalios, Karrie","Sonic Shapes: Visualizing Vocal Expression","","","","","http://hdl.handle.net/1853/51661","Sound has been an overlooked modality in visualization. Why? Because it is ephemeral. We experience it as it happens, often in community with others. Then, the sound is gone. Furthermore, sound in human communication is multidimensional and includes the semantic meaning of words, the meaning of expressive verbal gestures (paralingual and prosodic components), the nonvocal gestures, and relational gestures. Even though we are able to record sound and play it back, we typically focus more on the semantic meaning of words. In this paper, we describe a voice analytics toolkit and present visualizations that focus on the relational and expressive verbal gestures in speech. By making the overlooked channels of human communication visible and persistent, we make it possible to see beneath the surface of our words. This insight will potentially enable the development of new applications for speech therapy, the quantization and visualization of vocal trends common to speakers with medical conditions such as autism spectrum disorder (ASD), the characterization and visualization of communication patterns common in different kinds of relationships and cultures, and the development of new kinds of creative, multimodal works.","2013-07","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","Sonic Shapes","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BQM88TNU","journalArticle","2021","Morales, Esteban; James, Kedrick; Horst, Rachel; Takeda, Yuya; Yung, Effiam","The sound of our words: Singling, a textual sonification software","","","","","http://hdl.handle.net/1853/66342","As visualization struggles to grasp the intricate and temporal networks of meaning found in textual data, sonification emerges as a creative and effective way of representing language. Accordingly, this paper seeks to introduce Singling, a textual sonification software that allows users to create and manipulate auditory representations of a text's lexicogrammatical properties. To achieve this, we first present Singling's main features and interface. We then discuss an example of using this sonification software to explore—both analytically and aesthetically—three different poems. Overall, this paper seeks to introduce researchers, educators, and artists to the many possibilities of Singling and the practice of textual sonification, which includes data analysis, multimodal and collaborative narrative creation, and musical performance to name a few.","2021-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","The sound of our words","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RUY2JI5S","journalArticle","2011","Grimm, Giso; Hohmann, Volker; Ewert, Stephan","Object Fusion and Localization Dominance in Real Time Spatial Processing of Acoustics Sources Using Higher Order Ambisonics","","","","","http://hdl.handle.net/1853/51703","The psycho-acoustic properties fusion and localization dominance were measured for a rotating target and a fixed-position distractor, which was a delayed copy of the target. Higher-order Ambisonics was used for spatial presentation of the target. The relative delay between distractor and target, the listener position within the play- back system and the angular speed of the target were varied. For measurement of the localization dominance a pointer device was developed, which allows measurement of the perceived direction in real-time, synchronized to the target motion. The aim of this study was to find the limitations of the higher- order Ambisonics setup regarding the position of listeners and sources, as well as the potential influence of source speed and continuity on these limitations. A small effect of continuity on the breakdown of localization dominance was found. The results of this study are qualitatively in line with the predictions of the precedence effect. They can directly be used to optimize an artis- tic concert installation where acoustic sources are processed and presented as virtual moving sources. The setup is also suited for new hearing aid evaluation methods.","2011-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DL2ZTQTX","journalArticle","2002","Maffiolo, V.; Chateau, N.; Mersiol, M.","The impact of the sonification of a vocal server on its usability and its user-friendliness","","","","","http://hdl.handle.net/1853/51389","This paper deals with the evaluation of the impact of the addition of eight sounds in a vocal server on its usability and user-friendliness. Thirty-two subjects tested two versions of a voicemail service through eight scenarios, with and without sounds. Three types of data were collected: behavioral data, declarative data (answers to questionnaires), and galvanic skin responses. The main results show a learning effect from one version to another that is not dependent on the versions tested. No effect of the sonification on the usability and on the perceived user-friendliness of this voicemail server was found.","2002-07","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9JGP65FI","journalArticle","2014","Childs, Edward P.; Stephens, John; Childs, Benjamin","dataSonification Open Source Project for Real-Time Data Sonification","","","","","http://hdl.handle.net/1853/52033","A software platform for real-time data sonification is described in detail. The platform is designed primarily to process multiple real-time data streams simultaneously. The system was originally designed for processing financial data with the general goal to be able to monitor up to 20 different securities (stocks, bonds, financial indices) as their values were changing during a trading session. The sonifications were designed to make it easy to distinguish between different securities, and to convey as much information about each security’s activity using the shortest possible sound duration. The software platform was designed using multiple threads with a gatekeeper function to manage simultaneous sonifications events without confusion or system failure. This paper announces the release of this software together with its financial data stream implementation to the open source community. It is hoped that sonification researchers particularly interested in real-time data will use and adapt the software to their needs.","2014-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DH87GCAC","journalArticle","2005","Rober, Niklas; Masuch, Maic","Leaving the screen New perspectives in audio-only gaming","","","","","http://hdl.handle.net/1853/50168","The design of audio-based computer games possess several chal- lenges. In this paper we discuss both, the technical perspectives in the development, as well as the aesthetics in the design of audio- only computer games. We do not restrict our target audience to the visually impaired only, and assume that audiogames can be played and enjoyed by everyone. We further think that audio-only com- puter games, and audio based user interfaces in general, offer huge potentials in the form of mobile devices that can be used every- where and for nearly every application, including gaming. For this work we played and analyzed several existing audio- only computer games regarding their structure, aim, storytelling, sonification and the possible interactions, and derived some basic rules that are important for a successful game design and develop- ment. We further added additional techniques, which we believe are necessary to achieve a higher level of immersion and which assist in the perception and play of such games. To evaluate our concepts, we designed three simple action games and one story based adventure, that we integrated into our audio framework.","2005-07","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WZZPM5NK","journalArticle","2003","Rose, Jay","Reality (sound)bites: Audio tricks from the film and TV studio","","","","","http://hdl.handle.net/1853/50482","In the example-filled session that accompanies this paper, we'll listen to some of the ways sound designers fix–-or sometimes, break–-voices, music, and effects to help serve a director's vision. We'll start with how phoneme-level editing can change content, affect a dialect, or merge one voice with a completely different one. If time permits, we'll give some quick examples of how individual cookbook processes, such as equalization or delay, can be used in creative ways to change the nature of a sound. Finally, we'll examine how these processes are strung together in unusual ways, to simulate everything from an airplane interior to the sound of a classroom movie projector.","2003-07","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","Reality (sound)bites","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WBU88KDF","journalArticle","2002","Menzies, D.","Perceptual resonators for interactive worlds","","","","","http://hdl.handle.net/1853/51369","The simulation of sounds in an interactive world requires efficient and flexible algorithms, so that a potentially large number of different sounds can be generated simultaneously, in real-time. Diffuse resonators, such as wooden doors, pose a particular challenge because existing delay-feedback based methods of simulation are relatively expensive. A perceptual resonator is presented here as an alternative. In this the perceptually relevant characteristics of a resonator are simulated rather than all the low-level acoustics, gaining efficiency and control at the expense of some loss of detail, and strict linearity. The resonator also forms an interesting tool for perceptual investigation and a protoype for a new synthesis class.","2002-07","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7TI585M9","journalArticle","2007","Vogt, K.; Plessas, W.; de Campo, A.; Frauenberger, C.; Eckel, G.","Sonification of Spin Models. Listening to Phase Transitions in the Ising and Potts Model.","","","","","http://hdl.handle.net/1853/50040","In the interdisciplinary research project SonEnvir, we used sound to perceptualise data stemming from spin models. The advantages herein lie in the possibility of displaying more dimensions than in visual representation on one hand, and in the potential of the human auditory system on the other. Spin models provide an interesting test case for sonification in physics, as they model complex systems that are dynamically evolving and not satisfactorily visualisable. While the theoretical background is largely understood, their phase transitions have been an interesting subject for studies for decades, and results in this field can be applied to many scientific domains. Also, most classical methods of solving spin models rely on mean values, whereas especially at the critical point of phase transition the fluctuations of single spins are their most important feature. We found that sound is an ideal display mode to study these fluctuations and the dynamic evolution of the whole model. Our sonifications allow for identifying the different phases easily, independent of the dimension of the model and the number of spin states. Also one gets a first idea about the order of the phase transition.","2007-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DEUIW3AV","journalArticle","2012","McGee, Ryan Michael; Dickinson, Joshua; Legrady, George","Voice of Sisyphus: an image sonification multimedia installation","","","2168-5126","","http://hdl.handle.net/1853/44428","Voice of Sisyphus is a multimedia installation consisting of a projection of a black and white image sonified and spatialized through a 4 channel audio system. The audio-visual composition unfolds as several regions within the image are filtered, subdivided, and repositioned over time. Unlike the spectrograph approach used by most graphical synthesis programs, our synthesis technique is derived from raster scanning of pixel data. We innovate upon previous raster scanning image to sound techniques by adding frequency domain filters, polyphony within a single image, sound spatialization, and complete external control via network messaging. We discuss the custom software used to realize the project as well as the process of composing a multimodal artwork.","2012-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","Voice of Sisyphus","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NMIG6YZ2","journalArticle","2006","Kainulainen, A.; Turunen, M.; Hakulinen, J.","An architecture for presenting auditory awareness information in pervasive computing environments","","","","","http://hdl.handle.net/1853/50423","In this paper we present how awareness can be supported in pervasive computing environments through auditory information. We introduce an application which uses soundscapes to support people's awareness of each other's presence in an office environment. We describe several techniques for construction and control of such soundscapes. Finally, we present an architecture for designing and controlling soundscapes. The architecture is based on managers, agents, evaluators, a blackboard information storage, and a control language, it emphasizes reusability and extensibility, and it is built upon a common system framework.","2006-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5H4GEXQX","journalArticle","2017","Tomlinson, Brianna J.; Winters, R. Michael; Latina, Christopher; Bhat, Smruthi; Rane, Milap; Walker, Bruce N.","Solar System Sonification: Exploring Earth and its Neighbors Through Sound","","","","","http://hdl.handle.net/1853/58359","Informal learning environments (ILEs) like museums incorporate multi-modal displays into their exhibits as a way to engage a wider group of visitors, often relying on tactile, audio, and visual means to accomplish this. Planetariums, however, represent one type of ILE where a single, highly visual presentation modality is used to entertain, inform, and engage a large group of users in a passive viewing experience. Recently, auditory displays have been used as a supplement or even an alternative to visual presentation of astronomy concepts, though there has been little evaluation of those displays. Here, we designed an auditory model of the solar system and created a planetarium show, which was later presented at a local science center. Attendees evaluated the performance on helpfulness, interest, pleasantness, understandability, and relatability of the sounds mappings. Overall, attendees rated the solar system and planetary details very highly, in addition to providing open-ended responses about their entire experience.","2017-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","Solar System Sonification","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8PLZU2Z6","journalArticle","1998","Fernstrom, Mikael; Griffith, Niall","LiteFoot - Auditory display of footwork","","","","","http://hdl.handle.net/1853/50724","This paper describes the development of LiteFoot, an interactive floor space that tracks dancers steps, and converts the steps into auditory and visual display. The system can also record steps, for further analysis for use in dance research programmes, choreographic experimentation and training.","1998-11","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VLALLRMG","journalArticle","2019","Shafer, Seth; Larson, Timothy; diFalco, Elaine","The sonification of solar harmonics (SOSH) project","","","","","http://hdl.handle.net/1853/61544","The Sun is a resonant cavity for very low frequency acoustic waves, and just like a musical instrument, it supports a number of oscillation modes, also commonly known as harmonics. We are able to observe these harmonics by looking at how the Sun's surface oscillates in response to them. Although this data has been studied scientifically for decades, it has only rarely been sonified. The Sonification of Solar Harmonics (SoSH) Project seeks to sonify data related to the field of helioseismology and distribute tools for others to do the same. Creative applications of this research by the authors include musical compositions, installation artwork, a short documentary, and a full-dome planetarium experience.","2019-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E5885483","book","2009","Worrall, David","The use of sonic articulation in identifying correlation in capital market trading data","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51420","Despite intensive study, a comprehensive understanding of the structure of capital market trading data remains elusive. The one known application of audification to market price data reported in the 1990 that it was difficult to interpret the results probably because the market does not resonate according to acoustic laws. This paper reports on a technique transforming the data so it does resonate, so audification can be used as a means of identifying autocorrelation in capital market trading data. The results obtained indicate that the technique may have a wider application to other similarly structured time-series data.","2009-05","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"22ZVT8UM","journalArticle","2014","Hosseini, Seyedeh Maryam Fakhr; Riener, Andreas; Bose, Rahul; Jeon, Myounghoon","“Listen2dRoom”: Helping Visually Impaired People Navigate Indoor Environments Using an Ultrasonic Sensor-Based Orientation Aid","","","","","http://hdl.handle.net/1853/52085","People with visual impairments face considerable limitations with their mobility, but still there is little infrastructure in place to help them. In this study, we present a new wearable electronic travel aid (ETA), “Personal Radar”, which assists blind people in navigating in indoor environments using an ultrasonic sensors. After briefly describing our initial system design, we report the improvements from the pilot study. Then, we introduce our experiment in progress. In the experiment, blind folded students and visually impaired people will navigate through a maze and an empty room based on auditory and vibrotactile feedback of the device. This system could serve as an effective research platform for obstacle detection, current location awareness, and direction suggestion for the blind.","2014-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","“Listen2dRoom”","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X7PXIVMY","journalArticle","2014","Aswathanarayana, Shashank; Roginska, Agnieszka","I Hear Bangalore3D: Capture and Reproduction of Urban Sounds of Bangalore Using an Ambisonic Microphone","","","","","http://hdl.handle.net/1853/52081","This paper describes the project, I Hear Bangalore3D, which is an attempt to capture and render 3D recordings of various iconic locations of the city of Bangalore. First order ambisonic recordings were done and processed so that they can be played back using a speaker array configuration through real time matrixing or using headphones through binaural renderings of the recordings. This project has both aesthetic and informational use to it. Coming off a sister project, I Hear NY3D, which took a similar route in Manhattan, this project also aims at comparing the noise levels and other information of different cities in different parts of the world.","2014-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","I Hear Bangalore3D","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TYMKAL3F","journalArticle","2002","Scavone, G. P.; Lakatos, S.; Harbke, C. R.","The sonic mapper: An interactive program for obtaining similarity ratings with auditory stimuli","","","","","http://hdl.handle.net/1853/51391","The Sonic Mapper is an interactive Linux-based graphical program that affords increased methodological flexibility and sophistication to researchers who collect proximity data for auditory research. The Sonic Mapper consists of a mapping environment in which participants can position and group icons in the two-dimensional plane of the screen. Options for collecting data concerning hierarchical groupings, category prototypicality, and verbal labeling provide additional opportunities to test hypotheses in a convergent manner. The Sonic Mapper also offers an environment for traditional pairwise comparisons, as well as one for performing free sorting tasks. A pilot study that attempts to use many of the Sonic Mapper's key features is described briefly below.","2002-07","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","The sonic mapper","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3JKXZJSY","book","2009","Gygi, Brian; Shafiro, Valeriy","From signal to substance and back: Insights from environmental sound research to auditory display design","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51400","A persistent concern in the field of auditory display design has been how to effectively use environmental sounds, which are naturally occurring familiar non-speech, non-musical sounds. Environmental sounds represent physical events in the everyday world, and thus they have a semantic content that enables learning and recognition. However, unless used appropriately, their functions in auditory displays may cause problems. One of the main considerations in using environmental sounds as auditory icons is how to ensure the identifiability of the sound sources. The identifiability of an auditory icon depends on both the intrinsic acoustic properties of the sound it represents, and on the semantic fit of the sound to its context, i.e., whether the context is one in which the sound naturally occurs or would be unlikely to occur. Relatively recent research has yielded some insights into both of these factors. A second major consideration is how to use the source properties to represent events in the auditory display. This entails parameterizing the environmental sounds so the acoustics will both relate to source properties familiar to the user and convey meaningful new information to the user. Finally, particular considerations come into play when designing auditory displays for special populations, such as hearing impaired listeners who may not have access to all the acoustic information available to a normal hearing listener, or to elderly or other individuals whose cognitive resources may be diminished. Some guidelines for designing displays for these populations will be outlined.","2009-05","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","From signal to substance and back","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DEGKTV4I","journalArticle","1998","Somers, Eric","A pedagogy of creative thinking based on sonification of visual structures and visualization of aural structures","","","","","http://hdl.handle.net/1853/50717","This paper describes two related pedagogies for design education – one for teaching visual design and the other for teaching sound composition – in which the symbols and organizational principles of aural experience are transformed to the visual domain and vice versa. The purpose of the pedagogies is to develop in the student the ability to visualize and auralize from direct observation as an alternative to copying and modifying existing designs. The paper begins with a discussion of visual and aural perception and their relationship to thinking and imagination. The author then describes the use of semiotic transformation in teaching courses in sound composition and visual design.","1998-11","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZQVB6QFX","journalArticle","1996","Huopaniemi, Jyri; Savioja, Lauri; Takala, Tapio","DIVA virtual audio reality system","","","","","http://hdl.handle.net/1853/50806","We have created a model of real-time audio virtual reality. This model system includes model-based sound synthesizers, geometric room-acoustics modeling, binaural auralization for headphone or loudspeaker listening, and high-quality animation. This project aims to create a virtual musical event that is highly authentic in both audio and video. To reach this goal, we innovated this system with a real-time image-source algorithm for arbitrarily shaped rooms; shorter HRTF filter approximations for more efficient auralization; and a network-based distributed implementation of the audio-processing software and hardware.","1996-11","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PWZNJG6R","journalArticle","2002","Yagi, R.; Nishina, E.; Kawai, N.; Honda, M.; Maekawa, T.; Nakamura, S.; Morimoto, M.; Sanada, K.; Toyoshima, M.; Oohashi, T.","Auditory display for deep brain activation: Hypersonic effect","","","","","http://hdl.handle.net/1853/51345","We have found that sounds containing high-frequency components above the human audible range (20kHz) significantly increase the regional blood flow in the deep-lying brain structure and the power of the alpha frequency range of the spontaneous electroencephalogram (EEG), and positively affect sound perception. We have termed this phenomenon the “hypersonic effect” [1]. To carry out further experiments on this effect, we developed “Authentic Signal Disc” that is super audio CD (SACD) software containing the authentic signals for the hypersonic effect and “Authentic Hypersonic Audio System” that could reproduce the hypersonic effect. ”Authentic Hypersonic Audio System” consists of a custom-made SACD player, a de-emphasis controller, filters, two stereo power amplifiers, an output meter unit, and two-way stereo speakers and tweeters. Using these system and disc, we examined recorded EEGs, psychological evaluations based on Scheffe's method, and behavioral evaluations on the optimum listening level, and confirmed the introduction of the hypersonic effect. This system enables us to create some new applications of auditory display.","2002-07","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","Auditory display for deep brain activation","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6HPEUIC8","journalArticle","2001","Winberg, Fredrik; Hellstrom, Sten Olof","Qualitative aspects of auditory direct manipulation. A case study of the towers of Hanoi","","","","","http://hdl.handle.net/1853/50633","This paper presents the results from a qualitative case study of an auditory version of the game Towers of Hanoi. The goal of this study was to explore qualitative aspects of auditory direct manipulation and the subjective experience from playing the game. The results show that it is important to provide a way of focusing in the auditory space. Articulatory directness was also an important issue and feedback should support the movement of the objects in the auditory space.","2001-07","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PG4KVITE","book","2010","Taylor, Sean; Fernström, Mikael","Exploring Ambient Sonification of Water Toxicity","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49880","We explored the possibility of using ambient auditory display in the context of sonification of water toxicity. We looked at the existing work procedures carried out in an aquatic toxicity laboratory and developed a design that could replace or complement existing periodic visual monitoring of samples. The design was further developed as an art-science installation in a public exhibition in the Science Gallery in Dublin, Ireland, where visitors experienced through hearing the life and death of small aquatic crustaceans in real-time","2010-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BRGR3H8I","journalArticle","2021","Bakogiannis, Konstantinos; Polychronopoulos, Spyros; Andreopoulou, Areti; Georgaki, Anastasia","Data and information transmission in the context of sonification","","","","","http://hdl.handle.net/1853/66328","This paper illustrates the significance of the concept of information as a tool to expound sonification design. Previous works approached the concept of information systematically. However, its structural characteristics during the process of sonification have not been thoroughly discussed. In order to address the above, this paper presents a framework based on the definition of information from the fields of physics, communication engineering, cybernetics, and systems theory. According to this framework, the representation of a phenomenon into organized sound becomes possible by the propagation of organization within the components of the sonification communication model. Moreover, a distinction between the terms data and information is proposed and related to well-established sonification techniques (Audification, PMSon, MBS). The structural characteristics of the phenomenon (described in terms of entropy) are linked with sonification functions leading to new perspectives of sonification design.","2021-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T9XPS4KQ","journalArticle","2017","Bellona, Jon; Bai, Lin; Dahl, Luke; LaViers, Amy","Empirically Informed Sound Synthesis Application for Enhancing the Perception of Expressive Robotic Movement","","","","","http://hdl.handle.net/1853/58369","Since people often communicate internal states and intentions through movement, robots can better interact with humans if they too can modify their movements to communicate changing state. These movements, which may be seen as supplementary to those required for workspace tasks, may be termed “expressive.” However, robot hardware, which cannot recreate the same range of dynamics as human limbs, often limit expressive capacity. One solution is to augment expressive robotic movement with expressive sound. To that end, this paper presents an application for synthesizing sounds that match various movement qualities. Its design is based on an empirical study analyzing sound and movement qualities, where movement qualities are parametrized according to Laban’s Effort System. Our results suggests a number of correspondences between movement qualities and sound qualities. These correspondences are presented here and discussed within the context of designing movement-quality-to-sound-quality mappings in our sound synthesis application. This application will be used in future work testing user perceptions of expressive movements with synchronous sounds.","2017-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XSQ7P7UK","book","2015","Parker, J. no e","Sonification as art: Developing praxis for the audification of compost","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54122","This paper introduces compost as a rich site for creative exploration and expression via the medium of sonification art in the context of Composing [De]Composition, a large-scale audiovisual installation/performance work to be presented at University of California Riverside’s Culver Center for the Arts from June-October 2015. Here, the author non-reductively describes the multiagential and poly-temporal nature of compost through detailing the evolution of an artistic praxis involving: the observation, audification, and sonification of compost temperatures; the development new sensing methods for data-collection; and sound-mapping strategies. The main observable driving the project is incalescence—the heat generated by the composting process. Audification of this biological process brings a perceivably silent activity into the tangible reach of human hearing. The collection and real-time audification of temperature data using a custom interface to route sensor data to MAX/MSP enables listeners to better understand the complex ecology of a heterogeneous mass that is simultaneously decomposing, supporting a myriad of life forms while also enabling the bioavailability of macronutrients to the soil. In addition, the recontexualization of temporally-based temperature data into sound creates fertile ground for exploration in the compositional realm, as the collection of data over time depicts inherent patterns occurring in the systems analyzed, while the basis of music also builds upon the use of patterns (pitch based, rhythmic) through time. Sonification of these patterns enables the composer/sound artist to create compositions in partnership with her subject/phenomenon of study.","2015-07","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","Sonification as art","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WSJ5RDDM","journalArticle","2007","Peres, S. C.; Kortum, P.; Stallmann, K.","Auditory Progress Bars Preference Performance, and Aesthetics","","","","","http://hdl.handle.net/1853/49979","This paper presents the comparison of Auditory Progress Bars using segmented cello tones to those using sine tones in an on-hold telephony setting. Previous research suggests that for segmented sine tone APBs, there is an interaction between APB type (pitch or duration) and APB polarity (increasing or decreasing). However, for the cello tones, there was a main effect of direction, with increasing APBs resulting in better performance than decreasing APBs, and there was no effect of APB type (pitch or duration). As anticipated, overall performances were very similar for both types of segmented APBs. Contrary to expectations, users gave the cello tone APBs equally low subjective ratings to those they gave the sine tone APBs. Whole-song APBs produced very positive subjective ratings and performances similar to the sine and cello tone APBs.","2007-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-12","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RIZCUR8D","journalArticle","2007","Davison, Benjamin K.; Walker, Bruce N.","Sonification Sandbox Reconstruction: Software Standard for Auditory Graphs","","","","","http://hdl.handle.net/1853/50030","We report on an overhaul to the Sonification Sandbox. The Sonification Sandbox provides a cross-platform, flexible tool for converting tabular information into a descriptive auditory graph. It is implemented in Java, using the Java Sound API to generate MIDI output. An improved modular code structure provides a strong user interface and model framework for auditory graph representation and manipulation. A researcher can integrate part or the entire program into a different experimental implementation. The upgraded Sonification Sandbox provides a rich description of the auditory graph representation that can be saved or exported into various file formats. This description includes data representations of pitch, timbre, polarity, pan, and volume, along with graph contexts analogous to visual graph axes. Applications for the Sonification Sandbox include experimentation with various sonification techniques, data analytics beyond visualization, science education, auditory display for the blind, and musical interpretation of data.","2007-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-13","","","","","","","Sonification Sandbox Reconstruction","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6Q5CYZTE","journalArticle","2007","Cook, Perry R.","Din of an “Iniquity”: Analysis and Synthesis of Environmental Sounds","","","","","http://hdl.handle.net/1853/49988","This paper describes a series of related research and software projects in the analysis and synthesis of stochastic sounds in general, and more specifically, applications in the synthesis of environmental sounds. Analysis and synthesis of sounds as varied as a maraca (many beans bouncing around in a gourd), to groups of noise-making animals and insects, to human applause will be covered. Specific open-source project software will be described, such as the “Shakers” and “Flox” classes in the Synthesis ToolKit in C++ (STK), GaitLab (analysis and synthesis of walking sounds), ClapLab and ClaPD (synthesis of applause), TAPESTREA (Techniques and Paradigms for Expressive Synthesis and Transformation of Environmental Audio), and the new audio programming language ChucK.","2007-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-13","","","","","","","Din of an “Iniquity”","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RI9WEEE5","journalArticle","2013","Degara, Norberto; Nagel, Frederik; Hermann, Thomas","Sonex: An Evaluation Exchange Framework For Reproducible Sonification","","","","","http://hdl.handle.net/1853/51662","After 18 ICAD conferences, Auditory Display has become a mature research community. However, a robust evaluation and scientific comparison of sonification methods is often neglected by auditory display researchers. In the last ICAD 2012 conference, only one paper out of 53 makes a statistical comparison of several sonification methods and still no comparison with other stateof- the-art algorithms is provided. In this paper, we review profitable standards in other communities and transfer them to derive recommendations and best practices for auditory display research. We describe SonEX (Sonification Evaluation eXchange), a community-based framework for the formal evaluation of sonification methods. The goals, challenges and architecture of this evaluation platform are discussed. In addition, a simple example of a task definition according to the guidelines of SonEX is also introduced. This paper aims at starting a vivid discussion towards the establishment of thorough scientific methodologies for auditory display research and the definition of standardized sonification tasks.","2013-07","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-13","","","","","","","Sonex","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T5RZRMGI","journalArticle","2001","Lennox, Peter P.; Vaughan, John M.; Myatt, Tony","3D audio as an information-environment: Manipulating perceptual significance for differentiation and pre-selection","","","","","http://hdl.handle.net/1853/50511","Contemporary use of sound as artificial information display is rudimentary, with little 'depth of significance' to facilitate users' selective attention. We believe that this is due to conceptual neglect of 'context' or perceptual background information. This paper describes a systematic approach to developing 3D audio information environments that utilise known cognitive characteristics, in order to promote rapidity and ease of use. The key concepts are perceptual space, perceptual significance, ambience labelling information and cartoonification.","2001-07","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-13","","","","","","","3D audio as an information-environment","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RGG8BLFI","journalArticle","2018","Blandino, Michael V.","ToxSampler: Locative sound art exploration of the Toxic Release Inventory","","","","","http://hdl.handle.net/1853/60080","Regulatory geographic datasets that inform citizen’s lives are, in general, responsive to engaged search and visual, attentive browsing, but are not designed for directly informing the lived context. The density of sensors and software interfaces present in mobile devices allows for integration of these resources with contextual applications. ToxSampler is an iOS application that modifies the immediate environmental audio scene with associated data from the Toxic Release Inventory (TRI) of the United States Environmental Protection Agency. The application applies digital signal processing (DSP) to the microphone signal based upon the location of the participant and associated TRI data releases. The system, as a result, affords an informed awareness of the datascape through an immediate augmentation of the sensed setting.","2018-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-13","","","","","","","ToxSampler","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VNV6ECL9","journalArticle","2000","Hankinson, John CK; Edwards, Alistair DN","Musical phrase-structured audio communication","","","","","http://hdl.handle.net/1853/50676","It has previously been shown that musical grammars can impose structural constraints upon the design of earcons, thereby providing a grammatical basis to earcon combinations. In this paper, more complex structural combinations are explored, based upon linguistic phrases. By mapping between a musical grammar and a linguistic grammar, musical phrases can be generated which correspond to linguistic sentences. A large number of unique meanings can be presented in this way based upon a simple musical vocabulary. This is of great value to auditory designers. A user study has been undertaken which reveals that users can recognize these complex auditory phrases after a small amount of training.","2000-04","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z6JNJF5G","journalArticle","2014","Anderson, Mark","Use of Geospatial Data Sonification for Mobile Augmented Reality Audio Navigation","","","","","http://hdl.handle.net/1853/52057","This paper presents MeanderMaps: a non-speech Augmented Reality Audio (ARA) application for Apple iPhone that aids in navigation purposes by sonifying geospatial data. Users request directions to a specified location on a Google Map overlay, and MeanderMaps uses spatial auditory cues such as distance and direction to guide him/her to the destination. As the user travels to consecutive waypoints known as path nodes, auditory cues indicate whether an incorrect turn has been made or if the user is traveling in the wrong direction. Preliminary findings are reported using qualitative and quantitative methods, evaluating the overall sonification model in addition to individual audio cues that (a) worked, (b) worked somewhat well, and (c) needed to be improved. Future improvements and modifications to MeanderMaps are presented.","2014-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NQ83673N","journalArticle","2018","Frohmann, Lisa; Weger, Marian; Höldrich, Robert","Recognizability and perceived urgency of bicycle bells","","","","","http://hdl.handle.net/1853/60086","Raising awareness about how alarm sounds are perceived and evaluated by an individual in traffic scenery is important for developing new alarm designs, as well as for improving existing ones. Bearing a positive contribution to road safety, cyclists and pedestrians especially can benefit from appropriate alarming bell and horn sounds. Primarily, the alarm signal should evoke a precise idea of what is the source of the warning and the desired reaction to it. Furthermore, it should not be masked by other noises thus going undetected by the ear. Finally, an appropriate warning signal should transmit the urgency of a given situation, while at the same time, it should not cause other road users and pedestrians to startle. In two listening experiments, we examined the perception of commonly available bicycle bells and horns. Average typicality or recognizability as a bicycle bell among other everyday sounds has been investigated through a free identification task. In a second experiment, we tested perceived urgency of the warning sounds in relation to traffic noise. This article further provides a survey on non-verbal alarm design, as well as an analysis of acoustic properties of common bicycle bells and horns. Consequently, a linear regression model presents the relationship between named properties and perceived urgency. It is our intention to give an insight into the often unattended but important issue of the perception of auditory warning sounds in our everyday acoustic environment.","2018-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4XF96QV3","journalArticle","2016","Wolf, KatieAnna E.; Oda, Reid","MalLo March: A Live Sonified Performance With User Interaction","","","","","http://hdl.handle.net/1853/56573","In this extended abstract we present a new performance piece titled MalLo March that uses MalLo, a predictive percussion instrument, to allow for real-time sonification of live performers. The piece consists of two movements where in the first movement audience members will use a web application and headphones to listen to a sonification of MalLo instruments as they are played live on stage. During the second movement each audience member will use an interface in the web app to design their own sonification of the instruments to create a personalized version of the performance. We present an overview of the hardware and interaction design, highlighting various listening modes that provide audience members with different levels of control in designing the sonification of the live performers.","2016-07","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-13","","","","","","","MalLo March","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BGZSVXRP","journalArticle","1998","Wake, Sanae; Asahi, Toshiyuki","Sound retrieval with intuitive verbal expressions","","","","","http://hdl.handle.net/1853/50732","A sound retrieval method described in this paper enables users to easily obtain their desired sound. A sound representation experiment was conducted to study how people represent the sounds. Almost all representations were made with verbal descriptions that could be classified into ""description of sound itself"", ""description of sounding situation"" and ""description of sound impression"". The retrieval method, which adopts three keyword types, onomatopoeia, sound source, and adjective, was proposed based on the experimental results. The sound retrieval system's efficiency was discussed based on the subjective evaluation. Users can select a convenient retrieval method and adapt it to their idea of retrieval.","1998-11","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FFND6SFE","book","2010","Großhauser, Tobias; Hermann, Thomas","Wearable Setup for Gesture and Motion Based Closed Loop Audio Interaction","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/50068","The wearable sensor and feedback system presented in this paper is a type of audio-haptic display which contains onboard sensors, embedded sound synthesis, external sensors, and on the feedback side a loudspeaker and several vibrating motors. The so called “embedded sonification” in this case here is an onboard IC, with implemented sound synthesis. These are adjusted directly by the user and/or controlled in realtime by the sensors, which are on the board or fixed on the human body and connected to the board via cable or radio frequency transmission. Direct audio out and tactile feedback closes the loop between the wearable board and the user. In many situations, this setup can serve as a complement to visual output, e.g. exploring data in 3D space or learning motion and gestures in dance, sports or outdoor and every-day activities. A new metaphor for interactive acoustical augmentation is introduced, the so called “audio loupe”. In this case it means the sonification of minimal movements or state changes, which can sometimes hardly be perceived visually or corporal. This are for example small jitters or deviations of predefined ideal gestures or movements. Our system is easy to use, it even allows operation without an external computer. In some examples we outline the benefits of our wearable interactive setup in highly skilled motion learning scenarios in dance and sports.","2010-06","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"58XC285L","book","2009","Valle, Andrea; Lombardo, Vicenzo; Schirosa, Mattia","A graph-based system for the dynamic generation of soundscapes","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51292","This paper presents a graph-based system for the dynamic gen- eration of soundscapes and its implementation in an application that allows for an interactive, real-time exploration of the result- ing soundscapes. The application can be used alone, as a pure sonic exploration device, but it can also be integrated into a virtual reality engine. In this way, the soundcape can be acoustically in- tegrate in the exploration of an architectonic/urbanistic landscape. The paper is organized as follows: after taking into account the lit- erature relative to soundscape, a formal definition of the concept is given; then, a model is introduced; finally, a software application is described together with a case-study.","2009-05","2023-07-13 06:25:47","2023-07-13 06:25:47","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LA22DUFC","journalArticle","2018","Jette, Christopher; Buchholz, James H. J.","Fluor Sonescence: A Sonification of the Visualization of Brass Instrument Tones","","","","","http://hdl.handle.net/1853/60064","This paper is a discussion of the composition Fluor Sonescence, which combines trombone, electronics and video. The trombone and electronics are a mediated sonification of the video component. The video is a high framerate capture of the air motions produced by sound emanating from a brass instrument. This video material is translated into sound and serves as the final video component. The paper begins with a description of the data collection process and an overview of the compositional components. This is followed by a detailed description of the composition of the three components of Fluor Sonescence, while a discussion of the technical and aesthetic concerns is interwoven throughout. There is a discussion of the relationship of Fluor Sonescence to earlier works of the composer and the capture method for source material. The paper is an overview of a specific sonification project that is part of a larger trajectory of work.","2018-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","Fluor Sonescence","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TJU3H6M7","journalArticle","2008","Eerola, Tuomas; Tuuri, Kai","Could Function-Specific Prosodic Cues Be Used As a Basis for Non-Speech User Interface Sound Design?","","","","","http://hdl.handle.net/1853/49894","It is widely accepted that the nonverbal parts of vocal expression perform very important functions in vocal communication. Certain acoustic qualities in a vocal utterance can effectively communicate one's emotions and intentions to another person. This study examines the possibilities of using such prosodic qualities of vocal expressions (in human interaction) in order to design effective non-speech user interface sounds. In an empirical setting, utterances with four context-situated communicative functions were gathered from 20 participants. Time series of fundamental frequency (F0 ) and intensity were extracted from the utterances and analysed sta- tistically. Results show that individual communicative functions have distinct prosodic characteristics in respect of pitch contour and intensity. This implies that function-specific prosodic cues can be imitated in the design of communicative interface sounds for the corresponding functions in human-computer interaction.","2008-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B5NHINKL","journalArticle","2003","Brown, Lorna M.; Brewster, Stephen A.","Drawing by ear: Interpreting sonified line graphs","","","","","http://hdl.handle.net/1853/50453","The research presented here describes a pilot study into the interpretation of sonified line graphs containing two data series. The experiment aimed to discover the level of accuracy with which sighted people were able to draw sketches of the graphs after listening to them. In addition, it aimed to identify any differences in performance when the graphs were presented using different combinations of instruments–-either with piano representing both data series (same-instruments condition), or with piano representing one data series and trumpet representing the other (different-instruments condition). The drawings were evaluated by calculating the percentage of key features present. The results showed that accuracy was high (over 80% on average) in both conditions, but found no significant differences between the two. There were indications of some differences between the two conditions, but a larger study is necessary to discover whether these are significant. The results indicate that graph sonification systems should allow users to choose between these two presentation modes, depending on their preference and current task. The study showed that sonified graphs containing two data series can be interpreted, and drawn, by sighted people, and that evaluation with blind users (our target users) would be worthwhile.","2003-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","Drawing by ear","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VCDTP83X","journalArticle","2017","Jäger, Adrian; Hadjakos, Aristotelis","Navigation in an Audio-Only First Person Adventure Game","","","","","http://hdl.handle.net/1853/58363","Navigation in audio-only first person adventure games is challenging since the user has to rely exclusively on his or her sense of hearing to localize game objects and navigate in the virtual world. In this paper we report on observations that we made during the iterative design process for such a game and the results of the final evaluation. In particular we argue to provide a sufficient number of unique sound sources since players do not use a mental map of the virtual place for navigating but instead move from sound source to sound source in a more linear fashion.","2017-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZV2LXLQF","journalArticle","1998","Rubin, Benjamin U.","Audible information design in the New York City subway system: A case study","","","","","http://hdl.handle.net/1853/50716","Beginning with a detailed presentation of the use of audible signals, in the New York City subway stations and trains, we present an analysis of the information that is communicated by the existing sound design. We show the results of a survey of subway riders regarding their awareness and comprehension of the audible signals in the system. An analysis of the system's sound and information environment is presented, followed by a proposed sonic re-design to better serve both communication and aesthetic needs. We conclude by proposing that the principles of information design, sound design and music must be considered equally with those of acoustics and psychoacoustics when designing audio information and feedback systems.","1998-11","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","Audible information design in the New York City subway system","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PSQCM69I","journalArticle","2011","Lennox, Peter; Myatt, Tony","Perceptual Cartoonification in Multi-Spatial Sound Systems","","","","","http://hdl.handle.net/1853/51739","This paper describes large scale implementations of spatial audio systems which focus on the presentation of simplified spatial cues that appeal to auditory spatial perception. It reports a series of successful implementations of nested and multiple spatial audio fields to provide listeners with opportunities to explore complex sound fields, to receives cues pertaining to source behaviors within complex audio environments. This included systems designed as public sculptures capable of presenting engaging sound fields for ambulant listeners. The paper also considers questions of sound field perception and reception in relation to audio object scaling according to the dimensions of a sound reproduction system and proposes that a series of multiple, coordinated sound fields may provide better solutions to large auditorial surround sound than traditional reproduction fields which surround the audience. Particular attention is paid to the experiences since 2008 with the multi-spatial The Morning Line sound system, which has been exhibited as a public sculpture in a number of European cities.","2011-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2SF5Q7W9","journalArticle","2016","Landry, Steven; Tascarella, David; Jeon, Myounghoon; FakhrHosseini, S. Maryam","Listen To Your Drive: Sonification Architecture and Strategies for Driver State and Performance","","","","","http://hdl.handle.net/1853/56591","Driving is mainly a visual task, leaving other sensory channels open for additional information communication. As the level of automation increases in vehicles, monitoring the state and performance of the driver and vehicle shifts from the secondary to primary task. Auditory channels provide the flexibility to display a wide variety of information to the driver without increasing the workload of driving task. It is important to identify types of auditory displays and sonification strategies that provide integral information necessary for the driving task, and not overload the driver with unnecessary or intrusive data. To this end, we have developed an in-vehicle interactive sonification system using the medium-fidelity simulator and neurophysiological devices. The system is intended to integrate driving performance data and driver affective state data in real-time. The present paper introduces the architecture of our in-vehicle interactive sonification system and potential sonification strategies for providing feedback to the driver in an intuitive and non-intrusive manner.","2016-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","Listen To Your Drive","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9XLCBZD5","journalArticle","2013","Joliat, Nicholas; Mayton, Brian; Paradiso, Joseph A.","Spatialized anonymous audio for browsing sensor networks via virtual worlds","","","","","http://hdl.handle.net/1853/51643","We explore new ways to communicate sensor data by combining spatialized sonification with animated data visualization in a 3D virtual environment. A system is designed and implemented that implies a sense of anonymized presence in an instrumented building by manipulating navigable live and recorded spatial audio streams. Exploration of both real-time and archived data is enabled. In particular, algorithms for obfuscating audio to protect privacy and for time-compressing audio to allow exploration on diverse time scales are implemented. Synthesized sonification of diverse, distributed sensor data in this context is also supported within our framework.","2013-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3PAPK337","journalArticle","2003","Brown, Lorna M.; Brewster, Stephen A.; Ramloll, Ramesh; Burton, Mike; Riedel, Beate","Design guidelines for audio representation of graphs and tables","","","","","http://hdl.handle.net/1853/50454","Audio can be used to make visualisations accessible to blind and visually impaired people. The MultiVis Project has carried out research into suitable methods for presenting graphs and tables to blind people through the use of both speech and non-speech audio. This paper presents guidelines extracted from this research. These guidelines will enable designers to implement visualisation systems for blind and visually impaired users, and will provide a framework for researchers wishing to investigate the audio presentation of more complex visualisations.","2003-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8QCH483K","journalArticle","2016","Jang, Daeyoung; Yoo, Jae-hyoun; Lee, Tae Jin","Implementation and Evaluation of 10.2 channel Microphone for UHDTV Audio","","","","","http://hdl.handle.net/1853/56593","As broadcasting environments change rapidly to digital, user requirements for next-generation services that surpass the current HDTV service quality become more demanding. The next-generation of broadcasting services will change from HD to UHD and from 5.1 channel audio to more than 10 audio channels, including a height channel for a high quality realistic broadcasting service. In accordance with the estimated trends of future broadcasting services, we propose a 10.2 channel audio format for a Korean UHDTV broadcasting service. It can create almost similar spatial sound images as 22.2 channel audio with half the number of speakers. In this paper, we propose a 10.2 channel audio acquisition system for the creation of UHDTV content, and measurements and preliminary evaluation are carried out to determine whether the performance is acceptable for broadcasting.","2016-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5I6N2MIW","journalArticle","2005","Anderson, Janet","Creating an empirical framework for sonification design","","","","","http://hdl.handle.net/1853/50102","Sonification research and design is held back by a lack of empirical evidence on which to base design decisions. The purpose of this paper is to identify the crucial decisions that need to be made at each stage of the sonification design process and assess what research is required to fill the gaps in the empirical literature. Crucial research questions are identified with the aim of building a framework to guide the decision process.","2005-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DTDEY3ZP","journalArticle","2002","Brazil, E.; Fernstroem, M.; Tzanetakis, G.; Cook, P.","Enhancing sonic browsing using audio information retrieval","","","","","http://hdl.handle.net/1853/51352","Collections of sound and music of increasing size and diversity are used both by typical computer users and multimedia designers. Browsing audio collections poses several challenges to the design of effective user interfaces. Recent techniques in audio information retrieval allow the automatic extraction of audio content information. This information can be used to inform and enhance audio browsing tools. In this paper we describe how audio information retrieval can be utilized to create novel user interfaces for browsing of audio collections. More specifically we report on recent work on two system prototypes: the Sonic Browser and Marsyas and our current work on merging the two systems in a common flexible system.","2002-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GR6EE8AI","journalArticle","2006","McGregor, I.; Leplatre, G.; Crerar, A.; Benyon, D.","Sound and soundscape classification: establishing key auditory dimensions and their relative importance","","","","","http://hdl.handle.net/1853/50637","This paper investigates soundscape classification by using two different forms of data gathering and two different populations. The first method involves a questionnaire completed by 75 audio professionals. The second uses a speak-aloud experiment, during which 40 end users were asked to describe their audio environment. While both approaches are different and target a different audience, they provide an indication of key dimensions for the perception of soundscapes and their relative importance. Contrasts and similarities between the results of the questionnaire and speak-alouds are highlighted. Their implications with regards to the establishment of a set of common terms in order to aid future auditory designs are also discussed.","2006-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","Sound and soundscape classification","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EAHIGEMP","journalArticle","2007","Murphy, Emma; Kuber, Ravi; Strain, Philip; McAllister, Graham; Yu, Wai","Developing Sounds for a Multimodal Interface: Conveying Spatial Information to Visually Impaired Web Users","","","","","http://hdl.handle.net/1853/49991","A multimodal browser plug-in, with audio and haptic feedback, has been developed to explore how basic concepts in spatial navigation can be conveyed to web users with visual impairments. In this paper, the second version of an audio interface for this plug-in is described in terms of its development and integration with haptic feedback. This version of the system was evaluated within a collaborative setting, in order to investigate whether it is possible to use this tool in a working environment between visually impaired and sighted Internet users. The auditory interface is discussed based on user feedback from this evaluation, and future sound design plans are presented in relation to the new direction for the overall system.","2007-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","Developing Sounds for a Multimodal Interface","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P5ER6BYU","journalArticle","2021","Jeon, Myounghoon; Nadri, Chihab; Zhang, Yiqi","Introduction of a computational modelling approach to auditory display research: Case studies using the QN-MHP framework","","","","","http://hdl.handle.net/1853/66325","For more than two decades, a myriad of design and research methods have been proposed in the ICAD community. Neurological methods have been presented since the inception of ICAD, and psychological human-subjects research has become as a legitimate approach to auditory display design and evaluation. However, little research has been conducted on modelling approaches to formalize human behavior in response to auditory displays. To bridge this gap, the present paper introduces computational modelling in auditory displays using the Queuing Network- Model Human Processor (QN-MHP) framework. After delineating the advantages of computational modelling and the QN-MHP framework, the paper introduces four case studies, which modelled drivers' behavior in response to invehicle auditory warnings, followed by the implications and future work. We hope that this paper can spark lively discussions on computational modelling in the ICAD community and thus, more researchers can benefit from using this method for future research.","2021-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","Introduction of a computational modelling approach to auditory display research","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KEZXKSIW","journalArticle","2003","Cabrera, Densil; Tilley, Steven","Parameters for auditory display of height and size","","","","","http://hdl.handle.net/1853/50478","The vertical localization, and vertical and horizontal spread, of auditory images was investigated using a vertical 5- loudspeaker array, with the noise-band signal spectrum, loudness level, number of active loudspeakers, and center loudspeaker position as parameters. The signal spectrum and center loudspeaker position affected the image vertical center. Loudness level, and secondarily the signal spectrum, affected the auditory image size. These results, which are consistent with, and extend, previous studies into auditory volume and associations between stimulus frequency and vertical localization, suggest a robust basis for the auditory display of height and size.","2003-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GXNDKC7Z","book","2010","Barrass, Stephen; Schertenleib, Anton","A Social Platform for Information Sonification: Many-Ears.com","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49860","In this paper we describe the Many Ears project that will develop the first example of a social site for a community of practice in data sonification. This site will be modeled on the Many Eyes site for “shared visualization and discovery” that combines facilities of a social site with online tools for graphing data. Anyone can upload a dataset, describe it and make it available for others to visualize or download. The ease of use of the tools and the social features on Many Eyes have attracted a broad general audience who have produced unexpected political, recreational, cultural and spiritual applications that differ markedly from conventional data analysis. The Many Ears project seeks to find out what will happen when data sonification is made more available as a mass medium? What new audiences will listen to sonifications? Who will create sonifications and for whom? What unexpected purposes will sonification be put to?","2010-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","A Social Platform for Information Sonification","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ITBU2PZV","journalArticle","2014","Boren, Braxton; Musick, Michael; Grossman, Jennifer; Roginska, Agnieszka","I Hear NY4D: Hybrid Acoustic and Augmented Auditory Display for Urban Soundscapes","","","","","http://hdl.handle.net/1853/52082","This project, I Hear NY4D, presents a modular auditory display platform for layering recorded sound and sonified data into an immersive environment. Our specific use of the platform layers Ambisonic recordings of New York City and a palette of virtual sound events that correspond to various static and realtime data feeds based on the listener’s location. This creates a virtual listening environment modeled on an augmented reality stream of sonified data in an existing acoustic soundscape, allowing for closer study of the interaction between real and virtual sound events and testing the limits of auditory comprehension.","2014-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","I Hear NY4D","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LKJXSCLB","journalArticle","2008","Palladino, Dianne K.; Walker, Bruce N.","Efficiency of Spearcon-Enhanced Navigation of One Dimensional Electronic Menus","","","","","http://hdl.handle.net/1853/49908","This study investigated navigation through a cell phone menu in the presence of auditory cues (text-to-speech and spearcons), visual cues, or both. A total of 127 undergraduates navigated through a 50-item alphabetically listed menu to find a target name. Participants using visual cues (either alone or combined with auditory cues) responded faster than those using only auditory cues. Performance was not found to be significantly different among the two auditory only conditions. Although not significant, when combined with visual cues, spearcons improved navigational efficiency more than both text-to- speech cues and menus using no sound, and provided evidence for the ability of sound to enhance visual menus. Research results provide evidence applicable to efficient auditory menu creation.","2008-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UMI4W935","journalArticle","2001","Mellody, Maureen; Wakefield, Gregory H.","A tutorial example of stimulus sample discrimination in perceptual evaluation of synthesized sounds: discrimination between original and re-synthesized singing","","","","","http://hdl.handle.net/1853/50513","Stimulus sample discrimination (SSD) is an objective psychophysical procedure, in which samples are drawn from various signal distributions for comparison and an index of discrimination is measured. A key feature of SSD is the use of samples from a context distribution, which act either as additional or as distracting sources of information with respect to the discrimination task. When the context distribution provides information about the natural variations in the sounds from a musical instrument, SSD may prove useful as a measure of the perceptual accuracy of a sound synthesis algorithm. We report on results from a study in which SSD is applied to measure the degree to which singer identity is preserved in loworder synthesis of the female singers.","2001-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","A tutorial example of stimulus sample discrimination in perceptual evaluation of synthesized sounds","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CIC9EESN","journalArticle","2021","Elmquist, Elias; Ejdbo, Malin; Bock, Alexander; Rönnberg, Niklas","OpenSpace sonification: complementing visualization of the solar system with sound","","","","","http://hdl.handle.net/1853/66324","Data visualization software is commonly used to explore outer space in a planetarium environment, where the visuals of the software is typically accompanied with a narrator and supplementary background music. By letting sound take a bigger role in these kinds of presentations, a more informative and immersive experience can be achieved. The aim of the present study was to explore how sonification can be used as a complement to the visualization software OpenSpace to convey information about the Solar System, as well as increasing the perceived immersiveness for the audience in a planetarium environment. This was investigated by implementing a sonification that conveyed planetary properties, such as the size and orbital period of a planet, by mapping this data to sonification parameters. With a user-centered approach, the sonification was designed iteratively and evaluated in both an online and planetarium environment. The results of the evaluations show that the participants found the sonification informative and interesting, which suggest that sonification can be beneficially used as a complement to visualization in a planetarium environment.","2021-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","OpenSpace sonification","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3BBATZZ4","journalArticle","2007","Giordano, Bruno L.; McAdams, Stephen; McDonnell, John","Acoustical and Conceptual Information for the Perception of Animate and Inanimate Sound Sources","","","","","http://hdl.handle.net/1853/49975","Is the sound of a train whistling more similar to the sound of the wheels of a train or to the sound of your whistle? This question addresses the comparative relevance of acoustical and conceptual information to the perceived similarity of sound events. The answer to this question has theoretical and methodological consequences for the field of sound source perception, and for the behaviorally informed synthesis of environmental sounds. Hierarchical sorting was used to collect measures of the similarity of large sets of animate or inanimate sounds in naive listeners. Results were compared with those from two other conditions based on the same data–collection technique. Conceptual similarity was measured by presenting the sound source identification labels (written words) collected during a free–identification experiment. Acoustical similarity was measured on heard sounds, after participants received a training meant to minimize the effects of conceptual information on sorting. Acoustical similarity was only weakly correlated with conceptual similarity, proving the effectiveness of the training methodology in the acoustical condition. Also, naive listeners focused on conceptual and acoustical information when judging the similarity of animate and inanimate sound events, respectively. Theoretical and methodological consequences of these results are discussed.","2007-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DG5RIQ5A","book","2010","Barri, Tarik; Snook, Kelly","Versum: Data Sonification and Visualization in 3D","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/50067","Versum is an advanced, interactive 3D audiovisual composition environment which is augmented with a hardware and software front-end system that maps data into the environment for the purposes of exploratory scientific analysis. Originally intended as an audiovisual sequencer for real-time or automatable music and video performance, Versum also provides a unique environment for systematically investigating new data mappings for optimized human cognition of complex datasets.","2010-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","Versum","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7HUU6KCF","journalArticle","2011","Winters, R. Michael; Blaikie, Andrew; O'neil, Deva","Simulating the Electroweak Phase Transition: Sonification of Bubble Nucleation","","","","","http://hdl.handle.net/1853/51744","As an applicaton of sonification, a simulation of the early uni- verse was developed to portray the phase transition that occurred shortly after the Big Bang. The Standard Model of particle physics postulates that a hypothetical particle, the Higgs boson, is respon- sible for the breaking of the symmetry between the electromag- netic force and the weak force. This phase transition may have been responsible for triggering Baryogenesis, the generation of an abundance of matter over anti-matter. This hypothesis is known as Electroweak Baryogenesis. In this simulation, aspects of bub- ble nucleation in Standard Model Electroweak Baryogenesis were examined and modeled using Mathematica, and sonified using Su- perCollider3. The resulting simulation, which has been used for pedagogical purposes by one of the authors, suggests interesting possibilities for the integration of science and aesthetics as well as auditory perception. The sonification component in particular also had the unexpected benefit of being useful in debugging the Mathematica code.","2011-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","Simulating the Electroweak Phase Transition","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9467SAYC","journalArticle","2004","Leplaitre, G.; McGregor, I.","How to tackle auditory interface aesthetics? Discussion and case study","","","","","http://hdl.handle.net/1853/50858","This paper discusses the importance of auditory interface aesthetics and presents an empirical investigation of sound aesthetics in context. The theoretical discussion examines the relationship between sound aesthetics and user satisfaction and concludes that, despite the creation of numerous auditory design methods and guidelines, none are dedicated to achieving aesthetically pleasing designs. In a case study, an empirical investigation is conducted to evaluate the relationship between the functional and aesthetic value of an auditory interface. By investigating two different tasks, this study demonstrates that the nature of the tasks allocated to subjects has a significant impact on the aesthetic judgments made by the subjects. Consequently, functional and aesthetic properties of auditory cannot be dealt with independently.","2004-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","How to tackle auditory interface aesthetics?","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KZBVRXK7","journalArticle","2017","Alirezaee, Parisa; Girgis, Roger; Kim, TaeYong; Schlesinger, Joseph J.; Cooperstock, Jeremy R.","Did You Feel That? Developing Novel Multimodal Alarms for High Consequence Clinical Environments","","","","","http://hdl.handle.net/1853/58377","Hospitals are overwhelmingly filled with sounds produced by alarms and patient monitoring devices. Consequently, these sounds create a fatiguing and stressful environment for both patients and clinicians. As an attempt to attenuate the auditory sensory overload, we propose the use of a multimodal alarm system in operating rooms and intensive care units. Specifically, the system would utilize multisensory integration of the haptic and auditory channels. We hypothesize that combining these two channels in a synchronized fashion, the auditory threshold of perception of participants will be lowered, thus allowing for an overall reduction of volume in hospitals. The results obtained from pilot testing support this hypothesis. We conclude that further investigation of this method can prove useful in reducing the sound exposure level in hospitals as well as personalizing the perception and type of the alarm for clinicians.","2017-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","Did You Feel That?","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7TQ2T9SW","journalArticle","2016","Connors, Teresa Marie","The Aesthetics Of Causality: A Descriptive Account Into Ecological Performativity","","","","","http://hdl.handle.net/1853/56566","In this paper, I offer a perspective into a creative research practice I have come to term as Ecological Performativity. This practice has evolved from a number of non-linear audiovisual installations that are intrinsically linked to geographical and everyday phenomena. The project is situated in ecological discourse that seeks to explore conditions and methods of co-creative processes derived from an intensive data-gathering procedure and immersion within the respective environments. Through research the techniques explored include computer vision, data sonification, live convolution and improvisation as a means to engage the agency of material and thus construct non-linear audiovisual installations. To contextualize this research, I have recently reoriented my practice within recent critical, theoretical, and philosophical discourses emerging in the humanities, sciences and social sciences generally referred to as 'the nonhuman turn'. These trends currently provide a reassessment of the assumptions that have defined our understanding of the geo-conjunctures that make up life on earth and, as such, challenge the long-standing narrative of human exceptionalism. It is out of this reorientation that the practice of Ecological Performativity has evolved.","2016-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","The Aesthetics Of Causality","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NC3B8QCY","book","2010","Méndez, del Valle; García, Jorge","Road, River and Rail: A Multi-Interactive Composition","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49906","road, river and rail is not only a musical composition in the usual sense. It is also a sort of multi-interaction on different levels between real and virtual life. These interaction-levels comprise, not only a performance-interaction, but also an interactive creation by the composition as well as by the interpretation","2010-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","Road, River and Rail","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IFV3YAWB","journalArticle","2004","Gossmann, J.","Connecting virtual sound and physical space in audio-augmented environments","","","","","http://hdl.handle.net/1853/50833","This paper describes strategies to create multimodal coherences between sound and space in Audio-Augmented Environments. Visitors to AAEs wear motion-tracked wireless headphones displaying a location-aware, adaptive audio presentation augmenting the physical space with virtual sound-scenes. The work underlying this paper was done by the author during the course of the LISTEN project at Fraunhofer IMK[1][2]. In the first half, the concept of the `perceptual device' is introduced, which allows attaching audio content to a physical space. In the second part this is supported by practical examples from the publicly accessible AAE “Macke Labor”. The “Macke Labor” was installed at the Kunstmuseum Bonn, opening in October 15th 2003. It was developed by the author with an interdisciplinary team of collaborators. The paper is rounded off with the description of the sonification technique used in the evaluation of the recorded tracking and event data of the “Macke Labor”.","2004-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HGFJSQ3L","book","2015","Kuppanda, Thimmaiah; Degara, Norberto; Worrall, David; Thoshkahna, Balaji; Müller, Meinard","Virtual reality platform for sonification evaluation","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54116","In this paper we propose a game-based virtual reality platform for evaluation of sonification techniques. We study the task of localization of stationary objects in virtual reality using auditory cues. We further explore sonification techniques and compare the performance in this task using the proposed platform. The virtual reality environment is developed using Unity3D (game engine) and an Oculus Rift, a head mounted virtual reality display. Parameter mapping sonification techniques are employed to map the position of the object in virtual space to sound. Hence, the framework defined here constitutes an auditory virtual reality environment. This auditory display interface is subjectively evaluated in stationary object localization task. A statistical analysis of the subjective and objective measures of the listening test is performed resulting in a robust and scientific evaluation of the sonification methods.","2015-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TIAAFRXL","journalArticle","2003","Lemaitre, Guillaume; Susini, Patrick; Winsberg, Suzanne; McAdams, Stephen","Perceptively based design of new car horn sounds","","","","","http://hdl.handle.net/1853/50480","Due to the technologies used, there exist only a few different kinds of car horn sounds. We propose a method for the design of new sounds, based on a perceptual study of the actual sounds of car horns. Firstly we deal with recordings of existing car horns. We show that the different kinds of horn sounds can be divided into nine main families. Within these families we demonstrate secondly that the perception of timbre results from the integration of three elementary sensations. Thirdly, another experiment reveals that some sounds are better identified as car horns than others. A relationship between the perceived timbre of the sounds and their ability to be identified as car horns is established. Finally, we generalize our results to synthesized sounds. The synthesis method was designed to explore and enlarge the perceptual space. Studying these sounds confirms and generalizes the previous results. A model is proposed, which is able to synthesize sounds and predict their ability to be identified as car horns.","2003-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TCF5XS3D","journalArticle","2019","R. Michael, Winters; Kalra, Ankur; Walker, Bruce N.","Hearing artificial intelligence: Sonification guidelines & results from a case-study in melanoma diagnosis","","","","","http://hdl.handle.net/1853/61501","The applications of artificial intelligence are becoming more and more prevalent in everyday life. Although many AI systems can operate autonomously, their goal is often assisting humans. Knowledge from the AI system must somehow be perceptualized. Towards this goal, we present a case-study in the application of data-driven non-speech audio for melanoma diagnosis. A physician photographs a suspicious skin lesion, triggering a sonification of the system's penultimate classification layer. We iterated on sonification strategies and coalesced around designs representing three general approaches. We tested each in a group of novice listeners (n=7) for mean sensitivity, specificity, and learning effects. The mean accuracy was greatest for a simple model, but a trained dermatologist preferred a perceptually compressed model of the full classification layer. We discovered that training the AI on sonifications from this model improved accuracy further. We argue for perceptual compression as a general technique and for a comprehensible number of simultaneous streams.","2019-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","Hearing artificial intelligence","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QJMW58NU","journalArticle","2016","Sterkenburg, Jason; Landry, Steven; Jeon, Myounghoon; Johnson, Joshua","Towards An In-Vehicle Sonically-Enhanced Gesture Control Interface: A Pilot Study","","","","","http://hdl.handle.net/1853/56564","A pilot study was conducted to explore the potential of sonically-enhanced gestures as controls for future in-vehicle information systems (IVIS). Four concept menu systems were developed using a LEAP Motion and Pure Data: (1) 2x2 with auditory feedback, (2) 2x2 without auditory feedback, (3) 4x4 with auditory feedback, and (4) 4x4 without auditory feedback. Seven participants drove in a simulator while completing simple target-acquisition tasks using each of the four prototype systems. Driving performance and eye glance behavior were collected as well as subjective ratings of workload and system preference. Results from driving performance and eye tracking measures strongly indicate that the 2x2 grids yield better driving safety outcomes than 4x4 grids. Subjective ratings show similar patterns for driver workload and preferences. Auditory feedback led to similar improvements in driving performance and eye glance behavior as well as subjective ratings of workload and preference, compared to visual-only.","2016-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","Towards An In-Vehicle Sonically-Enhanced Gesture Control Interface","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CDFLMA5A","journalArticle","2002","Morimoto, M.","The relation between spatial impression and the precedence effect","","","","","http://hdl.handle.net/1853/51390","This paper reviews results of listening tests on auditory spatial impression (ASI) that describe the relation between individual characteristics of spatial impression and the precedence effect. ASI is a general concept defined as the spatial extent of the sound image, and is comprised at least two components. One is auditory source width (ASW), defined as the width of a sound image fused temporally and spatially with the direct (preceding) sound image; the other is listener envelopment (LEV), defined as the degree of fullness of the sound image surrounding the listener, and which excludes the direct sound image for which ASW is judged. Listeners can perceive separately these two components of ASI, and their subjective reports demonstrate that they can distinguish between them. The perception of ASW and LEV has close connection with The precedence effect (the law of the first wave front). Acoustic signal components that arrive within the time and amplitude limits of the effect contribute to ASW, and those beyond the upper limits contribute to LEV. It is possible to control ASW and LEV independently by controlling physical factors that influence each of the components. It is well-known, for example, that the degree of interaural cross-correlation (ICC) is an important physical factor in the control of ASI. ASW can be predicted from ICC (and thereby controlled by the manipulation of ICC) regardless of the number and directions of arrival of sound sources. But measurements of ICC within 1/3-octave bands are preferred for estimating ASW, whereas the use of wide band and 1-octave band signals, as described in the ISO standard, are not. On the other hand, LEV cannot be controlled only through manipulation of ICC, as LEV is also affected by the spatial distribution of sounds (e.g., front/back energy ratio).","2002-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N45FLQKJ","journalArticle","2001","Fernstrom, Mikael; Brazil, Eoin","Sonic browsing: An auditory tool for multimedia asset management","","","","","http://hdl.handle.net/1853/50644","In previous work, the Sonic Browser was used for browsing large data sets of music [1]. In this paper, we report results from an updated version of the Sonic Browser for managing general sound resources on personal computers. In particular, we have evaluated browsing of everyday sounds. The investigation was directed at comparing browsing single versus multiple stream audio. The problem of sound resource browsing for multimedia designers is the specific area of focus for our experiment. Finally, we conclude with current trends of our research for further improvement of the system.","2001-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","Sonic browsing","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4P97ZHNV","journalArticle","2002","McGregor, I.; Crerar, A.; Benyon, D.; Macaulay, C.","Soundfields and soundscapes: Reifying auditory communities","","","","","http://hdl.handle.net/1853/51380","This paper reports progress towards mapping workplace soundscapes. In order to design auditory interfaces that integrate effectively with workplace environments, we need a detailed understanding of the way in which end users inhabit these environments, and in particular, how they interact with the existing auditory environment. Our work concentrates first on mapping the physical soundfield, then overlaying this with a representation of the soundscape as experienced by its active participants. The ultimate aim of this work is to develop an interactive soundscape-mapping tool, analogous to the modeling tools available to architects. Such a tool would be of use to designers of physical, augmented and virtual environments and usable without professional musical or acoustical expertise.","2002-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","Soundfields and soundscapes","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U354FCZY","journalArticle","2006","Roginska, A.; Childs, E.; Johnson, M. K.","Monitoring real-time data: a sonification approach","","","","","http://hdl.handle.net/1853/50601","This paper describes an approach to sonifying and displaying remotely sensed data. A representative sample dataset was audified using synthetic sounds, and sonified using orchestral instruments. The resulting 14 data streams were streamed in real-time and rendered using 3 display methods. Audio spatialization using HRTF processing is compared with stereo and monophonic display. Listening tests show a marked preference for the sonified data processed using HRTFs.","2006-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","Monitoring real-time data","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BKD6N5TI","journalArticle","2016","Gionfrida, Letizia; Roginska, Agnieszka; Keary, James; Mohanraj, Hariharan; Friedman, Kent P.","The Triple Tone Sonification Method to Enhance the Diagnosis of Alzheimer’s Dementia","","","","","http://hdl.handle.net/1853/56570","For the current diagnosis of Alzheimer's dementia (AD), physicians and neuroscientists primarily call upon visual and statistical analysis methods of large, multi-dimensional positron emission tomography (PET) brain scan data sets. As these data sets are complex in nature, the assessment of disease severity proves challenging, and is susceptible to cognitive and perceptual errors causing intra and inter-reader variability among doctors. The Triple-Tone Sonification method, first presented and evaluated by Roginska et al., invites an audible element to the diagnosis process, offering doctors another tool to gain certainly and clarification of disease stages. Audible beating patterns resulting from three interacting frequencies extracted from PET brain scan data, the Triple-Tone method underwent a second round of subjective listening test and evaluation, this time on radiologists from NYU Langone Medical Center. Results show the method is effective at evaluation PET scan brain data.","2016-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"26PX5S62","journalArticle","2008","Langlois, Sabine; Suied, Clara; Lagear, Thierry; Charbonneau, Aude","Cross Cultural Study of Auditory Warnings","","","","","http://hdl.handle.net/1853/49889","Auditory human machine interfaces (HMI) are used in cars to provide the driver with information. For security reasons, sound design should respect information urgency scaling. In view of a continuous increase of Renault's sales worldwide, getting a better understanding of urgency perception of sounds is fundamental. Scientific knowledge is not very widespread about possible cross-cultural differences for auditory alarms, but seems to indicate that people from different countries would agree more on urgency perception of abstract sounds than environmental sounds. The aim of this study is to specify which acoustical parameters influence urgency perception of sounds, worldwide., A experiment was conducted in six countries, representative of Renault's customers: France, Germany, Great Britain, Turkey, Korea and the USA. Sixteen sounds were designed according to different acoustical parameters (frequency, timbre and onset of the pulses), and split into two sets depending on their tempo: one set at a fast tempo, one at a slower tempo. The results are very similar in the six countries. The auditory HMI are perceived as urgent when the frequency is high, at both tempi. At a fast tempo, a short attack time increases urgency perception. Abstract auditory HMI following frequency and onset guidelines should be perceived worldwide similarly along an urgency scale. These recommendations have been applied to design auditory HMI to be sounded by the instrument panel of vehicles recently released.","2008-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J38CXJHK","journalArticle","2007","Palladino, Dianne K.; Walker, Bruce N.","Learning Rates for Auditory Menus Enhanced with Spearcons Versus Earcons","","","","","http://hdl.handle.net/1853/50011","Increasing the usability of menus on small electronic devices is essential due to their increasing proliferation and decreasing physical sizes in the marketplace. Auditory menus are being studied as an enhancement to the menus on these devices. This study compared the learning rates for earcons (hierarchical representations of menu locations using musical tones) and spearcons (compressed speech) as potential candidates for auditory menu enhancement. We found that spearcons outperformed earcons significantly in rate of learning. We also found evidence that spearcon comprehension was enhanced by a brief training cycle, and that participants considered the process of learning spearcons much easier than the same process using earcons. Since the efficiency of learning and the perceived ease of use of auditory menus will increase the likelihood they are embraced by those who need them, this paper presents compelling evidence that spearcons may be the superior choice for such applications.","2007-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6MGJNXES","journalArticle","2005","Vickers, Paul","Whither and wherefore the auditory graph? Abstractions & aesthetics in auditory and sonified graphsh","","","","","http://hdl.handle.net/1853/50199","A good deal of attention has been paid by the auditory display community to the sonification of graphical data and the term au- ditory graph has been used to describe this class of auditory map- pings. We contend that definitions have become blurred leading to first-order sonifications of functions and data being treated as synonymous with the second- and higher-order mappings obtained when graphs of those functions and data are themselves sonified. This paper looks at the different types of sonifications currently known collectively as auditory graphs and, based on this analysis, proposes a purposeful distinction to be drawn between auditory graphs and sonified graphs. An example is taken from the domain of computer programming to further illustrate the argument.","2005-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","Whither and wherefore the auditory graph?","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"88RBKERF","journalArticle","2022","Seznec, Yann; Pauletto, Sandra","Towards a workshop methodology for involving non-expert stakeholders in the interactive sound design process: Connecting household sounds and energy consumption data","","","","","http://hdl.handle.net/1853/67381","Sonic interaction design and sonification have the potential to provide new ways to display and interpret data and information. Data from a number of domains have been sonified: astronomy, finance, health, security, and many more. However in recent years, research in auditory displays has highlighted the importance of using participatory methods to include stakeholders, often users who are not experts in sound, in the design loop. This raises the question of how to discuss sound with participants who may not be familiar with it, and how to discuss links and relationships between sound and the specific domain which is the focus of the design. In this paper we propose a methodology for a participatory workshop with stakeholders that could be applied to a variety of domains. We describe how we have deployed this methodology in a workshop that aimed to explore attitudes to both sound and energy usage in the home environment, and discuss what can be gained from such an approach.","2022-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","Towards a workshop methodology for involving non-expert stakeholders in the interactive sound design process","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A7LNFGW7","journalArticle","2001","van den Doel, Kees; Pai, Dinesh K.","JASS: A java audio synthesis system for programmers","","","","","http://hdl.handle.net/1853/50622","We describe a unit generator based audio synthesis programming environment written in pure Java. The environment is based on a foundation structure consisting of a small number of Java interfaces and abstract classes, and a potentially unlimited number of unit generators, which are created by extending the abstract classes and implementing a single method. Filter-graphs, sometimes called “patches”, are created by linking together unit generators in arbitrary complex graph structures. Patches can be rendered in real-time with special unit generators that communicate with the audio hardware, which we have implemented using the JavaSound API.","2001-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","JASS","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TAZYHYGI","journalArticle","2022","Weger, Marian; Hermann, Thomas; Höldrich, Robert","AltAR/table: A Platform for Plausible Auditory Augmentation","","","","","http://hdl.handle.net/1853/67378","Auditory feedback from everyday interactions can be augmented to project digital information in the physical world. For that purpose, auditory augmentation modulates irrelevant aspects of already existing sounds while at the same time preserving relevant ones. A strategy for maintaining a certain level of plausibility is to metaphorically modulate the physical object itself. By mapping information to physical parameters instead of arbitrary sound parameters, it is assumed that even untrained users can draw on prior knowledge. Here we present AltAR/table, a hard- and software platform for plausible auditory augmentation of flat surfaces. It renders accurate augmentations of rectangular plates by capturing the structure-borne sound, feeding it through a physical sound model, and playing it back through the same object in real time. The implementation solves basic problems of equalization, active feedback control, spatialization, hand tracking, and low-latency signal processing. AltAR/table provides the technical foundations of object-centered auditory augmentations, for embedding sonifications into everyday objects such as tables, walls, or floors.","2022-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","AltAR/table","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6II7DC9I","journalArticle","1997","Savioja, Lauri; Huopaniemi, Jyri; Lokki, Tapio; Vaananen, Rutta","Virtual environment simulation - advances in the DIVA project","","","","","http://hdl.handle.net/1853/50757","At ICAD'96, a real-time virtual audio reality model was presented, which included model-based sound synthesizers, geometric room acoustics modeling, binaural auralization for headphone and loudspeaker listening, and high-quality animation. The DIVA environment is an integrated implementation of a virtual reality system currently aiming at a virtual symphony orchestra performance. In the current version of the software, multiple sound sources (physical models of musical instruments) are conducted by a virtual conductor (controlled by a position tracker with 3 transmitters). The real-time calculation of auralization has been enhanced by accurate HRTF approximations, a new late reverberation model, and by an efficient image source method.","1997-11","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MDEDGZQ4","journalArticle","2019","Emsley, Iain","Exploring the interface effect in distant sonification","","","","","http://hdl.handle.net/1853/61534","I introduce ongoing research into the method that I am calling distant sonification as a response to understanding abstractions created through computational reading. My aim is to explore the interface effect and situate it in sonification and media theory. Discussing existing prototypes, I contextualise the visible interfaces within the wider design models, such as patterns, and computational materiality. Reflecting on experiments in media specific analysis, I suggest that there are different models with their own specificities that are brought together to create the interface. They might exist separately or are combined to create a wider effect that I explore through models and grammars. I suggest that there are different models with their own specificities that are brought together by humans and machines through layers.","2019-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JYF9XSVP","journalArticle","2001","Bonebright, Terri L.; Nees, Mike A.; Connerley, Tayla T.; McCain, Glenn R.","Testing the effectiveness of sonified graphs for education: A programmatic research project","","","","","http://hdl.handle.net/1853/50654","This programmatic research project builds on results from research on data sonification and from studies investigating comprehension of visual graphs. The purpose of the project is to explore the effectiveness of using sonified graphs of real data sets from disciplines to which students are exposed during academic courses. The primary question is whether sonified graphs can increase the comprehension of graphed data for students. The secondary question is whether stereo or monaural sonifications are most effective for graph comprehension. The third and final question of this project is whether sonified graphs with rhythm markers result in better comprehension than sonified graphs without them. The project consists of three laboratory experiments that explore whether students can match auditory representations with the correct visual graphs, whether they can comprehend graphed data sets more effectively by adding sonified components, and whether they can be trained to use sonified graphs better with practice. Results could provide new methods for teaching students with different learning styles quantitative skills in educational settings from kindergarten through college. They could also be extended to assist in teaching students with visual impairments about graphed data sets.","2001-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","Testing the effectiveness of sonified graphs for education","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B7F7TZFP","journalArticle","2006","Talbot, M.; Cowan, B.","Trajectory capture in frontal plane geometry for visually impaired","","","","","http://hdl.handle.net/1853/50693","Users who are blind, or whose visual attention is otherwise occu- pied, can benefit from an auditory representation of their imme- diate environment. To create it a video camera senses the envi- ronment, which is converted into synthetic audio streams that rep- resent objects. What aspects of the audio signal best encode this information? This paper compares four encodings that allow users to perceive the simultaneous motion of several objects. The comparisons are experimental: subjects hear trajectories of objects moving in a virtual 2D plane, encoded as audio streams with complex frequency spectra, and identify the represented mo- tions. One encoding uses panning for horizontal motion and pitch for vertical motion (the Pratt effect). A second uses best-fit head related transfer functions (HRTFs) to localize stream positions. The third combines the first two, using pitch to redundantly code elevation in a HRTF presentation. Finally, the fourth enhances the third, using best-fit HRTF to ‘vertically pan’ each audio stream at constant but unique elevations, for superior audio segregation. The fourth method outperforms the other three according to two measures, the accuracy of subjects’ perceptions, and the num- ber of replays needed to achieve those perceptions. With it sub- jects can perceive up to three different simultaneously-presented motions after minimal practice. The results show that the Pratt ef- fect is a more robust method than HRTF for representing vertical motion, and that, combined with the Pratt effect, vertical panning using a HRTF improves motion perception.","2006-06","2023-07-13 06:25:48","2023-07-21 08:31:43","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C4XNZT3A","journalArticle","2012","Bearman, Nick; Brown, Ethan","Who's sonifying data and how are they doing it? A comparison of ICAD and other venues since 2009","","","2168-5126","","http://hdl.handle.net/1853/44402","What disciplines are applying data sonification, and what synthesis tools are they using to make the sounds? These questions are basic to understanding the state of sonification today, but they are surprisingly difficult to answer. This short review attempts to fill this gap by distilling common patterns of data sonification research. We hope that this will complement other literature reviews and give potential and current sonification researchers a sense of what is happening in the ICAD community, show where there is room for new ventures, and where there is already a lot of active research to connect with. Additionally, we put ICAD in context of other academic publications.","2012-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","Who's sonifying data and how are they doing it?","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4CRE3E2U","journalArticle","2000","Larsson, Pontus; Kleiner, Mendel; Vastfjall, Daniel; Olsson, Conny; Dalenback, Bengt-Inge","Subjective testing of the performance of reverberation enhancement using virtual reality environmens","","","","","http://hdl.handle.net/1853/50680","Various systems for the purpose of performing subjective audiovisual tests have been evaluated. Auralizations and visualizations of two different halls in the Göteborg University School of Music have been made using CATT-Acoustic and VR-Creator/EON Studio. These simulations have been used in a subjective test for the purpose of evaluating the visual influence on room acoustical parameters, the realism and emotional parameters in a hall equipped with a reverberation enhancement system. The results show that depending on the room and type of stimuli, perceived room size, auditory source width and distance to sound source, are clearly influenced by the visual impression.","2000-04","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K9EEQJ3U","journalArticle","2008","Davison, Benjamin K.; Walker, Bruce N.","AudioPlusWidgets: Bringing Sound to Software Widgets and Interface Components","","","","","http://hdl.handle.net/1853/49873","Using sound as part of the user interface in a typical software application is still extremely rare, despite the technical capabilities of computers to support such usage. The ICAD community has developed several interface concepts, patterns, and toolkits, and yet the overall software scene has remained dominated by the visual-only user interface. AudioPlusWidgets is a software library offering scientifically grounded audio enhancements to the standard Java Swing API. Through metaphors and transparency, AudioPlusWidgets can be inserted into existing code with minimal changes, easily adding auditory capabilities to the interface components in the system. This library uses an event-based model and an audio manager to render speech, MIDI, and prerecorded sounds.","2008-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","AudioPlusWidgets","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QITJ8G6D","journalArticle","2022","Russo, Matt; Santaguida, Andrew","5000 Exoplanets: Listen to the Sounds of Discovery","","","","","http://hdl.handle.net/1853/67384","In March of 2022, NASA announced the discovery of the 5000th planet orbiting a star other than our sun (an exoplanet). We have created a sonification and visualization to celebrate this milestone and to communicate the exciting history of discovery to the general public. Our work provides a visceral experience of how humanity’s knowledge of alien worlds has progressed. A relatively simple and straightforward sonification mapping is used to make the informational content as accessible to the general public as possible. Listeners can see and hear the timing, number, and relative orbital periods of the exoplanets that have been discovered to date. The sonification was experienced millions of times through NASA’s social media channels and there are plans to update the sonification as future milestones are reached.","2022-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","5000 Exoplanets","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZMAW27U4","journalArticle","2014","Smith, Kevin M.; Claveau, David","The Sonification and Learning of Human Motion","","","","","http://hdl.handle.net/1853/52049","This paper examines how sonification can be used to help a student emulate the complex motion of a teacher with increasing spatial and temporal accuracy. The system captures a teacher’s motion in real-time and generates a 3-D motion path, which is recorded along with a reference sound. A student then attempts to perform the motion and thus recreate the teacher’s reference sound. The student’s synthesized sound will dynamically approach the teacher’s sound as the student’s movement becomes more accurate. Several types of sound mappings which simultaneously represent time and space deviations are explored. For the experimental platform, a novel system that uses low-cost camera-based motion capture hardware and open source software has been developed. This work can be applied to diverse areas such as rehabilitation and physiotherapy, performance arts and aiding the visually impaired.","2014-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WRHBX7QG","journalArticle","1996","LoPresti, Eric; Harris, W. Michelle","loudSPIRE, an auditory display schema for the SPIRE system","","","","","http://hdl.handle.net/1853/50808","SPIRE is a system for visualizing large amounts of document-based information. Auditory display was identified as a way to supplement SPIRE's existing visual interface; however, the diversity of data did not lend itself to a single sonification method. Researchers at the Pacific Northwest National Laboratory developed a schema that organizes the auditory display into layers so that multiple sonification methods can be used without sacrificing intuitiveness and usability. This paper describes the design motivations underlying the schema, gives examples of constituent data-to-sound mappings, and describes a prototype implementation named ""loudSPIRE."" Keywords: auditory display, information visualization, document analysis, sonification, multimedia","1996-11","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4NSRGWKX","journalArticle","2003","Fontana, Federico; Rocchesso, Davide","A physics-based approach to the presentation of acoustic depth","","","","","http://hdl.handle.net/1853/50432","A virtual listening environment providing localization cues is proposed for the reproduction of acoustic depth. By simulating the propagation of acoustic waves inside a tube it allows to change the source/listening point positions interactively, in a way that listeners experience various spatial configurations depending on the source/listener mutual position, and, correspondingly, perceive different auditory cues of depth. The quantitative relationship existing between physical and auditory distance assessments suggests to represent the tube using a model which allows direct control of the depth parameter. Simulations and experiments demonstrate the effectiveness of the model and its relative robustness in applications contexts where state of the art equipment and ideal listening conditions cannot be guaranteed.","2003-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4JH9UCXS","book","2015","Peres, S. Camille; Verona, Daniel; Ritchey, Paul","The effects of various parameter combinations in parameter-mapping sonifications","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54198","This study will be investigating the design of parametermapping sonifications and investigating how different combinations of sound parameter mappings affect the user’s ability to understand and interpret sEMG data. The parameter mappings being used are all redundantly mapped and the specific parameter combinations are 1) pitch and loudness, 2) pitch, loudness, and attack time, and 3) loudness and attack time. There will be both spatialized (right and left) and nonspatialized versions of each of these mappings. These mappings will be used to present sonifications of two channels of sEMG data to participants to explore if they can identify muscle activation order (which muscle activates first) and relative muscle exertion levels (which muscle has a higher exertion). It is expected that participants will perform better with the spatialized mappings. It is also expected that the participants will perform better with the mappings that include attack time because this results in greater timbral variety.","2015-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ABLEK3Q9","journalArticle","2002","Cohen, M.","A survey of emerging and exotic auditory interfaces","","","","","http://hdl.handle.net/1853/51333","Anticipating some emerging audio devices and features, this paper surveys trends in mobile telephony (especially regarding mobile internet in Japan), wearable/intimate multimedia computing, handheld/ nomadic/portable interfaces, and embedded systems like multimedia furniture and spatially immersive displays, gleaned from recent press releases and advertisements, popular media, and publications by industrial and academic laboratories, especially the author's own research group. Representative instances are cited, and conferencing narrowcasting selection functions are reviewed. Keywords: audio interaction, CVEs (collaborative virtual environments), embedded systems, handheld/mobile/portable interfaces, integration of mobile devices and telecommunication, mobile information device, mobile internet, multimodal interaction, novel user interfaces, pervasive Java, telematics, telerobotics, ubicomp (ubiquitous computing) (a.k.a. ambient, calm, pervasive) technology, wearable/intimate multimedia computing.","2002-07","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZT6E32Y9","journalArticle","2018","Uno, Shin’ichiro; Suzuki, Yasuo; Watanabe, Takashi; Matsumoto, Miku; Wang, Yan","Sound-based image and positon recognition system: SIPReS","","","","","http://hdl.handle.net/1853/60070","We developed software called SIPReS, which describes two-dimensional images with sound. With this system, visually-impaired people can tell the location of a certain point in an image just by hearing notes of frequency each assigned according to the brightness of the point a user touches on. It can run on Android smartphones and tablets. We conducted a small-scale experiment to see if a visually-impaired person can recognize images with SIPReS. In the experiment, the subject successfully recognized if there is an object or not. He also recognized the location information. The experiment suggests this application’s potential as image recognition software.","2018-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","Sound-based image and positon recognition system","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MWEZDD22","journalArticle","2021","Cádiz, Rodrigo F.; Droppelmann, Lothar; Guzmán, Max; Tejos, Cristian","Auditory graphs from denoising real images using fully symmetric convolutional neural networks","","","","","http://hdl.handle.net/1853/66334","Auditory graphs are a very useful way to deliver numerical information to visually impaired users. Several tools have been proposed for chart data sonification, including audible spreadsheets, custom interfaces, interactive tools and automatic models. In the case of the latter, most of these models are aimed towards the extraction of contextual information and not many solutions have been proposed for the generation of an auditory graph directly from the pixels of an image by the automatic extraction of the underlying data. These kind of tools can dramatically augment the availability and usability of auditory graphs for the visually impaired community. We propose a deep learning-based approach for the generation of an automatic sonification of an image containing a bar or a line chart using only pixel information. In particular, we took a denoising approach to this problem, based on a fully symmetric convolutional neural network architecture. Our results show that this approach works as a basis for the automatic sonification of charts directly from the information contained in the pixels of an image..","2021-06","2023-07-13 06:25:48","2023-07-13 06:25:48","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CBVGU2EI","journalArticle","1998","Wegner, Kristen","Surgical navigation system and method using audio feedback","","","","","http://hdl.handle.net/1853/50733","We discuss an experimental audio feedback system and method for positional guidance in real-time surgical instrument placement tasks. This system is intended for future usability testing in order to ascertain the efficacy of the use of the aural modality for assisting surgical placement tasks in the operating room. The method is based on translating spatial parameters of a surgical instrument or device, such as its position or velocity with respect to some coordinate system, into a set of audio feedback parameters along the coordinates of a generalised audio space. Error signals that correspond to deviations of the actual instrument trajectory from an optimal trajectory are transformed into a set of audio signals that indicate to the user whether correction is necessary. An experimental hardware platform was assembled using commercially available hardware. A system for 3-D modelling, surgical procedure planning, real-time instrument tracking and audio generation was developed. Prototype software algorithms for generating audio feedback as a function of instrument navigation were designed and implemented. The system is sufficient for future usability testing. This technology is still in an early stage of development, with formal usability and performance testing yet to be done. However, informal usability experiments in the course of the basic engineering process indicate the use of audio is a promising alternative to, or redundancy measure in support of visual display technology for intra-operative navigation.","1998-11","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RX9ZTXP9","journalArticle","2017","Sun, Yuanjing; Barnes, Jaclyn; Jeon, Myounghoon","Multisensory Cue Congruency In The Lane Change Test","","","","","http://hdl.handle.net/1853/58353","Drivers interact with a number of systems while driving. Taking advantage of multiple modalities can reduce the cognitive effort of information processing and facilitate multitasking. The present study aims to investigate how and when auditory cues improve driver responses to a visual target. We manipulated three dimensions (spatial, semantic, and temporal) of verbal and nonverbal cues to interact with visual spatial instructions. Multimodal displays were compared with unimodal (visual-only) displays to see whether they would facilitate or degrade a vehicle control task. Twenty-six drivers participated in the Auditory-Spatial Stroop experiment using a lane change test (LCT). The preceding auditory cues improved response time over the visual-only condition. When dimensions conflicted, spatial (location) congruency had a stronger impact than semantic (meaning) congruency. The effects on accuracy was minimal, but there was a trend of speed-accuracy trade-offs. Results are discussed along with theoretical issues and future works.","2017-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CICACPX9","journalArticle","2011","Hermann, Thomas; Zehe, Sebastian","Sonified Aerobics - Interactive Sonification of Coordinated Body Movements","","","","","http://hdl.handle.net/1853/51764","This paper introduces a new hard-/ and software system for the interactive sonification of sports movement involv- ing arm- and leg movements. Two different sonifications are designed to convey rhythmical patterns that become au- ditory gestalt so that listeners can identify features of the underlying coordinated movement. The Sonification is de- signed for the application to enable visually impaired users to participate in aerobics exercises, and also to enhance the perception of movements for sighted participants, which is useful for instance if the scene is occluded or the head pos- ture is incompatible with the observation of the instructor or fitness professional who shows the practices in parallel. Furthermore, the system allows to monitor fine couplings in arm/leg coordination while jogging, as auditory feedback may help stabilizing the movement pattern. We present the sensing system, two sonification designs, and interaction examples that lead to coordination-specific sound gestalts. Finally, some qualitative observations are reported from the first uses of the prototype.","2011-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CBX85A3A","journalArticle","2007","Grand, Florian","Organized Data for Organized Sound Space Fitting Curves in Sonification","","","","","http://hdl.handle.net/1853/50019","In this paper, we introduce space filling curves (SFC) as a useful possibility to organize data for sonification. First, we give a brief overview about the history of SFCs and their graphical construction. Then we focus on the mapping properties of SFCs from 2D to one dimension. We present the acoustic results of an implementation of the described method, in which we took the Hilbert curve as one particular example of an SFC. The actual sonification program features different methods for real-time interaction. These methods take advantage of the particular properties of SFCs. We further discuss their restrictions, how they can be circumvented, and give an outlook to future applications, where we also make suggestions as to how the properties of SFCs can be combined with methods of data reduction.","2007-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7NJFRWLF","journalArticle","2014","Han, Yoon Chung; Han, Byeong-jun","Skin Pattern Sonification Using NMF-based Visual Feature Extraction and Learning-based PMSon","","","","","http://hdl.handle.net/1853/52068","This paper describes the use of sonification to represent the scanned image data of skin pattern of the human body. Skin Patterns have different characteristics and visual features depending on the positions and conditions of the skin on the human body. The visual features are extracted and analyzed for sonification in order to broaden the dimensions of data representation and to explore the diversity of sound in each human body. Non-negative matrix factorization (NMF) is employed to parameterize skin pattern images, and the represented visual parameters are connected to sound parameters through support vector regression (SVR). We compare the sound results with the data from the skin pattern analysis to examine how much each individual skin patterns are effectively mapped to create accurate sonification results. Thus, the use of sonification in this research suggests a novel approach to parameter mapping sonification by designing personal sonic instruments that use the entire human body as data.","2014-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PMDVFG7C","book","2010","Liljedahl, Mats","Awesome - A Tool for Simulating Sound Environments","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49857","Sounds are (almost) always heard and perceived as parts of greater contexts. How we hear a sound depends on things like other sounds present, acoustic properties of the place where the sound is heard, the distance and direction to the sound source etc. Moreover, if the sound bear any meaning to us or not and what the meaning is, if any, depends largely on the listener’s interpretation of the sound, based on memories, previous experiences etc. When working with the design of sounds for all sorts of applications, it is crucial to not only evaluate the sound isolated in the design environment, but to also test the sound in possible greater contexts where it will be used and heard. One way to do this is to sonically simulate one or more environments and use these simulations as contexts to test designed sounds against. In this paper we report on a project in which we have developed a system for simulating the sounding dimension of physical environments. The system consists of a software application, a 5.1 surround sound system and a set of guidelines and methods for use. We also report on a first test of the system and the results from this test.","2010-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BAWH5EUH","book","2015","Worrall, David","Realtime sonification and visualisation of network metadata","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54204","The development of the NetSon project is described, from its exploratory origins in a polymedia work for an art and technology event, to real-time continuous sonifications of network metadata. The project currently exists in two forms: bespoke multichannel installations and a centrally-configured live video version streamed to the internet. These sonifications are accompanied by realtime visualizations that have been developed to assist in the immediate recognition of the dynamically configurable sonification mappings. A review of related work and a detailed discussion of the sonification mapping models are outside the scope of this paper.","2015-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3AHEPXKE","journalArticle","2012","Fan, Yuan-Yi; Weber, René","Capturing audience experience via mobile biometrics","","","2168-5126","","http://hdl.handle.net/1853/44416","Different from computer vision based approaches in audience participation research, such as in Glimmer [1] and in Flock [2], this paper presents a mobile approach to collecting and visualizing bodily responses from audience members. A mobile biometric application is designed as a novel medium that interfaces audience members to experienced content. To realize our goal on a mobile platform, a combination of video-imaging-based heart rate measurement and Zeroconf networking technology [3] (Bonjour) is implemented. As a proof of concept, we successfully collect continuous heart rate values from 3 mobile phones devices simultaneously and use the derived heart rate statistics to drive artistic audio and visual rendering. Preliminary results include two iOS applications and two mobile-biometric-enabled media arts installations.","2012-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EXBPHNQ3","journalArticle","1996","Flowers, John H.; Buhman, Dion C.; Turnage, Kimberly D.","Data sonification from the desktop: Should sound be part of standard data analysis software?","","","","","http://hdl.handle.net/1853/50804","The design of auditory formats for data display is presently focused on applications for blind or visually impaired users, specialized displays for use when visual attention must be devoted to other tasks, and some innovative work in revealing properties of complex data that may not be effectively rendered by traditional visual means. With the availability of high quality and flexible sound production hardware in standard desktop computers, the potential exists for using sound to represent characteristics of typical ""small and simple"" samples of data in routine data inspection and analysis. Our research has shown that basic properties of simple functions, distribution properties of data samples, and patterns of covariation between two variables can be effectively displayed by simple auditory graphs involving patterns of pitch variation over time. While such developments have implications for specialized applications and populations of users, these displays are easily comprehended by normal users with minimal practice. Providing further software enhancement to encourage exploration of data representation by sound may lead to a variety of useful creative developments in data display technology.","1996-11","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","Data sonification from the desktop","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BXHMLXHJ","journalArticle","2003","Lemmens, Paul M. C.; de Haan, Ab; van Galen, Gerard P.","Do location and context operate independently?","","","","","http://hdl.handle.net/1853/50458","To investigate if location and content operate independently, two experiments were carried out in which redundant auditory co–messages to a visual categorization task were manipulated to enable left and right ear only presentation, as well as binaural presentation. The location cues were irrelevant to the main task and the auditory co–messages (in one experiment earcons and in the other auditory icons) contained redundant information from the primary task. The findings seem to indicate that location and content of the co–messages operate independently because no interaction between these factors were found.","2003-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VQKQ6WLZ","journalArticle","1994","Axen, Ulrike; Choi, Insook","Using additive sound synthesis to analyze simplicial complexes","","","","","http://hdl.handle.net/1853/50881","We present a new technique for traversing simplicia1 complexes and producing sounds from the output of this traversal. The traversal algorithm was invented in order to extract temporal information from static geometric structures; this information is used as input to a sound synthesis algorithm. A systematic traversal of complexes and associating data to parameters of sound synthesis has many possible applications: the analysis of objects of dimension 4 or higher, exploration of large data sets and music composition. In this paper we limit ourselves to bdimensional objects and present the experimental results from the first phase of our work.","1994-11","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EYHC5NV8","journalArticle","2021","Nadri, Chihab; Lee, Seul Chan; Kekal, Siddhant; Li, Yinjia; Li, Xuan; Lautala, Pasi; Nelson, David; Jeon, Myounghoon","Investigating the effect of earcon and speech variables on hybrid auditory alerts at rail crossings","","","","","http://hdl.handle.net/1853/66329","Despite rail industry advances in reducing accidents at Highway Rail Grade Crossings (HRGCs), train-vehicle collisions continue to happen. The use of auditory displays has been suggested as a countermeasure to improve driver behavior at HRGCs, with prior research recommending the use of hybrid sound alerts consisting of earcons and speech messages. In this study, we sought to further investigate the effect of auditory variables in hybrid sound alerts. Nine participants were recruited and instructed to evaluate 18 variations of a hybrid In-Vehicle Auditory Alert (IVAA) along 11 subjective ratings. Results showed that earcon speed and pitch contour design can change user perception of the hybrid IVAA. Results further indicated the influence of speech gender and other semantic variables on user assessment of HRGC IVAAs. Findings of the current study can also inform and instruct the design of appropriate hybrid IVAAs for HRGCs.","2021-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KI72BWWA","journalArticle","2017","Broderick, James; Duggan, Jim; Redfern, Sam","Using Auditory Display Techniques to Enhance Decision Making And Perceive Changing Environmental Data Within a 3D Virtual Game Environment","","","","","http://hdl.handle.net/1853/58352","When it comes to understanding our environment, we use all our senses. Within the study and implementation of virtual environments and systems, huge advancements in the quality of visuals and graphics have been made, but when it comes to the audio in our environment, many people have been content with very basic sound information. Video games have strived towards powerful sound design, both for player immersion and information perception. Research exists showing how we can use audio sources and waypoints to navigate environments, and how we can perceive information from audio in our surroundings. This research explores using sonification of changing environmental data and environmental objects to improve user's perception of virtual spaces and navigation within simulated environments, with case studies looking at training and for remote operation of unmanned vehicles. This would also expand into how general awareness and perception of dynamic 3D environments can be improved. Our research is done using the Unity3D game engine to create a virtual environment, within which users navigate around water currents represented both visually and through sonification of their information using Csound, a C based programming language for sound and music creation.","2017-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NXHRWBRH","book","2009","Bulut, Zeynep","Revisiting the phenomenon of sound as ""empty container"": the acoustic imagination in Kurt Schwitters's “ursonata”","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51412","Dadaist artist and composer Kurt Schwitters’ s Ursonata (1922-1932) is a sound poem for solo voice based on a made-up verbal language that uses phonetics in German. Percussionist Steven Schick and composer/sound designer Shahrokh Y adegari have arranged a multimedia interpretation of Ursonata, (The New) UrSonata (2006) that amplifies the sounds of the voice as spatiotemporal events. Addressing the spatiotemporal voice in the (The New) UrSonata, this paper raises two goals: (1) to unfold the perception and reception of sound as acoustic imagination, and (2) to discuss acoustic imagination as “empty container” in Henri Lefebvre’s terms, that generates spatiality and bodily thought. To examine the notion of acoustic imagination, I will refer to Henri Lefebvre’s metaphor of “empty container”, which indicates a pure interiority to be filled in. Lefebvre qualifies the ontological status of space as empty container. I intend to use the same metaphor to formulate acoustic imagination. I will elaborate the connection between “empty container” and acoustic imagination by exemplifying the sounds of a coffee machine. Imagine the rhythmic drops of a coffee machine. Listening to the drops, we resonate with the sounds, we map a space through the physical nexus of the sounds, and we orient ourselves within the actual space by the help of the sounds. In other words, being physically and psychically extended by the sound, we draw a space. Acoustic imagination is pure interiority filled with such extension. This very extension produces spatial thought. In his Phenomenology of Perception, Maurice Merleau-Ponty draws our attention to spatiality as ‘bodily thought”. Furthering Merleau-Ponty’ s idea, I will suggest that acoustic imagination constitutes bodily thought. I will then return to Schwitter’s Ursonata and (The New) UrSonata, and situate the spatiotemporal sounds of the voice at the heart of our listening experience. While listening to crystallized fragments of sound, how do we conceive Schick’s voice? How do we hear, imagine, and build symmetries or asymmetries between his voice and our own voices?","2009-05","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","Revisiting the phenomenon of sound as ""empty container""","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GEWTQJAZ","journalArticle","2014","Droumeva, Milena; McGregor, Iain","A Method for Comparitive Evaluation of Listening to Auditory Displays by Designers and Users","","","","","http://hdl.handle.net/1853/52062","The process of designing and testing auditory displays often includes evaluations only by experts, and where non-experts are involved, training is commonly required. This paper presents a method of evaluating sound designs that does not require listener training, thus promoting more ecological practices in auditory display design. Complex sound designs can be broken down into discrete sound events, which can then be rated using a set of sound attributes that are meaningful to both designers and listeners. The two examples discussed in this paper include an auditory display for a commercial vehicle, and a set of sound effects for a video game. Both are tested using a repertory grid approach. The paper shows that the method can highlight similarities and differences between designer and user listening experiences thus informing design decisions and subsequently reception.","2014-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7I6I9QT2","book","2015","Hermann, Thomas; Hildebrandt, Tobias; Langeslag, Patrick; Rinderle-Ma, Stefanie","Optimizing aesthetics and precision in sonification for peripheral process-monitoring","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54209","We have developed the SoProMon process monitoring system to evaluate how real-time sonifications can increase awareness of process states and to support the detection and resolving of critical process situations. Our initial design conveys analogue information as process-data-driven soundscape that users can blend out in favor of a primary task, however the sonification attracts the user's attention even before things become critical. As result of a first user study we gained and present here insights into usability and acceptance of the sounds. Although effective, the aesthetic qualities were not rated highly. This motivated us to create a new design that sacrifices some functional aspects to emphasize long-term use compatibility. We present and compare the new designs and discuss our experiences in creating pleasant sonifications for this application area.","2015-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R5JJDBCQ","journalArticle","2000","Rozier, Joseph; Karahalios, Karrie; Donath, Judith","Hear & there: An augmented relity system of linked audio","","","","","http://hdl.handle.net/1853/50674","This paper presents an augmented reality system using audio as the primary interface. Using the authoring component of this system, individuals can leave ""audio imprints,"" consisting of several layers of music, sound effects, or recorded voice, at a location outdoors. Using the navigation component, individuals can hear imprints by walking into the area that the imprint occupies. Furthermore, imprints can be linked together, whereby an individual is directed from one imprint to related imprints in the area.","2000-04","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","Hear & there","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CXURC93G","journalArticle","2018","Cornejo, Stuart Duncan Haffenden","Towards Ecological and Embodied Design of Auditory Display","","","","","http://hdl.handle.net/1853/60078","Auditory display research has been criticised over a perceived lack of progress in tackling key issues relating to usability and user-experience. However, emerging trends in design-thinking present new tools for addressing the usability concerns that have long beleaguered this field of inquiry. In this paper, we provide an in-depth analysis on the emergence of design-based approaches in auditory display research by mapping out the progression of current research in the field. Through an ecological and embodied approach to perception and cognition, we then evaluate user-centric design strategies as tools for better understanding complex design spaces and improving usability. We then present a discussion to elucidate the benefits to auditory display research of employing user-centric design strategies for future projects.","2018-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5A62LU8Z","journalArticle","2006","Ferguson, S.; Cabrera, D.; Beilharz, K.; Song, H. J.","Using psychoacoustical models for information sonification","","","","","http://hdl.handle.net/1853/50694","Psychoacoustical models provide algorithmic methods of estimating the perceptual sensation that will be caused by a given sound stimulus. Four primary psychoacoustical models are most often used: `loudness', `sharpness', `roughness', and `fluctuation strength', models for which have been presented by Zwicker and Fastl [1]. These four models have been used extensively for optimising product sound quality in industrial sound design applications. However, they also may be applied for auditory display purposes. This paper presents a method for their application and discusses effects and implications of using this method for designing auditory displays. This paper is primarily theoretical – however, sound examples of auditory graphing based on psychoacoustical models will be presented at the conference for discussion.","2006-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AZTKPEAZ","journalArticle","2004","Cabrera, D.; Nguyen, A.; Choi, Y.","Auditory versus visual spatial impression: A study of two auditoria","","","","","http://hdl.handle.net/1853/50760","Spatial impression refers to the attributes of subjective space beyond localization. In the field of auditorium acoustics, auditory spatial impression is often divided into `apparent source width', `envelopment' and sometimes `intimacy'. In separate experiments, this study considers how visual and auditory spatial impression vary within two auditoria, and hence similarities between these two sensory modes. In the visual experiment, the `spaciousness', `envelopment', `stage dominance', `intimacy' and target distance were judged by subjects using grayscale projected photographs, taken from various positions in the audience areas of the two auditoria when a visual target was on stage. In the auditory experiment, the `apparent source width', `envelopment', `intimacy' and performer distance were judged using an anechoic orchestral recording convolved with binaural impulse responses measured from the same positions in the two auditoria. Results show target distance to be of primary importance in auditory and visual spatial impression – thereby providing a basis for covariance between some attributes of auditory and visual spatial impression. Nevertheless, some attributes of spatial impression diverge between the senses.","2004-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","Auditory versus visual spatial impression","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H8AEHM82","journalArticle","2005","O'Sullivan, Conor; Chang, Angela","Dimensional design: Explorations of the auditory and haptic correlate for the mobile device","","","","","http://hdl.handle.net/1853/50105","Designing for the mobile device currently allows for the exploration of additional dimensions beyond that of simple graphic treatment. The relationship between two of these dimensions, audio and haptics is discussed here. A method for creation of content for the generation of audio-haptic feedback is discussed. A particular evaluation of such content and its application to design for the auditory and haptic correlate is offered. General observations on this work and the need for further exploration are described.","2005-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","Dimensional design","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6T23MM5H","book","2015","Mihalas, George; Popescu, Lucian; Naaji, Antoanela; Andor, Minodora; Paralescu, Sorin; Tudor, Anca; Neagu, Adrian","Adding sound to medical data representation","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54195","Some preliminary results of a project aiming to develop tools for adding sound associated to medical data are presented. The description of our sonification procedure is followed by two different examples. The first refers to monitoring the heart rate (HR) during exercise, either in clinical settings or in self monitoring conditions. The second example is an application from molecular biology / cellular kinetics, for analysis of protein-protein interaction, with a specific reference to a computer simulation of P53 – MDM2 interaction, which exhibits, under certain conditions, an oscillatory behavior. Pending issues and future work are finally discussed.","2015-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LI4LHJFQ","journalArticle","2021","Fitzpatrick, Joe; Neff, Flaithri","A web guide to perceptually congruent sonification","","","","","http://hdl.handle.net/1853/66321","Sonification is an increasingly popular mechanism for data exploration, promoting the need for a greater understanding of human auditory perception and of how sonified information is designed, presented, and interpreted. In this paper, perceptual modelling is used to explore and demonstrate how perceptual phenomena are accounted for in sonification design. The framework, extracted from a larger body of work, links perceptual phenomena such as stream segregation to sonification mappings to provide a systematic approach to identifying and addressing perceptually-driven problems in applied sonification. A web guide functions to situate and guide designers through the complex theoretical constituents of auditory perception incorporated in the Perceptually Congruent Sonification (PerCS) framework. This web guide (hosted on sonification.ie) highlights and summarises the perceptual phenomena most relevant to sonification design and uses simple audio-visual interactions to demonstrate their effect. Preliminary qualitative feedback from a brief survey elucidates a small number of enduser concerns and comments.","2021-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UQYSRDCY","journalArticle","2003","Walker, Bruce N.; Cothran, Joshua T.","Sonification sandbox: A graphical toolkit for auditory graphs","","","","","http://hdl.handle.net/1853/50490","Motivated by the need for a multi-platform, multipurpose toolkit for sonifying data, the Sonification Sandbox allows users to map data to multiple auditory parameters and add context using a graphical interface. The Sonification Sandbox is a cross-platform application authored in Java, using the Java Sound API to generate MIDI output. The software allows users to independently map several data sets to timbre, pitch, volume, and pan, with full control over the default, minimum, maximum, and polarity for each attribute. It is also possible to add context to the sonification using a percussive “click track” or a constant, repeating, or notifying tone at the minimum, maximum, or mean of a given set of data. Applications for the Sonification Sandbox include auditory display for the blind, science and mathematics education, data exploration, and experimenting with the effectiveness of various sonification techniques and parameters.","2003-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","Sonification sandbox","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K9CTBX9R","journalArticle","2019","Komatsu, Takanori; Hayashi, Eiji","A design guide-line of auditory display for electric appliance","","","","","http://hdl.handle.net/1853/61495","The auditory channel is important for communication between computers and users because of its properties, such eye-free communication and strong attention grabbing properties. However, interpreting the meanings of sounds is not a trivial task. Users have to learn and memorize the mapping between sounds and their meanings for each device. Therefore, as the number of devices increases, this becomes challenging for users. To mitigate the challenge, it is desirable to use sounds that users can understand intuitively. Thus, investigating the intuitiveness of sounds is of significant interest. In this work, we investigated 2,012 sounds consisting of 48 earcons, 80 auditory icons, and 1,884 beep sequences through a series of user studies using Amazon Mechanical Turk as well as a lab study that validated the results of the Mechanical Turk studies. The results provided a guideline for designing sounds that users might understand more intuitively.","2019-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DWPXCN3W","journalArticle","2003","Hermann, Thomas; Niehus, Christian; Ritter, Helge","Interactive visualization and sonification for monitoring complex processes","","","","","http://hdl.handle.net/1853/50467","This paper introduces AVDisplay, a versatile auditory and visual display for monitoring, querying and accessing information about modules or processes in complex systems. In the context of a collaborative research effort (SFB360, artificial communicators) at Bielefeld University, a cognitive robotics system for humanmachine interaction is being developed. The AVDisplay provides the central interface for monitoring and debugging this system, currently involving about 20 computers hosting more than 30 complex processes. The display is designed to provide a summary over the system's activities combining visualization and sonification techniques. The dynamic visualization allows inference of correlated activity of processes. A habituation simulation process automatically sets a perceptional focus on interesting and relevant process activities. The sonification part is designed to integrate emotional aspects – if the system suffers from poor sensory quality, the sound conveys this by sounding uncomfortable.","2003-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W35SLP7F","journalArticle","1998","Conversy, Stephane","Ad-hoc synthesis of auditory icons","","","","","http://hdl.handle.net/1853/50709","This article introduces ad-hoc synthesis, an approach to designing auditory icons and synthesis algorithms that emphasizes the perception of the sounds by users instead of the analysis of actual sources and sound. We describe two substractive synthesis algorithms for generating and controlling wind and wave sounds in real-time by means of high-level parameters. Even though these sounds are not audiorealistic, they convey information in a non-intrusive way and therefore are suitable for monitoring background activities. These sounds capture the main invariants of the sounds they imitate, enabling users to recognize and understand them easily. We then push the approach further by showing how an auditory illusion, i.e. a sound that does not exist in the real world, can be used to convey the notion of speed in a natural and non-intrusive way.","1998-11","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VW432VAX","journalArticle","2017","Laughner, Joshua L.; Canfield-Dafilou, Elliot Kermit","Illustrating Trends in Nitrogen Oxides Across the United States Using Sonification","","","","","http://hdl.handle.net/1853/58354","Leveraging the human auditory system, sonification can be used as an educational tool for non-experts to engage with data in a different mode than visualization. Without oversimplifying the data, this project presents a sonification tool for exploring NO₂ and O₃ data from the BErkeley High Resolution (BEHR) tropospheric NO₂ and OMO3PR ozone profile datasets. By allowing the listener control over the data-to-sound mapping and synthesis parameters, one can experience and learn about the interplay between NO₂ tVCDs and O₃ concentrations. Furthermore, interannual trends can be perceived across different types of locations.","2017-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LA3MBXG3","journalArticle","2004","Stevens, C.; Brennan, D.; Parker, S.","Simultaneous Manipulation of Parameters of Auditory Icons to Convey Direction, Size, and Distance: Effects on Recognition and Interpretation","","","","","http://hdl.handle.net/1853/50916","Auditory icons – or environmental sounds – have the potential to convey information by non-verbal means quickly and accurately. In addition, human listeners are quick to determine many qualities of an auditory object, such as location, distance, size, and motion, from acoustics of the signal. An experiment tests these two coupled assumptions in a controlled laboratory context. Stimuli consisted of auditory icons “loaded” with information achieved through systematic manipulation of the acoustic parameters pitch, volume ramping, and reverberation. Sixty adult listeners were asked to recognize and describe four auditory icons wherein object size, distance and direction of motion were captured in the parameters of each 1-second sound. Participants were accurate at recognizing and interpreting the icons 70-80% of the time. Recognition rate was consistently high when participants responded to one, two or three parameters. However, recognition was significantly poorer when in response to all four parameters. There was a significant effect of icon type and parameter manipulation: dog bark was the most easily recognized icon, and the direction parameter interpreted most accurately. Implications of the findings for applied contexts are discussed.","2004-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","Simultaneous Manipulation of Parameters of Auditory Icons to Convey Direction, Size, and Distance","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"63DPPJ59","journalArticle","2016","Genovese, Andrea F.; Juras, Jordan; Miller, Chris; Roginska, Agnieszka","Investigation of ITD Symmetry in Measured HRIRs","","","","","http://hdl.handle.net/1853/56594","The Interaural Time Difference is one of the primary localization cues for 3D sound. However, due to differences in head and ear anthropometry across the population, ITDs related to a sound source at a given location around the head will differ from subject to subject. Furthermore, most individuals do not possess symmetrical traits between the left and right pinnae. This fact may cause an angle-dependent ITD asymmetry between locations mirrored across the left and right hemispheres. This paper describes an exploratory analysis performed on publicly available databases of individually measured HRIRs. The analysis was first performed separately for each dataset in order to explore the impact of different formats and measurement techniques, and then on pooled sets of repositories, in order to obtain statistical information closer to the population values. Asymmetry in ITDs was found to be consistently more prominent in the rear-lateral angles (approximately between 90° and 130° azimuth) across all databases investigated, suggesting the presence of a sensitive region. A significant difference between the peak asymmetry values and the average asymmetry across all angles was found on three out of four examined datasets. These results were further explored by pooling the datasets together, which revealed an asymmetry peak at 110° that also showed significance. Moreover, it was found that within the region of sensitivity the difference between specular ITDs exceeds the just noticeable difference values for perceptual discrimination at all frequency bands. These findings validate the statistical presence of ITD asymmetry in public datasets of individual HRIRs and identify a significant, perceptually-relevant, region of increased asymmetry. Details of these results are of interest for HRIR modeling and personalization techniques, which should consider implementing compensation for asymmetric ITDs when aiming for perceptually accurate binaural displays. This work is part of a larger study aimed at binaural-audio personalization and user-characterization through non-invasive techniques.","2016-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BFFS44WW","book","2010","Grond, Florian; Droßard, Trixi; Hermann, Thomas","Sonicfunction: Experiments with a Function Browser for the Visually Impaired","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/50056","We present in this paper SonicFunction, a prototype for the interactive sonification of mathematical functions. Since many approaches to represent mathematical functions as auditory graphs exist already, we introduce in SonicFunction three new aspects related to sound design. Firstly, SonicFunction features a hybrid approach of discrete and continuous sonification of the function values f(x). Secondly, the sonification includes information about the derivative of the function. Thirdly, SonicFunction includes information about the sign of the function value f(x) within the timbre of the sonification and leaves the auditory graph context free for an acoustic representation of the bounding box. We discuss SonicFunction within the context of existing function sonifications, and report the results from an evaluation of the program with 14 partially sighted and blind students.","2010-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","Sonicfunction","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"85CGLPCR","journalArticle","2011","Bruckner, Hans-Peter; Bartels, Christopher; Blume, Holger","PC-Based Real Time Sonification of Human Motion Captured by Inertial Sensors","","","","","http://hdl.handle.net/1853/51705","This paper presents a low latency system for real time sonification of human motion captured by inertial sensors. Exemplarily the position of the wrist estimated by two inertial sensors located at upper arm and forearm is transformed into a continuous synthetic sound. The body segment orientation is captured by Xsens MTx sensors and used to compute the position of the wrist relative to the shoulder joint. The accessible motion parameters provided by inertial sensors are three axial segment orientations, three axial accelerations and angular rates, and all derived quantities like position in three dimensional space. Motion data sonification is performed by the Sound Synthesis Toolkit (STK), a set of C++ classes for audio signal processing and sound synthesis. The proposed framework enables future research in continuous real time sonification of human motion to improve the process of motion learning during stroke rehabilitation. Through software profiling the proposed framework is benchmarked in terms of latency induced by signal transmission and processing to evaluate maximal process- able sampling rates and inertial sensor counts.","2011-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VUYJAVBU","book","2015","Boschi, Lapo; Paté, Arthur; Holtzman, Ben; Le Carrou, Jean-Loïc","Can auditory display help us categorize seismic signals?","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54177","Recordings of the Earth’s surface oscillation (seismograms) can be sonified such that most of the signal’s frequency spectrum falls in the audible range. Then, the pattern-recognition capabilities of the human auditory system can be applied to auditory analysis of seismic data. We sonify seismograms associated with a magnitude 5.6 earthquake. A group of volunteers listen to our sonified data set via headphones and software allowing them to reproduce each signal as many times as they want by clicking on the corresponding icon. Following the “free categorization” approach, listeners are asked to group icons corresponding to sounds perceived as “similar.” The goal of this test is to determine whether the human auditory system can perceive relevant “clues” in sonified seismograms, and whether humans can group such stimuli accordingly. Our results suggest that this is indeed the case, and allow us to identify at least one categorization strategy followed by the majority of listeners, which suggests that auditory analysis of seismic data is feasible and possibly useful. Our findings encourage further work, where we plan to take advantage of recent progress in auditory scene synthesis algorithms and spatial audio technology.","2015-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9W2U5L92","book","2010","Cordeiro, João; Makelberge, Nicolas","Hurly-Burly: An Experimental Framework for Sound Based Social Networking","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49890","This project deals with the topic of social interrelations; its aim is to achieve a deeper understanding of the underlying mechanisms of these relations through the use of sound and mobile devices/ubiquitous computing. The proposed framework follows two interdependent directions: 1) using environmental sounds as input data for context analysis, 2) using sound as an output to express results (sonification). This project is part of a long-term research project concerning sound based social networks, conducted at the Research Centre for Science and Technology in Arts (CITAR). The aim of this paper is to share some initial results, both practical and conceptual in form of a related work overview on social networking technologies, a conceptual design for a Facebook® application based on the project initial idea (including an IPhone® graphic interface proposal) and last but not least, an experimental framework for data communication between an IPhone® and a computer (using Pure Data through RjDj).","2010-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","Hurly-Burly","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EVTMG67Z","journalArticle","2013","Andreopoulou, Areti; Rogińska, Agnieszka; Mohanraj, Hariharan","Analysis Of The Spectral Variations In Repeated Head-Related Transfer Function Measurements","","","","","http://hdl.handle.net/1853/51670","This paper discusses the range of spectral variations between HRTF sets measured on the same subjects. The analysis is done in a corpus of 40 HRTF datasets of 4 subjects (10 datasets per subject). Variations are observed as a function of frequency, distance of the ears to the sound source (ipsilateral or contralateral), and location. Assessments of the spatial quality of all datasets were made through a subjective study which confirmed that despite their spectral variations, all individually measured HRTF sets maintained a high degree of spatial realism. An understanding of the variability in HRTFs can offer new intuition on objective binaural filter evaluation, and has significance in the research fields of spatial audio reproduction and virtual auditory display.","2013-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J4VAYTF7","journalArticle","1994","Das, Sumit; DeFanti, Tom; Sandin, Dan","An organization for high-level interactive control of sound","","","","","http://hdl.handle.net/1853/50824","The state of computer generated sound has advanced rapidly, and there exist many different ways of conceptualizing the abstract sound structures that comprise music and other complex organizations of sound. Many of these methods are radically diierent from one another, and so are not ususally used within the same system. One problem that almost all methods share is one of control, as large amounts of data are needed to specify sounds. How do we create, examine, and modify these complex structures? The problem is exacerbated if we consider the realm of interactively controlled sound. This chapter presents an organization which, rather than forcing a particular way of thinking about sound, allows multiple arbitrarily high-level views to coexist, all sharing a common interface. The methods or algorithms are abstracted into a objects called auditory actors. This encapsulation allows different algorithms to be used concurrently. All communication with and between these actors is carried out through message-passing, which allows arbitrary types of information (such as other messages) to be easily communicated. This standardizes control without limiting it to a particular type of data. A prototype system was implemented using this model. This system was used by a number of diierent developers to create audio interfaces for interactive virtual reality applications, which were demonstrated at the SIGGRtlPH 94 conference in Orlando, Florida. Compared to earlier systems, developers were able to create more complex audio interfaces in a shorter time.","1994-11","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ENENVDF6","journalArticle","1998","Bussemakers, Myra P.; de Haan, Abraham","Using earcons and icons in categorization tasks to improve multimedia interfaces","","","","","http://hdl.handle.net/1853/50737","In this study, the modality appropriateness hypothesis that originated from experiments in perception is tested for human computer interaction situations. In multimodal information processing users need to integrate the data coming from various sources into one message. In a visual and auditory categorisation task with accessory stimuli in the other modality, containing a mood, it was shown that in tasks where choices need to be made based on the meaning of the stimuli, the visual modality seems more appropriate. From the results can be concluded that users do not always benefit from having information in more than one modality.","1998-11","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q7W3HYWS","journalArticle","2005","Peres, S. Camille; Lane, David M.","Auditory graphs: The effects of redundant dimensions and divided attention","","","","","http://hdl.handle.net/1853/50093","An experiment is presented comparing the effectiveness of three parameters of sound for the auditory presentation of statistical data or auditory graphs. The dimensions of pitch, loudness, and time were used alone and redundantly to map the values of a box plot to an auditory graph. While previously, temporal mappings had resulted in better performance than mappings using pitch, panning, or loudness, these benefits were not consistently found in the current paradigm. Furthermore, to investigate possible benefits of mappings using two dimensions redundantly over mappings using one dimension, this experiment, compared mappings using integral and separable dimensions of sound - specifically, pitch and loudness (integral) and pitch and timing (separable). There was a benefit of a redundant design when the dimensions of sound used were integral whereas there was no benefit when they were separable. Finally, a task closer to a real-life application of auditory graphs was used where two sources of information were monitored simultaneously. The results support the argument that auditory graphs can be used effectively in “eyes busy” situations where more than one source of information is being monitoring.","2005-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","Auditory graphs","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XNNAGZCH","book","2015","Tordini, Francesco; Bregman, Albert S.; Cooperstock, Jeremy R.","The loud bird doesn’t (always) get the worm: Why computational salience also needs brightness and tempo","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54145","Salience shapes the involuntary perception of a sound scene into foreground and background. A computational model of salience would provide a strong perceptual baseline for the sonification designer. However, there is a lack of ground truth to evaluate the proposed models and to measure their performance with respect to human perception. This paper describes three contributions. First, we introduce a behavioral definition of salience. We describe an experiment based on our definition that tests a corpus of natural communication sounds. Our results suggest that salience is well described by three perceptual dimensions: not only loudness, but also, tempo and brightness. Second, we extract the most significant acoustical features and analyze their relation with salience, as measured by our ground truth. The context effects emerging from our analysis confirm the difference between salience and novelty. Finally, we suggest some necessary characteristics of the computational salience model based on the analyzed features.","2015-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","The loud bird doesn’t (always) get the worm","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2JBGNPMB","journalArticle","2013","Barrass, Stephen; Barrass, Tim","Embedding Sonifications In Things","","","","","http://hdl.handle.net/1853/51659","This paper describes three experimental prototypes that explore the embedding of sonifications in things. Through previous work with mobile sonifications we identified requirements for Òsonifications in the wildÓ as being embedded, expendable, and extendeable. Three prototypes, called Flotsam, Jetsam and Lagan, investigate technologies, sounds, materials and metaphors to define and illustrate the design space. The knowledge gained from these prototypes has led to the development of the open source Mozzi sonification library on the Arduino microprocessor.","2013-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WQNLD8UW","journalArticle","2012","Terasawa, Hiroko; Parvizi, Josef; Chafe, Chris","Sonifying ECoG seizure data with overtone mapping: a strategy for creating auditory gestalt from correlated multichannel data","","","2168-5126","","http://hdl.handle.net/1853/44445","This paper introduces a mapping method, harmonic mapping, which projects multichannel time-series data onto a harmonic series structure. Because of the common fate effect of gestalt principle, correlated signals are perceived as a unity, while uncorrelated signals are perceived segregated. This method is first examined with sonification of simple, generic datasets. Then, harmonic mapping is applied to sonification of an ECoG data of an epileptic seizure episode. The relationship between the gestalt formation the correlation in the data across channels is discussed in detail using a 16 channels reduced dataset. Finally, sonification of a 56 channel ECoG dataset is provided to demonstrate the advantage of the harmonic mapping.","2012-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","Sonifying ECoG seizure data with overtone mapping","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y7MGR75F","journalArticle","2007","Menzies, Dylan","Physical Audio for Virtual Environments, Phya in Review","","","","","http://hdl.handle.net/1853/50024","A review is presented of a library that has emerged out of the development of physical audio capability within a physical computer game environment. Technical aspects are covered with emphasis on practical requirements, as well as broader issues concerning the uptake of audio modeling within industry. Some future directions are considered.","2007-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GLV3KN4E","journalArticle","2016","Nesbitt, Keith; Williams, Paul; Ng, Patrick; Blackmore, Karen; Eidels, Ami","Designing Informative Sound to Enhance A Simple Decision Task","","","","","http://hdl.handle.net/1853/56583","In this paper we examined the role of informative sound in a simple decision-making game task. A within-subject experiment with 48 participants measured the response time, success rate and number of timeouts of the players in a number of eight-second decision tasks. As time proceeds, the task becomes easier at the risk of players timing out and reducing the overall opportunities they will have to attempt the task. We designed a simple informative sound display that uses a tone that increases in amplitude over the duration of the task. We test player performance in three conditions, no sound (visual-only), constant (non-informative) sound and increasing (informative) sound. We found that the increasing sound display significantly reduced timeouts when compared with the visual only and constant sound versions of the task. This reduction in timeouts did not impair the players' performance in terms of their success rate nor response time.","2016-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BV5HS6FM","book","2015","Nakayama, Yuki; Takano, Yuji; Matsubara, Masaki; Suzuki, Kenji; Terasawa, Hiroko","Real-time smile sonification using surface EMG signal and the evaluation of its usability","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54121","We propose a real-time, interactive system for smile recognition and sonification using surface electromyography (EMG) signals. When a user smiles, a sound is played. The surface EMG signal is mapped to pitch using a conventional scale. The timbre of the sound is a synthetic sound mimicking bubbles. In the experiment, eight participants evaluated the effects of smile-based sonification feedback. Participants expressed smiles in a condition that there was feedback or no feedback. We investigated what type of effects the feedback had on smiling by analyzing surface EMG signals and interviewing the subjects. The results suggest that the sonified feedback could facilitate the expression of spontaneous smiles. In addition, results suggested that both the attack and release of a smile were similarly perceived using visual or auditory feedback.","2015-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D3EUFU4D","journalArticle","2011","Bujacz, Michal; Skulimowski, Piotr; Strumillo, Pawel","Sonification of 3D Scenes Using Personalized Spatial Audio to Aid Visually Impaired Persons","","","","","http://hdl.handle.net/1853/51746","The research presented concerns the development of a sonification algorithm for representation of 3D scenes for use in an electronic travel aid (ETA) for visually impaired persons. The proposed sonar-like algorithm utilizes segmented 3D scene images, personalized spatial audio and musical sound patterns. The use of segmented and parametrically described 3D scenes allowed to overcome the large sensory mismatch between visual and auditory perception. Utilization of individually measured head related transfer functions (HRTFs), enabled the application of illusions of virtual sound sources. The selection of sounds used was based on surveys with blind volunteers. A number of sonification schemes, dubbed sound codes, were proposed, assigning sound parameters to segmented object parameters. The sonification algorithm was tested in virtual reality using software simulation along with studies of virtual sound source localization accuracy. Afterwards, trials in controlled real environments were made using a portable ETA prototype, with participation of both blind and sighted volunteers. Successful trials demonstrated that it is possible to quickly learn and efficiently use the proposed sonification algorithm to aid spatial orientation and obstacle avoidance.","2011-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9YUDU7FT","journalArticle","2006","McGregor, I.; Crerar, A.; Benyon, D.; Leplatre, G.","Workplace soundscape mapping: A trial of Macaulay and Crerar's Method","","","","","http://hdl.handle.net/1853/50696","This paper describes a trial of Macaulay and Crerar's method of mapping a workplace soundscape [1] to assess its fitness as a basis for an extended soundscape mapping method. Twelve participants took part within 14 separate environments, which included academic, commercial and domestic locations. Results were visualized and subsequently collapsed to produce typical responses to typical environments, as well as specialist responses to a shared workplace.","2006-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","Workplace soundscape mapping","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4IAPHI5B","journalArticle","2005","Suied, Clara; Susini, Patrick; Misdariis, Nicolas; Langlois, Sabine; Smith, Bennett K.; McAdams, Stephen","Toward a sound design methodology: Application to electronic automotive sound","","","","","http://hdl.handle.net/1853/50192","In the field of Human Machine Interfaces (HMI), there is a potential to convey numerous different messages by non- verbal-sounds. In the eighties, different approaches to the design of information-bearing sounds were proposed. These approaches and corresponding guidelines focused on the acoustical properties of auditory displays. In particular, psychophysical approaches to urgency perception have identified relationships between acoustic parameters and different degrees of urgency perception. We performed an experiment with sounds currently used in automotive HMI. It was found that these sounds are not satisfactory and do not fulfill their intended function, even though some of them match the existing guidelines. Thus, we propose that a new methodology should be used to design more adequate HMIs. This methodology draws on two different theoretical frameworks: acoustics and semiotics (science of signs). In order to investigate the important hypotheses on which the methodology is based, we describe specific experiments that can be used to validate or invalidate the method, when applied to a specific sound design problem. Finally, we discuss the potential use of our new methodology.","2005-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","Toward a sound design methodology","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"83RYYZEW","book","2015","McIlraith, Rick; Walton, Paul; Brereton, Jude","The Spatialised Sonification of Drug-Enzyme Interactions","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54182","This paper presents the preliminary work into the creation of an interactive spatial sonification system used to model the interactions between drug molecules and their target biomolecules within the human body. With the aid of sonification and a 3D soundscape, the user is able to optimize these interactions to a much greater precision than the sole use of the current visual model. This system gives a promising means to aid the rapid design of new drug molecules that can interact more strongly with the enzyme’s active site, therefore creating more effective drugs for the treatment of cancer and other diseases. This paper gives a full account of the relevant theory, the techniques used and details of preliminary user testing.","2015-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KNHEB732","book","2015","Diatkine, Coralie; Bertet, Stéphanie; Ortiz, Miguel","Towards the holistic spatialization of multiple sound sources in 3D, implementation using ambisonics to binaural technique","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54178","This abstract describes a modular tool, dedicated to the real time spatialization of multiple sources in three dimensions, based on a mixed Ambisonics or Higher Order Ambisonics (HOA) to binaural technique, coupled with an interface that allows to position sound sources using free-hand gestures in a visual 3D environment. It is implemented in the real-time programming environment Max/MSP.","2015-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XZG2J45V","journalArticle","2002","Ritter, H.; Hermann, T.","Crystallization sonification of high-dimensional datasets","","","","","http://hdl.handle.net/1853/51347","This paper introduces Crystallization Sonification, a sonification model for exploratory analysis of high-dimensional datasets. The model is designed to provide information about the intrinsic data dimensionality (which is a local feature) and the global data dimensionality, as well as the transitions between a local and global view on a dataset. Furthermore the sound allows to display the clustering in high-dimensional datasets. The model defines a crystal growth process in the high-dimensional data-space which starts at a user selected “condensation nucleus” and incrementally includes neighboring data according to some growth criterion. The sound summarizes the temporal evolution of this crystal growth process. For introducing the model, a simple growth law is used. Other growth laws which are used in the context of hierarchical clustering are also suited and their application in crystallization sonification offers new ways to inspect the results of data clustering as an alternative to dendrogram plots. In this paper, the sonification model is described and example sonifications are presented for some synthetic high-dimensional datasets.","2002-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TA4BPVDX","journalArticle","2018","Quinton, Michael; McGregor, Iain; Benyon, David","Investigating effective methods of designing sonifications","","","","","http://hdl.handle.net/1853/60075","This study aims to provide an insight into effective sonification design. There are currently no standardized design methods, allowing a creative development approach. Sonifcation has been implemented in many different applications from scientific data representation to novel styles of musical expression. This means that methods of practice can vary a greatly. The indistinct line between art and science might be the reason why sonification is still sometimes deemed by scientists with a degree of scepticism. Some well established practitioners argue that it is poor design that renders sonifications meaningless, in-turn having an adverse effect on acceptance. To gain a deeper understanding about sonification research and development 11 practitioners were interviewed. They were asked about methods of sonification design and their insights. The findings present information about sonification research and development, and a variety of views regarding sonification design practice.","2018-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LA83TSHV","book","2015","Horsak, Brian; Iber, Michael; Bauer, Karin; Kiselka, Anita; Gorgas, Anna-Maria; Dlapka, Ronald; Doppler, Jakob","A wireless instrumented insole device for real-time sonification of gait","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54114","The treatment of gait disorders or impairments is one major challenge in physical therapy. The broad and fast development in low-cost, miniaturized and wireless sensing technologies supported the development of embedded and unobtrusive systems for robust gait-related data acquisition and analysis. Next to their application as portable and lowcost diagnosis tools, such systems bear also the capability of using them as feedback devices during gait retraining to foster motor learning processes. The approach described within this project applies movement-based sonification of gait to foster motor learning aspects during gait retraining. In detail the aim of this manuscript is threefold: (1) present a prototype (the SONIGait device) of a pair of wireless, sensor insoles instrumented with force-sensors for real-time data transmission and acquisition on a mobile client, (2) present the development of a set of sonification prototypes for realtime audible feedback and (3) evaluate the sonification prototypes as well as the SONIGait device within a pilot study","2015-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5IBMYZZB","book","2015","Monache, S. Delle; Rocchesso, D.; Baldan, S.; Mauro, D. A.","Growing the practice of vocal sketching","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54105","Sketch-thinking in the design domain is a complex representational activity, emerging from the reflective conversation with the sketch. A recent line of research on computational support for sound design has been focusing on the exploitation of voice, and especially vocal imitations, as effective representation strategy for the early stage of the design process. A set of introductory exercises on vocal sketching, to probe the communication effectiveness of vocal imitations for design purposes, are presented and discussed, in the scope of the research-through-design workshop activities of the EU project SkAT-VG.","2015-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EBQM33XN","journalArticle","2008","Schneider, Max","Database Concept for Medical Auditory Alarms","","","","","http://hdl.handle.net/1853/49901","With this paper we want to explore the benefits of an online available database for medical functional sounds and auditory alarms. The concept is inspired by an extensive context research in the medical field and aims to evoke discussion and trigger possible corporations in the future. The database so far exists as a concept design and aims to be realized as a research project in university using funds form the EXIST program and other interested research institutions as well as hospitals. The database is conceived to be a WIKI based platform, content will be supplied by manufacturers, hospitals and designers, nevertheless aims are to supply an initial baseline of medical device sound sets to show functionality and benefits for the users in the hospital and company context can draw from it. The paper will introduce basic elements of the database, a brief summary of the current situation and outlook on the possibilities when incorporating the database into a design process.","2008-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q8RG88NA","journalArticle","2014","Laffineur, Ludovic; Giot, Rudi; Commére, Louis","Audiovisual and Pedagogical Network Installation","","","","","http://hdl.handle.net/1853/52078","This paper presents an interactive installation designed to inform visitors of the network flow and the risks to connect their devices to any Wi-Fi hotspots. The system developed in C++ grabs packets using LibPCap, analyses them at low level (e.g., packet length) and also provides high-level information (e.g., port number). This new approach is based on the network flow analysis as well as on network services analysis. The software communicates with ChucK through the OSC protocol and is developed with the Open- Frameworks library in order to create unique visualisation. Users can actively take part to an interactive and didactic audiovisual exhibition system using their mobile device to send e-mails, listen to a web radio, surf on a website, read RSS feeds, in short, the experience begins once visitors exchange data with the network.","2014-06","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S7J7IMNC","book","2015","Yang, Jiajun; Hunt, Andy","Real-time sonification of biceps curl exercise using muscular activity and kinematics","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54152","In this research, we developed a real-time sonification system to be used in biceps curl. The sonification is generated using a parameter mapping method based on exercise information collected from a muscle sensor and Kinect camera. A crossover trial (AB-BA method) using biceps curl exercises was conducted, which included 14 healthy subjects equally assigned to two different groups. The first group started their sessions without any feedback then received sonification in the last sessions. The other group completed the sessions with the sonic feedback in the early stages. The experimental results show that the sonification worked well at portraying temporal information to help subjects improve the pacing of their movement. Results also show greater improvement in exercise metrics (greater average repetition range and total effort) when participants exercised with sonification, but not statistically significant. However, a significant result is that participants enjoyed the training more with the sonification than without. Positive comments were made on the sound feedback. The study demonstrates the potential for a real-time auditory feedback oriented training device to be used in fitness training or physical rehabilitation.","2015-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WHH7SCUR","book","2015","Dayé, Christian","Crafting sonifications: levels of interdisciplinary exchange in group discussions and their empirical assessment","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54104","This paper proposes a methodology to empirically assess the level of cross-disciplinary collaboration. Drawing on a variety of concepts established within Science & Technology Studies (STS), it develops an operationalization of a framework developed by Michael E. Gorman. The data this methodology relies on are quantified transcripts of discussions within groups trying to collaboratively develop sonifications of given scientific data sets. The proposed methodology has been applied to data from a sonification workshop to evaluate its usability, and some results are reported.","2015-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","Crafting sonifications","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LWYKZNEM","book","2015","Barrass, Stephen","Diagnostic singing bowls","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54173","The Hypertension singing bowl is a CAD object shaped by a year of blood pressure data, 3D printed in steel so it resonates when stuck or rung. But can blood pressure really be diagnosed by listening to singing bowls shaped by blood pressure datasets? This paper presents work in progress to answer this question.","2015-07","2023-07-13 06:25:49","2023-07-13 06:25:49","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y4K6WZCP","journalArticle","2005","Coleman, Graeme W.; Hand, Chris; Macaulay, Catriona; Newell, Alan F.","Approaches to auditory interface design - Lessons from computer games","","","","","http://hdl.handle.net/1853/50087","The computer game has begun to establish itself within the wider entertainment industry, and has thus attracted considerable interest from more general interaction designers. However, while computer game audio has become increasingly sophisticated, it remains a discipline largely overlooked by the research community. We begin by outlining similarities between each discipline, highlighting those which we believe provide interesting opportunities for designers of auditory interfaces. We also suggest that, through an understanding of the everyday practices of computer game sound designers and their colleagues within the industry, the process of sound design for alternative forms of interfaces can be considerably informed. To discover and understand some of these practices, we present our experiences conducting a field study using ethnographic methods with a major UK-based computer game developer. We highlight discoveries which we believe are pertinent for the design of auditory interfaces and thus merit further research. Our study forms part of our wider research to develop a grounded theory (i.e. a theory conceived via the data collected during the field study) to understand the reality of sound design within the computer games industry, relationships to the design of more general interfaces and thus how we approach the design of contemporary auditory interfaces.","2005-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZZE6XR5R","journalArticle","2004","Bandt, R.","Sounding public space: Sound artists in the public domain","","","","","http://hdl.handle.net/1853/50908","The right to quiet has been defined as a public commons (Franklin, 1993). Public space in Australia is becoming increasingly sound designed. This presentation investigates the variety of approaches by sound artists who have installed public space drawing on the three year ARC Australian Sound Design Project's research, website, http://www.sounddesign.unimelb.edu.au and public outreach Hearing Place. Current trends and practices will be compared and contrasted and conclusions drawn about the implications for Australia's soundscape in the future.","2004-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","Sounding public space","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GAVJRYQI","journalArticle","1997","Walker, Bruce N.; Ehrenstein, Addie","Congruency effects with dynamic auditory stimuli: design implications","","","","","http://hdl.handle.net/1853/50743","Since pitch is a commonly varied parameter in auditory displays, we investigated whether it is possible to attend to relative pitch while ignoring changes in pitch, and whether changes in pitch could be assessed independently of the overall pitch of a dynamic auditory stimulus. Stimuli were defined as either congruent (e.g., high pitch stimulus that became higher in pitch) or incongruent (e.g., high pitch stimulus that became lower in pitch). In this experiment, faster responses to congruent stimuli indicated a failure of selective attention. This effect was uniform for pitch judgments with all stimuli, but varied with the overall pitch for pitch-change judgments. The performance difference between congruent and incongruent trials was greatest for the extreme (high or low) stimuli. Moreover, pitch information intruded more into responses to pitch change than vice versa. Auditory display designers can use congruent stimuli to help distinguish between high and low pitches. If pitch change is the important dimension, designers should restrict the range over which stimulus pitches vary.","1997-11","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","Congruency effects with dynamic auditory stimuli","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8ESDCHJM","journalArticle","2013","Walsh, Riana; Walsh, Michael","Sonic Expressions Of Changes In The Percentage Cover Of Purple Moor-Grass, By Altitude, Under Hill Sheep Grazing In Western Ireland","","","","","http://hdl.handle.net/1853/51654","Preliminary investigations into the sonification of data representing the percentage cover of Purple Moor-grass as a function of altitude over a 14-year period from 1995-2008 are presented. The source of the vegetation data was the Teagasc Hill Sheep Farm in Leenaun, Co. Mayo, which was grazed at 0.8 ewes/ha/yr. Purple Moor-grass was the dominant vegetation species. It underwent several contrasting changes by altitude over the 14-year period. Sonification was applied to these changes in order to enhance the comprehension and interpretation of their graphical presentation for attendees at lectures/conferences who suffer from impaired visual but not acoustic acuity. Five auditory graphs were created, each representing unique changes in percentage cover of the vegetation, especially Purple Moor-grass by altitude. Each auditory graph was designed to convey an accurate representation of the data, highlighting important features. Visual graphs corresponding to the auditory graphs are also presented.","2013-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MRLU8HA9","journalArticle","2014","Choi, Insook","A Priori Attunement for Two Cases of Dynamical Systems","","","","","http://hdl.handle.net/1853/52079","An application of a tuning function adopts a space metaphor in scientific methods for representing state space of non-linear dynamical systems. To achieve an interactive exploration of the systems through sounds, attunement is defined as an a priori process for conditioning a playable space for an auditory display. To demonstrate this process, two cases of dynamical systems are presented. The first case employs Chua’s circuit, in which system parameters are defined as energy introduction to the system and energy governance within the system. The second case employs a swarm simulation, defined as a set of rules to dictate social agents’ behaviors. Both cases exhibit complex dynamics and emergent properties. The paper synthesizes a comparative review of auditory display for the two cases while defining playable space with generalizable tuning functions. The scope of the discussion focuses on the relationship between playable space as a canonical architecture for auditory display workflow and its realization through attunement in applications of dynamical systems.","2014-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6SK2GSTC","journalArticle","2008","Dombois, Florian","SONIFYER A Concept, a Software, a Platform","","","","","http://hdl.handle.net/1853/49949","In this article we would like to focus on two structural questions, that could be useful for improving the general acceptance of sonification research: On the one hand we are argueing for a new generation of sonification software that should be easy and intuitively to use for all kind of researchers. On the other hand our pledoyer goes for a sound forum of our sonification community to faster exchange and discuss the listenable results. For both aspects we describe our ideas and prototypes as a possible starting ground for discussion.","2008-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7B2CMHTY","journalArticle","2002","Menzies, D.","W-panning and o-format, tools for object spatialization","","","","","http://hdl.handle.net/1853/51395","Real acoustic objects have spatial width and characteristic radiation patterns. Techniques are described for efficiently synthesizing these qualities, by encoding with spherical harmonics. This approach naturally lends itself to Ambisonic reproduction, although it can be usefully applied to other forms of reproduction","2002-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3NRYNPU9","journalArticle","2019","Ziemer, Tim; Schultheis, Holger","Psychoacoustical signal processing for three-dimensional sonification","","","","","http://hdl.handle.net/1853/61499","Physical attributes of sound interact perceptually, which makes it challenging to present a large amount of information simultaneously via sonification, without confusing the user. This paper presents the theory and implementation of a psychoacoustic signal processing approach for three-dimensional sonification. The direction and distance along the dimensions are presented via multiple perceptually orthogonal sound attributes in one auditory stream. Further auditory streams represent additional elements, like axes and ticks. This paper describes the mathematical and psychoacoustical foundations and discusses the three-dimensional sonification for a guidance task. Formulas, graphics and demo videos are provided. To facilitate use at virtually all places the approach is mono-compatible and even works on budget loudspeakers.","2019-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FA7TNRQ7","journalArticle","1998","Cooley, Millicent","Sound + image in computer-based design: Learning from sound in the arts","","","","","http://hdl.handle.net/1853/50728","Sound is underutilized in software and on the web, in spite of its obvious value to other media, such as film. Many practitioners in computer-based design, particularly those with backgrounds in programming and print design, are simply unfamiliar with the medium of sound. The performing arts has a long history of creating sound which makes a powerful impression on human perception and emotion, and has accumulated a rich body of theories and practical insights for how this is done. These theories and insights should be explored for their usefulness in improving sound design in software. The purpose of the research discussed in this paper has been to learn from the principles and practices of sound design in the performing arts, and to discuss and demonstrate ways in which some of these ideas might be helpful to designers of computer-based media and software. This research considers performing arts theory with an emphasis on sound, validates some of this theory in the form of a series of interactive multimedia exercises, and describes commentary from performing arts professionals who discuss practical and theoretical issues in sound design from an experienced perspective. Games tend to make better use of sound than other computerbased products and, because of their narrative qualities, occupy a place in design somewhere between traditional performing arts and software. For these reasons, games have lessons to offer to other areas of computer-based design in terms of sound use, and some analysis of game design is included here, as well.","1998-11","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","Sound + image in computer-based design","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LEQ7L5DT","journalArticle","2013","Hammerschmidt, Jan; Tunnermann, Rene; Hermann, Thomas","Infodrops: sonification for enhanced Awareness of resource Consumption in the shower","","","","","http://hdl.handle.net/1853/51642","Although most of us strive to develop a sustainable and less resource-intensive behavior, this unfortunately is a difficult task, because often we are unaware of relevant information, or our focus of attention lies elsewhere. Based on this observation, we present a new approach for an unobtrusive and affective ambient auditory information display to become and stay aware of water and energy consumption while taking a shower. Using the interaction sound of waterdrops falling onto the bathtub as a carrier for information, our system supports users to be in touch with resource-related variables. We explore the usage of an affective dimension as an additional layer of information and introduce our 4/5-factor approach to adapt the auditory display’s output so that it supports a slow but steady adjustment of the personal showering habit over time. We present and discuss several alternative sound and interaction designs.","2013-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","Infodrops","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2KAYMM3S","journalArticle","2002","O'Sullivan, C.","A formulation of holophonic display","","","","","http://hdl.handle.net/1853/51339","This paper presents the results of a project that aims to develop an algorithm to synthesise a natural auditory phenomenon. The aspiration was the construction of a listener's holophonic experience of these auditory events, to develop a sonification that was both realistic and accurate, as a reflection of how we hear our own environment. The natural aural stimulation in the virtual environment is that of rain hitting leaves. This sound is synthesised and then processed to create a type of holophonic display so that physical, psychoacoustical and perceptual aspects of the spatial localisation are accounted for. Research that has been subsequently accomplished in the areas of digital waveguide techniques, head-related transfer functions (HRTFs), mathematics, signal processing and digital audio is assembled and applied to the formulation of this algorithm. An implementation in the Pd (Pure-Data) software package is also offered.","2002-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AZBLWI83","journalArticle","2001","Lorho, Gaetan; Marila, Juha; Hipakka, Jarmo","Feasibility of multiple non-speech sounds presentation using headphones","","","","","http://hdl.handle.net/1853/50619","This paper describes a study of listeners' ability to segregate spatially separated sources of non-speech sounds. Short sounds from musical instruments were played over headphones at different spatial positions using stereo panning or 3-D audio processing with Head-Related Transfer Functions. The number of sound positions was limited to five in this study. One, three or five sound items were played to the listener, multiple sounds being presented with four different onset times from simultaneous to successive replay. The subjects had to spatially discriminate one sound item, i.e. identify a given instrument and find its position. Performance was assessed by measure of response time and error-rate. A preference grading was also included in this test to compare the two headphone presentation techniques employed.","2001-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"73EG4RNB","journalArticle","2007","Reissel, L. M.; Pai, Dinesh K.","High-Resolution Analysis and Resynthesis of Environmental Impact Sounds","","","","","http://hdl.handle.net/1853/50001","Impact sounds produced by everyday objects are an important source of information about contact interactions in virtual environments and auditory displays. Impact signals also provide a rich class of real and synthetic percussive musical sounds. However, their perceptually acceptable resynthesis and modification requires accurate estimation of mode parameters, which has proved difficult using traditional methods. In this paper we describe some of the problems posed by impact phenomena when applying standard methods, and present a phase-constrained high-resolution algorithm which allows more accurate estimation of modes and amplitudes for impact signals. The phase-constrained algorithm is based on least squares estimation, with initial estimates obtained from a modified ESPRIT algorithm, and it produces better resynthesis results than previously used methods. We give examples with everyday object impact sounds.","2007-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2M27M5KR","journalArticle","2018","Ziemer, Tim; Schultheis, Holger","A psychoacoustic auditory display for navigation","","","","","http://hdl.handle.net/1853/60072","A psychoacoustic auditory display for navigation in two-dimensional space is presented. The auditory display is examined in an experiment with novice users. Trajectory analysis indicates that users were able to a) accurately find sonified targets b) analyze the sonification axis-by-axis c) integrate the sonified dimensions to approach the target on the shortest path. Techniques developed in this work appear to work equally well with three-dimensional coordinates.","2018-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"45FH52RM","journalArticle","2008","Gossmann, Joachim; West, Ruth; Hackbarth, Ben","Scalable Auditory Data Signatures for Discovery Oriented Browsing in an Expressive Context","","","","","http://hdl.handle.net/1853/49951","To be useful for browsing in vast multidimensional databases, auditory representations need to be able to scale in depth and detail. The concepts of quantitative and qualitative listening are presented and Scalable Auditory Data Signatures introduced conceptually. The implementation of these concepts within the interdisciplinary project “ATLAS in silico” is described.","2008-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"43SBIH5C","journalArticle","2003","Targett, Sue; Fernstrom, Mikael","Audio games: Fun for all? All for fun!","","","","","http://hdl.handle.net/1853/50444","In this paper we investigate if it is possible to create entertaining computer games that use only non-speech aural feedback and if such games could be used for skills acquisition or in therapeutic applications. To answer these questions we developed two computer games, Os & Xs (Tic Tac Toe) and Mastermind, representing all necessary information through auditory display. User testing confirmed that the games were playable and early indications are that the games can be entertaining, particularly for the blind community. Testing also suggested that playing audio games could assist in increasing both memory and ability to concentrate, thus showing potential for both skills acquisition and therapeutic applications.","2003-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","Audio games","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZLM3PKTP","journalArticle","2011","Lopez, Jose J.; Cobos, Maximo; Pueo, Basilio","Wafe-Field Synthesis: State of the Art and Future Applications","","","","","http://hdl.handle.net/1853/51923","Wave-Field Synthesis (WFS) has become one of the most promising spatial sound reproduction systems. The most basic difference of WFS in comparison to other available systems is that the acoustic field is accurately synthesized using loudspeaker arrays in a broad area, suppressing the sweet spot that characterizes conventional surround systems and giving an accurate and deep immersion for all the listeners. In this paper, a review of the main concepts related to WFS, from its fundamentals to the latest applications and developments is presented. The limitations and drawbacks of WFS are listed and succinctly described, giving also the main proposal to overcome these limitations. Among them, a solution to include elevation in WFS is presented and some recent techniques to perform stereo to WFS up-mixing are also commented. Finally, applications of WFS to different engineering areas such as immersive videoconference and auditory display systems are presented.","2011-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","Wafe-Field Synthesis","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9PGCTEJG","journalArticle","2004","Godfrey, Justin; Lindsay, Jeffrey; Walker, Bruce N.","The audio abacus: Representing a wide range of values with accuracy and precision","","","","","http://hdl.handle.net/1853/51318","Point estimation is a relatively unexplored facet of sonfication. We present a new computer application, the Audio Abacus, designed to transform numbers into tones following the analogy of an abacus. As this is an entirely novel approach to sonifying exact data values, we have begun a systematic line of investigation into the application settings that work most effectively. Results are presented for an initial study. Users were able to perform relatively well with very little practice or training, boding well for this type of display. Further investigations are planned.","2004-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","The audio abacus","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2QL6GW25","journalArticle","2012","Bretan, Mason; Weinberg, Gil; Freeman, Jason","Sonification for the Installation Drawn Together","","","2168-5126","","http://hdl.handle.net/1853/44410","This extended abstract describes Drawn Together, an interactive art installation in which a person takes turns drawing with a computer. We describe the process of the interaction and the methods used to creatively sonify the process and the animations. There are three main states in the interactive process that are sonically represented using audio samples in a mix of background and foreground sounds. The lines drawn by the computer are sonified using a set of features describing length, rate of time drawn, location, and curviness.","2012-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M5P53FFF","journalArticle","2007","Droumeva, Milena; de Castell, Suzanne; Wakkary, Ron","Investigating Sound Intensity Gradients as Feedback for Embodied Learning","","","","","http://hdl.handle.net/1853/50003","This paper explores an intensity-based approach to sound feedback in systems for embodied learning. We describe a theoretical framework, design guidelines, and the implementation of and results from an informant workshop. The specific context of embodied activity is considered in light of the challenges of designing meaningful sound feedback, and a design approach is shown to be a generative way of uncovering significant sound design patterns. The exploratory workshop offers preliminary directions and design guidelines for using intensity-based ambient sound display in interactive learning environments. The value of this research is in its contribution towards the development of a cohesive and ecologically valid model for using audio feedback in systems, which can guide embodied interaction. The approach presented here suggests ways that multi-modal auditory feedback can support interactive collaborative learning and problem solving.","2007-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FJ8EP8YG","book","2015","Wersényi, György; Nagy, Hunor; Csapó, Ádám","Evaluation of reaction times to sound stimuli on mobile devices","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54148","When developing new devices for assistive technology, it is important to consider auditory response times to different kinds of warning and navigational sounds. Perception, processing and action following the presentation of auditory stimuli depends on several parameters, the most important being the stimuli themselves and the method used for providing feedback. With the growing market penetration of mobile devices (smartphones, tablets etc.) and increasing popularity of crowdsourced solutions, we have chosen to develop a mobile application for the measurement of reaction times with respect to a wide range of stimuli, including sine tones, speech and various kinds of clicks and noises. During tests, participants are asked to indicate the direction of sound samples by pressing the appropriate button on the touch screen. Stereo panning can be used up to five directions. In this paper, our goal is to demonstrate the viability of this approach through a set of basic (at this time, not yet crowdsourced) tests performed using the application. A rudimentary statistical evaluation of measured response times and success rates was performed. Results were compared to an earlier study using similar categories of stimuli. As in that study, some relative differences between the stimuli types were found, i.e. the 1 kHz panned sine and pink noise categories were shown to be somewhat more favorable than speech and click-trains. Future enhancements to the application will include tilt-based input control – allowing for the participation of visually impaired test subjects who cannot see the response buttons – as well as extensions allowing for the logging and analysis of large-scale crowdsourced test results.","2015-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q4ZJEBMU","journalArticle","2021","Lindborg, PerMagnus","Feeling Loki's pain: Designing and evaluating a DIY 3D auditory display for geodata sonification","","","","","http://hdl.handle.net/1853/66341","Loki's Pain is an immersive 3D audio installation artwork, a sonification of seismic activity. Visitors take the place of Loki, who was punished by the gods and caused earthquakes. We designed an auditory display in the shape of a hemidodecahedron and built a prototype with a low-budget, DIY approach. Seismic data were retrieved from the Internet. Location, magnitude, and epicentre depth of hundreds of recent earthquakes were sonified with physical modelling synthesis into a 10-minute piece. The visitor experience was evaluated in a listening experiment (N = 7), comparing the installation with a version for headphones. Differences on eight semantic scales were small. A content analysis of focus group discussions nuanced the investigated topics, and qualitative interpretation strengthened the quantitative findings. Verbal expressions of immersivity were stronger in the installation, which stimulated longer and more detailed responses. Aspects such as audio quality, the structure's physical-visual shape, and multisensorial design evoked both positive and negative emotions, and elicited imagination and memory recall. However, the assumed capacity of the LOKI structure to stimulate a richer social experience than that of headphone listening was not supported by the responses in this study","2021-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","Feeling Loki's pain","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WPWQWKWW","journalArticle","1997","Mitsopoulos, Evangelos N.; Edwards, Alistair D. N.","Auditory scene analysis as the basis for designing auditory widgets","","","","","http://hdl.handle.net/1853/50739","This paper presents a methodology for the design of fast-rate auditory presentations based on the Auditory Scene Analysis of Bregman. The auditory scene is hierarchically organized and based on the concept of auditory streams described in terms of two types of structures, one across streams (at an instant) and one within each stream (over time). Each stream constitutes a perceptual entity on which attention can be focused. Integrating information across streams requires effort and practice because the auditory system is inclined to derive properties such as temporal order or rhythm on a within-stream basis. The methodology distinguishes between the structure of the scene and the physical dimensions of sound used to produce this structure. Task performance may be critically affected by the structure of the scene. For this reason the structure is designed with respect to the tasks to be supported. When the structural aspects of the auditory scene have been defined, the physical dimensions of sounds are selected so as to provide the desired structure.","1997-11","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DKTJQCLD","book","2009","Ramakrishnan, Chandrasekhar; Greenwood, Steven","Entropy sonification","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51298","We present entropy sonification, a technique to bring interesting data to the foreground of a sonification and push uninteresting data into the background. To achieve this, the data is modeled as an in- formation source, and the underlying sonification is converted into sound grains. Information-theoretic attributes of the data are used to determine the amplitude envelope and duration of the grains. The information source model adds an additional avenue for con- trol. By altering the information source model, one can focus on different aspects of the data via entropy zooming.","2009-05","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"64MPDV8U","journalArticle","1996","Barrass, Stephen","TaDa! Demonstrations of auditory information design","","","","","http://hdl.handle.net/1853/50817","The prospect of computer applications making ""noises"" is disconcerting to some. Yet the soundscape of the real world does not usually bother us. Perhaps we only notice a nuisance? Sounds can support information-processing activities by providing information that is useful and relevant. The TaDa method focuses on designing an auditory representation to meet information requirements, so that the sounds are information rather than ""noise."" The design process integrates task analysis, a database of sound examples, a rule-based design aid, and interactive sound design tools. The method and tools are demonstrated in scenarios from mining exploration, resource management, and climatology. The multimedia interfaces that were implemented show that sounds can provide information that is difficult to obtain visually, and can improve the directness and usefulness of an information display.","1996-11","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UJR26GNF","journalArticle","2007","Ibrahim, Asri; Hunt, Andy","Systematic Usability Inspection Approach for Sonification Applications","","","","","http://hdl.handle.net/1853/50033","In previously reported research, most sonification designers have needed to develop at least a working prototype for user testing. The results are interpreted and analysed to look for possible problems and solutions to further improve the design. This paper introduces a new systematic usability inspection approach for the design of sonification applications design before they go to the initial development phase. This process gives an alternative for designers to evaluate their design, detect possible problems and improve the design before they start developing it. It uses two of our models - the Sonification Application (SA) model and the User Interpretation Construction (UIC) model. In this paper we discuss the steps of this process, which include preparing inspection materials, implementing inspection and managing the results.","2007-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GM52ZNXT","journalArticle","2018","Lenzi, Sara; Gleria, Francesca","Humanising data through sound: Res Extensae and a user-centric approach to data sonification","","","","","http://hdl.handle.net/1853/60068","In this paper, starting from a case study (the mixed-media data sonification installation Res Extensae), we discuss a number of assumptions on the efficacy of sound as a means to represent and communicate numerical data. The discussion is supported by the results of a questionnaire aimed at validating our assumptions and conducted with fifteen of the participants to the experience. At the same time, we have the ambition to contribute to a wider debate on the value of data sonification. We introduce the first stage of a research on sonification as a design-driven, user-centred and multi-modal experience, in that closer to data design practices rather than to traditional composition and computer music. We describe the usage of physical objects to help users to put sounds and data into a wider context, improving the user experience and facilitating the comprehension and retention of the meaning of data.","2018-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","Humanising data through sound","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DVFC4RGA","journalArticle","2019","Bukvic, Ivica Ico; Earle, Gregory; Sardana, Disha; Joo, Woohun","Studies in spatial aural perception: establishing foundations for immersive sonification","","","","","http://hdl.handle.net/1853/61498","The Spatial Audio Data Immersive Experience (SADIE) project aims to identify new foundational relationships pertaining to human spatial aural perception, and to validate existing relationships. Our infrastructure consists of an intuitive interaction interface, an immersive exocentric sonification environment, and a layer-based amplitude-panning algorithm. Here we highlight the systemﾒs unique capabilities and provide findings from an initial externally funded study that focuses on the assessment of human aural spatial perception capacity. When compared to the existing body of literature focusing on egocentric spatial perception, our data show that an immersive exocentric environment enhances spatial perception, and that the physical implementation using high density loudspeaker arrays enables significantly improved spatial perception accuracy relative to the egocentric and virtual binaural approaches. The preliminary observations suggest that human spatial aural perception capacity in real-world-like immersive exocentric environments that allow for head and body movement is significantly greater than in egocentric scenarios where head and body movement is restricted. Therefore, in the design of immersive auditory displays, the use of immersive exocentric environments is advised. Further, our data identify a significant gap between physical and virtual human spatial aural perception accuracy, which suggests that further development of virtual aural immersion may be necessary before such an approach may be seen as a viable alternative.","2019-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","Studies in spatial aural perception","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MXH4I9ZY","journalArticle","2011","Schaffert, Nina; Mattes, Klaus; Effenberg, Alfred O.","The sound of Rowing Stroke Cycles as Acoustic Feedback","","","","","http://hdl.handle.net/1853/51919","Acoustic feedback offers promising opportunities to enhance the perception of athletes in regards of the modification of movement patterns and control in technique training. Sound conveys time-critical structures that are perceived subliminally, which is of crucial importance for the precision when modifying movements to improve their execution. Technological advances allow the design of innovative feedback systems to communicate information audibly to athletes. This paper describes a concept for providing acoustic feedback online during on-water training sessions to elite rowers with the final aim to improve the mean boat velocity by a reduction of intracyclic interruptions in the boat acceleration. Following the initial analysis of technical and biomechanical requirements, the acoustic feedback system Sofirow was designed and field-tested with elite athletes. This rowing specific training system presents the boat acceleration-time trace audibly and online to athletes and coaches. The results showed a significant increase in the mean boat velocity when the acoustic feedback was used compared to sections without. It thus seems promising to implement acoustic feedback regularly into training processes for elite athletes. A behavioural dynamics approach was recommended to provide a theoretical basis for this concept.","2011-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F73AZPG6","book","2015","Newbold, Joseph W.; Hunt, Andy; Brereton, Jude","Chemical spectral analysis through sonification","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54197","Chemical spectra are an important part of how research chemists analyse the outcomes of experiments. However these complex spectra can be very difficult and time consuming to analyse. This paper outlines an investigation into using sonification to improve the understanding and ease of analysis of chemical spectral data. The project specifically uses sonification techniques to display Nuclear Magnetic Resonance (NMR) spectra. Two sonification methods were designed to offer different perspectives on the data; “Spectral Audification” allows a quick overview of the data while maintaining its subtleties whereas a simple parameter mapping method allows more in-depth analysis of the spectra such as the use of rhythmic patterns to make sets of peaks easily identifiable","2015-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QLF46GVI","journalArticle","2003","Hoffmann, Heiko; Dachselt, Raimund; Meissner, Klaus","An independent declarative 3D audio format on the basis of XML","","","","","http://hdl.handle.net/1853/50434","This paper describes the development of an XML-based format called Audio3D for the declarative description of acoustic environments and sound sources for 3D auditory displays by an audio designer without the need of programming efforts. The format is platform and API independent and suitable for realtime and offline sound rendering. It can be used together with other XML-formats for 3D graphics such as X3D and is based on the concept of a hierarchical scene graph. Acoustic environments can be described in any level of detail using reflecting and absorbing surfaces or reverberation parameters for an abstract representation of multiple acoustic rooms.","2003-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8K9W8Z28","book","2010","Flint, Tom; Turner, Phil; Leplâtre, Gregory; McGregor, Iain","Soundscape Mapping: A Tool for Evaluating Sounds and Auditory Environments","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/50060","This paper describes a soundscape mapping tool, and provides an illustration of its use in the evaluation of an in-car auditory interface. The tool addresses three areas: communicating what people are listening to, showing how soundscapes can be visualized, and demonstrating how the approach can be used by a designer during the evaluation of an auditory display. The strengths and limitations of this approach are discussed and future work identified.","2010-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","Soundscape Mapping","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N93EY3U4","book","2010","Fink, Alex; Mechtley, Brandon; Wichern, Gordon; Liu, Jinru; Thornburg, Harvey; Spanias, Andreas; Coleman, Grisha","Re-Sonification of Geographic Sound Activity using Acoustic, Semantic and Social Information","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49909","Sonic representations of spaces have emerged as a means to capture and present the activity that conventional representations, such as maps, do not encapsulate. Therefore, to convey the activity information of regions, both large and small, we use sounds and information provided by regional communities in the automated design of soundscapes to re-sonify geographic sound activity. To quantify this community knowledge, we have developed an ontological framework to determine the importance of sound and concepts to one another using acoustic, semantic, and social information. This framework is then used in the automated design of a generative soundscape model purposed to identify and re-sonify sounds that impart relevant information about a geographic region. Furthermore, we are developing a social networking website to facilitate the collection and re-sonification of sounds and data.","2010-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TR8XZ8I2","journalArticle","2021","Kalonaris, Stefano; Zannos, Iannis","High-order surrogacy for the audiovisual display of dance","","","","","http://hdl.handle.net/1853/66319","The current pandemic (COVID-19) has had considerable impact on many fronts, not least on the physical presence of humans, affecting how we relate to one another and to the natural environment. To investigate these two interactions, the notion of surrogacy, originally described by Smalley as remoteness between source and sonic gesture, is considered and extended to include bodily gesture, for the rendering of contemporary dance performances into abstract audiovisual compositions/objects. To this end, for a given dance performance, sonification of the motion capture data is combined with video-frame processing of the video recording. In this study, we focus on higher order surrogacy and associate this with 1) a soundscape ecology-inspired approach to sonification, whereby three species of sounds coexist and adapt in the environment according to the symbiotic paradigm of mutualism, and 2) a wave space method to sonify their coevolution. Aesthetic implications of this procedure in the context of multimodal, telematic/remote and virtual systems are discussed as disembodied presence emerges as a dominant trope in our daily experience.","2021-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NQ8IJJJT","journalArticle","2016","Romigh, Griffin D.; Simpson, Brian D.; Iyer, Nandini","In Ear to Out There: A Magnitude Based Parameterization Scheme for Sound Source Externalization","","","","","http://hdl.handle.net/1853/56582","While several potential auditory cues responsible for sound source externalization have been identified, less work has gone into providing a simple and robust way of manipulating perceived externalization. The current work describes a simple approach for parametrically modifying individualized head-related transfer function spectra that results in a systematic change in the perceived externalization of a sound source. Methods and results from a subjective evaluation validating the technique are presented, and further discussion relates the current method to previously identified cues for auditory distance perception.","2016-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","In Ear to Out There","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4YKX5AGL","journalArticle","2007","Brewer, Scott","Creating a Virtual Suikinkutsu","","","","","http://hdl.handle.net/1853/49985","This paper describes the process undertaken to construct a virtual suikinkutsu through sound synthesis. Firstly a description is given of a physical suikinkutsu and its inherent unique sound qualities. The suikinkutsu's physical qualities provide a model for the characteristics required for use by the virtual suikinkutsu. A brief discussion of related works which will aid in informing the virtual suikinkutsu is given. The finished virtual model is described and a comparison is undertaken between recordings made on the virtual suikinkutsu and recordings taken of a physical suikinkutsu. Finally a look into future work on the model is undertaken.","2007-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YACT7P8N","book","2009","Frissen, Iljia; Katz, Brian F. G.; Gustavino, Catherine","Perception of reverberation in large single and coupled volumes","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51408","The aim of the presented research is to quantify how sensitive the human ear is to subtle changes in reverberation. We quantified the discrimination thresholds for reverberations that are representative for large rooms such as concert halls (reverberation times around 1.8 s). For exponential decays, simulating an ideal simple room, thresholds are around 6% (Experiment 1). We found no difference in thresholds between a short noise burst and a male voice spoken word, suggesting that discrimination is not dependent on the type, or spectral content, of the sound source (Experiment 2). In a final experiment we matched coupled room, non-exponential decay stimuli to exponential ones, and vice versa, in an attempt to quantify the complex former in terms of the simpler latter.","2009-05","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RHBCYB8S","book","2010","Windt, Katja; Iber, Michael; Klein, Julian","Grooving Factory - Bottleneck Control in Production Logistics through Auditory Display","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49888","Grooving Factory is the name of an interdisciplinary research project in the fields of production logistics engineering and auditory display. It aims to reveal bottlenecks in industrial productions and to improve the achievement of logistic targets by using sonification in production planning and control (PPC). Since data sets derived from production processes are time related, processes in production can be displayed as oscillating complex sounds, e.g. via additive synthesis. In this study, the feedback data of operations at 33 workstations of a circuit board manufactory served as a model for auditory display. The workload of the workstations was compared to their actual performance, which indicated their work in process (WIP) level, i.e. the balance of their input and output. The results of the auditory display were compared to WIP related bottlenecks identified by the bottleneck oriented logistic analysis including logistic operating curves. The research project includes the development of a new PPC method realized as a prototype in a software tool.","2010-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3KK9VAXG","journalArticle","1997","Darvishi, Alireza","A visual user interface for creation and manipulation of auditory scenes","","","","","http://hdl.handle.net/1853/50738","This paper describes a software prototype which allows visual composition and manipulation of everyday sounds based on a suggested concept called ""Auditory Scenes"" [3]. The description of user interface components in the Java software prototype is the main topic of this paper. The concept of ""Auditory Scenes"" assumes various perceptual attributes for each individual sound in the scene as well as the temporal, spatial and other relationships between them. An introduction to the suggested concept ""Auditory Scenes"" is given first. Various components of the implemented user interface and a conclusion are discussed in the following section.","1997-11","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T2J8HE76","journalArticle","2007","Usher, John; Martens, William L.","Perceived Naturalness of Speech Sounds Presented Using Personalized Versus Non-Personalized HRTFs","","","","","http://hdl.handle.net/1853/50025","Speech sound sources were spatially processed using measured HRTF data that were obtained from nine individuals. The speech signals were auditioned via headphones by two groups of listeners via a paired comparison task in which listeners were asked to judge which of two stimuli sounded more natural. One group of listeners was composed of those whose HRTFs had been used to create subsets of the stimuli that were presented, while a second group of listeners were never presented with stimuli that were processed using their own HRTFs. Results from the first group showed that stimuli generated using an individual's own HRTFs will not necessarily be judged as more natural than those generated using HRTF data from other individuals. However, this was not because one set of HRTF data gave the most natural listening experience for all listeners, since the stimulus ranked highest differed between individuals. An analysis of the Interaural Level Difference (ILD) showed that the frequency dependence of ILD for an individual's HRTFs was quite similar to that of the HRTFs that produced for them an auditory image that was ranked as the most natural sounding. The results suggest that the interaural spectral difference presented via HRTF-based processing can affect perceived naturalness as strongly as the overall spectral shape that is related to source tone coloration.","2007-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IV4G2QUJ","journalArticle","2019","Ismailogullari, Abdullah; Ziemer, Tim","Soundscape clock: Soundscape compositions that display the time of day","","","","","http://hdl.handle.net/1853/61510","This paper presents an ambient auditory display that communicates the time of day. Four soundscapes represent different quadrants of the clock. Auditory icons divide the quadrants into three parts that represent hours, and four partitions that represent every quarter of an hour. The auditory display is little intrusive and only informative to those who are privy to its principles. Suitable application areas are offices where staff can derive the time from the soundscape, while customers stay unaware and may only enjoy the calm, auditory nature scene. To experience the calm ambient character of the auditory display we suggest you to play the demo while reading the paper: https://tinyurl.com/y4yd8zkh .","2019-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","Soundscape clock","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZU9XUZQV","journalArticle","2011","Moulster, Andrew; Stockman, Tony","On the Road to Design: Developing a Sonified Route Navigator for Cyclists","","","","","http://hdl.handle.net/1853/51704","This paper describes the development of a system that uses sonification to assist cyclist’s navigation. The focus of the paper is on the design process, the decisions made and the methods used to make design choices. An important aspect was the use of listening tests undertaken by potential users of the system while cycling in order to obtain data used to underpin key design decisions regarding the timing and representation of route elements. The architecture and usage of the developed system is described, as well as details of on and off road evaluations of the system.","2011-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","On the Road to Design","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J6A3JPIW","journalArticle","2001","Larsson, Pontus; Vastfjall, Daniel; Kleiner, Mendel","Ecological acoustics and the multi-modal perception of rooms: Real and unreal experiences of auditory-visual virtual environments","","","","","http://hdl.handle.net/1853/50616","An ecological approach to multimodal perception of virtual environments suggests that different perceptual mechanisms should cooperate in forming an impression of the complex surrounding. Traditionally, Virtual Environments (VE's) has primarily been developed for the visual modality. It is hypothesized that multi-modal stimulation in VE's raises the experience of presence perceived by the user. Furthermore, it is believed that auditory cues also can improve memory. In Experiment 1, 40 subjects were assigned either to a unimodal (vision only) or bimodal (vision and hearing) virtual environment. The subjects had two memory- and navigation tasks, one where auditory cues had no apparent connection to visual information and one where auditory and visual cues carried similar information. Completion time for both tasks was measured. Statistical analysis showed as expected that no improvement of memory occurred for the unrelated task, while the auditory information yielded a significant effect in the second memory task. Ratings showed that subjects in the bimodal condition experienced significantly higher presence, were more focused on the situation and enjoyed the VE more than subjects receiving unimodal information did. Experiment 2 tested the hypothesis that varying degrees of visual realism would affect judgments of aural room qualities in a betweensubjects design using 80 undergraduates. The results suggested that auditory stimuli in virtual environments can serve both as an information-carrying channel as well as way to improve the experience of presence in a VE and that memory performance may serve as a measure of presence in VE's.","2001-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","Ecological acoustics and the multi-modal perception of rooms","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U834NEFG","journalArticle","2004","Barrass, S.; Gardner, H.; Jacob, T.; Sood, G.; Sheridan, J.","Soundstudio4D - A VR interface for gestural composition of spatial soundscapes","","","","","http://hdl.handle.net/1853/50914","We describe a software system which enables computergenerated soundscapes to be synthesised, spatialised and edited using a gestural interface. Iterative design and testing of the software interface has taken place in a walk-in, immersive, virtual-reality theatre. Sound spatialisation has been implemented for an 8-speaker array using a Vector Base Amplitude Planning algorithm. The software has been written in Java, JSyn and Java3D with native method calls to sound-cards and sensors.","2004-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F9USUMML","book","2010","Sadikali, Akil; Boyd, Jeffrey","Rhythmic Gait Signature from Video without Motion Capture","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49903","The goal of gait biometrics is usually to identify individual people from a distance, often without their knowledge. As such, gait biometrics provide a source of data that ties a visible pattern of motion to an individual. We describe our work to convert one particular biometric gait signature into a rhythmic sound pattern that is unique for different individuals. We begin with a camera viewing a person walking on a treadmill, then extract a phase configuration that describes the timing pattern of motions in the gait. The timing pattern is then converted to a rhythmic percussion pattern that allows one to hear differences and similarities across a population of gaits. We can also hear phase patterns in a gait independent of the actual frequency of the gait. Our approach avoids the inconvenience and cost of traditional motion capture methods. We demonstrate our system with the sonification of 25 gaits from the CMU Motion of Body database","2010-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D3H5I6SS","journalArticle","2017","Shortridge, Woodbury; Gable, Thomas M.; Noah, Brittany E.; Walker, Bruce N.","Auditory and Head-Up Displays for Eco-Driving Interfaces","","","","","http://hdl.handle.net/1853/58360","Eco-driving describes a strategy for operating a vehicle in a fuel-efficient manner. Current research shows that visual ecodriving interfaces can reduce fuel consumption by shaping motorists’ driving behavior but may hinder safe driving performance. The present study aimed to generate insights and direction for design iterations of auditory eco-driving displays and a potential matching head-up visual display to minimize the negative effects of using purely visual headdown eco-driving displays. Experiment 1 used a sound cardsorting task to establish mapping, scaling, and polarity of acoustic parameters for auditory eco-driving interfaces. Surveys following each sorting task determined preferences for the auditory display types. Experiment 2 was a sorting task to investigate design parameters of visual icons that are to be paired with these auditory displays. Surveys following each task revealed preferences for the displays. The results facilitated the design of intuitive interface prototypes for an auditory and matching head-up eco-driving display that can be compared to each other.","2017-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9QLBGFX7","journalArticle","2016","Yamauchi, Takuya","Designing Sound Representations for Responsive Environments","","","","","http://hdl.handle.net/1853/56561","In this paper, we demonstrate a responsive sound installation consisting of computer-linked thermometers and cameras installed in both interior and exterior locations that detect the states of these spaces based on image and temperature data. The system simultaneously produces and modifies sound pictograms in different spaces in order to convey information on motion or temperature changes. We also propose a sound design method that represents sounds made by physical objects using the above-described installation and sound design method.","2016-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XPGU2PL7","journalArticle","2019","Adhitya, Sara","London bus tunes: Using sound to improve the safe navigation of London's bus system","","","","","http://hdl.handle.net/1853/61540","This work-in-progress introduces a proposal to incorporate sound in the passenger navigation system of the London Bus. First, we present the problems of accessibility concerning Londonﾒs complex bus system. Then, we introduce our proposal of using sound and sonification in particular to aid in the navigation of Londonﾒs bus system. We explain our sonification strategy and describe a recent preliminary trial of our sonification prototype, implemented as an installation during an accessibility event held by Transport for London at the ExCel centre in London on 19 March 2019. We discuss the feedback obtained from this trial and conclude with proposed future work in terms of both the development of our sonification strategy as well as its implementation in Londonﾒs public transport system.","2019-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","London bus tunes","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CU8SNVPN","journalArticle","2003","Cohen, Michael; Kawaguchi, Makoto","Narrowcasting operations for mobile phone CVE chatspace avatars","","","","","http://hdl.handle.net/1853/50474","We have developed an interface for narrowcasting (selection) functions for a networked mobile device deployed in a collaborative virtual environment (CVE). Featuring a variable number of icons in a “2.5D” application, the interface can be used to control motion, sensitivity, and audibility of avatars in a teleconference or chatspace. The interface is integrated with other CVE clients through a “servent” (server/client hybrid) HTTP TCP/IP gateway, and interoperates with a heterogeneous groupware suite to interact with other clients, including stereographic panoramic browsers and spatial audio backends and speaker arrays. Novel features include mnemonic conferencing selection function keypad operations, multiply encoded graphical display of such non-mutually exclusive attributes, and explicit multipresence features.","2003-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9R63EZDL","journalArticle","2002","Tsuchida, Y.","Review of aspects of auditory signal studies in Japan","","","","","http://hdl.handle.net/1853/51375","Sound is very important in human communication. It evokes an attention and conveys much information even if a listener does not pay attention to the signal. These are very big advantages for information transmission. Information transmission that uses sound in addition to a speech have similar characteristics. Therefore, a lot of auditory auditory signal sounds other than language, such as alarms and warning signals are used frequently. Social conditions surrounding auditory signals and recent researchs are reviewed here.","2002-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7ZRYD8UB","journalArticle","2007","Bliss, James P.; Spain, Randall D.","Sonification and Reliability - Implications for Signal Design","","","","","http://hdl.handle.net/1853/50028","Sonifications of complex data streams represent a new way for task designers to convey important information to task operators. In recent years, researchers have applied sonification technology in a variety of task domains, including medical device monitoring, complex task instruction, and visualization of data streams and sets. The use of sonifications as emergency signals has been suggested as a way to convey continuous task state information to operators. However, researchers have focused mostly on acoustic properties of sonifications, and have not considered operator trust of them. Past research has shown predictable operator trust-driven reactions to conventional alarms. It is necessary to extend such investigations to sonifications, so that designers may know whether sonifications might represent a technological solution to foster more rapid and appropriate real-time trust calibration by task operators. In this paper, we describe prior research with alarm mistrust, and highlight the potential benefits of further research combining signal reliability and sonification.","2007-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9I7AD83G","journalArticle","2011","Gomez, Juan Diego; Bologna, Guido; Deville, Benolt; Pun, Thierry","Multisource Sonification for Visual Substitution in an Auditory Memory Game: One, or Two Fingers?","","","","","http://hdl.handle.net/1853/51702","The See ColOr project aims at developing a mobility system for blind persons based on image color sonification. Within this project the present work addresses the optimal use of auditory-multi-touch interaction, and in particular the matter of the number of fingers needed for efficient exploration. To determine the actual significance of mono and multi-touch interaction onto the auditory feedback, a color matching memory game was implemented. Sounds of this game were generated by touching a tablet with one or two fingers. A group of 20 blindfolded users was tasked to find color matches into an image grid represented on the tablet by listening to their associated color-sound representation. Our results show that for an easy task aiming at matching few objects, the use of two fingers is moderately more efficient than the use of one finger. Whereas, against our intuition, this cannot be statistically confirmed in the case of similar tasks of increasing difficulty.","2011-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","Multisource Sonification for Visual Substitution in an Auditory Memory Game","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NI58PFHH","journalArticle","2005","Brock, Derek; Ballas, James A.; McFarlane, Daniel C.","Encoding urgency in legacy audio alerting systems","","","","","http://hdl.handle.net/1853/50106","Despite ongoing modernization efforts, the U.S. Navy expects that it will continue to make highly effective use of legacy systems for many years to come. This and a mandate to maintain fully mission capable platforms have made the service slow to place new audio alerting technologies in command and control environments, despite their demonstrated effectiveness in the laboratory and elsewhere. However, recent upgrade programs for decision support systems have brought with them opportunities to revise and improve standing audio alert techniques. In this paper, the authors describe how the legacy audio component of a Navy decision support workdesk was revised to encode appropriate levels of urgency for incoming action and information alerts. The preliminary design process and issues germane to it are discussed, and the results of an empirical design study are presented. In addition, the implemented solution and the results of a subsequent empirical evaluation are briefly described and discussed.","2005-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZTQH5IYC","journalArticle","2019","Phillips, Sean; Cabrera, Andres","Sonification workstation","","","","","http://hdl.handle.net/1853/61529","Sonification Workstation is an open-source application for general sonification tasks, designed with ease-of-use and wide applicability in mind. Intended to foster adoption of sonification across disciplines, and increase experimentation with sonification by non-specialists, Sonification Workstation distills tasks useful in sonification and encapsulates them in a single software environment. The novel interface combines familiar modes of navigation from Digital Audio Workstations, with a highly simplified patcher interface for creating the sonification scheme. Further, the software associates methods of sonification with the data they sonify, in session files, which will make sharing and reproducing sonifications easier. It is posited that facilitating experimentation by non-specialists will increase the potential growth of sonification into fresh territory, encourage discussion of sonification techniques and uses, and create a larger pool of ideas to draw from in advancing the field of sonification. Source code is available at https://github.com/Cherdyakov/sonificationworkstation. Binaries for macOS and Windows, as well as sample content, are available at http://sonificationworkstation.org.","2019-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TKPD2R9C","journalArticle","2003","Barrass, Stephen","Sonification design patterns","","","","","http://hdl.handle.net/1853/50483","Most product designers have little or no experience with sonifications. Designers from a range of different domains use a common method called Design Patterns to describe “solutions to problems in context” in a way that can be readily understood and reused. Design Patterns may provide a way to communicate sonification research results with product designers and other design communities. I have written a handful of prototype Sonification Design Patterns from papers in the ICAD 2002 proceedings. The papers I selected had clear statements of hypotheses, results to support them, and repeated examples elsewhere in the proceedings. These Patterns are now on the SonificationDesignPatterns site on the WikiWeb and can be edited and added to using any internet browser. The lively development of SonificationDesignPatterns by the ICAD community may help build sonification-specific vocabulary, identify sonification hypotheses, and allow product designers to pick up and apply our research.","2003-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D4K7ZECL","journalArticle","2004","O'Dwyer, M. F.; Potard, G.; Burnett, I.","A 16-speaker audio visual display interface and control system","","","","","http://hdl.handle.net/1853/50781","This paper details the CHESS system developed at the University of Wollongong. CHESS aims to provide a hardware and software platform for the creation, manipulation and playback of complex three-dimensional sound scenes. Ambisonic techniques are used to render a virtual sound scene on sixteen speakers arranged hemispherically around a user. A 3D visual representation of the scene is provided, which may be viewed on one or more display devices, including a virtual reality headset. User input may be provided via a 3D glove. These elements are combined to produce highly configurable immersive audio-visual applications with true three-dimensional audio. A control system has been developed for this system, which allows users to easily create complicated applications. We conclude by considering an example application of the system: cockpit and air traffic control systems.","2004-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8S3HNAA6","journalArticle","2008","Rinott, Michal","The Laughing Swing: Interacting with Non-Verbal Human Voice","","","","","http://hdl.handle.net/1853/49964","This paper looks at non-speech uses of the human voice in interactive objects. A collection of projects using non-verbal voice, as input and as output, is briefly reviewed. The Laughing Swing - an interactive object using non-verbal voice as output, created by the author and associates - is described in terms of motivations, sound design, sonic behavior implementation and user responses. The significance and potential of interactions with non verbal voice is discussed.","2008-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","The Laughing Swing","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5G2H5DKH","journalArticle","2021","Peng, Tristan; Choi, Hongchan","SIREN: A case study in web audio based sonification","","","","","http://hdl.handle.net/1853/66345","SIREN is an open-source, web-based sonification workstation that provides an accessible entry point to data mapping sonification and aims to demonstrate a use case for the Web Audio API. With plug-and-play functionality, and numerous methods to customize and create meaningful auditory display, SIREN provides useful features for pedagogy, methods for exploratory data sonification, and an extensible, open-ended development platform. Inspired by common digital audio workstation (DAW) workflows, SIREN provides a familiar and intuitive layout based upon data matrices as tracks that can be chained together as channels, thus allowing values in one data sequence to control a parameter of another data array. This paper describes the application's design philosophy and provides a case study as usage examples. We place the program in a comparative context with other data-mapping front-ends, and describe future goals.","2021-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","SIREN","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RP3WBCBY","journalArticle","2002","Maeda, K.; Matsuo, K.; Matsumoto, Shinji; Saito, Y.","Example design process for audio signals in a digital camera","","","","","http://hdl.handle.net/1853/51353","Design guidelines were proposed to provide a general workflow for the creation of audio signals for digital cameras, which recently continue to diversify. The steps for the initial phase–- from the formulation of basic sound design concepts to the extraction of the design item and the construction of an outline for creating an actual sound–-have been defined, and trial audio signals that are expected to be incorporated into a wide variety of electrical appliances have been created. In addition, hypotheses on the circumstances under which the equipment will be used have been deduced, and the evaluation methods of trial audio signals, points of improvement in the sound quality and the necessary precautions for incorporating the audio signal into the equipment have been investigated.","2002-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"62IA5PDF","book","2010","Edwards, Alistair; Hunt, Andy; Hines, Geneviève; Jackson, Vanessa; Podvoiskis, Alyte; Roseblade, Richard; Stammers, Jon","Sonification Strategies for Examination of Biological Cells","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/50055","Cervical cancer is one of the most preventable forms of the disease thanks to the fact that pre-cancerous changes can be detected in cervical cells. These cells are examined visually under microscopes, but the objective of this project was to ascertain whether their examination could be improved if the visual inspection were accompanied by an auditory representation. A number of different sound mappings were tested. This paper also traces the way the sound experiments evolved in parallel with the underlying research on cell image analysis. The main conclusion is that in this kind of application, the important parameters to sonify are the ‘badness’ of the cell and the reliability of that rating, and some likely sound mappings to convey this information have been identified.","2010-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JXW4KNYY","journalArticle","2019","Liu, Danyi; van der Heide, Edwin","Interactive auditory navigation in molecular structures of amino acids: A case study using multiple concurrent sound sources representing nearby atoms","","","","","http://hdl.handle.net/1853/61523","We are interested in sonifying the molecular structures of amino acids. This paper describes the context and the first design choices for our approach. So far, we believe an amino acid molecule is too complex to be perceived at once. Therefore, we have designed an interactive form of sonification in which the listener navigates through the molecule over the network of carbon atoms. We describe our different approaches and discuss the topic of immediacy: the time it takes to recognize the structure surrounding the listenerﾒs position while navigating. Furthermore, we touch upon the question how many atoms we can sonify simultaneously and the role auditory masking plays in this context. To overcome auditory masking, we propose to use irregular but easy to recognize sounds. We conclude with an interest in a three-dimensional navigation environment using general molecular structures for further research and development.","2019-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","Interactive auditory navigation in molecular structures of amino acids","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IMAWBNGX","journalArticle","2005","Walker, Bruce N.; Stamper, Kevin","Mobile audio designs monkey: An audio augmented reality designer's tool","","","","","http://hdl.handle.net/1853/50172","Audio Augmented Reality (AR) design is currently a very difficult task. To develop audio for an AR environment a designer must have technical skills which are unrelated to the design process. The designer should be focusing on the creativity, design, and the logic of the AR rather than the details of the audio. To support the design process, an audio AR designers' tool called Mobile Audio Designs (MAD) Monkey was developed. MAD Monkey was developed using the standard User Centered Design process. The stages of the iterative design process are described here, and the features of the resulting system are discussed. Evaluation of the prototype and plans for further development are also enumerated.","2005-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","Mobile audio designs monkey","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DZGS5I4L","journalArticle","2022","Herrebrøden, Henrik; Gonzalez Sanchez, Victor Evaristo; Vuoskoski, Jonna; Jensenius, Alexander Refsum","Pre-recorded sound file versus human coach: Investigating auditory guidance effects on elite rowers","","","","","http://hdl.handle.net/1853/67379","We report on an experiment in which nine Norwegian national team rowers (one female) were tested on a rowing ergometer in a motion capture lab. After the warm-up, all participants rowed in a neutral condition for three minutes, without any instructions. Then they rowed in two conditions (three minutes each), with a counterbalanced order: (1) a coaching condition, during which they received oral instructions from a national team coach, and (2) a sound condition, during which they listened to a pre-recorded sound file that was produced to promote good rowing technique. Performance was measured in terms of distance traveled, and subjective responses were measured via a questionnaire inquiring participants about how useful the two interventions were for rowing efficiency. The results showed no significant difference between the two conditions of main interest–the pre-recorded sound file and traditional coaching–on any measure. Our study indicates that auditory guidance can be a cost-efficient supplement to athletes’ training, even at higher levels.","2022-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","Pre-recorded sound file versus human coach","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"982VRGJX","journalArticle","2014","Alexander, Robert L.; O’Modhrain, Sile; Gilbert, Jason A.; Zurbuchen, Thomas H.","Auditory and Visual Evaluation of Fixed-frequency Events in Time-varying Signals","","","","","http://hdl.handle.net/1853/52069","This study directly compares the auditory and visual analysis capabilities of participants in a structured data analysis task. This task involved the identification of transient fixed-frequency sinusoid events that were embedded within white noise and noise derived from solar wind time series. It was hypothesized that participants would be able to identify the number of embedded events more quickly and accurately through auditory data analysis than through visual analysis. While visual analysis outperformed auditory analysis overall, additional investigation revealed that auditory analysis outperformed vision in instances where these events were embedded in solar wind data. This task - involving the detection of transient periodic activity occurring within background turbulence - closely mirrors a type of spectral analysis conducted by heliospheric scientists. Additionally, several data examples contained embedded events that were correctly identified through audition while being consistently overlooked through visual inspection. The largest disparity between visual and auditory performance was found in the analysis of white noise spectra that contained no embedded events. In these instances, auditory analysis regularly resulted in the identification of events when none were present; a potential reasoning for these false positives is discussed. The results of this study suggest that the analysis capabilities of each modality may vary based largely on the complexity of the masking stimuli that are present.","2014-06","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RVV3RELW","journalArticle","2005","Massimino, Paolo","From marked text to mixed speech and sound","","","","","http://hdl.handle.net/1853/50157","Loquendo TTS is a commercial Multilanguage/Multivoice Text-To-Speech synthesizer, attaining great acoustic naturalness and linguistic accuracy. Currently available languages are: Catalan, Chinese, Dutch, British English, American English, French, German, Greek, Italian, Portuguese, Brazilian, Castilian, Argentine, Chilean, Mexican and Swedish. Loquendo TTS is a flexible engine, based on multi-language external knowledge-bases, efficient and platform-independent. It performs text-to-speech conversion as a real-time “software-only” process. The Loquendo TTS integrated audio mixer allows mixing sound files and synthetic voice. It's possible to mix one or more sound files simultaneously, at the same time. Thanks to explicit tags embedded in the text, easy synchronization between audio files and speech is guaranteed even if the text is modified. Every sound effect is treated as an independent track, with independent timeline, volume and sample rate. Commands such Mix, Play, Stop, Pause, Resume, Loop and Fade allow users to have complete control on the audio sources. In order to make easy the use of the integrated audio mixer, a multi-platform application is shipped with Loquendo TTS SDK: TTSDirector. Loquendo TTS Director is a Java multiplatform development tool intended for helping the user in the design of the application prompts. The text of the application prompt can be written in the edit box and interactively refined by means of a “listen & edit” procedure, allowing to tune the TTS behavior by means of the Loquendo TTS User Control Tags. This paper gives details about the previous topics and can be used as basis for the workshop demonstration, concerning the use of the audio mixer, integrated into the Loquendo TTS, and other functionalities.","2005-07","2023-07-13 06:25:50","2023-07-13 06:25:50","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7N6WIRNY","journalArticle","2007","Visell, Yon; Cooperstock, Jeremy","Modeling and Continuous Sonification of Affordances for Gesture-Based Interfaces","","","","","http://hdl.handle.net/1853/50016","Sonification can play a significant role in facilitating continuous, gesture-based input in closed loop human computer interaction, where it offers the potential to improve the experience of users, making systems easier to use by rendering their inferences more transparent. The interactive system described here provides a number of gestural affordances which may not be apparent to the user through a visual display or other cues, and provides novel means for navigating them with sound or vibrotactile feedback.The approach combines machine learning techniques for understanding a user's gestures, with a method for the auditory display of salient features of the underlying inference process in real time. It uses a particle filter to track multiple hypotheses about a user's input as the latter is unfolding, together with Dynamic Movement Primitives, introduced in work by Schaal et al [1][2], which model a user's gesture as evidence of a nonlinear dynamical system that has given rise to them. The sonification is based upon a presentation of features derived from estimates of the time varying probability that the user's gesture conforms to state trajectories through the ensemble of dynamical systems. We propose mapping constraints for the sonification of time-dependent sampled probability densities. The system is being initially assessed with trial tasks such as a figure reproduction using a multi degree-of-freedom wireless pointing input device, and a handwriting interface.","2007-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LCUE9X8T","journalArticle","2018","Brittell, Megen","Seeking a reference frame for cartographic sonification","","","","","http://hdl.handle.net/1853/60082","Sonification of geospatial data must situate data values in two (or three) dimensional space. The need to position data values in space distinguishes geospatial data from other multi-dimensional data sets. While cartographers have extensive experience preparing geospatial data for visual display, the use of sonification is less common. Beyond availability of tools or visual bias, an incomplete understanding of the implications of parameter mappings that cross conceptual data categories limits the application of sonification to geospatial data. To catalyze the use of audio in cartography, this paper explores existing examples of parameter mapping sonification through the framework of the geographic data cube. More widespread adoption of auditory displays would diversify map design techniques, enhance accessibility of geospatial data, and may also provide new perspective for application to non-geospatial data sets.","2018-06","2023-07-13 06:25:51","2023-07-21 08:32:34","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7ZCPI9PG","journalArticle","2012","Vogt, Katharina; Goudarzi, Visda; Höldrich, Robert","SysSon - a systematic procedure to develop sonifications","","","2168-5126","","http://hdl.handle.net/1853/44447","The newly started research project SysSon will develop a systematic procedure to develop sonifications, and test the procedure with climate data. The SysSon approach addresses the relevant obsta cles that are met when introducing sonification in a new scientific domain: the cultural bias, usability and technical issues. This paper presents the research approach that shall be put up for discussion. Furthermore, first results of the preparatory steps will be presented.","2012-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2V5EU3C2","journalArticle","2007","Nasir, Tooba; Roberts, Jonathan C.","Sonification of Spatial Data","","","","","http://hdl.handle.net/1853/50035","Sonification is the use of sound and speech to represent information. There are many sonification examples in the literature from simple realizations such as a Geiger counter to representations of complex geological features. The data that is being represented can be either spatial or non-spatial. Specifically, spatial data contains positional information; the position either refers to an exact location in the physical world or in an abstract virtual world. Likewise, sound itself is spatial: the source of the sound can always be located. There is obviously a synergy between spatial data and sonification. Hence, this paper reviews the sonification of spatial data and investigates this synergy. We look at strategies for presentation, exploration and what spatial interfaces and devices developers have used to interact with the sonifications. Furthermore we discuss the duality between spatial data and various sonification methodologies.","2007-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"78X48DZR","journalArticle","2014","Arditi, Aries","Auditory Display of Coarse Optical Imagery: Concept for a Rehabilitation Aid for Blind Spatial Orientation","","","","","http://hdl.handle.net/1853/52080","We introduce a concept for a rehabilitation aid for blind persons that will present, on a sonic display, coarse optical information obtained from a spectacle-mounted camera. The aid will serve blind persons who have no light sense or who can at most detect ambient light. The approach is to map luminous intensity to loudness of continuous tones of distinct timbre representing a small number of directions relative to that of the user’s head.","2014-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","Auditory Display of Coarse Optical Imagery","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6K3YPJ3T","journalArticle","2008","Dingler, Tilman; Lindsay, Jeffrey; Walker, Bruce N.","Learnabiltiy of Sound Cues for Environmental Features: Auditory Icons, Earcons, Spearcons, and Speech","","","","","http://hdl.handle.net/1853/49940","Awareness of features in our environment is essential for many daily activities. While often awareness of such features comes from vision, this modality is sometimes unavailable or undesirable. In these instances, auditory cues can be an excellent method of representing environmental features. The study reported here investigated the learnability of well known (auditory icons, earcons, and speech) and more novel (spearcons, earcon-icon hybrids, and sized hybrids) sonification techniques for representing common environmental features. Spearcons, which are speech stimuli that have been greatly sped up, were found to be as learnable as speech, while earcons unsurprisingly were much more difficult to learn. Practical implications are discussed.","2008-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","Learnabiltiy of Sound Cues for Environmental Features","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JBSS7CEP","journalArticle","2016","Dyer, John; Stapleton, Paul; Rodger, Matthew","Sonification of Movement for Motor Skill Learning in a Novel Bimanual Task: Aesthetics and Retention Strategies","","","","","http://hdl.handle.net/1853/56574","Here we report early results from an experiment designed to investigate the use of sonification for the learning of a novel perceptual-motor skill. We find that sonification which employs melody is more effective than a strategy which provides only bare timing information. We additionally show that it might be possible to �refresh' learning after performance has waned following training - through passive listening to the sound that would be produced by perfect performance. Implications of these findings are discussed in terms of general motor performance enhancement and sonic feedback design.","2016-07","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","Sonification of Movement for Motor Skill Learning in a Novel Bimanual Task","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DVZ5ABJT","journalArticle","2006","Pauletto, S.; Hunt, A.","The sonification of EMG data","","","","","http://hdl.handle.net/1853/50692","This paper describes the sonification of electromyographic (EMG) data and an experiment that was conducted to verify its efficacy as an auditory display of the data. A real-time auditory display for EMG has two main advantages over the graphical representation: it frees the eyes of the analyst, or the physiotherapist, and it can be heard by the patient too who can then try to match with his/her movement the target sound of a healthy person. The sonification was found to be effective in displaying known characteristics of the data. The `roughness' of the sound was found to be related to the age of the patients. The sound produced by the sonification was also judged to be appropriate as an audio metaphor of the data it displays; a factor that contributes to its potential to become a useful feedback tool for the patients.","2006-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9BD2MK77","journalArticle","2013","Lunn, Paul; Hunt, Andy","Phantom Signals: Erroneous Perception Observed During The Audification Of Radio Astronomy Data","","","","","http://hdl.handle.net/1853/51675","This paper describes the work in progress of an investigation into utilizing audification techniques upon radio astronomy data, generated by the Search for Extraterrestrial Intelligence (SETI). The proposed system involves subjects listening to the data presented as background noise. The initial tests established that subjects are able to detect the presence of simulated signals when presented with white noise; however it was observed that there were significant reports of signals that were not present in the test files. Subjects regularly reported perceiving these “phantom signals”. Further experimentation confirmed that phantoms were reported when listeners were presented with pure white noise and were asked to identify signals with this data. Exposing subjects to examples of potential signals prior to the test has a heavy influence on the prevalence and sonic characteristics of the illusory signals reported.","2013-07","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","Phantom Signals","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HTAFT2D3","journalArticle","2001","Waters, Dean; Adulula, Husam","The virtual bat: Echolocation in virtual reality","","","","","http://hdl.handle.net/1853/50658","Work in progress is being presented on the effectiveness of using sonar to orientate and navigate in a virtual reality system. The sonar system is based on those of bats, using ultrasonic frequency modulated signals reflected from simple targets. The model uses the reflectivity characteristics of ultrasound, but the frequency and temporal structure used are scaled down by a factor of ten to bring the frequency range and temporal resolution within the capabilities of the human auditory system. Orientation with respect to the ensonified target is achieved by time of flight time delays to give target range, and binaural location information derived from interaural timing differences, interaural intensity differences, and head-related transfer functions. Data on the ability to locate targets as a function of signal frequency, bandwidth, duration and sweep pattern is presented.","2001-07","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","The virtual bat","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R39S9P9X","journalArticle","2008","Absar, Rafa; Guastavino, Catherine","Usability of Non-Speech Sounds in User Interfaces","","","","","http://hdl.handle.net/1853/49965","We review the literature on the integration of non-speech sounds to visual interfaces and applications from a usability perspective and subsequently recommend which auditory feedback types serve to enhance human interaction with computers by conveying useful and comprehensible information. We present an overview over varied tasks, functions and environments with a view to establishing the best practices for introducing non-speech sounds in order to improve the overall experience of users.","2008-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T8FS6EF4","journalArticle","2012","Beyls, Peter","Interfacing the Earth","","","2168-5126","","http://hdl.handle.net/1853/44404","We provide a short introduction to WindChime, a real-time web-driven audiovisual installation. Weather data from many world locations is gathered from a server and accommodated in a dynamic visual representation. The dynamics of the wind at specific world locations exercises influence over a mass of floating particles in a virtual parallel world. Particles in turn influence the production of complex sounds. In effect, a rewarding aesthetic experience results from the appreciation of the intricate interplay of two complex dynamical systems; one of natural origin (the earth), the other of cultural design (the program).","2012-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"79R288II","journalArticle","2002","Susini, P.; Vieillard, S.; Deruty, E.; Smith, B.; Marin, C.","Sound navigation: Sonified hyperlinks","","","","","http://hdl.handle.net/1853/51386","This article deals with the idea of hyperlinks in the auditory realm, “sonified hyperlink”, which is analog to the visual hyperlinks of the HTML language. The aim of the present article is to propose acoustical recommendations for sounds used to underline a word in spoken text.","2002-07","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","Sound navigation","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VWU36FQ7","journalArticle","2007","Kainulainen, Anssi; Turunen, Markku; Hakulinen, Jaakko; Melto, Aleksi","Soundmarks in Spoken Route Guidance","","","","","http://hdl.handle.net/1853/50034","Route guidance is an emerging mobile computing application domain. Soundscapes or acoustic environments are a perceptually important part of people's location awareness and navigation. In this paper, we present how nonspeech audio can be used to complement speechbased and graphical route information in mobile public transport guidance. We present TravelMan, a mobile multimodal pedestrian and public transport route guidance application. Based on TravelMan, we also present a soundmarkbased route description design. Auditory icons describe methods of transport and identify spatial points of interest. They support users as a less intrusive, awareness supporting information source. Initial test results indicate that combining speech and nonspeech sounds are not a trivial task, and that there is need for further development.","2007-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YVH9KB5N","journalArticle","1996","van de Doel, Kees; Pai, Dinesh K.","Synthesis of shape dependent sounds with physical modeling","","","","","http://hdl.handle.net/1853/50816","We describe a general framework for the simulation of sounds produced by colliding physical objects in a real time graphics environment. The framework is based on a physical model of the vibration dynamics of bodies. The computed sounds depend on the material of the body and its shape. A key contribution of this work is that the sounds also depend on the location of impact on a struck object. This adds important realism to virtual environments. The framework has been implemented in a Sonic Explorer program, which simulates a room with several objects such as a chair, tables, and rods. After a preprocessing stage, the user can hit the objects at different points to interactively produce realistic sounds. We also have created an online demo of the sound synthesis, written in Java.","1996-11","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LM4LZ8QZ","journalArticle","2005","Pauletto, Sandra; Hunt, Andy","A comparision of audio & visual analysis of complex time-series data sets","","","","","http://hdl.handle.net/1853/50092","This paper describes an experiment to compare user understanding of complex data sets presented in two different modalities, a) in a visual spectrogram, and b) via audification. Many complex time-series data sets taken from helicopter flight recordings were presented to the test subjects in both modalities separately. The aim was to see if a key set of attributes (noise, repetitive elements, regular oscillations, discontinuities, and signal power) were discernable to the same degree in the different modalities. Statistically significant correlations were found for all attributes, which shows that audification can be used as an alternative to spectrograms for this type of analysis.","2005-07","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R6XC5B8S","journalArticle","2003","Susini, Patrick; Gaudibert, Piotr; Deruty, Emmanuel; Dandrel, Louis","Perceptive study and recommendation for sonification categories","","","","","http://hdl.handle.net/1853/50481","In the field of audio signaletics, most sound designers have their own recipes to make samples that convey a certain meaning, which we could call auditory function. The aim of the present article is to compare the perceptive representation and the functional representation with the usual sound categories designed to fulfill specific actions of user's interface. The article finally proposes recommendations for the designers according to perceptive results.","2003-07","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X8CPJLB5","book","2015","Storek, Dominik; Stuchlik, Jan; Rund, Frantisek","Modifications of the surrounding auditory space by augmented reality audio: Introduction towarped acoustic reality","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54143","Augmented Reality Audio (ARA) is mostly employed in terms of adding virtual sound source into an existing auditory scene. Common goal of such system is to preserve the highest level of fidelity and natural character of both real and virtual components. This paper introduces another approach to the ARA systems based on intentional modification of the native parameters of both the real and the virtual auditory segments. The system employs binaural microphone-equipped earphones used as microphone-hearthrough device, which are able to capture and immediately reproduce the auditory scene. A processing unit is included in the microphone-earphone signal path. The unit enables to apply various audio effects to both the captured and the virtual sound allowing warped perception of the auditory reality. The real-time processing algorithms were implemented in Pure Data environment. The proposed system provides immersive audio performance and has potential to be used in specific virtual reality applications, such as simulation of reality perception by people with mental disorders.","2015-07","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","Modifications of the surrounding auditory space by augmented reality audio","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V26JCY4A","journalArticle","2018","Ballora, Mark; Roman, Christopher; Pockalny, Robert; Wishner, Karen","Sonification and science pedagogy: preliminary experiences and assessments of earth science data presented in an undergraduate general education course","","","","","http://hdl.handle.net/1853/60069","This paper describes preliminary investigations into how sonifications of scientific graphs are perceived by undergraduate students in an introductory course in oceanography at the University of Rhode Island. The goal is to gather data that can assist in gauging students’ levels of engagement with sonification as a component of science education. The results, while preliminary, show promise that sonified graphs improve understanding, especially when they are presented in combination with visual graphs.","2018-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","Sonification and science pedagogy","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8YQJSQWI","book","2015","Geronazzo, Michele; Avanzini, Federico; Fontana, Federico","Use of personalized binaural audio and interactive distance cues in an auditory goal-reaching task","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54107","While the angular spatialization of source sounds through individualized Head-related transfer functions (HRTFs) has been extensively investigated in auditory display research, also leading to effective real-time rendering of these functions, conversely the interactive simulation of egocentric distance information has received less attention. The latter, in fact, suffers from the lack of realtime rendering solutions also due to a too sparse literature on the perception of dynamic distance cues. By adding a virtual environment based on a Digital waveguide mesh (DWM) model simulating a small tubular shape to a binaural rendering system through selection techniques of HRTF, we have come up with an auditory display affording interactive selection of absolute 3D spatial cues of angular spatialization as well as egocentric distance. The tube metaphor in particular minimized loudness changes with distance, hence providing mainly direct-to-reverberant and spectral cues. A goal-reaching experiment assessed the proposed display: participants were asked to explore a virtual map with a pen tablet and reach a sound source (the target) using only auditory information; then, subjective time to reach and traveled distance were analyzed. Results suggest that participants achieved a first level of spatial knowledge, i.e., knowledge about a point in space, by performing comparably to when they relied on more robust, although relative, loudness cues. Further work is needed to add fully physical consistency to the proposed auditory display.","2015-07","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V395J6IJ","journalArticle","2019","Lutz, Otto Hans-Martin; Kroger, Jacob Leon; Schneiderbauer, Manuel; Hauswirth, Manfred","Surfing in sound: Sonification of hidden web tracking","","","","","http://hdl.handle.net/1853/61533","Web tracking is found on 90% of common websites. It allows online behavioral analysis which can reveal insights to sensitive personal data of an individual. Most users are not aware of the amout of web tracking happening in the background. This paper contributes a sonification-based approach to raise user awareness by conveying information on web tracking through sound while the user is browsing the web. We present a framework for live web tracking analysis, conversion to Open Sound Control events and sonification. The amount of web tracking is disclosed by sound each time data is exchanged with a web tracking host. When a connection to one of the most prevalent tracking companies is established, this is additionally indicated by a voice whispering the company name. Compared to existing approaches on web tracking sonification, we add the capability to monitor any network connection, including all browsers, applications and devices. An initial user study with 12 participants showed empirical support for our main hypothesis: exposure to our sonification significantly raises web tracking awareness.","2019-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","Surfing in sound","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TXEVDVNP","journalArticle","2014","Neate, Timothy; Degara, Norberto; Hunt, Andy; Nagel, Frederik","A Generic Evaluation Model for Auditory Feedback in Complex Visual Searches","","","","","http://hdl.handle.net/1853/52063","This paper proposes a method of evaluating the effect of auditory display techniques on a complex visual search task. The approach uses a pre-existing visual search task (conjunction search) to create a standardized model for audio, and non-audio assisted visual search tasks. A pre-existing auditory display technique is evaluated to test the system. Using randomly generated images, participants were asked to undertake a series of visual search tasks of set complexities, with and without audio. It was shown that using the auditory feedback improved the participant’s visual search times considerably, with statistically significant results. Additionally, it was shown that there was a larger difference between audio and non-audio when the complexity of the images was increased. The same auditory display techniques were then applied to an example of a real complex visual search task, the results of which imply a significant improvement in visual search efficiency when using auditory feedback.","2014-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3V3ZD4SW","journalArticle","2002","Jeong, W.; Gluck, M.","Multimodal bivariate thematic maps with auditory and haptic display","","","","","http://hdl.handle.net/1853/51365","The purpose of this study is to explore the possibility of multimodal bivariate thematic maps by utilizing auditory and haptic displays. With four different modes of display, the completion time of tasks and the recall (retention) rate were measured in two separate experiments. In terms of the completion time, haptic displays seem to interfere with other modalities. However, Color-Auditory displays performed similarly to Color-Color displays. For the recall rate, multimodal displays have higher recall rates, with users performing the best on Auditory-Haptic displays. These findings confirmed the possibility of using auditory and haptic displays in visually dominant geographic information systems (GIS). We speculate that the natural quantitative hierarchies in auditory and haptic displays provide an advantage in the use of multiodal displays.","2002-07","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V66I6PX6","journalArticle","2006","Bovermann, T.; Hermann, T.; Ritter, H.","Tangible data scanning sonification model","","","","","http://hdl.handle.net/1853/50652","In this paper we develop a sonification model following the Modelbased Sonification approach that allows to scan high-dimensional data distributions by means of a physical object in the hand of the user. In the sonification model, the user is immersed in a 3D space of invisible but acoustically active objects which can be excited by him. Tangible computing allows to identify the excitation object (e.g. a geometric surface) with a physical object used as controller, and thus creates a strong metaphor for understanding and relating feedback sounds in response to the user's own activity, position and orientation. We explain the technique and our current implementation in detail and give examples at hand of synthetic and real-world data sets.","2006-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2ZMGSJXE","journalArticle","2008","Pirhonen, Antti; Palomaki, Henni","Sonification of directional and emotional content: Description of design challenges","","","","","http://hdl.handle.net/1853/49956","In the construction of sound objects into an application, the designer's skills to communicate through sounds is the cornerstone of the activity. In such an expertise, the knowledge about the human way of interpreting different properties of sounds is essential. This paper is a description of two experiments, in which the semantics of tempo change, pitch change and intensity change of sound has been studied by asking the participants of the experiments to combine sounds to visual images. The images in the first experiment were photos which had been validated in terms of their emotional content. In the second experiment, the images were arrows pointing in various directions. The results show that studying context independent semantics of non-speech sounds with the help of photos is problematic, but some tendencies can be revealed. On the other hand, simple information units like physical directions, can be illustrated with changes in intensity and tempo, but especially with change in pitch.","2008-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","Sonification of directional and emotional content","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VII6S59I","book","2009","Hug, Daniel","Using a systematic design process to investigate narrative sound design strategies for interactive commodities","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51422","Computing technologies turn everyday artifacts into narrative, pro- cedural objects. This observation suggests that the narrative sound design strategies used in films and video games could also be ap- plied for the design of interactive commodities. However, it is unknown whether these strategies from immersive media can be applied in physical artifacts of everyday use. In this paper we de- scribe methodological considerations and outline a structure of a revisable, design oriented, participatory research process, which allows to explore narrative sound designs and their possible appli- cation in interactive commodities in a systematic yet explorative way. The process, which focused on interpretational aspects, has been applied in two workshops and their results are reported and discussed.","2009-05","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SFG2XJBA","journalArticle","2004","Barrass, S.; Adcock, M.","Cultivating design patterns for auditory displays","","","","","http://hdl.handle.net/1853/50832","Auditory Displays are quite well known in the research community, but very little of this experience is being transferred to product designers. The method of Design Patterns is well known to a number of design domains and is used to describe “solutions to problems in context” in such a way that they can be reused again and again. Here we present six new prototype Design Patterns for Auditory Display: SystemMonitoring, SituationalAwareness, SonifiedLineGraph, AuditoryIcon, Attenson, and Personalisation. We invite the Auditory Display community to collaboratively cultivate these patterns and other patterns via the communal WikiWeb repository.","2004-07","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DIFQ7DBL","journalArticle","2005","Kildal, Johan; Brewster, Stephen A.","Explore the matrix: Browsing numerical data tables using sound","","","","","http://hdl.handle.net/1853/50153","When first approaching a two-dimensional (2D) data table, a user often wants to get a quick overview of the data before analysing them in more detail. Blind and visually impaired people cannot do this task at all using visual displays. Furthermore, speech synthesisers do not help sufficiently with browsing and pattern identification. The approach taken in this research work is to use other senses available to the users to provide efficient ways of exploring numerical data tables. Sighted users having to analyse complex data sets could also benefit from multimodally-presented data arrangements The first steps have been taken by using audio to access information in tables. A sonification of tabular data has been designed, as well as a novel way of accessing the sonified data. Each row or column is sonified as a single piece of information and they are accessed sequentially, reducing the overall complexity of the 2D data structure. Experimental results show that an overview of the data in a table is obtained faster using this sonification technique than using screen reading software, without a reduction in accuracy.","2005-07","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","Explore the matrix","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"STLLFQ5Y","book","2010","Bouchara, Tifanie; Katz, Brian F. G.; Jacquemin, Christian; Guastavino, Catherine","Audio-Visual Renderings for Multimedia Navigation","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/58425","Our study focuses on multimodal information access to audiovisual databases, and evaluates the effect of combining the visual modality with audio information. To do so, we have developed two new exploration tools, which extend two information visualization techniques, namely Fisheye Lens (FL) and Pan&Zoom (PZ), to the auditory modality. The FL technique combined coherent distortion of graphics, sound space and volume. The PZ technique was designed without visual distortion but with low audio volume distortion. Both techniques were evaluated perceptually using a target finding task with both visual-only and audio-visual renderings. We did not find significant differences between audio-visual and visual-only conditions in terms of completion times. However we did find significant differences in participant’s qualitative evaluations of difficulty and efficiency. In addition, 63% of participants preferred the multimodal interface. For FL, the majority of participants judged the visual-only rendering as less efficient and appreciated the benefit of the audio rendering. But for PZ, they were satisfied with the visual-only rendering and evaluated the audio rendering as distracting. We conclude with future design specifications.","2010-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4QYPJLI6","journalArticle","2005","Bonebright, Terri L.","A suggested agenda for auditory graph research","","","","","http://hdl.handle.net/1853/50085","This paper presents one option for a research agenda for future work in auditory graphs. The main agenda items suggested are effectiveness of auditory graphs; sonification tools; role of memory and attention; real-world applications; longitudinal studies of learning; and neurophysiological research. A brief review of past research in each area is given to provide general information about relevant studies and is meant to serve as a starting point rather than as a comprehensive overview of the literature on auditory graph studies.","2005-07","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EQGV94AU","journalArticle","2021","Seiça, Mariana; Roque, Licínio; Martins, Pedro; Cardoso, F. Amílcar","A systemic perspective for sonification aesthetics","","","","","http://hdl.handle.net/1853/66337","For more than twenty-five years, the sonification field has been attempting to establish itself as a primary body of knowledge communicating through sound. Despite multiple efforts to embrace the interdisciplinary nature of the field and the subjective nature of sound, we wonder: is the tendency for dealing with such challenges through an objective, functional communication, with a single interpretation criterion, limiting the epistemic boundaries of action? How can a subjectively perceived medium such as sound be embraced in all its aesthetic dimensions? We propose a conceptual transition through the reframing of a sonification as a living system for creating aesthetic experiences. This will be achieved by drawing notions from phenomenology, embodied perception, human-computer interaction and soundscape theory. A systemic sonification distinguishes itself as an ever-evolving system built on dynamic structures that actively responds to changes in its environment and interactions from surrounding beings. Driven by a series of emerging concepts of non-linearity, networks, nested systems and intertwined relationships, the system's resilience and adaptability grows with each interaction, recentring the human protagonist as the weaver of his/her aesthetic experience through a selftranscendent process that expands the perception field.","2021-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y9Z3XGRL","journalArticle","2006","Midgley, L.; Vickers, P.","Sonically-enhanced mouse gestures in the firefox browser","","","","","http://hdl.handle.net/1853/50638","The use of the mouse to allow interaction via gestures has attracted much interest recently and the popular FIREFOX web browser has been enhanced by an extension supporting mouse gestures. These gestures reveal an interaction problem: feedback is limited (often only a terse message in the browser's status bar) and navigation errors easily result when the user unknowingly executes a gesture when trying to accomplish some other task (e.g. copying text from a web page). This paper describes an attempt to improve the interaction experience by adding auditory cues to inform the user about the progress and progression of gestural commands. FIREFOX was chosen as it has an open extension architecture that is easily modified. Preliminary trials indicate increased user satisfaction and comprehension when using auditory-enhanced gestures over the non-enhanced gestures.","2006-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N5VJUD27","journalArticle","1997","Hamman, Michael; Goudeseune, Camille","Mapping data and audio using an event-driven audio server for personal computers","","","","","http://hdl.handle.net/1853/50749","Recent research suggests that auditory display offers new means forobserving and differentiating complex data. One standard method forrendering an auditory display is playback of previously generated audiofiles. This method is enhanced through playback modification usingtools such as Intel's RSX. MIDI-based synthesizers provide yet anothermethod for auditory display. These methods have various drawbacksthat are pressed to the limit when confronted with the requirements ofanalogical display systems. What is needed, therefore, is a way ofrendering audio that is to auditory display what a system like OpenGL isto graphical display. Audio Rendering Engine And Library (AREAL)is a real-time audio renderer and sound synthesis software library. It offers the software developer and auditory display designer a set oftools for developing high-quality audio applications for low-cost multimediacomputers using consumer or professional audio hardware. The primarypurpose of AREAL is to enable a ""model-based"" approach to audio which isbecoming common on high-end (and high-cost) workstations. This papergives a brief description of this software system and its potential foruse within the auditory display community.","1997-11","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7TAKFCHQ","journalArticle","1997","Saue, Sigurd; Fjeld, Ola Kr","A platform for audiovisual seismic interpretation","","","","","http://hdl.handle.net/1853/50740","We are describing a pilot study investigating the use of sonification techniques in seismic interpretation for oil exploration. Due to emerging development criteria of conformance and usefulness, the project was redirected from a free experimental tool box to a primitive but functioning work station for seismic interpretation. The emphasis was put on extracting interesting seismic parameters for sonification. The most important elements are objects (allowing encapsulation of structures), attributes (allowing multiple data sets for each object) and methods (how the chosen data is transformed to sound). Among the methods are standard mapping from data value to pitch, mapping from structural position to pitch, mapping from entire object content to sound with moving objects. All methods can be both mouse-driven and automatic. The audio is MIDI-based using common soundcards. Separation of different data streams are done by instrumentation, octave placement and panning. The major assets of using sound were evaluated to be multidimensionality, time resolution, pattern sensibility and new data representations (methods). The pilot study ended successfully insofar that it will be integrated in a new software package featuring 3D-visualization and audio for seismic interpretation.","1997-11","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XUV6R6SZ","book","2015","Boren, Braxton; Geronazzo, Michele; Brinkmann, Fabiann; Choueiri, Edgar","Coloration metrics for headphone equalization","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54097","Headphone equalization is necessary for accurate binaural reproduction over headphones, but so far no metrics have been adopted for evaluating human perception of spectral coloration in post-equalization headphone transfer functions (HpTFs). A metric for peak error is proposed that represents the average HpTF error from narrow peaks per third octave band. In addition, a new metric for broadband error is defined by subtracting the average error from narrow peaks and notches from that of an auditory filter bank model. Used together, the peak error and broadband error terms are shown to represent the critical information necessary for transparent headphone reproduction.","2015-07","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2AE3QTME","book","2010","Cook, Perry R.; Fellbaum, Christiane; Ma, Xiaojuan","Environmental Sounds as Concept Carriers for Communication","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49881","Sonification, the use of nonspeech audio to represent data and information, has been applied to industrial systems and computer interfaces via mechanisms such as auditory icons and earcons. In this paper, we explore a different application of sonification, which is to facilitate communication across language barriers by conveying commonly used concepts via environmental auditory representations. SoundNet, a linguistic database enhanced with natural nonspeech audio, is constructed for this purpose. The concept-sound associations which are building blocks of SoundNet were validated through a sound labeling study conducted on Amazon Mechanical Turk. We determine the factors that cause a sound to evoke a concept. We examine which aspects of the proposed auditory representations are evocative, and what kinds of confusions may occur. Our results show that sounds can effectively illustrate some concepts, especially those related to concrete entities and actions, and thus can be utilized in assistive communication applications.","2010-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GD3DCIEK","journalArticle","2018","Hermann, Thomas","Wave Space Sonification","","","","","http://hdl.handle.net/1853/60087","This paper introducesWave Space Sonification (WSS), a novel class of sonification techniques for time- (or space-) indexed data. WSS doesn’t fall into the classes of Audification, Parameter- Mapping Sonification or Model-based Sonification and thus constitutes a novel class of sonification techniques. It realizes a different link between data and their auditory representation, by scanning a scalar field – defined as wave space – along a data-driven trajectory. This allows both the highly controlled definition of the auditory representation for any area of interest, as well as subtle yet acoustically complex sound variations as the overall pattern changes. To illustrate Wave Space Sonification (WSS), we introduce three different WSS instances, (i) the Static Canonical WSS, (ii) Data-driven Localized WSS and (iii), Granular Wave Space Sonification (GWSS), and we demonstrate the different methods with sonification examples from various data domains. We discuss the technique and its relation to other sonification approaches and finally outline productive application areas.","2018-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IW5AXE2L","journalArticle","2001","Hansen, M. H.; Rubin, B.","Babble online: Applying statistics and design to sonify the internet","","","","","http://hdl.handle.net/1853/50612","A statistician (Hansen) and a media artist (Rubin) investigate the application of statistical methods and sound-design principles to the real-time sonification of Internet communications. This paper presents results from two applications: the sonification of browsing activity on Lucent's Web site, and the sonification of a large number of Internet chat sites in real-time. These experiments suggest new ways to experience the diverse and dynamic data streams generated by modern data networks. As an art-technology collaboration, the project outcomes range from the creation of art installations to the development of practical monitoring platforms. This paper discusses the interplay between these two perspectives, and suggests that each is motivated by a common interest in generating meaningful experiences with dynamic data.","2001-07","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","Babble online","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E2PYC4XB","journalArticle","2007","Guastavino, Catherine; Larcher, Veronique; Catusseau, Guillaume; Boussard, Patrick","Spatial Audio Quality Evaluation: Comparing Transaural, Ambisonics and Stereo","","","","","http://hdl.handle.net/1853/50032","Two experiments were conducted to investigate perceptual differences between three sound recording and reproduction techniques, namely transaural, ambisonics and stereophony, in terms of spatial quality (Exp.1) and localization (Exp. 2) on a variety of sound material. Results indicate a strong contrast between ambisonics and the other two techniques. Specifically, ambisonics provides a good sense of immersion and envelopment but a poor localization and readability of the scene, while stereophony and transaural provide a precise localization and a good readability but lack immersion and envelopment. These results suggest that a trade-off between immersion and precision may be difficult to achieve using these techniques.","2007-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","Spatial Audio Quality Evaluation","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7EIJ9JLN","journalArticle","2021","Bouchara, Tifanie; Bouchet, Mathieu; Misdariis, Nicolas","Towards a better understanding of mental models implied in sonic icon design and perception","","","","","http://hdl.handle.net/1853/66349","This paper presents our ongoing efforts to determine if there are shared references, i.e. mental models, across designers and potential users of sonic icons in mobile applications. First, 13 sound designer students had to conceive sonic icons regarding 11 common mobile functionalities. Their conceptual models were analyzed through lexical analyses of their self-report on their design and manual annotations of their renderings. Second, the 143 obtained icons were evaluated by 52 naïve listeners through a free categorization task. While deeper analyses are still required, results already indicate that some function/sonic icon links make strong consensus on how they should be realized, considering both the designers' productions and the listeners' clustering. This forms one key result of the study and could lead to useful guidelines in sonic icon design.","2021-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4JTWUWRD","journalArticle","2017","Yang, Jiajun; Hermann, Thomas","Parallel Computing of Particle Trajectory Sonification to Enable Real-Time Interactivity","","","","","http://hdl.handle.net/1853/58356","In this paper, we revisit, explore and extend the Particle Trajectory Sonification (PTS) model, which supports cluster analysis of high-dimensional data by probing a model space with virtual particles which are ‘gravitationally’ attracted to a mode of the dataset’s potential function. The particles’ kinetic energy progression of as function of time adds directly to a signal which constitutes the sonification. The exponential increase in computation power since its conception in 1999 enables now for the first time to investigate real-time interactivity in such complex interweaved dynamic sonification models. We speeded up the computation of the PTS model with (i) data optimization via vector quantization, and (ii) parallel computing via OpenCL. We investigated the performance of sonifying high-dimensional complex data under different approaches. The results show a substantial increase in speed when applying vector quantization and parallelism with CPU. GPU parallelism provided a substantial speedup for very large number of particles comparing to using CPU but did not show enough benefit for a low number of particles due to copying overhead. A hybrid OpenCL implementation is presented to maximize the benefits of both worlds.","2017-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NSA5CJF2","book","2009","Verron, Charles; Aramaki, Mitsuko; Kronland-Martinet, Richard; Pallone, Gregory","Analysis/synthesis and spatialization of noisy environmental sounds","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51286","The use of stochastic modeling is discussed for analysis/synthesis and transformation of environmental sounds. The method leads to perceptually relevant synthetic sounds based on the analysis of nat- ural sounds. Applications are presented, such as sound effects us- ing parametric signal transformations, or data compression. More- over, we propose a method which efficiently combines the stochas- tic modeling with 3D audio techniques. This architecture offers an efficient control of the source width rendering that is often an im- portant attribute of noisy environmental sounds. This control is of great interest for virtual reality applications to create immersive 3D scenes.","2009-05","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"393IFRM4","journalArticle","1998","Brewster, Stephen A.","Sonically-enhanced drag and drop","","","","","http://hdl.handle.net/1853/50727","This paper describes an experiment to investigate if the addition of non-speech sounds to the drag and drop operation would increase usability. There are several problems with drag and drop that can result in the user not dropping a source icon over the target correctly. These occur because the source can visually obscure the target making it hard to see if the target is highlighted. Structured non-speech sounds called earcons were added to indicate when the source was over the target, when it had been dropped on the target and when it had not. Results from the experiment showed that subjective workload was significantly reduced, and overall preference significantly increased, without sonically-enhanced drag and drop being more annoying to use. Results also showed that time taken to do drag and drop was significantly reduced. Therefore, sonicenhancement can significantly improve the usability of drag and drop.","1998-11","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7PQ5NU7R","journalArticle","2007","Avanzini, Federico","Synthesis of Environmental Sounds in Interactive Multimodal Systems","","","","","http://hdl.handle.net/1853/50027","This review paper discusses the literature on perception and synthesis of environmental sounds. Relevant studies in ecological acoustics and multimodal perception are reviewed, and physicallybased sound synthesis techniques for various families of environmental sounds are compared. Current research directions and open issues, including multimodal interfaces and virtal environments, automatic recognition and classification, and sound design, are discussed. The focus is especially on applications of physically based techniques for synthesis of environmental sounds in interactive multimodal systems. The paper reports on ongoing research on bimodal (audio-haptic) rendering of virtual objects.","2007-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2XLECSBL","journalArticle","2021","Apel, Ted; Johnson, Jeffrey B.","Portable real-time volcano infrasound auditory display devices","","","","","http://hdl.handle.net/1853/66353","Active open-vent volcanoes produce intense infrasound airwaves, and volcanoes with prominent craters can create strongly resonant signals, which are inaudible to humans, and often peak around 1 Hz. Study of volcano infrasound is used to model eruption dynamics, the structure of volcanic craters, and can be used as a component of volcano monitoring infrastructure. We have developed a portable on-site real-time sonification device that emits an audible sound in response to an infrasonic airwave. This device can be used near an active volcano both as a real-time educational aid and as an accessible tool for monitoring the state of volcano activity. This paper presents this device with its hardware and software implementation, its parameter mapping sonification algorithm, recommendations for its use in the field, and strategies for future improvements.","2021-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KLILJTIA","book","2009","Wersenyi, Gyorgy","Evaluation of auditory representations for selected applications of a graphical user interface","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51294","A survey with 50 blind and 100 sighted users included a questionnaire about their user habits during everyday use of personal computers. Based on their answers, the most important functions and applications were selected and results were compared. Special user habits and needs of blind users are highlighted. The second part of the investigation included collecting of auditory representations (auditory icons, spearcons etc.), mapping with visual information and evaluation with the target groups. Furthermore, a new design method for auditory events and class was introduced, the so called auditory emoticons. These use non-verbal human voice samples to represent additional emotional content. Blind and sighted users evaluated different auditory representations for the selected events, including Hungarian and German spearcons. Finally, an application can be created and sound samples can be implemented under JAWS or the Windows OS.","2009-05","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9WQYMBM4","book","2015","Mendonça, Catarina; Rummukainen, Olli; Pulkki, Ville","3D sound can have a negative impact on the perception of visual content in audiovisual reproductions","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54119","There is reason to believe that sound interacts with visual attention mechanisms. Practical implications of that interaction have never been analyzed in the context of spatial sound design for audiovisual reproduction. The study reported here aimed to test if sound spatialization could affect eye movements and the processing of visual events in audiovisual scenes. We presented participants with audiovisual scenes of a metro station. The sound was either mono, stereo, or 3D. Participants wore eye tracking glasses during the experiment and their task was to count how many people entered the metro. In the divided attention task, participants had to count people entering 3 doors of the metro. In the selective attention task, participants had to count how many people entered the middle door alone. It was found that sound spatialization did not affect the divided attention task. But in the selective attention task participants counted less visual events with 3D sound. In that condition, the number of eye fixations and time spent in the visual area of interest were smaller. It is hypothesized that, in the case of divided attention, the attention is already disengaged and fluctuating, which could explain why sound did not have any additional effect. In the selective attention task, participants must remain concentrated in only one visual area and competing well-spatialized sounds in peripheral areas might have a negative impact. These results should be taken into consideration when designing sound spatialization algorithms and soundtracks.","2015-07","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JLSGZARE","journalArticle","2008","Droumeva, Milena; Wakkary, Ron","Understanding Aural Fluency in Auditory Display Design for Ambient Intelligent Environments","","","","","http://hdl.handle.net/1853/49967","This paper presents the design and some evaluation results from the auditory display model of an ambient intelligent game named socio-ec(h)o. socio-ec(h)o is played physically by a team of four, and displays information via a responsive environment of light and sound. Based on a study of 56 participants involving both qualitative and preliminary quantitative analysis, we present our findings to date as they relate to the auditory display model, future directions and implications. Based on our design and evaluation experience we begin building a theoretical understanding for the unique requirements of informative sonic displays in ambient intelligent and ubiquitous computing systems. We develop and discuss the emerging research concept of aural fluency in ambient intelligent settings.","2008-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NKBISNAR","book","2015","Powell, Nicholas; Lumsden, Jo","Exploring novel auditory displays for supporting accelerated skills acquisition and enhanced performance in motorsport","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54124","This paper explores the design, development and evaluation of a novel real-time auditory display system for accelerated racing driver skills acquisition. The auditory feedback provides concurrent sensory augmentation and performance feedback using a novel target matching design. Real-time, dynamic, tonal audio feedback representing lateral G-force (a proxy for tire slip) is delivered to one ear whilst a target lateral G-force value representing the ‘limit’ of the car, to which the driver aims to drive, is panned to the driver’s other ear; tonal match across both ears signifies that the ‘limit’ has been reached. An evaluation approach was established to measure the efficacy of the audio feedback in terms of performance, workload and drivers’ assessment of selfefficacy. A preliminary human subject study was conducted in a driving simulator environment. Initial results are encouraging, indicating that there is potential for performance gain and driver confidence enhancement based on the audio feedback.","2015-07","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RKSV2QEF","journalArticle","2004","Walsh, M.","Aural maps, neural futures","","","","","http://hdl.handle.net/1853/50831","“If the mental objects of philosophy, art, and science have a place it will be in the deepest synaptic fissures, in the hiatuses, intervals, and mean-times of the non-objectifiable brain, in a place where to go in search of them will be to create.” It would be fair to assume that developing the sonification of neurological data would add to our knowledge of the mind. Taking the above quote seriously, however, implies that this addition would also be a creation. This paper is concerned with the relation of scientific and artistic processes in an effort to aurally map the mind. In particular, it is concerned with the affect of this relation on the design processes needed to develop such a method. It will suggest that the mind listening to itself is productively scientific and creative.","2004-07","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EPUSSFE9","journalArticle","2021","Temor, Lucas; MacDonald, Daniel E.; Natarajan, Thangam; Coppin, Peter W.; Steinman, David A.","Perceptually-motivated sonification of spatiotemporally-dynamic CFD data","","","","","http://hdl.handle.net/1853/66343","Everyday perception and action are fundamentally multisensory. Despite this, the sole reliance on visualization for the representation of complex 3D spatiotemporal data is still widespread. In the past we have proposed various prototypes for the sonification of dense data from computational fluid dynamics (CFD) simulations of turbulent-like blood flow, but did not robustly consider the perception and associated meaning-making of the resultant sounds. To reduce some of the complexities of these data for sonification, in this work we present a feature-based approach, applying ideas from auditory scene analysis to sonify different data features along perceptually-separable auditory streams. As there are many possible features in these dense data, we followed the analogy of ""caricature"" to guide our definition and subsequent amplification of unique spectral and fluctuating features, while effectively minimizing the features common between simulations. This approach may allow for better insight into the behavior of flow instabilities when compared to our previous sonifications and/or visualizations, and additionally we observed benefits when some redundancy was maintained between modalities.","2021-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I25EB2H2","journalArticle","2022","Madaghiele, Vicenzo; Pauletto, Sandra","The sonic carpet: Real-time feedback of energy consumption and emission data through sonic interaction design","","","","","http://hdl.handle.net/1853/67383","As buildings become increasingly automated and energy efficient, the relative impact of occupants on the overall building carbon footprint is expected to increase. Research shows that by changing occupant behaviour energy savings between 5 and 15 % could be achieved. A commonly used device for energy-related behaviour change is the smart meter, a visual-based interface which provides users with data about energy consumption and emissions of their household. This paper approaches the problem from a Sonic Interaction Design point of view, with the aim of developing an alternative, sound-based design to provide feedback about some of the data usually accessed through smart meters. In this work, we experimented with sonic augmentation of a common household object, a door mat, in order to provide a non-intrusive everyday sonic interaction. The prototype that we built is an energy-aware sonic carpet that provides real-time feedback on home electricity consumption and emissions through sound. An experiment has been designed to evaluate the prototype from a user experience perspective, and to assess how users understand the chosen sonifications.","2022-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","The sonic carpet","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4I3FUS3S","book","2015","Wolf, KatieAnna; Gliner, Genna; Fiebrink, Rebecca","End-user development of sonifications using soundscapes","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54150","Designing sonifications requires knowledge in many domains including sound design, sonification design, and programming. Thus end users typically do not create sonifications on their own, but instead work with sonification experts to iteratively co-design their systems. However, once a sonification system is deployed there is little a user can do to make adjustments. In this work, we present an approach for sonification system design that puts end users in the control of the design process by allowing them to interactively generate, explore, and refine sonification designs. Our approach allows a user to start creating sonifications simply by providing an example soundscape (i.e., an example of what they might want their sonification to sound like), and an example dataset illustrating properties of the data they would like to sonify. The user is then provided with the ability to employ automated or semi-automated design of mappings from features of the data to soundscape controls. To make this possible, we describe formal models for soundscape, data, and sonification, and an optimization-based method for creating sonifications that is informed by design principles outlined in past auditory display research.","2015-07","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TLGCK4DD","journalArticle","2014","Mehra, Ravish; Antani, Lakulish; Manocha, Dinesh","Source Directivity and Spatial Audio for Interactive Wave-based Sound Propagation","","","","","http://hdl.handle.net/1853/52060","This paper presents an approach to model time-varying source directivity and HRTF-based spatial audio for wave-based sound propagation at interactive rates. The source directivity is expressed as a linear combination of elementary spherical harmonic sources. The propagated sound field due to each spherical harmonic source is precomputed and stored in an offline step. At runtime, the timevarying source directivity is decomposed into spherical harmonic coefficients. These coefficients are combined with precomputed spherical harmonic sound fields to generate propagated sound field at the listener position corresponding to the directional source. In order to compute spatial audio for a moving and rotating listener, an efficient plane-wave decomposition approach based on the derivatives of the sound field is presented. The source directivity and spatial audio approach have been integrated with the Half-Life 2 game engine and the Oculus Rift head-mounted display to enable realistic acoustic effects for virtual environments and games.","2014-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VUBPZP6S","journalArticle","2004","Mueller-Tomfelde, C.","Interaction sound feedback in a haptic virtual environment to improve motor skil acquisition","","","","","http://hdl.handle.net/1853/50860","This paper describes the concept and the realisation of a research prototype of a haptic environment that is enhanced with sound feedback to impart implicit knowledge and to teach motor skills to trainees. The sound feedback is to be understood as an additional feedback that provides information to the trainee about his or her actions in relation to objects in a virtual haptic environment. Although the setup simulates the interaction with haptic objects, the additional auditory feedback goes beyond the imitation of real sound behaviour and enables new ways to convey information about user interaction, i.e., movements in space with or without forces applied to tangible objects. The goal of the interaction with additional auditory feedback is to enrich the virtual environment to a full multimodal interaction environment, to support precise movements and to optimise the motor skill learning curve of trainees. The prototype is focused on computer based learning in the context of surgical training and it uses a virtual document planner that provides the appropriate instructions and tailored feedback for the human computer interaction through reasoning on the basis of an explicit knowledge representation.","2004-07","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CMDAWUHR","journalArticle","2008","Boggards, Niels","Sound Editing on the Sonogram","","","","","http://hdl.handle.net/1853/49946","Sound editing applications commonly use a waveform display to graphically represent a sound signal. This representation not only conveys little information relevant to sound design, it also severely limits the ways in which the user can interact with the sound. By placing the sonogram at the center, AudioSculpt provides a more intuitive and insightful visualization, while at the same time allowing new ways of interacting with the sound's content, such as copy/paste of parts of the spectrum.","2008-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NUAPXUDB","journalArticle","2012","Hermann, Thomas; Nehls, Anselm Venezian; Eitel, Florian; Barri, Tarik; Gammel, Marcus","Tweetscapes - Real-time Sonification of Twitter data streams for Radio Broadcasting","","","2168-5126","","http://hdl.handle.net/1853/44424","This paper introduces Tweetscapes, a system that transforms message streams from Twitter in real-time into a soundscape that allows the listener to perceive characteristics of twitter messages such as their density, origin, impact, or how topics change over time. Tweetscapes allows the listener to be in touch with the social platform / medium Twitter and to understand its dynamics. We developed Tweetscapes with and for the Sound Art department of Germany-wide radio program Deutschlandradio Kultur where the sonifications are now broadcast several times per week for a few minutes since October 2011. The goal was to create a new sense of media awareness and an example for how sound can support monitoring applications differently than mere alarms. This paper introduces the methods, the ideas, the design, the sounds, and it discusses our experiences with, and novel interaction possibilities offered by Tweetscapes.","2012-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q9GJW9MP","book","2009","Pirhonen, Antti; Tuuri, Kai","Using multiple, role-related perspectives in the design of alarm sounds for safety critical context","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51423","The requirements for alarm sounds for safety critical contexts are many, some of which may be conﬂicting. This study concerns the design of alarm sounds for a hospital environment, in particular operating room conditions. We describe the process of capturing sound design ideas in a form that could be utilised in practical sound design. The process is an application of the Rich Use- Scenario method, and provides an example of how this method should be tailored in terms of the context of use. The central finding in this study derives from the contribu- tion of people in three different roles in the design process. These roles were those of contextual practitioner, non-expert (man in the street), and sound designer. The design case illustrates the importance of including all these perspectives in the design process.","2009-05","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4ZTXF93D","journalArticle","2005","Vilimek, Roman; Hempel, Thomas","Effects of speech and non-speech sounds on short-term memory and possible implications for in-vehicle use","","","","","http://hdl.handle.net/1853/50156","Using auditory output for presenting non-critical but relevant events to the car driver, we compared the effect of four groups of sounds (two speech, two non-speech) on short-term memory and on response time and accuracy. The results indicate that longer speech messages can disrupt short-term memory performance whereas earcons, auditory icons, and single keywords do not cause this effect. Earcons, in turn, lead to comparatively long response times. Based on these experimental data, the suitability of such stimuli for in-vehicle representation is discussed. The type of experimental set-up may enable transfer of the results to comparable settings.","2005-07","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6MLLJ5L4","book","2015","Bălan, Oana; Moldoveanu, Alin; Nagy, Hunor; Wersényi, György; Botezatu, Nicolae; Stan, Andrei; Lupu, Robert-Gabriel","Haptic-auditory perceptual feedback based training for improving the spatial acoustic resolution of the visually impaired people","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54096","3D binaural sounds play an important role in the development of navigational systems for the blind people. The use of generic HRTFs in virtual auditory displays significantly affects the acoustic spatial resolution and the listener’s ability to make localization judgments regarding the sound sources situated inside the cone of confusion. The aim of this paper is to investigate whether haptic-auditory feedback based training can enhance sound localization performance, front-back discrimination and the navigational skills of the visually impaired people. In our experiments, we assessed the sound localization performance of nine visually impaired subjects before and after a series of haptic-auditory training procedures aimed to enhance the perception of 3D sounds. The results of our tests demonstrate that our subjects succeeded to improve their sound localization performance, reduced the incidence of angular precision and reversal errors and became able to build an effective spatial representation map of the acoustic environment.","2015-07","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SQQKJJ4Y","journalArticle","2003","Potard, Guillaume; Burnett, Ian","A study on sound source apparent shape and wideness","","","","","http://hdl.handle.net/1853/50441","This work is intended as an initial investigation into the perception of wideness and shape of sound sources. A method that employs multiple uncorrelated point sources is used in order to form “sound shapes”. Several experiments were carried out in which, after some initial training, subjects were asked to indentify the shapes that were being played. Results indicate that differences in vertical and horizontal source wideness are easily perceived and scenes that use broad sound sources to represent normally large sound objects are selected 70% of the time over point source versions. However, shape identification was found to be more ambiguous except for certain types of signals where results were above statistical probability. The work indicates that shape and wideness of sound sources could be effectively used as extra cues in virtual auditory displays and generally improve the realism of virtual 3D sound scenes. This work was performed as a Core Experiment within the MPEG Audio Subgroup with the intention of possible integration of source wideness into MPEG-4 AudioBIFS.","2003-07","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q5A7BR4D","journalArticle","2005","Bruce, J. W.; Palmer, N. T.","SIFT: Sonification integrable flexible toolkit","","","","","http://hdl.handle.net/1853/50179","This paper describes work-in-progress on a platformindependent toolkit for sonification of scientific data. The data being displayed and the sonification control information can be provided in real-time and distributed over a wide area via Ethernet. The toolkit allows the designer to process, scale, and map data to a wide variety of sonification parameters and methods. Sonification processing and control commands are stored in standard XML syntax files and can be applied or modified in real-time. The toolkit described here is easily added to existing visualization applications and can be quickly expanded to use new data formats and sonification modalities. Early results of interactive auditory and visual analysis of an example domain are described, and extensive user tests are being planned.","2005-07","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","SIFT","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NRSWQRNQ","journalArticle","2011","Bearman, Nick","Using Sound to Represent Uncertainty in Future Climate Projections for the United Kingdom","","","","","http://hdl.handle.net/1853/51922","This paper compares different visual and sonic methods of representing uncertainty in spatial data. When handling large volumes of spatial data, users can be limited in the amount that can be displayed at once due to visual saturation (when no more data can be shown visually without obscuring existing data). Using sound in combination with visual methods may help to represent uncertainty in spatial data and this example uses the UK Climate Predictions 2009 (UKCP09) dataset; where uncertainty has been included for the first time. Participants took part in the evaluation via a web-based interface which used the Google Maps API to show the spatial data and capture user inputs. Using sound and vision together to show the same variable may be useful to colour blind users. Previous awareness of the data set appears to have a significant impact (p \textless 0.001) on participants ability to utilise the sonification. Using sound to reinforce data shown visually results in increased scores (p = 0.005) and using sound to show some data instead of vision showed a significant increase in speed without reducing effectiveness (p = 0.033) with repeated use of the sonification.","2011-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JPVIMYPR","journalArticle","2019","Dewhurst, David","The design and exploration of auditory display effects for blind drivers in autonomous vehicles","","","","","http://hdl.handle.net/1853/61545","This work forms a part of a wider project, in which the author is developing a system to present visual images, and other material, via sets of auditory (and tactile) display effects. The main contribution of this paper is to describe the design, and examine the effectiveness, of these effects in an automotive context, specifically in the context of blind drivers travelling in autonomous (self-driving) vehicles. This paper also brings together and summarizes auditory display effects and techniques that have previously been reported by the author, and describes several new features. The effects are termed tracers; polytracers; drone and matrix effects; imprints; and multi-level multi-talker ﾓfocusﾔ effects. The paper describes the potential automotive application of such auditory display effects in:- command and control; route presentation; maps/cartography; and enhancing the journey experience of blind travelers. Methods of presenting rectangular areas within a scene, (termed ﾓaudio previewsﾔ) are described and discussed, as is the concept of a small set of effects termed a ﾓglimpseﾔ. The results of informal assessment sessions with a totally blind person, and two sighted people, are described.","2019-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KY7S8GW3","book","2009","Schaffert, Nina; Mattes, Klaus; Effenberg, A. O.","A sound design for the purposes of movement optimisation in elite sport (using the example of rowing)","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51288","Monitoring sportive movements is essential for training processes to detect variations as well as progression, stagnations or even regressions. Visualization plays the dominant role in the technique analysis, even though the eyes’ ability to perceive information of time-related events are limited and less efficient in comparison to the ears. Sound represents the information more differentiated and can support motion sequences. Acoustic displays offer a promising alternative to visual displays. Therefore an appropriate sound is needed that represents the specific movement patterns of a cyclic motion. In this paper we present our current considerations towards basic requirements for a sound design that fulfils the specific purposes of movement optimisation and its acceptance in elite sport.","2009-05","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZKQGTUEV","journalArticle","2012","Parseihian, Gaëtan; Katz, Brian F. G.; Conan, Simon","Sound Effect Metaphors for Near Field Distance Sonification","","","2168-5126","","http://hdl.handle.net/1853/44435","This article presents a concept of distance sound source sonification for virtual auditory displays in the context of the creation of an assistive device for the visually impaired. In order to respond to user needs, three sonification metaphors of distance based on sound effects were designed. These metaphors can be applied to any type of sound and thereby satisfy all aesthetic desires of users. The paper describes the motivation to use this new type of sonification based on sound effects, and proposes guidelines for the creation of these three metaphors. It then presents a user evaluation of these metaphors by 16 subjects through a near field sound localization experiment. The experiment included a simple binaural rendering condition in order to compare and quantify the contribution of each metaphor on the distance perception.","2012-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GLUCN5XG","book","2010","Jagadeesan, Sharman; Gröhn, Matti","Real-time Sound Synthesis Using an Inexpensive Wireless Game Controller","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49910","Due to the development of the sensor technology it is possible to manufacture wireless multi-degrees of freedom controllers at reasonable costs. We have tested one manufactured by a small finnish start-up company in controlling real-time sound synthesis parameters. According to our experience, it is very suitable for it.","2010-06","2023-07-13 06:25:51","2023-07-13 06:25:51","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CEI9E87H","journalArticle","2008","Ballas, James A.; Nevitt, Justin","Using Web Services to Foster Global Collaboration in Sound Design","","","","","http://hdl.handle.net/1853/49966","The migration of client-server systems to web services using Service Oriented Architecture (SOA) design principles is widespread and likely to dominate the future evolution of computing. Use of web services is especially challenging for streaming content such as that which would be used for sound design. This paper describes the principles of a Service Oriented Architecture (SOA) and ways that it could support sound design and foster global collaboration across the web.","2008-06","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GH4ZZZDC","journalArticle","1996","Walker, Bruce N.; Kramer, Gregory","Mappings and metaphors in auditory displays: An experimental assessment","","","","","http://hdl.handle.net/1853/50812","Auditory displays are becoming increasingly common, but there are still no general guidelines for mapping data dimensions (e.g., temperature) onto display dimensions (e.g., pitch). This paper presents experimental research on different mappings and metaphors in a generic process-control task environment with reaction time and accuracy as dependent measures. It is hoped that this area of investigation will lead to the development of mapping guidelines applicable to auditory displays in a wide range of task domains. Some keywords in this paper include mapping, metaphor, and guidelines.","1996-11","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","Mappings and metaphors in auditory displays","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8E3XHJEW","journalArticle","2008","McGregor, Iain; Crerar, Alison; Benyon, David; Leplatre, Gregory","Visualising the Soundfield and Soundscape: Extending Macaulay and Crerar's 1998 Method","","","","","http://hdl.handle.net/1853/49968","The introduction of effective auditory warnings into a shared environment requires a prior understanding of the existing soundfield and soundscape. Reifying the physical and perceptual auditory environment enables a form of pre auditioning, as well as the evaluation of any auditory augmentation. This paper describes the development of a visualisation technique for soundscape mapping. Building on earlier published work in sound classification, we report data captured using eighteen participants in a shared office environment. The resulting sound classification is used as the basis of a pictorial soundscape and soundfield visualisation. We show how this representation can be used to model the experiences of individuals, as well as subsets of users of the space.","2008-06","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","Visualising the Soundfield and Soundscape","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PWQDCEZD","journalArticle","1998","Williams, David","'The sound of silence': A preliminary experiment investigating non-verbal auditory representations in telephone-based automated spoken dialogues","","","","","http://hdl.handle.net/1853/50708","At the lexical level, a typical human-computer dialogue in an aural-only spoken language system consists of two stages, system output and user input. As with human-human conversation, a good proportion of turn taking clues are given by lapses in talk. Unfortunately, in telephone-based automated spoken dialogues, silences on the system's part may not be so easily resolved. A pilot experiment examined the recogniser listening and processing states and showed that auditory icons representing these caused fewer incorrect user responses than the control condition. However, where system prompts explicitly requested a response, icons were not necessary if talkover was provided. Also, the effectiveness of auditory representations had a strong interaction with the expertise of the caller suggesting that expert users may require a period of acclimatisation to the use of sounds as they tend to listen to them due to novelty. Conversely, novice users with no experience acted correctly.","1998-11","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","'The sound of silence'","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PMD7BMU3","journalArticle","2021","Quinton, Michael; McGregor, Iain; Benyon, David","Sonification of exosolar system accretion discs","","","","","http://hdl.handle.net/1853/66327","This study investigated the design and evaluation of a sonification, created for an astronomer who studies exosolar accretion discs. User design methods were applied to sonify data that could allow the classification of accretion discs. The sonification was developed over three stages: a requirements gathering exercise that inquired about the astronomer's work and the data, design and development, as well as an evaluation. Twenty datasets were sonified and analysed. The sonification effectively represented the accretion discs allowing the astronomer to commence a preliminary, comparative classification. Multiple parameter mappings provide rich auditory stimuli. Spatial mapping and movement allow for easier identification of fast changes and peaks in the data which improved the understanding of the extent of these changes.","2021-06","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"94UURW3L","journalArticle","2013","Albrecht, Robert; Lokki, Tapio","Adjusting The Perceived Distance Of Virtual Speech Sources By Modifying Binaural Room Impulse Responses","","","","","http://hdl.handle.net/1853/51673","Effective control of the perceived location of virtual sound sources is an important aspect of auditory displays. While room acoustics modelling may be used to produce cues related to the sound source and listener location in a space, in many real-time applications it is more feasible to utilize ready-made room impulse responses. This paper looks at how the perception of distance can be affected by modifying the temporal envelopes of room impulse responses. Two measured binaural room impulse responses were modified by amplifying or attenuating different portions of them before convolving them with speech samples. Listeners were asked to judge the relative distances between these virtual speech sources presented over headphones. The results suggest that the perception of distance is more effectively altered by modifying an early-tolate energy ratio, where approximately 50–100 ms of the impulse response is included in the early energy, than by directly modifying the traditional direct-to-reverberant energy ratio.","2013-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IARX5P8T","journalArticle","1997","Miner, Nadine E.; Caudell, Thomas P.","Using wavelets to synthesize stochastic-based sounds for immersive virtual environments","","","","","http://hdl.handle.net/1853/50755","Stochastic, or non-pitched, sounds fill our real world environment. Humans almost continuously hear stochastic sounds such as wind, rain, motor sounds, and different types of impact sounds. Because of their prevalence in real-world environments, it is important to include these types of sounds for realistic virtual environment simulations. This paper describes a synthesis approach that uses wavelets for modeling stochastic-based sounds. Parameterizations of the wavelet models yield a variety of related sounds from a small set of models. The result is dynamic sound models that can change according to changes in the virtual environment. This paper contains a description of the sound synthesis process, several developed models, and the on-going perceptual experiments for validating the sound synthesis veracity. The developed models and results demonstrate proof of the concept and illustrate the potential of this approach.","1997-11","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CZX9NHYN","journalArticle","2004","Janata, P.; Childs, E.","Marketbuzz: Sonification of real-time financial dataa","","","","","http://hdl.handle.net/1853/50899","A system for the sonification of real-time financial data, currently in use by financial traders in five pilot projects, is described. Anecdotal feedback from the pilot projects suggests that the auditory display is more effective and consistent for monitoring the movement of volatile market indices. The same system has also been tested in two experiments carried out at the Department of Psychological and Brain Sciences at Dartmouth College. In the first experiment, subjects performed “change of direction” monitoring tasks of varying difficulty with and without auditory display. The results indicated a significant increase in accuracy when the auditory display was used. In the second experiment, subjects performed the same monitoring task with and without auditory display but were given a second, “number-matching” task which forced them to direct their visual attention away from the “change of direction” task from time to time. The auditory display increased accuracy more dramatically than in the first experiment, since the subjects were able to rely on the sonification to perform the “change of direction” monitoring task when they were distracted with the “number-matching” task.","2004-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","Marketbuzz","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"58KZYW4K","journalArticle","1998","Macaulay, Catriona; Crerar, Alison","'Observing' the workplace soundscape: Ethanography and auditory interface design","","","","","http://hdl.handle.net/1853/50707","This paper identifies a gap in the research agenda of the auditory display community – the study of work practice and the uses (current and potential) of the workplace `soundscape'. The paper presents a case study derived from a one year activity theory-oriented ethnographic study of information gathering work at a UK daily newspaper. We consider the soundscape aspects of mediating collaborative activity in the newsroom, and conclude with a discussion of the issues arising from this attempt to utilise ethnographic techniques within the auditory display design domain.","1998-11","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","'Observing' the workplace soundscape","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SNH9X92F","journalArticle","2001","Martens, William L.","Psychophysicl calibration for controlling the range of a virtual sound source: Multidimensional complexity in spatial auditory display","","","","","http://hdl.handle.net/1853/50628","Just as control over perceived azimuth and elevation of a virtual sound source should be psychophysically calibrated in spatial auditory display, so should perceived range; however, in contrast to azimuth and elevation display, precise control over auditory range has been difficult to achieve. This is partly due to the multidimensional complexity of the human response to spatial auditory stimulation, but it is also due to the multidimensional complexity of the acoustic stimulus for range, which includes a substantial number of independent parameters even in the case of static spatial positioning of sound source relative to listener. In the static case, there is strong dependence of perceived range upon at least the following display parameters: direct sound level, indirect sound level, interaural cross-correlation, and the relation between direct and indirect sound spectra associated with air absorption and close-range head-related effects. If the sound source range varies smoothly over time, other display parameters (such as dynamic variation in pitch of the direct sound, or Doppler shift, and also dynamic variation in the initial time gap) become significant, and interact with the above-listed parameters to produce changes in auditory range that have proven difficult to successfully model. In the absence of a model that integrates variation in all of these display parameters and successfully predicts range variation, two reasonable solutions to the problem of range control present themselves. The first is to base control upon highly realistic simulation, relying on the relatively good match between perceived range and specified range that can be observed when nearly all displayed auditory spatial information is consistent with an adequate physical model. The second solution is to base control upon psychophysical range judgments under conditions of expected use of the display, relying on an inversion of a range prediction model fit to the judgments using multiple regression analysis. This paper presents two examples of successful psychophysical calibration for auditory range control for spatially static sources: One case employed a simplified model of range-dependence in simulated head-related transfer functions for headphone display of virtual sources at close range (within the listener's personal space). The other case employed a room-related (rather than head-related) loudspeaker-based sound simulation to create auditory imagery of sources relatively far away from a group of simultaneous listeners. This room-related loudspeaker system was designed with the pragmatic goal of reducing reliance upon fixed, known listening locations. In both cases, adequate control over the range of a set of sound sources (short speech samples) was achieved using a look-up table derived by inverting the range prediction equation fit to collected human range ratings.","2001-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","Psychophysicl calibration for controlling the range of a virtual sound source","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9J7C9RDK","journalArticle","2012","Pirrò, David; Wankhammer, Alexander; Schwingenschuh, Petra; Höldrich, Robert; Sontacchi, Alois","Acoustic interface for tremor analysis","","","2168-5126","","http://hdl.handle.net/1853/44437","In this paper we introduce new methods for real-time acoustical tremor diagnosis. We outline the problems of tremor diagnosis in the clinical context and discuss how sonification can complement and expand the existing tools neurologists have at their disposal. Based on three preliminary sonification experiments upon recorded tremor movement data, we show how temporal as well as spectral characteristics of a tremor can be made audible in realtime. Our first observations indicate that differences among tremor types can be made recognizable via sonification. Therefore, we suggest that the proposed methods could allow for the formulation of more confident diagnoses. At the end of the paper, we will also shortly outline the central topics of future research.","2012-06","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FVTUWZ72","journalArticle","2006","Walker, Bruce N.; Nance, Amanda; Lindsay, Jeffrey","Spearcons: speech-based earcons improve navigation performance in auditory menus","","","","","http://hdl.handle.net/1853/50642","With shrinking displays and increasing technology use by visually impaired users, it is important to improve usability with non-GUI interfaces such as menus. Using non-speech sounds called earcons or auditory icons has been proposed to enhance menu navigation. We compared search time and accuracy of menu navigation using four types of auditory representations: speech only; hierarchical earcons; auditory icons; and a new type called spearcons. Spearcons are created by speeding up a spoken phrase until it is not recognized as speech. Using a within-subjects design, participants searched a 5 x 5 menu for target items using each type of audio cue. Spearcons and speech-only both led to faster and more accurate menu navigation than auditory icons and hierarchical earcons. There was a significant practice effect for search time, within each type of auditory cue. These results suggest that spearcons are more effective than previous auditory cues in menu-based interfaces, and may lead to better performance and accuracy, as well as more flexible menu structures.","2006-06","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","Spearcons","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WVE2YY2Y","journalArticle","2001","Muller-Tomfelde, Christian; Steiner, Sascha","Audio-enhanced collaboration at an interactive electronic whiteboard","","","","","http://hdl.handle.net/1853/50514","This paper describes an experimental setup to investigate new possibilities to support cooperative work of a team with audio feedback on a large interactive electronic whiteboard, called DynaWall®. To enrich the interaction and the feedback qualities within a team work situation the DynaWall is equipped with a set of loudspeakers which are invisibly integrated into the environment. Different forms of audio feedback are realized and discussed to meet the requirements for collaborative team work situations. An audio feedback for a gesture interface with sound cues is implemented to improve the use of gestures to execute commands. Furthermore a spatial sound property of moved and thrown information objects on the surface of the otherwise silent electronic whiteboard is introduced to add an imitated natural sound behavior. The focus of the setup is to experiment with sound feedback for a non-standard computer environment useful in a cooperative team work situation.","2001-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GMHIXMIY","book","2015","Spagnol, Simone; Avanzini, Federico","Anthropometric tuning of a spherical head model for binaural virtual acoustics based on interaural level differences","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54141","In this paper we propose a method to customize a spherical head model for binaural sound rendering based on the listener’s anthropometry. Interaural level difference (ILD) information from a HRTF database is used to subjectively tune the radius parameter of the spherical model so as to best fit individual measures. Multiple linear regression on anthropometric data is performed, yielding a closed formula relating the three head dimensions to the ILD-optimized radius. The effectiveness of the proposed radius estimation method in predicting the correct ILD with a spherical model is compared to that of alternative methods from the literature. Results show that the average spectral distortion between experimental and predicted ILDs with our method is significantly lower than with other estimation methods for lateral source locations. The proposed customization approach provides substance towards the development and evaluation of personal auditory displays for binaural virtual acoustics.","2015-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X3ASBQL3","journalArticle","2001","Tsingos, Nicolas","A versatile software architecture for virtual audio simulations","","","","","http://hdl.handle.net/1853/50609","Existing real-time audio rendering architectures provide rigid development frameworks which are not adapted to a wide range of applications. In particular, experimenting with new rendering techniques is virtually impossible. In this paper, we present a novel, platform-independent software architecture that is well suited for experimenting with multichannel audio mixing, geometrical acoustics and 3D audio processing in a single framework. Our architecture is divided into two layers. A low level DSP layer is responsible for streaming and processing audio buffers using a general filter-based formalism. Built on top is an audio rendering layer responsible for general geometry-based audio rendering and configuration of the rendering setup. In particular, we introduce sequence objects which we use to define and control arbitrary sound propagation paths in a geometrical 3D virtual environment. We discuss implementation details and present a variety of prototype applications which can be efficiently designed within the proposed framework.","2001-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DNJN24V5","journalArticle","2007","McGee-Lennon, Marilyn R.; Wolters, Maria; McBryan, Tony","Audio Reminders in the Home Environment","","","","","http://hdl.handle.net/1853/49977","In this paper we report an experimental comparison between three different types of audio reminders in the home setting: speech, earcons, and a simple pager sound. We examine how quickly and accurately participants were able to interpret the reminders, and to what extent presentation of the reminders interfered with a digit span background task. In addition, a questionnaire was used to gather user preferences and attitudes towards the different types of reminders. Although participants perform best with speech reminders, there are large inter-subject differences in performance, and over 50% prefer non-speech audio reminders. The implications for the design and application of auditory interfaces for home-based reminder systems are discussed.","2007-06","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FE992J36","journalArticle","2000","Leplatre, Gregory; Brewster, Stephen A.","Designing non-speech sounds to support navigation in mobile phone menus","","","","","http://hdl.handle.net/1853/50670","This paper describes a framework for integrating non-speech audio to hierarchical menu structures where the visual feedback is limited. In the first part of this paper, emphasis is put on how to extract sound design principles from actual navigation problems. These design principles are then applied in the second part, through the design, implementation and evaluation of a set of sounds in a computer-based simulation of the Nokia 6110 mobile phone. The evaluation indicates that non-speech sound improves the performance of navigational tasks in terms of the number of errors made and the number of keypresses taken to complete the given tasks. This study provides both theoretical and practical insights about the design of audio cues intended to support navigation in complex menu structures.","2000-04","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AE48EJPC","journalArticle","2004","Potard, G.; Burnett, I.","Control and measurement of apparent sound source width and its applications to sonification and virtual auditory displays","","","","","http://hdl.handle.net/1853/50849","The aim of this paper is to investigate the possibility of using the spatial extent of sound sources as a mean of carrying information in sonification designs. To do so, we studied the accuracy of the perception of artificially produced sound source extent in a 3D audio environment. We found that the source extent perceived by subjects matched relatively well the intended source extent. Thus source extent could be used as a tool to represent areas, sizes and regions in virtual auditory displays. This paper also reviews the technologies involved in the reproduction and measurement of spatially extended sound sources. Finally, it is shown that the perception of sound source extent can be sensitive to temporal and spectral variations thereby adding extra sonification parameters","2004-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GPAZHY9B","journalArticle","2007","Reiter, Ulrich; Weitzel, Mandy","Influence of Interaction on Perceived Quality in Audiovisual Applications: Evaluation of Cross-Modal Influence","","","","","http://hdl.handle.net/1853/50009","This paper presents a subjective assessment among 32 test subjects performed to investigate the question of possible cross-modal division of attention in interactive audiovisual application systems. We give an overview on recent related research, and we describe in detail the experimental setup, the procedure and the analysis of the data obtained. As a result, the experiment described here verifies that interaction or task can have an influence upon the perceived audio quality, even if the interaction / task is performed in another modality.","2007-06","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","Influence of Interaction on Perceived Quality in Audiovisual Applications","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZNM9FNKE","journalArticle","2002","Hermann, T.; Meinicke, P.; Bekel, H.; Ritter, H.; Mueller, H. M.; Weiss, S.","Sonifications for EEG data analysis","","","","","http://hdl.handle.net/1853/51378","This paper presents techniques to render acoustic representations for EEG data. In our case, data are obtained from psycholinguistic experiments where subjects are exposed to three different conditions based on different auditory stimuli. The goal of this research is to uncover elements of neural processing correlated with high-level cognitive activity. Three sonifications are presented within this paper: spectral mapping sonification which offers a quite direct inspection of the recorded data, distance matrix sonification which allows to detect nonlinear long range correlations at high time resolution, and differential sonification which summarizes the comparison of EEG measurements under different conditions for each subject. This paper describes the techniques and presents sonification examples for experimental data.","2002-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TINK3JZV","journalArticle","2001","Kamel, Hesham M.; Roth, Patrick; Sinha, Rashmi","Graphics and user's exploration via simple sonics (GUESS): Providing interrelational representation of objects in a non-visual environment","","","","","http://hdl.handle.net/1853/50620","In this research we investigated the use of the GUESS system in the exploration of auditory pattern perception by blind and visually impaired people. We have compared three different techniques for presenting graphical scenes via non-speech sounds: one based on the physical tablet, one on the virtual-sonic grid, and one on sound localization techniques. In each technique we utilized a 2D sound plane to represent different geometric shapes. As an input device, we used a graphical tablet in order to explore the images rendered. We have conducted a pilot study with three groups of four participants each. Our results have shown that with the second and third techniques, blind people were able, within a relatively short space of time, to precisely identify the interrelation of simple geometric shapes. They have also shown that, in the second technique, assigning a non-speech sound to a region located in the center of the tablet reduced the navigation time when relocating specific shapes. As to the first technique, it received the lowest time rating for relocating objects. Our findings indicate that the method of presenting interrelation in auditory interface designs does indeed play an important role in assisting users comprehend the diagrams communicated.","2001-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","Graphics and user's exploration via simple sonics (GUESS)","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RKBE3ZER","journalArticle","2011","McGee, Ryan; Van der Veen, Jatila; Wright, Matthew; Kuchera-Morin, JoAnn; Alper, Basak; Lubin, Philip","Sonifying the Cosmic Microwave Background","","","","","http://hdl.handle.net/1853/51765","We present a new technique to sonify the power spectrum of the map of temperature anisotropy in the Cosmic Microwave Background (CMB), the oldest observable light of the universe. According to the Standard Cosmological Model, the universe be- gan in a hot, dense state, and the first 380,000 years of its ex- istence were dominated by a tightly coupled plasma of baryons and photons, which was permeated by gravity-driven pressure os- cillations - sound waves. The imprint of these primordial sound waves remains as light echoes in the CMB, which we measure as small-amplitude red and blue shifts in the black body radiation of the universe, with a typical angular scale of one degree. With our software, users can observe how the temperature map and power spectrum of the CMB change in response to different compositions of baryonic matter, dark matter, and dark energy, and explore these different universes in ‘sound space.’ Our simulation is designed to enhance understanding of how we can infer properties of the uni- verse from the power spectrum of CMB temperature anisotropies. We discuss the theory, the software, and potential applications in education.","2011-06","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IMRB5DJR","journalArticle","2002","Kilander, F.; Loennqvist, P.","A whisper in the woods - an ambient soundscape for peripheral awareness of remote processes","","","","","http://hdl.handle.net/1853/51338","The concept of a weakly intrusive ambient soundscape (WISP) is presented as a means to provide a peripheral awareness of processes beyond a user's immediate attention. The WISP is a component in a larger environment for ubiquitous computing, centered around a conference room scenario. The experiences from a demonstration prototype indicate that the choice of sounds and the intensity of their presentation can greatly influence the way the WISP is perceived. The work relates in various ways to the sonification of data and audio-based techniques for maintaining peripheral awareness.","2002-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E55KJVLA","journalArticle","2013","Tordini, Francesco; Bregman, Albert S.; Cooperstock, Jeremy R.; Ankolekar, Anupryia; Sandholm, Thomas","Toward An Improved Model Of Auditory Saliency","","","","","http://hdl.handle.net/1853/51667","While visual saliency models are approaching maturity, their auditory counterparts remain in their infancy. This is mainly due to the difficulties of gathering basic data, and oversimplifications such as an assumption of monaural signals. Moreover, conventional testing approaches for evaluating auditory saliency models tend to be overly simplistic. To address these shortcomings, we developed an experimental procedure for testing auditory saliency along with more formalized stimulus-selection criteria to support more versatile and ecologically relevant saliency models. This work is described, along with an analysis of some relevant acoustical correlates that emerge from the experiments. The results motivate the formulation of a measure of sound complexity and appear to favor time-domain, rather than frequency-domain analysis to describe saliency. Finally, some conclusions are drawn regarding the definition of an expanded feature set to be used for auditory saliency modeling and prediction in the context of natural, everyday sounds.","2013-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UWEJ34F7","journalArticle","2016","Quinton, Michael; McGregor, Iain; Benyon, David","Sonifying the Solar System","","","","","http://hdl.handle.net/1853/56572","Sound is potentially an effective way of analysing data and it is possible to simultaneously interpret layers of sounds and identify changes. Multiple attempts to use sound with scientific data have been made, with varying levels of success. On many occasions this was done without including the end user during the development. In this study a sonified model of the 8 planets of our solar system was built and tested using an end user approach. The sonification was created for the Esplora Planetarium, which is currently being constructed in Malta. The data requirements were gathered from a member of the planetarium staff, and 12 end users, as well as the planetarium representative tested the sonification. The results suggest that listeners were able to discern various planetary characteristics without requiring any additional information. Three out of eight sound design parameters did not represent characteristics successfully. These issues have been identified and further development will be conducted in order to improve the model.","2016-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TJ4RH85Z","book","2009","Larsson, Pontus","Earconsampler: a tool for designing emotional auditory driver-vehicle interfaces","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51295","EarconSampler is a simple tool for designing and modifying auditory driver-vehicle interfaces. It allows for creating melodic patterns of wav-snippets and easy adjustment of parameters such as tempo and pitch. It also contains an analysis section where sound quality parameters, urgency and emotional response to the sound is calculated / predicted, so that the user directly can see how a certain parameter affects perception and emotional response.","2009-05","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","Earconsampler","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GMSIIW28","journalArticle","2012","Revuelta Sanz, Pablo; Ruiz-Mezcua, Belén; Sánchez Pena, Jose M.","A sonification proposal for safe travels of blind people","","","2168-5126","","http://hdl.handle.net/1853/44439","Sonification is one of the most natural ways to complete the information perceived by the blinds. Thus, it has been widely applied to create assistive products to help these collective in their daily life. In our case, we are working in a mobility device which transforms the depth map of a scene into a set of sounds, comprehensible by the user. Our sonification proposal is based on the opinions of experts and potential users, recovered by different interviews which crystallize in the herein explained sonification. This proposal follows the so-called point transform, which allows real-time sonification and quite accurate localization of the sound sources. However, some modifications to avoid ambiguous situations are also implemented and explained in this study.","2012-06","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I8CXU6GS","journalArticle","2001","Ilmonen, Tommi","Mustajuuri - An application and toolkit for interactive audio processing","","","","","http://hdl.handle.net/1853/50624","Mustajuuri is a freeware application and toolkit for audio signal processing. It is designed for quick prototyping, testing and combination of audio or MIDI processing modules. Its main focus is on efficient and low-latency real-time operation. Mustajuuri offers an extremely flexible plugin architecture. By creating new plugins programmmers can extend Mustajuuri to meet new needs. The C++ API takes into account the necessary features of audio signal processing and application development: low-latency real-time audio signal processing, easy debugging, graphical and text-mode user interfaces, internationalization and portability (currently supported on IRIX and Linux, but Windows port is possible). Mustajuuri also offers developers a rich set of support libraries that contain audio-related signal processing and graphics tools. Mustajuuri is licensed under the Library GNU Public License and it is available for download from the home-page: http://www.tml.hut.fi/ tilmonen/mustajuuri/.","2001-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TCL2VWTL","journalArticle","2007","Harrar, Lila; Stockman, Tony","Designing Auditory Graph Overviews: An Examination of Discrete vs. Continuous Sound and the Influence of Presentation Speed","","","","","http://hdl.handle.net/1853/49990","A number of studies have reported that auditory graphs (AGs) can be used successfully by individuals to gain an overview of data series. Very little however is known about the effects that changing presentation parameters of AGs has on user's' ability to gain an overview or identify specific graph characteristics. This study investigates the effect of varying graph complexity, speed and mode of presentation of AGs. We examine the effects of these variations on graph comprehension as a whole and on specific graph analysis tasks such as point estimation.","2007-06","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","Designing Auditory Graph Overviews","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5T9QAESX","book","2015","Metatla, Oussama; Bryan-Kinns, Nick; Stockman, Tony; Martin, Fiore","Sonifications for digital audio workstations: Reflections on a participatory design approach","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54120","Methods to engage users in the design process rely predominantly on visual techniques, such as paper prototypes, to facilitate the expression and communication of design ideas. The visual nature of these tools makes them inaccessible to people living with visual impairments. Additionally, while using visual means to express ideas for designing graphical interfaces is appropriate, it is harder to use them to articulate the design of non-visual displays. We applied a user-centred approach that incorporates various participatory design techniques to help make the design process accessible to visually impaired musicians and audio production specialists to examine how auditory displays, sonification and haptic interaction can support some of their activities. We describe this approach together with the resulting designs, and reflect on the benefits and challenges that we encountered when applying these techniques in the context of designing sonifications to support audio editing.","2015-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","Sonifications for digital audio workstations","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5G2T6CUC","journalArticle","2004","Avanzini, F.; Rocchesso, D.; Serafin, S.","Friction sounds for sensory substitution","","","","","http://hdl.handle.net/1853/50855","This paper explores the use of a physics-based sound model of continuous contact for auditory display in interactive settings. An audio-visual interactive display is developed in which the sound model is controlled by the user's gestures. The display is used to investigate to what extent audition can substitute for haptic feedback in conveying perception of inertial properties of a manipulated object. In a first experiment the audio-visual display is controlled through a standard pointing device (a marble mouse, or trackball). A second experiment uses a tangible object and a computer vision system that tracks the object motion. Early results suggest that the perception of effort is a cross-modal phenomenon in which auditory feedback plays a relevant role.","2004-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KD8QWNWH","journalArticle","2008","Ferguson, Sam; Cabrera, Densil","Exploratory Sound Analysis: Sonifying Data About Sound","","","","","http://hdl.handle.net/1853/49905","Sound is commonly analysed subjectively by listening to it. However, when we want to analyse a sound objectively, we often switch domains, and change to visual or numerical displays. While it is likely that, generally speaking, the visual sense dominates other senses, when the data being explored are sound the question naturally arises as to whether these data may be statistically represented in that same domain. This paper describes and demonstrates a general scheme for building statistical representations of sound that exist entirely within the auditory domain, and use the original audio data to present descriptive data.","2008-06","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","Exploratory Sound Analysis","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ILMLG5SJ","journalArticle","2002","Groehn, M.","Localization of a moving virtual sound source in a virtual room, the effect of a distracting auditory stimulus","","","","","http://hdl.handle.net/1853/51362","An audio localization test of moving virtual sound sources was carried out in a spatially immersive virtual environment, using loudspeaker array with vector based amplitude panning for reproduction of sound sources. Azimuth and elevation error in localization was measured. In this experiment the main emphasis was to explore the effect of a distracting auditory stimulus. Eight subjects accomplished a set of localization tasks. In these tasks they perceived the azimuth more accurately than the elevation. The distracting auditory stimulus decreased the localization accuracy. There was large variation between the subjects. The median error in azimuth for the most inaccurate subject was approximately twice as much as for the most accurate subject. The amount of the localization blur was dependent on angular distance from virtual sound source position to the nearest loudspeaker. The localization blur increased while the angular distance increased. Results of this experiment were compared with the results achieved in our previous experiment without the distracting stimulus.","2002-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GDGY37TY","journalArticle","2021","Huang, Mincong (Jerry); Chabot, Samuel; Braasch, Jonas","Panoptic reconstruction of immersive virtual soundscapes using human-scale panoramic imagery with visual recognition","","","","","http://hdl.handle.net/1853/66346","This work, situated at Rensselaer's Collaborative-Research Augmented Immersive Virtual Environment Laboratory (CRAIVELab), uses panoramic image datasets for spatial audio display. A system is developed for the room-centered immersive virtual reality facility to analyze panoramic images on a segment-by-segment basis, using pre-trained neural network models for semantic segmentation and object detection, thereby generating audio objects with respective spatial locations. These audio objects are then mapped with a series of synthetic and recorded audio datasets and populated within a spatial audio environment as virtual sound sources. The resulting audiovisual outcomes are then displayed using the facility's human-scale panoramic display, as well as the 128-channel loudspeaker array for wave field synthesis (WFS). Performance evaluation indicates effectiveness for real-time enhancements, with potentials for large-scale expansion and rapid deployment in dynamic immersive virtual environments.","2021-06","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MPU4R7J6","journalArticle","2004","van den Doel, K.","Physically-based models for liquid sounds","","","","","http://hdl.handle.net/1853/50904","A physically based liquid sound synthesis methodology is developed. The fundamental mechanism for the production of liquid sounds is identified as the acoustic emission of bubbles. After reviewing the physics of vibrating bubbles as it is relevant to audio synthesis, a sound model for isolated single bubbles is developed and validated with a small user study. A stochastic model for the real-time interactive synthesis of complex liquid sounds such as produced by streams, pouring water, rivers, rain, and breaking waves is based on the synthesis of single bubble sounds. It is shown how realistic complex high dimensional sound spaces can be synthesized in this manner.","2004-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8HGADIZS","book","2015","Tünnermann, René; Leichsenring, Christian; Bovermann, Till; Hermann, Thomas","Upstairs: A calm auditory communication and presence system","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54147","For decades, researchers have been creating and evaluating so-called media spaces. Most of those were virtual spaces that bridge physical distance in order to create a common shared space. In the tradition of these spaces, upstairs supports peripheral awareness between non-colocated spaces but follows a different approach. Instead of creating a large unifying space, it makes use of the metaphor of wall-diffused noises commonly known from neighbors living upstairs or next door. When sharing a space, people are subconsciously aware of other people’s activities, mainly because of their interaction with the environment. We designed upstairs to extend today’s telepresence and social presence systems (i. e. most notably the telephone and videoconferencing solutions) that mostly focus on the transmission of the conscious part of communication and thereby enrich these systems by supporting peripheral awareness to allow for a permanent connection without distracting too much. In this paper we present the design decisions that led to realized system, the technical setup and the study we conducted over a two week time frame in the homes of couples in long distance relationships.","2015-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","Upstairs","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GGI69TLG","journalArticle","2013","Poirier-Quinot, David; Touraine, Damien; Katz, Brian F. G.","Blendercave: A Multimodal Scene Graph Editor For Virtual Reality","","","","","http://hdl.handle.net/1853/51672","This paper presents the BlenderCAVE project, which extends the 3D creation content software Blender and its Game Engine (BGE) to Virtual Reality (VR) applications. Based on a multi-screen nonstereoscopic adaptation of the BGE [Gascon et al., 2010], Blender- CAVE now integrates a complete framework dedicated to Virtual Reality (VR), compatible with the three main Operating Systems for any given VR architecture configuration. It has been developed by audio and VR researchers with support from the Blender Community on LIMSI’s state of the art VR platforms. Acting as a Scene Graph, BlenderCAVE handles multi-screen/multi-user tracked stereoscopic rendering through an efficient low-level master/ slave synchronization process while controlling spatial audio rendering (ambisonic, multi-user binaural, WFS, etc.) and haptic events through OSC and VRPN protocols. The scene creation process itself is reduced to simple Blender manipulations including basic python programming easily carried out using standard laptops. OSC client and spatial audio rendering methods have thus far been implemented in the Max/MSP Audio Programming Environment.","2013-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","Blendercave","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y4VHCFNG","book","2015","Hinde, Alistair F.; Evans, Michael; Tew, Anthony I.; Howard, David M.","Onset asynchrony in spoken menus","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54112","The menu is an important interface component, which appears unlikely to be completely superseded by modern search-based approaches. For someone who is unable to attend a screen visually, however, alternative non-visual menu formats are often problematic. A display is developed in which multiple concurrent words are presented with different amounts of onset asynchrony. The effect of different amounts of asynchrony and word length on task durations, accuracy and workload are explored. It is found that total task duration is significantly affected by both onset asynchrony and word duration. Error rates are significantly affected by both onset asynchrony, word length and their interaction, whilst subjective workload scores are only significantly affected by onset asynchrony. Overall, the results appear to suggest that the best compromise between accuracy, workload and speed may be achieved through presenting shorter or temporally-compressed words with a short inter-stimuli interval.","2015-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EUJD2MDV","journalArticle","2014","Brock, Derek; Gaumond, Charles F.; Wasylyshyn, Christina; McClimens, Brian","Collaboratively Identifying and Referring to Sounds with Words and Phrases","","","","","http://hdl.handle.net/1853/52070","Machine classification of underwater sounds remains an important focus of U.S. Naval research due to physical and environmental factors that increase false alarm rates. Human operators tend to be reliably better at this auditory task than automated methods, but the attentional properties of this cognitive discrimination skill are not well understood. In the study presented here, pairs of isolated listeners, who were only allowed to talk to each other, were given a collaborative soundordering task in which only words and phrases could be used to refer to and identify a set of impulsive sonar echoes. The outcome supports the premise that verbal descriptions of unfamiliar sounds are often difficult for listeners to immediately grasp. The method of “collaborative referring” used in the study is proposed as new technique for obtaining a verified perceptual vocabulary for a given set of sounds and for studying human aural identification and discrimination skills.","2014-06","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CCCYC42M","journalArticle","2018","Arbon, Robert E.; Jones, Alex J.; Bratholm, Lars A.; Mitchell, Tom; Glowacki, David R.","Sonifying stochastic walks on biomolecular energy landscapes","","","","","http://hdl.handle.net/1853/60093","Translating the complex, multi-dimensional data produced by simulations of biomolecules into an intelligible form is a major challenge in computational chemistry and biology. The so-called “free energy landscape” is amongst the most fundamental concepts used by scientists to understand both static and dynamic properties of biomolecular systems. In this paper we use Markov models to design a strategy for mapping features of this landscape to sonic parameters, for use in conjunction with visual display techniques such as structural animations and free energy diagrams. This allows for concurrent visual display of the physical configuration of a biomolecule and auditory display of characteristics of the corresponding free energy landscape. The resulting sonification provides information about the relative free energy features of a given configuration including its stability.","2018-06","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KW4TWT2Z","journalArticle","2004","Serafin, G.; Serafin, S.","Sound design to enhance presence in photorealistic virtual reality","","","","","http://hdl.handle.net/1853/50913","The role of soundscape design to enhance the sense of presence in virtual reality is discussed and tested. Preliminary results which compare real versus virtual soundscapes are reported.","2004-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HGEZJ2N6","journalArticle","2007","Cabrera, Densil; Ferguson, Sam","Sonification of Sound: Tools for Teaching Acoustics and Audio","","","","","http://hdl.handle.net/1853/50029","This paper describes a collection of examples of how the teaching of acoustics and technical audio may be aided through sonification. Examples are fixed demonstrations, small programs in Max/MSP, and sonifications as part of a sound analysis program (PsySound3) developed by the authors. Examples include auditory graphs of frequency-dependent parameters used in architectural acoustics, sonifications of room modal distributions, sonifications of room impulse responses, simplification of input sound using spectral moments and the Hilbert transform, interactive sonification of head-related transfer functions and vowel formants, and sonifications of sound analysis parameters, including the output of psychoacoustical models.","2007-06","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","Sonification of Sound","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZZCIACPP","book","2015","Andreopoulou, Areti; Katz, Brian F. G.","On the use of subjective HTRF evaluations for creating global perceptual similarity metrics of assessors and assessees","","978-3-902949-01-1","","","http://hdl.handle.net/1853/54095","In the absence of a well suited measure for quantifying binaural data variations, this study presents the use of a global perceptual distance metric which can describe both HRTF as well as listener similarities. The metric is derived based on subjective evaluations of binaural renderings of a sound moving along predefined trajectories in the horizontal and median planes. Its characteristics and advantages in describing data distributions based on perceptually relevant attributes are discussed. In addition, the use of 24 HRTFs from two different databases of origin allows for an evaluation of the perceptual impact of some database-dependent characteristics on spatialization. The effectiveness of the experimental design as well as the correlation between the HRTF evaluations of the two plane trajectories are also discussed.","2015-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5HILN874","journalArticle","2006","Brazil, E.; Fernstroem, M.","Investigating concurrent auditory icon recognition","","","","","http://hdl.handle.net/1853/50593","This paper an investigation of the identification of concurrently presented auditory icons1. The motivation for this work was to get a better understanding of the identification of an everyday sound scene. We collected a set of descriptions for a set of everyday sounds as classified by the participants, using their free text responses. Two different experiments were conducted. The first experiment used no sub-categorisation or classification information when choosing the auditory icons. The second experiment used object and action descriptors in the selection of auditory icons. Our hypotheses was that by ensuring auditory icons did not have the same object or action descriptors, the identification of auditory icons would improve. Both experiments used an onset-to-onset gap of 300 ms between auditory icons. The results show that when there was no overlap between the object and the action descriptors of the concurrent auditory icons, the identification of the auditory icons was significantly improved.","2006-06","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MPGI5GG4","journalArticle","2021","Ziemer, Tim; Schultheis, Holger","The CURAT sonification game: Gamification for remote sonification evaluation","","","","","http://hdl.handle.net/1853/66332","As sonification is supposed to communicate information to users, experimental evaluation of the subjective appropriateness and effectiveness of the sonification design is often desired and sometimes indispensable. Experiments in the laboratory are typically restricted to short-term usage by a small sample size under unnatural conditions. We introduce the multi-platform CURAT Sonification Game that allows us to evaluate our sonification design by a large population during long-term usage. Gamification is used to motivate users to interact with the sonification regularly and conscientiously over a long period of time. In this paper we present the sonification game and some initial analyses of the gathered data. Furthermore, we hope to reach more volunteers to play the CURAT Sonification Game and help us evaluate and optimize our psychoacoustic sonification design and provide feedback on the game and recommendations for future developments.","2021-06","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","The CURAT sonification game","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IZ7WJI3U","journalArticle","2016","Bukvic, Ivica Ico","3D Time-Based Aural Data Representation Using D4 Library’s Layer Based Amplitude Panning Algorithm","","","","","http://hdl.handle.net/1853/56592","The following paper introduces a new Layer Based Amplitude Panning algorithm and supporting D4 library of rapid prototyping tools for the 3D time-based data representation using sound. The algorithm is designed to scale and support a broad array of configurations, with particular focus on High Density Loudspeaker Arrays (HDLAs). The supporting rapid prototyping tools are designed to leverage oculocentric strategies to importing, editing, and rendering data, offering an array of innovative approaches to spatial data editing and representation through the use of sound in HDLA scenarios. The ensuing D4 ecosystem aims to address the shortcomings of existing approaches to spatial aural representation of data, offers unique opportunities for furthering research in the spatial data audification and sonification, as well as transportable and scalable spatial media creation and production.","2016-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GM6I2RHQ","journalArticle","2004","Brewster, S.; McGookin, D.","Space, the final frontearcon: The identification of concurrently presented earcons in a synthetic spatialized auditory environment","","","","","http://hdl.handle.net/1853/50910","Two experiments which investigate the impact of spatialised presentation on the identification of concurrently presented earcons are described. The first experiment compared the identification of concurrently presented earcons based on the guidelines for individual earcon design and presentation of Brewster, Wright and Edwards [1] which were presented in spatially distinct locations, to the identification of non-spatially presented earcons which incorporated guidelines for concurrent presentation from McGookin and Brewster [2]. It was found that a significant increase in earcon identification occurred, as well as an increase in earcon register identification when earcons were spatially presented. The second experiment compared the identification of concurrently presented earcons based on the guidelines of Brewster, Wright and Edwards [1] which were presented in spatially distinct locations, to the identification of spatially presented earcons which incorporated guidelines for the presentation of concurrent earcons from McGookin and Brewster [2]. The incorporation of the concurrent earcon guidelines was found to significantly increase identification of the timbre attribute but did not significantly effect the overall identification of earcons.","2004-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","Space, the final frontearcon","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2LAZA23W","journalArticle","2003","Miele, Joshua A.","Smith-Kettlewell display tools: A sonification toolkit for MATLAB","","","","","http://hdl.handle.net/1853/50485","The Smith-Kettlewell Display Tools (SKDtools) is an accessibility toolkit for the popular modeling and data manipulation environment MATLAB. It provides blind and visually-impaired users with a variety of sonification and tactile display options, and is assembled from tools already existing within the MATLAB environment. Two distinct sonification methods are implemented, and one of the intentions of the project is to examine the relative effectiveness of these two methods using rigorous psychophysical techniques. SKDtools was developed at The Smith-Kettlewell Eye Research Institute's Rehabilitation Engineering Research Center under a grant from the National Institute for Disability and Rehabilitation Research.","2003-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","Smith-Kettlewell display tools","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6R83BCKV","journalArticle","1998","Lehmann, Thomas","Case study of a software structure for a 3D-Audio-Device","","","","","http://hdl.handle.net/1853/50719","The simulation of an auditory environment is audio signal processing depending on an acoustical model method and a given scene model. For signal processing we use a set of signal processing objects which can be glued to a signal processing network. The object structure depends on the requested acoustic model method in consideration of real-time aspects of an interactive virtual reality (VR). The final processing is executed on a common computer system. In this paper I would like to briefly show the inner structure of this signal processing tool for 3D-audio and the communication interface to the visual animation tool. I outline the structures and the generation of the signal processing network depending on two acoustical model methods for the simulation of an auditory environment. Furthermore concepts for some improvements of these basic structures are shown considering the limited processing resources in real-time systems.","1998-11","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AI484FVI","journalArticle","2008","Franinovic, Karmen; Visell, Yon","Strategies for Sonic Interaction Design: from Context to Basic Design","","","","","http://hdl.handle.net/1853/49950","We advocate a new approach to the design of interactive and sonically augmented artifacts. It is aimed at enriching the context within which design takes place, while integrating the level of structured exploration that has been instrumental to formalizing design processes for nearly a century. The proposed process combines the systematic approach of basic design with exploratory studies within an existing everyday setting. The approach is particularly salient for auditory display in products, due to the relative lack of design examples and methods that exist for those working in this area to draw upon. We describe a study undertaken in domestic kitchen, a setting that has long been recognized as ripe with expressive, sonic interactions. The results of this contextual research have been used for the design of sonically-augmented lamps. We analyze the relevant results, and describe plans for integrating assessment methods.","2008-06","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","Strategies for Sonic Interaction Design","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AHDPZC5T","journalArticle","2001","Ballas, James A.; Brock, Derek; Stroup, Janet; Fouad, Hesham","The effect of auditory rendering on perceived movement: Loudspeaker density and HRTF","","","","","http://hdl.handle.net/1853/50653","Until recently, multiple speaker systems for Virtual Environment (VE) applications were limited to a few front and rear speakers. Utilizing the Virtual Audio Server (VAS) with a Vector Base Amplitude Panning (VBAP) algorithm for multiple speaker control, an array of 24 speakers was constructed to test large speaker configurations. Localization of complex movement was superior with the 24-speaker system, compared to an 8-speaker configuration or HRTF spatialization.","2001-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","The effect of auditory rendering on perceived movement","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PKESTE6E","journalArticle","2013","Nees, Michael A.; Best, Kathryn","Modality And Encoding Strategy Effects On A Verification Task With Accelerated Speech, Visual Text, And Tones","","","","","http://hdl.handle.net/1853/51678","An experiment examined performance on a speeded comparison verification task with accelerated speech (spearcons), visual text, and auditory tones (sonifications). ParticipantsÕ task was to encode the state (increasing or decreasing) of a stock depicted in the first (study) stimulus for comparison with the state depicted in the second (verification) stimulus. We also instructed participants to remember the study stimulus according to a prescribed encoding strategyÑeither as words (a verbal working memory processing code) or tones (a tonal, auditory imagery working memory processing code). Results generally offered evidence that the accelerated speech stimuli assumed the same verbal working memory code as the visual text stimuli. Interestingly, however, both speech and tones also exhibited lingering, stimulus-specific perceptual effects during verification despite recoding in working memory. Results are discussed in terms of auditory imagery in working memory, and the relevance of results to auditory interface design is discussed.","2013-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NP6V2QLH","journalArticle","2002","Martens, W. L.","Rapid psychophysical calibration using bisection scaling for individualized control of source elevation in auditory display","","","","","http://hdl.handle.net/1853/51373","In an effort to reduce problems stemming from individual differences in spatial hearing, a rapid method for customizing an interactive spatial auditory display for individual users was developed and tested. This paper describes how new users of a DSP-based spatial auditory display system perform a short series of psychophysical calibration tasks via realtime manipulation of the elevation of a virtual sound source removed from the median plane by a constant angle (on a “cone of confusion” centered on the interaural axis). The user first produces five settings indicating the point at which the perceived elevation of a virtual source matches their own internal standard for “ear-level” incidence. The median of these settings provides an anchoring stimulus for creating an individualized psychophysical scale for controlling source elevation as perceived by the user of the display system. The experimentally-derived anchoring stimulus is regarded as the origin for an angular bisection session that enables the rapid construction of a look up table (LUT) for the full range of elevations produced by the display for each individual user. In contrast to systems that base source elevation control upon individualized head-related transfer functions (HRTFs), the tested system uses a generic set of HRTFs, and manipulates only the values in the LUT for the elevations produced by each HRTF. The method does not attempt to find for each individual listener a single best frequency scaling for the generic set of HRTFs, but attempts to map the useful range of elevations produced by them. Though such a LUT for perceived elevation can be based upon angular estimates made for virtual sources created using each of many HRTFs, the bisection task presented here requires users to complete only a short listening session in which they adjust the elevation of a comparison stimulus to bisect the angle subtended by a pair of reference stimuli. In contrast to other rapid methods of customization, such as those based upon a user's subjective preferences, the current method is based upon active spatial manipulation of a virtual source. The adjustments are referenced to the user's internal standard for “ear-level” incidence, which is tangibly defined and quite easily explained to new users.","2002-07","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4DM8BQKX","journalArticle","2017","Cherston, Juliana; Paradiso, Joseph A.","Rotator: Flexible Distribution of Data Across Sensory Channels","","","","","http://hdl.handle.net/1853/58351","'Rotator’ is a web-based multisensory analysis interface that enables users to shift streams of multichannel scientific data between their auditory and visual sensory channels in order to better discern structure and anomaly in the data. This paper provides a technical overview of the Rotator tool as well as a discussion of the motivations for integrating flexible data display into future analysis and monitoring frameworks. An audio-visual presentation mode in which only a single stream is visualized at any given moment is identified as a particularly promising alternative to a purely visual information display mode. Auditory and visual excerpts from the interface are available at http://resenv.media.mit.edu/rotator.","2017-06","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","Rotator","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HVPV25T5","journalArticle","1998","Petrie, Helen; Morley, Sarah","The use of non-speech sounds in non-visual interfaces to the MS Windows GUI for blind computer users","","","","","http://hdl.handle.net/1853/50734","Two studies investigated the use of non-speech sounds (auditory icons and earcons) in non-visual interfaces to MS-Windows for blind computer users. The first study presented sounds in isolation and blind and sighted participants rated them for their recognisability, and appropriateness of the mapping between the sound and the interface object/event. As a result, the sounds were revised and incorporated into the interfaces. The second study investigated the effects of the sounds on user performance and perceptions. Ten blind participants evaluated the interfaces, and task completion time was significantly shorter with the inclusion of sounds, although interesting effects on user perceptions were found.","1998-11","2023-07-13 06:25:52","2023-07-13 06:25:52","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9V24F96H","journalArticle","2012","Hermann, Thomas; Ungerechts, Bodo; Toussaint, Huub; Grote, Marius","Sonification of Pressure Changes in Swimming for Analysis and Optimization","","","2168-5126","","http://hdl.handle.net/1853/44423","This paper introduces new methods for the sonification of pressure sensor data measured while executing crawl stroke swimming. Swimming research aims at better understanding the flow conditions and detailed dynamics in order to adapt swimming strokes to achieve maximal speed with minimal energy consumption. The fact that during interaction of body and water a pressure field is induced while water masses are displaced is considered rarely. In reaction to the changes of the pressure field the swimming speed of the body is changed. The Sonification of the pressure data measured at one stroking hand (palm and back of hand), shoulder and elbow turns the hydrodynamic situation into a complex sonic rhythmical motive, which supports the recognition of auditory gestalts and the differentiation therein, and may in future real-time sonification help the swimmers to optimize their motions. Furthermore the sonifications can be synchronized to video recordings for data analysis and coaching. We describe several alternative Sonification prototypes and discuss the resulting sounds in their ability to bring different patterns to attention. The application gives an example for newly introduced intermediate sonifications that bridge the gap between action and effect sonification.","2012-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R7T4UZQL","journalArticle","2001","Teppo, Aki; Vuorimaa, Petri","Speech interface implementation for XML browser","","","","","http://hdl.handle.net/1853/50649","The growing popularity of digital cellular phones and personal digital assistants (PDA) is setting new demands for Internet content producers. One problem with these devices is the small visual display. WWW pages are usually designed for traditional desktop computers and they are difficult to view with small displays. In this paper, a solution is presented that uses the audio capabilities of such mobile devices in addition to optional visual display. The idea is to transform XML data into VoiceXML in addition to some traditional display layout language. This approach could also make Internet browsing possible for visually handicapped people.","2001-07","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YHBRTRPL","journalArticle","2002","Dombois, Florian","Auditory seismology on free oscillations, focal mechanisms, explosions and synthetic seismograms","","","","","http://hdl.handle.net/1853/51334","The method of audifying seismograms and interpreting seismological data by ear enables a wide range of geophysical questions to be studied in a new manner. In this article we report about the actual state of research in Auditory Seismology. Some tests were carried out in the area of free oscillation phenomena, focal mechanisms of earthquakes, explosion signals and synthetic seismograms. The results confirm the difference between visual and acoustic approach as having different foci: Free oscillations as a phenomenon of resonance are easily accessible for the ear, whereas synthetic seismograms give acoustically less insight than visually. Beyond the seismological investigation the article gives a historical introduction of Auditory Seismology up to the present, and a summary of earlier results to contextualize these and assist the process of establishing the acoustical approach as an individual research field.","2002-07","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3GLDX9T3","journalArticle","2003","Schmandt, Chris; Vallejo, Gerardo","“Listenin” to domestic enviroments from remote locations","","","","","http://hdl.handle.net/1853/50509","This paper describes ListenIn, work in progress using audio as a monitoring medium, with emphasis on domestic environments inhabited by elder parents. The primary goal of this monitoring is to provide a continuous but peripheral awareness of the monitored site and a remote location, or to a mobile user. Sound gathering and classification in the home is done in a distributed architecture server with multiple components. At transitions of activity, as measured by change in sound, a remote server receives and plays a few seconds of an “iconic” sound, the actual sound, or a “garbled” version of the actual sound, depending on confidence in the classification and whether speech is detected.","2003-07","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LHVT2FFU","journalArticle","2006","Guillaume, A.; Rivenez, M.; Chastres, V.; Blancard, C.; Pellieux, L.","Identification of environmental sounds: Role of rhythmic properties","","","","","http://hdl.handle.net/1853/50594","Studies that have dealt with the effect of sound spectral and temporal properties on environmental sound identification have focused on a narrow range of sounds [1, 2, 3]. The purpose of the research was to evaluate the effect of sound temporal characteristics on the identification of 72 different environmental sounds, 29 of them having a rhythmic structure. We used a gating paradigm, involving a successive presentation of increasing increment of gates stimulus [4] that listeners had to identify. The minimum amount of time presentation (uniqueness point) for which an environmental sound was correctly recognized was recorded. We found that rhythmic sounds were identified earlier (tR = 160 ms) than non rhythmic (tNR = 239 ms). Furthermore, for rhythmic sounds, we observed a significant correlation between the uniqueness point and the duration of the first inter onset interval (r = 0.65). Our results suggest that sound rhythmic structure is an informative parameter in the identification process.","2006-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","Identification of environmental sounds","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X4QFI3AA","journalArticle","2019","Nees, Michael A.","Eight components of a design theory of sonification","","","","","http://hdl.handle.net/1853/61522","Despite over 25 years of intensive work in the field, sonification research and practice continue to be hindered by a lack of theory. In part, sonification theory has languished, because the requirements of a theory of sonification have not been clearly articulated. As a design science, sonification deals with artifacts- artificially created sounds and the tools for creating the sounds. Design fields require theoretical approaches that are different from theory-building in natural sciences. Gregor and Jones [1] described eight general components of design theories: (1) purposes and scope; (2) constructs; (3) principles of form and function; (4) artifact mutability; (5) testable propositions; (6) justificatory knowledge; (7) principles of implementation; and (8) expository instantiations. In this position paper, I examine these components as they relate to the field of sonification and use these components to clarify requirements for a theory of sonification. The current status of theory in sonification is assessed as it relates to each component, and, where possible, recommendations are offered for practices that can advance theory and theoretically-motivated research and practice in the field of sonification.","2019-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XEWE9DA5","journalArticle","2011","Hoferlin, Benjamin; Hoferlin, Markus; Michael, Raschke; Heidemann, Gunther; Weiskopf, Daniel","Interactive Auditory Display to Support Situational Awareness in Video Surveillance","","","","","http://hdl.handle.net/1853/51700","A key element for efficient video surveillance is situational awareness. Characteristics of human perception (e.g., inattentional blindness) as well as surveillance practice (e.g., CCTV operators have multiple responsibilities) often hinder comprehensive visual recognition of the activities in the monitored area. We support sit- uational awareness and reduce the workload of CCTV operators by complementing the video display by an auditory display. Tra- jectories of moving objects extracted from surveillance video are sonified by auditory icons. These icons are interactively assigned by the user to each object category of the video and, in this way, form a sonic ecology. We use a spatial auditory display to rep- resent location, direction and velocity of a trajectory with respect to a virtual listener. This facilitates orientation in virtual auditory space in a natural and realistic way that meets users’ expectations. Modification areas are introduced to allow the users to define areas in which auditory icons are modified to further improve situational awareness. We put emphasis on efficient interaction between users and the auditory display to adjust the system according to the mon- itored area. Finally, we evaluate our approach by a user study and discuss benefits and shortcomings of the proposed sonification in the light of psychology, cognitive science, and neuroscience.","2011-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7CJ6ZSUN","journalArticle","2018","Bukvic, Ivica Ico; Earle, Gregory D.","Reimagining human capacity for location-aware aural pattern recognition: A case for immersive exocentric sonification","","","","","http://hdl.handle.net/1853/60083","The following paper presents a cross-disciplinary snapshot of 21st century research in sonification and leverages the review to identify a new immersive exocentric approach to studying human capacity to perceive spatial aural cues. The paper further defines immersive exocentric sonification, highlights its unique affordances, and presents an argument for its potential to fundamentally change the way we understand and study the human capacity for location-aware audio pattern recognition. Finally, the paper describes an example of an externally funded research project that aims to tackle this newfound research whitespace.","2018-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","Reimagining human capacity for location-aware aural pattern recognition","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AFI2SWYU","book","2009","Brazil, Eoin; Fernstrom, Mikael","Subjective experience methods for early conceptual design of auditory display","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51413","We review a cross-section of subjective experience methods fo- cused on the early conceptual design of auditory displays. The motivation of this review is to support expert and novice design- ers in creating auditory displays in human-computer interaction by introducing them to these methods. A range of available guid- ance and current practice is firstly analysed. Subsequently, the key methods and their concepts are discussed with examples from ex- isting studies. A complementary framework is presented to high- light how these methods can be used together by auditory display designer at the early conceptual design stage. The results from these studies help to demonstrate the need for a greater awareness and use of this type of method in early conceptual design to un- cover pragmatic mental models and associated salient cognitive attributes. The attributes can be related to subjective judgements such as quality, preference, or context among many. This type of approach differs from many quantitative approaches which are strictly focused on the usage aspects of auditory displays. The manner of quantitative approaches is to use hypothesis and valida- tion criteria, however these cannot deal in a structured way with ephemeral judgements such as emotion, mood, or with subject de- pendant information such as tacit knowledge. The increasing use of interactive auditory displays is one area where this type of early conceptual design method can help in ensuring the designed in- teraction and the concrete mapping it uses reflects the considered behaviour of potential users including aspects of the inner needs, desires, and tacit knowledge. This approach will help in consider- ing the emotional, intellectual, and sensual aspects of interactions when designing auditory displays. We close by reflecting on the results and discussing future lines of research using these methods.","2009-05","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T8SV8WLY","journalArticle","2007","Stockman, Tony; Rajgor, Neil; Metatla, Oussama; Harrar, Lila","The design of interactive audio soccer","","","","","http://hdl.handle.net/1853/50045","The questions involved in the design of an interactive, audio only computer-based football game are explored. The game design process starts by exploring basic questions such as size of playing area, orientation, awareness of team mates and opponents and basic navigation. The project goes on to explore more advanced design issues, not addressed by previous audio only ball games, involving the provision of a multi-player perspective, requiring the provision of an intuitive means of supporting changes in the focus of the interaction in audio. In general the dynamic, multi-player perspective poses interesting questions of how to provide real time and interactive sonification of ball and player positions and how these should be managed within the context of the changes in interaction focus mentioned above. A further interesting issue relates to how, within an auditory game context, to handle aspects of the game which are essentially silent, such as the sides of the pitch, positions of the goals and players who are not currently moving. To assist with these and other design questions, advice was sort from past and present players of the British blind soccer squad. The information gathered ranged from basic facts about the rules and conditions under which games are played, through to discussions about the role of echo location in providing an awareness of physical features of the pitch and the proximity of other players. This in turn led to the question of how realistically to present the information provided through echo location in a virtual auditory display. The paper concludes with a discussion of the potential roles of this type of system in team coaching, exploring the practical applications of audio game representations to realistic coaching scenarios.","2007-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9HUM8NKB","journalArticle","2000","Saue, Sigurd","A model for interaction in exploratory sonification displays","","","","","http://hdl.handle.net/1853/50666","This paper presents a general model for sonification of large spatial data sets (e.g. seismic data, medical data) based on ideas from ecological acoustics. The model incorporates not only what we hear (the sounds), but also how we listen (the interaction). Metaphorically speaking the interpreter is walking along paths in areas of the data set, listening to locally and globally defined sound objects. The time aspects of sonification are given special attention, introducing the notion of temporalization. Some features of a preliminary Windows NT implementation are summarized.","2000-04","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3GHTB2JZ","journalArticle","1997","Sawhney, Nitin; Schmandt, Chris","Design of spatialized audio in nomadic environments","","","","","http://hdl.handle.net/1853/50744","This paper describes an on-going research project at the MIT Media Lab, exploring the use of audio as a primary modality for nomadic computing applications. We are developing a framework for use on a wearable audio platform, Nomadic Radio, that presents dynamic information within a spatialized audio environment. The contextual state of the nomadic listener indicated by time of day, physical positioning, scheduled tasks, message content, and level of interruption is used to present relevant information in the user's listening space. In this paper we will consider issues related to auditory presentation and spatial techniques for awareness and browsing of audio messages on wearable computing.","1997-11","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7VIWBEWE","journalArticle","2001","Demarey, Catherine; Plenacoste, Patricia","User, sound context and use context: What are their roles in 3D sound metaphors design?","","","","","http://hdl.handle.net/1853/50659","The sound metaphor is considered as an assistant tool for the user activity. In order to create such new use, the classical design process must be adapted to the design object and viewed with a creative and prospective approach. As shown, the cognitive capacities of the user, the use context and the sound context play a specific and important role in the design process of the sound metaphor. We propose a structure for the sound metaphor, which is about to be validated. In our point of view, designing sound metaphors can be based on psychological studies of analogy. The search of analogy is led by goals, manipulated objects and mainly by activation of the knowledge of the subject regarding the situation and the use of context [1][2]. It depends on the degree of abstraction between the representation of the situation (target) and the activated knowledge (source). So, we use the context of perception of 3D natural environment. Our studies have shown importance of the use of 3D sounds and context for identifying sounds and for activating the knowledge related to this sound.","2001-07","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","User, sound context and use context","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S28457NK","journalArticle","1997","Brewster, Stephen A.; Clarke, Catherine V.","The design and evaluation of a sonically-enhanced tool palette","","","","","http://hdl.handle.net/1853/50754","This paper describes an experiment to investigate the effectiveness of adding sound to tool palettes. Palettes have usability problems because users need to see the information they present but they are often outside the area of visual focus. We used non-speech sounds called earcons to indicate the current tool and when tool changes occurred so that users could tell what tool they were in wherever they were looking. Results showed a significant reduction in the number of tasks performed with the wrong tool. Therefore users knew what the current tool was and did not try to perform tasks with the wrong tool. All of this was not at the expense of making the tool palettes any more annoying to use.","1997-11","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PH8PZGUW","journalArticle","2014","Riker, Paul; Acevedo, Daniel","iEAR: Immersive Environmental Audio for Photorealistic Panoramas","","","","","http://hdl.handle.net/1853/52104","This paper presents iEAR, a flexible spatial audio rendering tool for use with photorealistic monoscopic and stereoscopic panoramas across various display systems. iEAR allows users to easily present multichannel audio scenes over variable speaker arrangements, while maintaining tight integration with the corresponding visual elements of the display media. Built in the Max/MSP Audio Programming Environment, iEAR utilizes well-established panning methods to accommodate a wide range of speaker configurations. Audio scene orientation is tied to the visual scene using an OSC connection with the visualization software, allowing users to render and spatialize multichannel environmental audio recordings in tandem with the changing perspective in the visual scene.","2014-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","iEAR","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y8EC2DKD","journalArticle","2003","Peres, S. Camille; Lane, David M.","Sonification of statistical graphs","","","","","http://hdl.handle.net/1853/50486","Two experiments are presented that compare the effectiveness of different parameters of sound for the auditory presentation of box plots. Temporal mapping was found to be better than pitch or panning mapping. In the first experiment, the mapping condition that used two dimensions (the redundant condition) did not result in a better performance than those mappings that used one dimension. However, subjects showed a strong preference for the redundant condition. Finally, subjects' overall level was not very high and performance did not increase with practice as much as might have been expected.","2003-07","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ABJXQKHY","journalArticle","2004","Kuester, F.; Hutchinson, T. C.; Ramaswamy, L.","Towards enhancing earthquake response interpretation using sonification","","","","","http://hdl.handle.net/1853/51316","In this paper, we select the application domain of earthquake engineering for utility of sonification, where signals are of random frequency and amplitude content. In particular, we focus on the response of structures to particular earthquake time histories. Given a random ground motion input, the resulting response signal will vary in the time and frequency domain, and show large variations in acceleration, velocity and displacement space. We illustrate the utility of different simple, yet robust sonification techniques to the study of the response of a variety of linear elastic single-degree-offreedom (SDOF) oscillators, with different natural periods Tn and associated damping ratios zn, subjected to a pair of earthquake motions. In the system study, we augment the representation of the response results of these SDOF structures with both visual and aural cues.","2004-07","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DXSLWD4K","journalArticle","2000","Runkle, Paul; Yendiki, Anastasia; Wakefield, Gregory H.","Active sensory tuning for immersive spatialized audio","","","","","http://hdl.handle.net/1853/50665","Unlike their visual counterparts, immersive spatialized audio displays are highly sensitive to individual differences in the signal processing parameters associated with source placement in azimuth and elevation. We introduce Active Sensory Tuning (AST) as a general framework within which human observers can efficiently search through large design spaces. The application of AST to individualizing spatialized audio displays is demonstrated and its use in a broader range of auditory data processing and synthesis is discussed.","2000-04","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IY7UBHYJ","journalArticle","2000","Walker, Bruce N.; Kramer, Gregory; Lane, David M.","Psychophysical scaling of sonification mappings","","","","","http://hdl.handle.net/1853/50679","We determined preferred data-to-display mappings by asking experiment participants directly and then examined the psychophysical scaling functions relating perceived data values to underlying acoustic parameters. Presently, we are extending and validating the scaled mappings in practical data interpretation tasks. The resulting scaling functions, in conjunction with the experimental paradigm developed here, should spark further research in this area and have implications for the design of future sonifications.","2000-04","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T9FFG96I","journalArticle","2019","Lenzi, Sara; Terenghi, Ginevra; Taormina, Riccardo; Galelli, Stefano; Ciuccarelli, Paolo","Disclosing cyber attacks on water distribution systems: an experimental approach to the sonification of threats and anomalous data","","","","","http://hdl.handle.net/1853/61518","Water distribution systems are undergoing a process of intensive digitalization, adopting networked devices for monitoring and control. While this transition improves efficiency and reliability, these infrastructures are increasingly exposed to cyber-attacks. Cyber-attacks engender anomalous system behaviors which can be detected by data-driven algorithms monitoring sensors readings to disclose the presence of potential threats. At the same time, the use of sonification in real time process monitoring has grown in importance as a valid alternative to avoid information overload and allowing peripheral monitoring. Our project aims to design a sonification system allowing human operators to take better decisions on anomalous behavior while occupied in other (mainly visual) tasks. Using a state-of-the-art detection algorithm and data sets from the Battle of the Attack Detection Algorithms, a series of sonification prototypes were designed and tested in the real world. This paper illustrates the design process and the experimental data collected, as well results and plans for future steps.","2019-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","Disclosing cyber attacks on water distribution systems","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BT9GX5FF","journalArticle","2003","Avanzini, F.; Rocchesso, D.; Belussi, A.; Dal Palu, A.; Dovier, A.","Acqua alta a venezia: Design of a urban scale auditory warning system","","","","","http://hdl.handle.net/1853/50429","A new warning system for high tide in Venice has been designed to replace the existing network of electro-mechanical sirens. The project was divided into four sections: (i) optimal placement of loudspeakers via constraint logic programming, (ii) simulation and visualization of the acoustic field in the city, (iii) design of the warning sounds, (iv) validation of the warning sounds. This paper reports the strategies and results of all four project stages, with special emphasis on sound design and validation.","2003-07","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","Acqua alta a venezia","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YYKZVGET","journalArticle","2011","Seldess, Zachary; Yamaoka, So; Kuester, Falko","Sonnotile: Audio Annotation and Sonification for Large Tiled Audio/Visual Display Environments","","","","","http://hdl.handle.net/1853/51766","We present “Sonnotile”, a multi-modal rendering framework to enhance scientific data exploration, representation, and analysis within tiled-display visualization environments. Sonnotile aims to assist researchers in the customization and embedding of sound objects within their data sets. These sound objects may act as way-finding markers within a media space, as well as allow researchers to attach and recall various sonic descriptions or representations of an arbitrary number of regions within a data set. In designing the software, our initial efforts have been centered on the challenges of sound “annotation” within large- scale pyramidal TIFF files.","2011-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","Sonnotile","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WXDW732R","journalArticle","2003","Fernstrom, Mikael; Brazil, Eoin; Ottaviani, Laura","A new experimental technique for gathering similarity ratings for sounds","","","","","http://hdl.handle.net/1853/50431","Multidimensional scaling techniques (MDS) are a vibrant area of research with much development and advancement in last few decades. In this paper we focus on an interactive 2-dimensional interface for creating a similarity space for audio. Classical MDS techniques can place great demands upon participants overwhelming their sensory and cognitive abilities to make choices across such datasets using only pairwise comparison. We outline the results of an investigation to obtain multidimensional similarity ratings for a mixed sound collection containing both recorded sounds and synthesised sounds, and we describe briefly a computer application for collecting such data that reduces task demands by use of a 2-dimensional visual display with multistream audio for organising sound similarity and returns the results in the context of the whole stimuli set.","2003-07","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CAMY7HQT","journalArticle","2021","Asendorf, Malte; Kienzle, Moritz; Ringe, Rachel; Ahmadi, Fida; Bhowmik, Debaditya; Chen, Jiumeng; Huynh, Kelly; Kleinert, Steffen; Kruesilp, Jatawan; Lee, Ying Ying; Wang, Xin; Luo, Wei; Jadid, Navid Mirzayousef; Awadin, Ahmed; Raval, Varun; Schade, Eve Emily Sophie; Jaman, Hasanur; Sharma, Kashish; Weber, Colin; Winkler, Helena; Ziemer, Tim","Tiltification: An accessible app to popularize sonification","","","","","http://hdl.handle.net/1853/66331","This paper presents Tiltification, a multi modal spirit level application for smartphones. The non-profit app was produced by students in the master project ""Sonification Apps"" in winter term 2020/21 at the University of Bremen to learn how to conceptualize, implement, market and test smartphone apps. In the app, psychoacoustic sonification is used to give feedback on the device's rotation angles in two plane dimensions, allowing users to level furniture or take perfectly horizontal photos. Tiltification supplements the market of spirit level apps with detailed auditory information presentation. This provides for additional benefit in comparison to a physical spirit level and for more accessibility for visually and cognitively impaired people. We argue that the distribution of sonification apps through mainstream channels is a contribution to establish sonification in the market and make it better known to users outside the scientific domain.","2021-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","Tiltification","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E4C7E7PN","journalArticle","2019","Paterson, Estrella; Sanderson, Penelope; Paterson, Neil; Loeb, Robert","Design and evaluation of a new auditory display for the pulse oximeter","","","","","http://hdl.handle.net/1853/61532","information about a patient's oxygen saturation (SpO2) and heart rate via visual and auditory displays. An audible tone is emitted after every detected pulse (indicating heart rate), and the pitch of the tone is mapped to SpO2. However, clinicians cannot reliably judge SpO2 using only the current auditory display. In a series of three studies, we compared auditory displays based on current pulse oximeters with displays designed to provide more information about SpO2 levels using additional acoustic properties. Results from the first two laboratory studies show that the new auditory displays support better identification of specified ranges of SpO2, and better detection of when saturation transitions a critically relevant threshold. The analysis of a third study in a highfidelity simulator is currently under way. An auditory display that provides more information about SpO2 levels and when SpO2 changes from one range to another may be useful for clinicians when they are engaged in other visually demanding tasks but have to detect and treat patient deterioration, often in time-pressured and stressful situations.","2019-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U93DBXKH","journalArticle","2014","Godbout, Andrew; Thornton, Chris; Boyd, Jeffery E.","Mobile Sonification for Athletes: A Case Study in Commercialization of Sonification","","","","","http://hdl.handle.net/1853/52048","Several companies, including Under Armour, Nike and Adidas, are taking advantage of advances in sensor technology to sell wearable systems that measure, record, and analyze the motion of athletes. To date, these systems make little, if any use of sonification. Therefore, there is an opportunity for sonification methods in this domain, including the potential to reach a mass market. In the fall of 2013, Under Armour and NineSigma created the Armour39 Challenge, an open call for proposals to build new technology for the Armour39, Under Armour’s wearable motion and heart-rate sensor. The authors of this paper responded to the challenge, proposing novel sonification systems to exploit the data from the Armour39. This paper presents these systems, including issues, solutions, and tools for sonification performed on a mobile device with a wearable sensor. The sonifications are rhythmic, exploiting the periodicity of human motion, and are demonstrated by sonifying athletic performance metrics in real-time for speed skating and running.","2014-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","Mobile Sonification for Athletes","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P8EA88DF","journalArticle","2005","Walker, Bruce N.; Nees, Michael A.","An agenda for research and development of multimodal graphs","","","","","http://hdl.handle.net/1853/50098","Effective multimodal graphing tools can be beneficial to both sighted and visually impaired students and scientists. However, before this can become a reality, considerable research is required on the auditory graphing components. We suggest mappings, polarities, scaling, context, and training be studied in particular. We point to previous work in these areas and make suggestions for expanded research questions. We recommend that more complex and realistic data sets be used, and that visually impaired participants play a larger role in the research. The design of multimodal graphing software should be informed by empirical findings. Effective research and useful software tools will bring a broader perspective to data analysis for all who use graphs, regardless of visual ability.","2005-07","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FLI7RZWM","journalArticle","2005","Walker, Bruce N.; Nees, Michael A.","Brief training for performance of a point estimation sonification task","","","","","http://hdl.handle.net/1853/50101","This study examined different types of brief training for a point estimation task with auditory graphs. Participants estimated the price of a stock at a specific times in a 10-hour trading day as depicted in a sonified graph of the stock price data. Forty Georgia Tech undergraduates completed a pre-test, an experimental training session, and a post-test for the point estimation task. In an extension of Smith and Walker [1], a highly conceptual, task analysis-derived method of training was compared to training paradigms that used either prompting of correct responses or feedback for correct answers during training. Two additional groups, one receiving only practice as training and another completing a filler task, were also included. Results indicate that practice with feedback for the point estimation task produced better post-test performance than all other training conditions.","2005-07","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BP9WAJGD","journalArticle","2014","Wersényi, György; Répás, József","Performance Evaluation of Blind Persons in Listening Tests in Different Environmantal Conditions","","","","","http://hdl.handle.net/1853/52056","Visually impaired people are often in target groups of various investigations, including basic research, applied research, research and development studies. Experiments in the development of assistive technologies - navigation aids or computer interfaces (auditory displays) - aim to incorporate the results of testing with blind subjects during development. Listening tests concerning the localization performance of blind subjects can be installed in various environments using different excitation signals. Generally, results can be collected only from a small number of participants and they are compared with results of blindfolded sighted subjects. The goal of this study was to include different environmental conditions (virtual reality, real life, free-field), different localization tasks and a larger number of participants both blind and sighted for comparison. Results indicate that blind subjects’ performance is generally not superior to sighted subjects’ performance from the engineering point of view, but further psychological evaluation is recommended.","2014-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"42CGEA98","journalArticle","2008","Barrass, Stephen; Best, Virginia","Stream-Based Sonification Diagrams","","","","","http://hdl.handle.net/1853/49945","The van Noorden Diagram describes the auditory streaming of two tones with changes in pitch difference and intertone onset interval (IOI). There are regions where the listener hears one stream or two streams, and an ambiguous region between where listening attention affects what is heard. The ambiguous region dominates at IOI \textgreater 200ms which is where many sonifications are designed. We propose a Stream-Based Sonification region at IOI \textless 200ms to control streaming and reduce the ambiguous effects of attention. In this paper we generalise this region in a series of four Stream-Based Sonification Diagrams. The first is a repetition of the original van Noorden Diagram at higher temporal resolution in the SBS region. The other three show the same general pattern of regions for new mappings of the brightness, amplitude and pan of a noise. The results show that streaming by brightness and pitch are closely related. They also show a new coherence boundary for streaming by amplitude, and that streaming by spatial panning is relatively unaffected by IOI. The palette of Stream-Based Sonification Diagrams developed here provides a foundation for the design of sonifications that control streaming and take listening attention into account.","2008-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JC2GPNMR","journalArticle","2022","Nadri, Chihab; Zieger, Scott; Lautala, Pasi; Nelson, David; Jeon, Myounghoon","Preliminary evaluation of lead time variation for rail crossing in-vehicle alerts","","","","","http://hdl.handle.net/1853/67389","In-Vehicle Auditory Alert (IVAA) effectiveness depends on several auditory factors. Lead time has been shown to significantly influence IVAA effectiveness for automotive displays, although applications for Highway-Rail Grade Crossings (HRGCs) have yet to modulate and determine an appropriate lead time. To address this research gap, we conducted a small-scale driving simulator study to investigate the effect of lead time variation on driving performance and gaze behavior at rail crossings. We recruited 11 participants who drove through three experimental drives with different alert state conditions. Preliminary results show that a seven second lead time led to statistically higher temporal demand, a slower approach speed to crossings, and better gaze behavior than the no IVAA condition. The seven second lead time condition had similar higher values than the advanced warning condition, although they were not statistically significant. Findings of the current study offer insight into auditory display guidance for HRGCs, although future work involving a larger recruitment pool is needed to confirm study findings.","2022-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2DXRDWBZ","journalArticle","2005","Stockman, Tony; Frauenberger, Christopher; Hind, Greg","Interactive sonification of spreadsheets","","","","","http://hdl.handle.net/1853/50166","This paper describes the second phase in a project exploring the application of sonification to improve the accessibility of spreadsheets. The principal target population of the study has been visually impaired users, though the approach has potential in a range of other application areas, such as the eyes-free monitoring of large data spaces, or to preserve screen space in mobile use. The paper begins by examining previous sonification research relevant to the auditory display of spreadsheets, before briefly reviewing the results and requirements arising from the first phase of this project. Phase 2 of the project began with a more detailed requirement investigation, the results of which are outlined. In particular, the requirement for increased interactivity is examined. The way in which these requirements are realised in a further prototype are described, and the results of evaluations of this second prototype discussed.","2005-07","2023-07-13 06:25:53","2023-07-21 08:33:23","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"77WBMBLL","journalArticle","2000","Lokki, Tapio; Grohn, Matti; Savioja, Lauri; Takala, Tapio","A case study of auditory navigation in virtual acoustic environments","","","","","http://hdl.handle.net/1853/50664","We report results of an auditory navigation experiment. In auditory navigation sound is employed as a navigational aid in a virtual environment. In our experiment, the test task was to find a sound source in a dynamic virtual acoustic environment. In dynamic auralization the movements of the subject are taken into account in acoustic modeling of the room. We tested the effect of three different factors (stimulus, panning method and acoustic environment) to the number of errors and to the time spent in the test in finding the target. The results, which were also statistically validated, proved that noise is the best stimulus, reverberation complicates the navigation and simple models of spatial hearing give enough cues for auditory navigation.","2000-04","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NF3ZU4YB","journalArticle","2014","McKenzie, Ian; Lennox, Peter; Wiggins, Bruce","Hearing Without Ears","","","","","http://hdl.handle.net/1853/52051","We report on on-going work investigating the feasibility of using tissue conduction to evince auditory spatial perception. Early results indicate that it is possible to coherently control externalization, range, directionality (including elevation), movement and some sense of spaciousness without presenting acoustic signals to the outer ear. Signal control techniques so far have utilised discrete signal feeds, stereo and 1st order ambisonic hierarchies. Some deficiencies in frontal externalization have been observed. We conclude that, whilst the putative components of the head related transfer function are absent, empirical tests indicate that coherent equivalents are perceptually utilisable. Some implications for perceptual theory and technological implementations are discussed along with potential practical applications and future lines of enquiry.","2014-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6EFCV5BJ","journalArticle","2002","McGookin, D.; Brewster, S.","Dolphin: The design and initial evaluation of multimodal focus and context","","","","","http://hdl.handle.net/1853/51350","In this paper we describe a new focus and context visualisation technique called multimodal focus and context. This technique uses a hybrid visual and spatialised audio display space to overcome the limited visual displays of mobile devices. We demonstrate this technique by applying it to maps of theme parks. We present the results of an experiment comparing multimodal focus and context to a purely visual display technique. The results showed that neither system was significantly better than the other. We believe that this is due to issues involving the perception of multiple structured audio sources.","2002-07","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","Dolphin","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7UG9FN2A","book","2009","Mariette, Nicholas","Navigation performance effects of render method and latency in mobile audio augmented reality","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51405","This paper describes a pilot study and main experiment that as- sess user performance at navigating to spatialised sound sources using a mobile audio augmented reality system. Experiments use a novel outdoor paradigm with an application-relevant navigation task to compare perception of two binaural rendering methods un- der several head-turn latencies. Binaural rendering methods ex- amined were virtual, 6-speaker, first-order Ambisonic and virtual 12-speaker VBAP techniques. This study extends existing indoors research on the effects of head-turn latency for seated listeners. The pilot study examined the effect of capture radius (of 1, 2, 3, 4 and 5 metres) on mean distance efficiency for a single user’s navigation path to sound sources. A significant performance degradation was found to occur for radii of 2 m. The main exper- iment examined the effect of render method and total system la- tency to head-turns (176 ms minimum plus 0, 100, 200, 400 and 800 ms) on mean distance efficiency and subjective stability rating (on a scale of 1-5), for 8 participants. Render method significantly affected distance efficiency and 800 ms of added head-turn latency significantly affected subjective stability. Also, the study revealed a significant interaction effect of render method and head-turn la- tency: Ambisonic rendering didn’t significantly affect subjective stability due to added head-turn latency, while VBAP rendering did. Thus, it appears rendering method can mitigate or potentiate stability effects of head-turn latency. The study also exemplifies that the novel experimental paradigm is capable of revealing statis- tically significant performance differences between mobile audio AR implementations.","2009-05","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NXXN5ZKH","journalArticle","2007","Pauletto, Sandra; Hunt, Andy","Interacting with Sonifications: An Evaluation","","","","","http://hdl.handle.net/1853/50008","The aim of the experiment described in this paper is to evaluate and compare three different methods for interacting with an algorithm for the sonification of data streams. The experiment was carried out using an existing Interactive Sonification Toolkit as a high fidelity prototype. The experiment focused on measuring and comparing the efficiency and effectiveness of three interaction methods which differ in the degree of real-time control allowed to the user. Subjects were also asked to answer a questionnaire which gathered information about their perception of using the different interaction methods. The experiment shows that the method providing the lowest degree of real-time control to the user is the least efficient. This method is also perceived to be the least pleasant, fast, clear and intuitive. There are no significant differences in terms of effectiveness and efficiency for the remaining two methods both in terms of objective measures and user perception. Finally the method allowing a medium degree of control to the user is judged to be significantly more pleasant than the others.","2007-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","Interacting with Sonifications","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ARQYBDDF","journalArticle","2001","Pellegrini, Renato S.","Quality assessment of auditory virtual environments","","","","","http://hdl.handle.net/1853/50631","Practical applications for auditory virtual environments (AVEs) are ever increasing. The achievable quality of AVEs has reached a level, where realworld problems can often be solved in a convenient way by using AVEs, meaning e.g. less expensive or more flexible than a real-world solution. Nowadays, the quality of AVEs is often measured in terms of their technical capacity to approximate the physical behavior of a real environment. As the design goals for AVEs shift from “reproducing the physical behavior of a real environment as accurate as possible” to “stimulating the desired perception directly” this comparative quality measure is no longer feasible. This paper describes parameters influencing the perceived quality of AVEs. Moreover, the dependence of perceived quality on the application is emphasized.","2001-07","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"39FWIN3E","journalArticle","2005","McKeown, Denis","Candidates for within-vehicle auditory displays","","","","","http://hdl.handle.net/1853/50104","Speech, auditory icons (sounds imitating real world events), environmental sounds (naturally occurring sounds), and abstract warnings are all candidates for user interfaces. Such auditory displays and warnings for within-vehicle use must satisfy certain criteria such as being appropriately urgent, acceptable to users and commanding accurate and appropriately fast response times. Here such criteria are investigated and compared for these different forms of auditory display as an interface for a broad range of driving scenarios. In a computer task of identifying learned mappings of sound to scenario, speech and auditory icons produced both faster response times and greatest accuracy. Abstract sounds produced the slowest response times and least accuracy. Environmental sounds showed an intermediate pattern of performance for accuracy but the response times were similar to the abstract sounds. Urgency and pleasantness judgments showed an interesting contrast. Speech utterances were similarly and consistently rated as pleasant, but also of intermediate urgency (that is, speech sounds did not differ according to situational urgency). On the other hand the three other sound types mapped successfully onto their specified situational urgency levels, and showed a consistent relationship: sounds mapped to highly urgent scenarios were also judged less pleasant.","2005-07","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BLXEQ79J","journalArticle","2005","Bovermann, Till; Hermann, Thomas; Ritter, Helge","The local heat exploration model for interactive sonification","","","","","http://hdl.handle.net/1853/50184","This paper presents a new sonification model for the exploration of topographically ordered high-dimensional data (multi-parameter maps, volume data) where each data item consists of a position and feature vector. The sonification model implements a common metaphor from thermodynamics that heat can be interpreted as stochastic motion of 'molecules'. The latter are determined by the data under examination, and 'live' only in the feature space. Heat-induced interactions cause acoustic events that fuse to a granular sound texture which conveys meaningful information about the underlying distribution in feature space. As a second ingredient of the model, data selection is achieved by a separated navigation process in position space using a dynamic aura model, such that heat can be induced locally. Both, a visual and an auditory display are driven by the underlying model. We exemplify the sonification by means of interaction examples for different high-dimensional distributions.","2005-07","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LJ88NUW6","book","2010","Riedenklau, Eckard; Hermann, Thomas; Ritter, Helge","Tangible Objects and Interactive Sonification as a Scatter Plot Alternative for the Visually Impaired","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/50065","In this paper we present an approach that enables visually impaired people to explore multivariate data through scatter plots. Our approach combines Tangible Active Objects (TAOs) and Interactive Sonification into a non-visual multi-modal data exploration interface and thereby translates the visual experience of scatter plots into the audio-haptic domain. Our system and the developed sonification techniques are explained in this paper and a first user study is presented.","2010-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LAJE2VFU","journalArticle","2006","Berman, L. I.; Gallagher, K. B.","Listening to program slices","","","","","http://hdl.handle.net/1853/50597","Comprehending a computer program can be a daunting task. There is much to understand, including the interaction among different portions of the code. Program slicing can help one to understand this interaction. Because present-day visual development environments tend to become cluttered, the authors have explored sonification of program slices in an attempt to determine if it is practical to offload some of the visual information. Three slice sonification techniques were developed, resulting in an understanding of how to sonify slices in a manner appropriate for the software developer undertaking program comprehension activities. The investigation has also produced a better understanding of sonification techniques that are musical yet non-melodic and non-harmonic. These techniques were demonstrated to a small set of developers, each reporting that the techniques are promising and useful.","2006-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2QBBMNGL","journalArticle","2002","Chateau, N.; Maffiolo, V.; Ehrette, T.; d'Alessandro, C.","Modelling the emotional quality of speech in a telecommunication context","","","","","http://hdl.handle.net/1853/51364","This paper presents a study of the perception, the analysis, and the modelling of the emotional quality of speech. Speech emotional quality is defined as the qualities of speech samples in terms of the emotional content that describe the listeners' global impressions as elicited by their audition. For this study, twenty professional female speakers recorded a welcome prompt of a vocal server in five elocution styles. The sound corpus was submitted to psychoacoustic tests and to signal analysis. From the psychoacoustic tests, twenty subjective criteria could be extracted that characterize the perceived emotional quality. These criteria can be used to draw perceptive portraits of the speech samples. Linear models connecting the perceptive portraits to physical data derived from signal analysis were developed.","2002-07","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J72EL9E9","journalArticle","2007","Liljedahl, Mats; Papworth, Nigel; Lindberg, Stefan","Beowulf: A game experience built on sound effects","","","","","http://hdl.handle.net/1853/49983","A computer game with most of the traditional graphics removed and replaced with a detailed and realistic soundscape, can give immersive gaming experiences. By reducing the graphical, explicit output of information from the game, the player becomes free to concentrate on interpreting the implicit information from a rich soundscape. This process of interpretation seems to have the power to invoke clear inner, mental images in the player, which in turn gives strong and immersive experiences. This paper describes a project that explores some of these mechanisms and points out some new potential directions for computer games and game play design","2007-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","Beowulf","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L9I7BWWQ","journalArticle","2014","Boevé, Jean-Luc; Giot, Rudi","Volatiles that Sound Bad: Sonification of Defensive Chemical Signals from Insects against Insects","","","","","http://hdl.handle.net/1853/52067","Defensive chemicals such as volatiles are essential for many insects against the attack of predatory insects, but in the research domain of chemical ecology there remains a need to better understand how intrinsic physicochemical constants of volatiles determine the intra- and interspecific diversification of such compounds produced by prey insects, knowing that many predatory insects primarily rely on chemical cues during foraging. To apprehend and explore the diversity of emitted chemicals as related to the receiver’s perception, here we aim to transform chemical into acoustic signals by a process of sonification, because odours and sounds are similarly perceived in their spatiotemporal dynamics. Since insects often emit a complex mixture of repellents, we prototyped a sonification software to process physicochemical parameters of individual molecules, prior mixing these sonified data by following the chemical profile of specific insect defensive secretions. In a proof of concept, the repellence of insectivorous ants towards single chemicals was compared with the repulsive response of humans towards the auditorily translated signals. Expected outreaches of our ongoing project called 'SonifChem' are, among others, to explore the repulsive and even the attractive bioactivities of chemicals emanating from any (biological) source.","2014-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","Volatiles that Sound Bad","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZXRU3JEZ","journalArticle","2012","André, Cédric R.; Embrechts, Jean-Jacques; Verly, Jacques G.; Rébillat, Marc; Katz, Brian F. G.","Sound for 3D cinema and the sense of presence","","","2168-5126","","http://hdl.handle.net/1853/44399","While 3D cinema is becoming more and more established, little effort has focused on the general problem of producing a 3D sound scene spatially coherent with the visual content of a stereoscopic-3D (s-3D) movie. As 3D cinema aims at providing the spectator with a strong impression of being part of the movie (sense of presence), the perceptual relevance of such spatial audiovisual coherence is of significant interest. Previous research has shown that the addition of stereoscopic information to a movie increases the sense of presence reported by the spectator. In this paper, a coherent spatial sound rendering is added to an s-3D movie and its impact on the reported sense of presence is investigated. A short clip of an existing movie is presented with three different soundtracks. These soundtracks differ by their spatial rendering quality, from stereo (low spatial coherence) to Wave Field Synthesis (WFS, high spatial coherence). The original stereo version serves as a reference. Results show that the sound condition does not impact on the sense of presence of all participants. However, participants can be classified according to three different levels of presence sensitivity with the sound condition impacting only on the highest level (12 out of 33 participants). Within this group, the WFS soundtrack provides a lower reported sense of presence than the other custom soundtrack. The analysis of the participants' heart rate variability (HRV) shows that the frequency-domain parameters correlate to the reported presence scores.","2012-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H4K4WGQM","journalArticle","1998","Evans, Michael J.","Synthesising moving sounds","","","","","http://hdl.handle.net/1853/50729","Auditory display designers are making increasingly effective and creative use of our ability to localise sound; to particular auditory events as occurring at particular locations. Many applications in which spatial audio has been applied could also benefit from exploiting another important ability of the auditory system; the detection and identification of sound source motion. The display of moving sources could improve usability, provide additional variables in sonification, make virtual environments more perceptually realistic and provide new creative possibilities for designers. Transaural cancellation allows the creation of spatial audio with just two loudspeakers. These techniques are now extended to create the illusion of a sound source moving along an arbitrary trajectory at an arbitrary rate. This paper discusses the application of synthesised sound source movement to a number of practical applications in auditory display. We seek to extend the use of Head-Related Transfer Functions (HRTFs) in stationary sound spatialisation to encompass movement synthesis. The detection of moving sources is not time-invariant so we propose and demonstrate the use of time-frequency spectrograms as a mechanism for characterising source movement. There are an infinite number of such trajectory-related spectrograms and we address the need for a continuous directional model to accommodate this.","1998-11","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LN5JPFJQ","journalArticle","2012","Supper, Alexandra","""Trained ears"" and ""correlation coefficients"": A social science perspective on sonification","","","2168-5126","","http://hdl.handle.net/1853/44444","This paper presents a social science perspective on the field of sonification research. Adopting a perspective informed by constructivist science and technology studies (STS), the paper begins by arguing why sonification is an interesting case study to reconsider the role of sensory representation in scientific practice, and in particular the creation of credibility in science. It then focuses on a debate in which the meaning of objectivity is negotiated within the sonification community, showing that different notions of objectivity and scientific quality co-exist within the community, which are linked to different research questions being asked with the sonifications, different users that are envisaged for the sonifications, and different disciplinary backgrounds of the sonification researchers.","2012-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","""Trained ears"" and ""correlation coefficients""","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z8E2AHBD","journalArticle","2019","Ferguson, Jamie; Brewster, Stephen","Evaluating the magnitude estimation approach for designing sonification mapping topologies","","","","","http://hdl.handle.net/1853/61521","A challenge in sonification design is mapping data parameters onto acoustic parameters in a way that aligns with a listener's mental model of how a given data parameter should sound. Studies have used the psychophysical scaling method of magnitude estimation to systematically evaluate how participants perceive mappings between data and sound parameters - giving data on perceived polarity and scale of the relationship between the data and sound parameters. As of yet, there has been little research investigating whether data-to-sound mappings that are designed based on results from these magnitude estimation experiments have any effect on usersﾒ performance in an applied auditory display task. This paper presents an experiment that compares data-to-sound mappings in which the mappingﾒs polarity is based on results from a previous magnitude estimation experiment against mappings whose polarities are inverted. The experiment is based around a simple task in which participants need to rank WiFi networks based on how secure they are, where security is represented using an auditory display. Results suggest that for a simple auditory display like the one used here, whether or not the polarities of the data-to-sound mappings are based on magnitude estimation does not have a substantial effect on any objective performance measures gathered during the experiment. Finally, potential areas for future work are discussed that may continue to investigate the problems addressed by this paper.","2019-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6UEYX2SA","journalArticle","2004","Pauletto, S.; Hunt, A.","A toolkit for interactive sonification","","","","","http://hdl.handle.net/1853/50827","This paper describes work-in-progress on an Interactive Sonification Toolkit which has been developed in order to aid the analysis of general data sets. The toolkit allows the designer to process and scale data sets, then rapidly change the sonification method. The human user can then interact with the data in a fluid manner, continually controlling the position within the set. The interface used by default is the computer mouse, but we also describe plans for multiparametric interfaces which will allow real-time control of many aspects of the data. Early results of interactive sonic analysis of two example domains are described, but extensive user tests are being planned.","2004-07","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7ZC74PV2","journalArticle","2007","Mariette, Nick","Mitigation of Binaural Front-Back Confusions by Body Motion in Audio Augmented Reality","","","","","http://hdl.handle.net/1853/50015","Front-back confusions are a well-known phenomenon of spatial hearing whereby the listener incorrectly localizes a source to its mirror image position across the frontal plane. This type of localization error can occur for real and synthetically spatialised sound sources. Experiments have shown the listener can resolve front-back ambiguities by rotating their head; also that sound source movement can resolve confusions if the listener is aware of the intended direction of source movement. The present outdoors experiment studies the mitigation of front-back confusions for synthetic binaural spatial audio interactive with body movement but not head-turns. This partly disabled mobile augmented reality system renders sound source positions relative to the world reference frame, (so the listener may walk past a stationary spatialised sound), but it renders instantaneous source bearing relative to the listener's reference frame. Experiment participants walked past synthetic binaural sound sources with initial azimuths of {\textbackslashpm\(40{\textbackslash,ˆ{\textbackslashcirc}\, 60{\textbackslash,ˆ{\textbackslashcirc}\, 80{\textbackslash,ˆ{\textbackslashcirc}\, 100{\textbackslash,ˆ{\textbackslashcirc}\, 120{\textbackslash,ˆ{\textbackslashcirc}\ and 140{\textbackslash,ˆ{\textbackslashcirc}\) and initial distance of 20 metres. Walk distances were chosen to result in azimuth changes of 4{\textbackslash,ˆ{\textbackslashcirc}\, 8{\textbackslash,ˆ{\textbackslashcirc}\, 12{\textbackslash,ˆ{\textbackslashcirc}\ and 16{\textbackslash,ˆ{\textbackslashcirc}\ between initial and final source bearings. Each factor combination resulted in a corresponding source distance change over the course of the walk. Front or back judgments of the initial source positions were recorded before and after walking. Results show statistically significant improvement of front-back localization for source azimuth changes of 12{\textbackslash,ˆ{\textbackslashcirc}\ or 16{\textbackslash,ˆ{\textbackslashcirc}\, and source distance changes of at least 0.21 of the initial distance.","2007-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4R2T5URQ","journalArticle","2006","Hermann, T.; Baier, G.; Stephani, U.; Ritter, H.","Vocal sonification of pathologic EEG features","","","","","http://hdl.handle.net/1853/50695","We introduce a novel approach in EEG data sonification for process monitoring and exploratory as well as comparative data analysis. The approach uses an excitory/articulatory speech model and a particularly selected parameter mapping to obtain auditory gestalts (or auditory objects) that correspond to features in the multivariate signals. The sonification is adaptable to patient-specific data patterns, so that only characteristic deviations from background behavior (pathologic features) are involved in the sonification rendering. Thus the approach combines data mining techniques and case-dependent sonification design to give an application-specific solution with high potential for clinical use. We explain the sonification technique in detail and present sound examples from clinical data sets.","2006-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M4PBT2GF","journalArticle","2013","Tünnermann, René; Hammerschmidt, Jan; Hermann, Thomas","Blended Sonification –Sonification For Casual Information Interaction","","","","","http://hdl.handle.net/1853/51656","In recent years, graphical user interfaces have become almost ubiquitous in form of notebooks, smartphones and tablets. These systems normally force the user to attend to an often very specific and narrow screen and thus squeeze the information through a chokepoint. This ties the users’ attention to the device and affects other activities and social interaction. In this paper we introduce Blended Sonifications as sonifications that blend into the users’ environment without confronting users with any explicitly perceived technology. Blended Sonification systems can either be used to display information or to provide ambient communication channels. We present a framework that guides developers towards the identification of suitable information sources and appropriate auditory interfaces. We aim at improving the design of interactions and experiences. Along with the introduction and definition of the framework, this paper presents interface examples, both for mediated communication and information display applications.","2013-07","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YSU9SYRP","journalArticle","2019","Aziz, Nida; Stockman, Tony; Stewart, Rebecca","An investigation into customisable automatically generated auditory route overviews for pre-navigation","","","","","http://hdl.handle.net/1853/61507","While travelling to new places, maps are often used to determine the specifics of the route to follow. This helps prepare for the journey by forming a cognitive model of the route in our minds. However, the process is predominantly visual and thus inaccessible to people who are either blind or visually impaired (BVI) or doing an activity where their eyes are otherwise engaged. This work explores effective methods of generating route overviews, which can create a similar cognitive model as visual routes, using audio. The overviews thus generated can help users plan their journey according to their preferences and prepare for it in advance. This paper explores usefulness and usability of auditory routes overviews for the BVI and draws design implications for such a system following a 2-stage study with audio and sound designers and users.The findings underline that auditory route overviews are an important tool that can assist BVI users to make more informed travel choices. A properly designed auditory display might contain an integration of different sonification methods and interaction and customisation capabilities. Findings also show that such a system would benefit from the application of a participatory design approach.","2019-06","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"27X7Y7A6","journalArticle","2001","Zacharov, Nick; Koivuniemi, Kalle","Audio descriptive analysis & mapping of spatial sound displays","","","","","http://hdl.handle.net/1853/50610","This study presents a method termed Audio Descriptive Analysis & Mapping (ADAM) to a study of spatial sound displays. Several subjective tasks were performed including a preference experiment, descriptive language development and lastly scaling of all stimuli based on developed attribute scales. The process associated with the descriptive language and attribute scale development is described in detail. A large number of stimuli (104) were employed comprising of 8 audio recording/reproduction techniques in 13 different sound environments, in an effort to broadly evaluated spatial sound displays. Preference data was submitted to a principle components analysis to study the underlying structure of the data. In order to study how subjective preference is formulated, preference data and direct attribute data were submitted to a preference mapping procedure employing partial least square regression.","2001-07","2023-07-13 06:25:53","2023-07-13 06:25:53","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KHHQP9ZA","book","2010","Stockman, Tony; Al-Thanki, Dena","Development and Evaluation of a Cross-Modal XML Schema Browser","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49871","We describe the development and evaluation of across-modal XML (Extensible Mark-up Language) schema browser. The aim of developing the system is to investigate cross-modal collaboration between users. The browser provides an audio representation of XML schema documents in a way that preserves the structure of documents and supports multi-level navigation. The project has two principle objectives: 1) to overcome the difficulties faced by visually impaired users and sighted people using small screen devices when browsing XML schema files, 2) To explore usability issues when users collaborate using the auditory and visual interfaces of the system. The paper also examines differences between sighted and visually impaired users of the developed auditory interface. The overall results of the usability evaluations demonstrate that both sighted and visually impaired users were able to perform tasks using the audio modality efficiently and accurately, and the same was true of sighted users interactions with the GUI. The use of the system to support collaboration where each user employs a different mode (audio or visual) of the system clearly demonstrated that cross-modal collaboration is effectively supported, enabling users to collaborate and successfully complete a complex shared task.","2010-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HD67ZY9H","journalArticle","2019","Larsson, Pontus; Maculewicz, Justyna; Fagerlönn, Johan; Lachmann, Max","Auditory displays for automated driving - challenges and opportunities","","","","","http://hdl.handle.net/1853/61513","The current position paper discusses vital challenges related to the user experience design in unsupervised, highly automated cars. These challenges are: (1) how to avoid motion sickness, (2) how to ensure users' trust in the automation, (3) how to ensure usability and support the formation of accurate mental models of the automation system, and (4) how to provide a pleasant and enjoyable experience. We argue for that auditory displays have the potential to help solve these issues. While auditory displays in modern vehicles typically make use of discrete and salient cues, we argue that the use of less intrusive continuous sonic interaction could be more beneficial for the user experience.","2019-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SCRYDYKD","journalArticle","2006","Milczynski, M.; Hermann, T.; Bovermann, T.; Ritter, H.","A malleable device with applications to sonification-based data exploration","","","","","http://hdl.handle.net/1853/50424","This article introduces a novel human computer interaction device, developed in the scope of a Master's Thesis. The device allows continuous localized interaction by providing a malleable interaction surface. Diverse multi-finger as well as multi-handed manipulations can be applied. Furthermore, the device acts as a tangible user interface object, integrated into a tangible computing framework called tDesk. Software to convert the malleable element's shape into an internal surface representation has been developed. Malleable interactions are applied to a new Modelbased Sonification approach for exploratory data analysis. Highdimensional data are acoustically explored via their informative interaction sound in result to the user's excitation.","2006-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6JQMQB9H","journalArticle","2002","Martens, W. L.","Subjective evaluation of auditory spatial imagery associated with decorrelated subwoofer signals","","","","","http://hdl.handle.net/1853/51379","Although only a single subwoofer is typically used in two-channel and multichannel stereophonic sound reproduction, the use of two subwoofers enables manipulation of low-frequency interaural crosscorrelation (IACC), and this manipulation is particularly effective in producing variation in auditory spatial imagery. In order to document this variation objectively, a series of listening experiments were executed using a set of stimuli generated at five correlation values and presented in two reproduction modes. Both modes used two subwoofers, but in one of the reproduction modes identical signals were applied to the two subwoofers. The results of both exploratory and confirmatory listening experiments showed that the range of variation in both perceived auditory source width (ASW) and perceived auditory source distance (ASD) is reduced when negatively correlated signals are not reproduced at low frequencies. Global dissimilarity judgments were made for this set of ten stimuli in an exploratory study designed to reveal the salient perceptual dimensions of the stimuli. A subsequent confirmatory study employed a two-alternative forced-choice task in order to determine how identifiably different the stimuli were with respect to the two perceptual attributes revealed in the exploratory study, those two attributes being ASW and ASD. The implications of these findings for loudspeaker-based spatial auditory display are discussed.","2002-07","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2P9FYBD3","journalArticle","2001","Rocchesso, Davide","Acoustic cues for 3-D shape information","","","","","http://hdl.handle.net/1853/50608","Three-dimensional resonators, such as cavities or rooms, affect the perceived timbral character of any sound source there enclosed. It is well understood how the resonator size, or the material of the enclosure, are conveyed to the listener by means of specific features of acoustic signals. On the other hand, the perception of the shape of a resonator is a much subtler issue that we investigate in this paper, taking the sphere and the cube as reference cases. The perceptual study is motivated by the availability of a compact resonator model whose parameters can be tuned to represent different shapes.","2001-07","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7XRQRJVQ","book","2010","Wakkary, Ron; Droumeva, Milena","Socio-ec(h)o: Focus, Listening and Collaboration in the Experience of Ambient Intelligent Environments","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/50054","In this paper, we aim to conceptualize the connection between embodied interactions and the experience of understanding a dynamic auditory display response. We have termed this concept aural fluency and hereby continue previous work documenting in more detail the listening patterns that emerge in users’ experiences with ambient intelligent environments. Aural fluency describes the acquired listening competency and focus on sonic feedback that users form over time in systems utilizing responsive ambient audio display and collaborative embodied interaction. We describe listening positions that characterize the concept and show the stages of aural fluency. The concept arose from the design, analysis and evaluation of an embedded interaction system named socio-ec(h)o – a project upon which we also build on from previous work in the hopes of elucidating further the complex experiences of listening attentions and thus offer insights to the field of auditory displays.","2010-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","Socio-ec(h)o","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2FNB3JYP","journalArticle","2007","Menzies, Dylan","Nearfield Synthesis of Complex Sources with High-Order Ambisonics, and Binaural Rendering","","","","","http://hdl.handle.net/1853/50018","A scheme is presented for encoding general complex sources in high-order Ambisonic soundfields, with control over position and orientation. Also reviewed is related work by the author on the binaural rendering of nearfield sources, accounting fully for the physical constraints of this problem. Together these developments provide a means for creating high quality nearfield auditory displays over headphones.","2007-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R5J7F2QM","journalArticle","2017","Gable, Thomas M.; Tomlinson, Brianna; Cantrell, Stanley; Walker, Bruce N.","Spindex and Spearcons in Mandarin: Auditory Menu Enhancements Successful in a Tonal Language","","","","","http://hdl.handle.net/1853/58357","Auditory displays have been used extensively to enhance visual menus across diverse settings for various reasons. While standard auditory displays can be effective and help users across these settings, standard auditory displays often consist of text to speech cues, which can be time intensive to use. Advanced auditory cues including spindex and spearcon cues have been developed to help address this slow feedback issue. While these cues are most often used in English, they have also been applied to other languages, but research on using them in tonal languages, which may affect the ability to use them, is lacking. The current research investigated the use of spindex and spearcon cues in Mandarin, to determine their effectiveness in a tonal language. The results suggest that the cues can be effectively applied and used in a tonal language by untrained novices. This opens the door to future use of the cues in languages that reach a large portion of the world’s population.","2017-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","Spindex and Spearcons in Mandarin","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T45LLVVE","journalArticle","2004","Duraiswami, R.; Schneiderman, B.; Plaisant, C.; Zhao, H.","Sonification of geo-referenced data for auditory information seeking: Design principle and pilot study","","","","","http://hdl.handle.net/1853/50918","We present an Auditory Information Seeking Principle (AISP) (gist, navigate, filter, and details-on-demand) modeled after the visual information seeking mantra [1]. We propose that data sonification designs should conform to this principle. We also present some design challenges imposed by human auditory perception characteristics. To improve blind access to georeferenced statistical data, we developed two preliminary sonifications adhering to the above AISP, an enhanced table and a spatial choropleth map. Our pilot study shows people can recognize geographic data distribution patterns on a real map with 51 geographic regions, in both designs. The study also shows evidence that AISP conforms to people's information seeking strategies. Future work is discussed, including the improvement of the choropleth map design.","2004-07","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","Sonification of geo-referenced data for auditory information seeking","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JWQKSG9T","journalArticle","2007","Cabrera, Densil","Control of Perceived Room Size Using Simple Binaural Technology","","","","","http://hdl.handle.net/1853/49986","The localization of auditory images and their size forms the bulk of the research literature in spatial auditory perception using binaural technology. Nevertheless, binaural technology conveys many other spatial characteristics of sound environments, and the present paper is concerned with one of these: auditory room size perception. This paper reviews the potential cues to room size perception conveyed through simple binaural technology. Statistical room acoustics is shown to provide indications of room size through energy relations between direct sound, early reflections and late reflections. However, binaural hearing could be important in distinguishing the concept of room size from source distance. These theoretical notions are considered in relation to experimental findings on room size perception using simple binaural technology.","2007-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HN4KG4PA","journalArticle","2021","El Hajj, Tracey","Network sonification and the algorhythmics of everyday life","","","","","http://hdl.handle.net/1853/66333","Today, public concern with the extent to which they influence people's routines, and how much they affect cultures and societies, has grown substantially. This paper argues that, by listening to networks, people can begin to apprehend, and even comprehend, the complex, ostensibly ""magical"" nature of network communications. One problem is that listening semantically to networks is incredibly difficult, if not impossible. Networks are very noisy, and they do not, for instance, use alphabetic language for internal or external communication. For the purpose of interpreting networks, I propose ""tactical network sonification"" (TNS), a technique that focuses on making the materiality of networks sensibly accessible to the general public, especially people who are not technology experts. Using an electromagnetic transduction device — Shintaro Miyazaki and Martin Howse's Detektor — TNS results in crowded sound clips that represent the complexity of network infrastructure, through the many overlapping rhythms and layers of sound that each clip contains.","2021-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V5RCXAM3","journalArticle","2008","Bovermann, Till; Hermann, Thomas; Ritter, Helge","AudioDB. Get in Touch With Sound","","","","","http://hdl.handle.net/1853/49872","In this paper we present AudioDB, a system to collaboratively navigate sound databases via a spatial audio-haptic setup. It provides an environment to sonically sort, group and select sounds which are represented as physical artifacts on a tabletop surface. We give an introduction and insights on implementing interactive overviews for sound databases followed by first impressions of a qualitative analysis of the system.","2008-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LYL99C68","journalArticle","2012","Droumeva, Milena; McGregor, Iain","Everyday listening to auditory displays: lessons from acoustic ecology","","","2168-5126","","http://hdl.handle.net/1853/44414","In order to design auditory displays that function well within the cultural, informational and acoustic ecology of everyday situations designers as well as researchers in psychoacoustics need to continue to gain a better understanding of how listeners hear and make sense of information in more ecological settings and outside the lab! In this paper the authors present a preliminary study that builds on past work and theoretical ideas from acoustic ecology, exploring the practice of everyday listening in settings containing auditory displays. This pilot study involves 10 participants who are asked to listen to two separate soundscapes and describe in three tasks, both verbally and in writing, what they hear and how they make sense of these aural environments. The results suggest directions for understanding everyday listening form a holistic perspective in order to inform both the design of auditory displays, and the development of other research tools and instruments for measuring auditory perception ecologically. The bigger study which involves 100 participants has been completed and is expected to be published shortly as a journal article.","2012-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","Everyday listening to auditory displays","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HIPMFJ8I","journalArticle","2012","Brueckner, Hans-Peter; Wielage, Matthis; Blume, Holger","Intuitive and Interactive Movement Sonification on a Heterogeneous RISC / DSP Platform","","","2168-5126","","http://hdl.handle.net/1853/44412","A major requirement for effective and interactive sonification in rehabilitation is the availability of a mobile platform. Portable state of the art motion capturing is achieved with inertial sensors. This paper presents a real-time, low latency sonification demonstrator based on an low power consumption ARM Cortex A8 processor, which is designed for mobile usage. The sonification demonstrator is based on the Texas Instruments C6A816x / AM389x development board. It enables research in continuous real time sonification of human motion to improve the process of motion learning in stroke rehabilitation. Profiling results are used to benchmarked the Integra software application against a PC based version in terms of signal processing latency. Furthermore, a new sonification mapping, basing on the beat effect, is introduced. This mapping is especially usable for people suffering from partial deafness. A subjective test series shows the understandability of this mapping for healthy subjects, in comparison to a previously proposed sonification mapping.","2012-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q6SAXTJR","journalArticle","1996","Storms, Russell; Cockayne, William; Barham, Paul; Falby, John; Brutzman, Don; Zyda, Michael; Biggs, Lloyd","The auralization and acoustics laboratory","","","","","http://hdl.handle.net/1853/50819","As an expansion of the NPSNET Research Group (NRG), the Auralization and Acoustics Laboratory (AA-Lab) at the Naval Postgraduate School studies the integration of aural cues into virtual environments. Currently, the AA-Lab focuses on spatial-acoustic sound rendering via headphones (closed-field) and loudspeakers (open-field).","1996-11","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"INBMYWY5","book","2010","Gamper, Hannes; Lokki, Tapio","Audio Augmented Reality in Telecommunication Through Virtual Auditory Display","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49841","Audio communication in its most natural form, the face-to-face conversation, is binaural. Current telecommunication systems often provide only monaural audio, stripping it of spatial cues and thus deteriorating listening comfort and speech intelligibility. In this work, the application of binaural audio in telecommunication through audio augmented reality (AAR) is presented. AAR aims at augmenting auditory perception by embedding spatialised virtual audio content. Used in a telecommunication system, AAR enhances intelligibility and the sense of presence of the user. As a sample use case of AAR, a teleconference scenario is devised. The conference is recorded through a headset with integrated microphones, worn by one of the conference participants. Algorithms are presented to compensate for head movements and restore the spatial cues that encode the perceived directions of the conferees. To analyse the performance of the AAR system, a user study was conducted. Processing the binaural recording with the proposed algorithms places the virtual speakers at fixed directions. This improved the ability of test subjects to segregate the speakers significantly compared to an unprocessed recording. The proposed AAR system outperforms conventional telecommunication systems in terms of the speaker segregation by supporting spatial separation of binaurally recorded speakers.","2010-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CA2KFFH4","book","2010","Tuuri, Kai; Pirhonen, Antti","Communicative Functions of Sounds which we call Alarms","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49867","The design of alarm or warning sounds appears to be far from a trivial challenge. Even if the basic principles of creating an alarming quality for a sound have been widely accepted and applied, there seems to be a constant need for knowledge about what a ”good” alarm should sound like. In this paper, we analyse the challenge of alarm sound design. The analysis is carried out in terms of an application context, which is an anaesthesia workstation in an operating room. We conclude that to result in satisfactory sounds, the design should not only concentrate on stereotypic qualities of expected alarms, like a strong psycho-physiological reaction but should also take more aspects into an account. It is proposed that these context dependent aspects, in turn, are extracted from the communicative functions of the sound’s intended usage. For such a conceptual design of alarm sounds, a basic taxonomy of communicative functions in terms of alarm priority levels is proposed. Even though this report concentrates on one application area, the approach would be applicable in several areas. Sound design for other safety critical applications, in particular, would benefit from our findings.","2010-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QSGV5CTI","journalArticle","2016","Martin, Fiore; Metatla, Oussama; Bryan-Kinns, Nick; Stockman, Tony","Accessible Spectrum Analyser","","","","","http://hdl.handle.net/1853/56586","This paper presents the Accessible Spectrum Analyser (ASA) developed as part of the DePic project (Design Patterns for Inclusive collaboration) at Queen Mary University of London. The ASA uses sonification to provide an accessible representation of frequency spectra to visually impaired audio engineers. The software is free and open source and is distributed as a VST plug-in under OSX and Windows. The aim of reporting this work at the ICAD 2016 conference is to solicit feedback about the design of the present tool and its more generalized counterpart, as well as to invite ideas for other possible applications where it is thought that auditory spectral analysis may be useful, for example in situations where line of sight is not always possible.","2016-07","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RDZHFBLW","journalArticle","2018","Aldana Blanco, Andrea Lorena; Grautoff, Steffen; Hermann, Thomas","CardioSounds: A portable system to sonify ECG rhythm disturbances in real-time","","","","","http://hdl.handle.net/1853/60089","CardioSounds is a portable system that allows users to measure and sonify their electrocardiogram signal in real-time. The ECG signal is acquired using the hardware platform BITalino and subsequently analyzed and sonified using a Raspberry Pi. Users can control basic features from the system (start recording, stop recording) using their smartphone. The system is meant to be used for diagnostic and monitoring of cardiac pathologies, providing users with the possibility to monitor a signal without occupying their visual attention. In this paper, we introduce a novel method, anticipatory mapping, to sonify rhythm disturbances such as Atrial Fibrillation, Atrial flutter and Ventricular Fibrillation. Anticipatory mapping enhances perception of rhythmic details without disrupting the direct perception of the actual heart beat rhythm. We test the method on selected pathological data involving three of the most known rhythm disturbances. A preliminary perception test to assess aesthetics of the sonifications and its possible use in medical scenarios shows that the anticipatory mapping method is regarded as informative discerning healthy and pathological states, however there is no agreement about a preferred sonification type.","2018-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","CardioSounds","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4TWHLKF6","journalArticle","2005","Stockman, Tony; Nickerson, Louise Valgerour; Hind, Greg","Auditory graphs: A summary of current experience and towards a research agenda","","","","","http://hdl.handle.net/1853/50097","In this paper we shall briefly review previous work we have found directly relevant to our own research on the use of auditory graphs. We will then summarise previous unpublished experiences of us- ing auditory graphs in the domain of medical signal analysis, and further recent work on the use of auditory graphs for analysing spreadsheet data. We conclude by outlining issues we believe to be relevant in the formation of a research agenda for the design and evaluation of the technology.","2005-07","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","Auditory graphs","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TPSY2ZR2","journalArticle","2022","Ziemer, Tim; Jadid, Navid Mirzayousef","Recommendations to Develop, Distribute, and Market Sonification Apps","","","","","http://hdl.handle.net/1853/67377","After decades of research, sonification is still rarely adopted in consumer electronics, software and user interfaces. Outside the science and arts scenes the term sonification seems not well known to the public. As a means of science communication, and in order to make software developers, producers of consumer electronics and end users aware of sonification, we developed, distributed, and promoted Tiltification. This smartphone app utilizes sonification to inform users about the tilt angle of their phone, so that they can use it as a torpedo level. In this paper we report on our app development, distribution and promotion strategies and reflect on their success in making the app in particular, and sonification in general, better known to the public. Finally, we give recommendations on how to develop, distribute and market sonification apps.This article is dedicated to research institutions without commercial interests.","2022-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9BA2RUYI","book","2010","Alexander, Robert; Zurbuchen, Thomas H.; Gilbert, Jason; Lepri, Susan; Raines, Jim","Sonification of Ace Level 2 Solar Wind Data","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49913","This paper provides a brief overview of the sonification research conducted by the Solar and Heliospheric Research Group at the University of Michigan. The team collaborated with composer and multimedia artist Robert Alexander to gain a new perspective of the underlying patterns behind recurring solar wind phenomena. This sonification effort was one in which a high level of creative freedom was provided to the composer, while scientific accuracy was maintained through adherence to the original data set. An interface was constructed in Max/MSP that allowed ACE-SWICS Level 2 solar wind data to be graphed visually and represented aurally through both acoustic and synthesized timbres. This document will explore the sonification methods behind iteration 1.1, which is a sonification of solar wind activity from 2003.","2010-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CSMBJ5AQ","journalArticle","2019","Wolf, KatieAnn; Fiebrink, Rebecca","Toward supporting end-user design of soundscape sonifications","","","","","http://hdl.handle.net/1853/61520","In this paper, we explore the potential for everyday Twitter users to design and use soundscape sonifications as an alternative, “calm” modality for staying informed of Twitter activity. We first present the results of a survey assessing how 100 Twitter users currently use and change audio notifications. We then present a study in which 9 frequent Twitter users employed two user interfaces - with varying degrees of automation – to design, customize, and use soundscape sonifications of Twitter data. This work suggests that soundscapes have great potential for creating a calm technology for maintaining awareness of Twitter data, and that sound scapes can be useful in helping people without prior experience in sound design think about sound in sophisticated ways and engage meaningfully in sonification design.","2019-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E38AZINM","journalArticle","2008","Papetti, Stefano; Devallez, Delphine; Fontana, Federico","Depthrow: A Physics-based Audio Game","","","","","http://hdl.handle.net/1853/49900","We present an interactive audio game designed around the auditory perception of distance and the use of physics-based models for simulations of the dynamics, the sound source, and the acoustical environment. The game consists in throwing a virtual sound- ing object inside a virtual open-ended tube which is inclined. The task is to keep the object inside the tube, in other words the user should adjust the initial velocity applied to the object such that the latter does not fall out at the far end of the tube. The position of the object inside the tube is provided by continuous audio feedback. User performance is closely related to its ability to perceive the dynamic distance of the object in the virtual tube. Therefore, this game represents a potential tool for exploring the usability of auditory distance information in interaction design.","2008-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","Depthrow","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZMSB5XMF","journalArticle","2006","de Gotzen, A.; Rocchesso, D.","Peek-a-book: Playing with an interactive book","","","","","http://hdl.handle.net/1853/50604","This demonstration is about a prototype of a new digitally augmented book for children, using sensors to allow continuous user interaction and to generate (not just play back) sounds in real time. During the demonstration the user will experience the book, intuitively modifying and controlling the sound generation process.","2006-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","Peek-a-book","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SY5K7YYG","journalArticle","1997","Pair, Jarrell; Kooper, Rob","COOLVR: Implementing audio in a virtual environments toolkit","","","","","http://hdl.handle.net/1853/50742","COOLVR (Complete Object Oriented Library for Virtual Reality) is a toolkit currently being developed at the Graphics, Visualization, and Usability Center (GVU) at Georgia Tech. The toolkit is written to allow programmers to easily create virtual environments (VE's) which will compile cross platform. Unlike most VE toolkits which focus effort primarily on the visual senses, COOLVR aims to equally engage both the sense of sight and the sense of hearing. One of the main design goals of the COOLVR toolkit is to give the programmer an intuitive method to enrich the virtual world with auditory cues. COOLVR uses a set of cross platform audio rendering modules to conduct real time sound processing. By providing potential designers with the capability of easily integrating spatial audio in a virtual world, a heightened level of immersivity or presence can be achieved in COOLVR environments.","1997-11","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","COOLVR","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ADT5BRIZ","journalArticle","2014","Nees, Michael A.; Gable, Thomas M.; Jeon, Myounghoon; Walker, Bruce N.","Prototype Auditory Displays for a Fuel Efficiency Driver Interface","","","","","http://hdl.handle.net/1853/52089","We describe work-in-progress prototypes of auditory displays for fuel efficiency driver interfaces (FEDIs). Although research has established that feedback from FEDIs can have a positive impact on driver behaviors associated with fuel economy, the impact of FEDIs on driver distraction has not been established. Visual displays may be problematic for providing this feedback; it is precisely during fuel-consuming behaviors that drivers should not divert attention away from the driving task. Auditory displays offer a viable alternative to visual displays for communicating information about fuel economy to the driver without introducing visual distraction.","2014-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6V2BE2ZQ","journalArticle","2014","Peres, S. Camille; Faisst, Cody; Slota, Nathan; Verona, Daniel; Williams, Chase; Ritchey, Paul","Sonification Synthesizer for Surface Electromyography","","","","","http://hdl.handle.net/1853/52103","Surface electromyography (sEMG) is a means for measuring muscle activity. sEMG data are typically displayed graphically on a computer screen and while this can be a useful way to display the data, it is not always ideal. This extended abstract details the development of a sonification tool that allows users to sonify sEMG data in real-time. The tool will allow the user to independently control the sound of each channel, similar to a software synthesizer. Independent real-time control of each channel will allow the user to create sonification models, which are mappings of certain sounds to specific muscle groups. A prototype of the tool is currently being developed using SuperCollider in parallel with a Delsys Trigno Wireless sEMG system. This tool will allow users to easily explore various kinds of sEMG sonification models and test them for intuitiveness and effectiveness.","2014-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LAVKGHVG","journalArticle","2011","Brown, Ethan","The R Programming Language as a Unified Environment for Data Sonification","","","","","http://hdl.handle.net/1853/51917","This short article explores the statistical programming environ- ment R as a sonification interface using the add-on package playitbyr, currently in a pre-alpha stage of development. R’s growing and vibrant community of users is a promising audience for sonification tools, and sonification can provide a welcome ad- dition to R’s arsenal of tools for making sense of multidimensional data. playitbyr attempts to provide intuitive functions and data structures for mapping data to sonic parameters, using MIDI or Csound for synthesis.","2011-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M9S7WRRF","journalArticle","2005","Shenkar, Orit; Weiss, Patrice L.; Algom, Daniel","Auditory representation of visual stimuli: Mapping versus association","","","","","http://hdl.handle.net/1853/50095","Two methods for representing visual images by sounds were explored. The analytic method used a rule-based representation by which values of continuous auditory variables defined each point in space. The metaphoric method governed the association of a unique sound with each image. The study demonstrated the usefulness and potential applicability of the analytic method.","2005-07","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","Auditory representation of visual stimuli","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LUT2U5EY","journalArticle","2005","Frysinger, Steven P.","A brief history of auditory data representation to the 1980s","","","","","http://hdl.handle.net/1853/50089","The field of Auditory Data Representation, which addresses the representation of quantitative data through the use of auditory, rather than visual, displays, has seen considerable activity in the last twenty years. On the occasion of the first Symposium on Auditory Graphs it is well to consider the roots of this field. This paper presents a brief history of the field, leading up to the beginning of the 1980s, and accompanies a demonstration of a multivariate time series representation developed by the author and his colleagues in 1980.","2005-07","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z4MXN8BA","journalArticle","2000","Fouad, Hesham; Ballas, James A.; Brock, Derek","An extensible toolkit for creating virtual sonic environments","","","","","http://hdl.handle.net/1853/50662","The Virtual Audio Server (VAS) is a toolkit designed for exploring problems in the creation of realistic Virtual Sonic Environments (VSE). The architecture of VAS pr","2000-04","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KW38TGFG","book","2009","Fernstrom, Mikael; Brazil, Eoin","The Shannon Portal: Designing an Auditory Display for Casual Users in a Public Environment","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51417","We developed an installation for a public environment and casual users where auditory display was a significant element to facilitate user interaction. We used an iterative design process, starting from simple onomatopoeic representations, to complex sound object models in Pure Data. The system was evaluated at each stage, from the lab to the final public setting. The problems addressed covered the representations of left-right, up-down, and the amount of movement by the user or groups of users. In addition to this, it was important that the auditory display would attract attention when users were within control range of the system, i.e. an affordance that invited and allowed users to discover functionality.","2009-05","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","The Shannon Portal","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NB4XBZ9U","journalArticle","2014","Worrall, David; Thoshkahna, Balaji; Degara, Norberto","Detecting Components of an ECG Signal for Sonification","","","","","http://hdl.handle.net/1853/52074","In recent state-of-the-art electocardiogram (ECG) studies, many authors mention that they had to manually correct automatically detected peaks or exclude artifact-loaded segments from the automatically annotated data they were studying. Given the importance of accurate feature detection for signal analysis, this is clearly a limiting factor. Our investigation into the use of sonification for analysis of ECG data for medical and diagnostic purposes is also hampered by the lack of such a reliable ground truth. In order to be able to undertake a comparative analysis of sonification and numerical techniques, we are investigating ways to improve algorithmic feature detection, particularly more robust algorithms for the detection of important landmarks in the signal in the presence of noise, whilst accounting for the variability in the very nature of the signal. This paper is a work-in-progress report of our efforts to date.","2014-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SCKQLMX2","journalArticle","2019","Osinski, Dominik; Bizon, Patrycja; Midtfjord, Helene; Wierzchon, Michal; Hjelme, Dag Roar","Designing auditory color space for color sonification systems","","","","","http://hdl.handle.net/1853/61538","Designing of color sonification systems provides a possibility of contribution to various fields ranging from rehabilitation of visually impaired through color perception, multisensory art experience to consciousness studies. The design process itself requires understanding and integrating knowledge from many difficult and inherently different branches of science and the resulting sonification method will be highly dependent on the purpose of the system. We present work in progress on designing and experimental verification of color sonification method that will be implemented in Colorophone, a wearable assistive device for the visually impaired, which enables perception of the information about color through sound. Although our system shows promising results in color and object recognition, we would like to enhance the existing color sonification method by designing a framework for experimental verification of our color sonification algorithm. The goal of this paper is therefore to briefly describe our way of thinking in order to provide the basis for the discussion.","2019-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J67DUMT5","journalArticle","2006","Candey, R. M.; Schertenleib, A. M.; Diaz Merced, W. L.","Xsonify sonification tool for space physics","","","","","http://hdl.handle.net/1853/50697","xSonify is a concentrated project to extend the space physics data capabilities of the NASA Space Physics Data Facility (SPDF) [1] for use by visually-impaired students and researchers, by developing a sonification data analysis tool using the JavaSound API and accessing data locally or via web services. xSonify is an open-source publicly-available Java application and can be easily installed (using WebStart) and run on most platforms. With sonification, a large fraction of the space physics data collection is opened to a completely new and now excluded audience (both professional and public). Besides meeting a compelling need for a more effective non-visual approach to displaying science data, this extends SPDF's goals of improving access to space physics data and helps achieve NASA's goals of diversity and public outreach. Wanda Diaz Merced, a visually-impaired astrophysicist from Puerto Rico, is instrumental in advising on and testing the tool. Anton Schertenleib is the initial developer, as part of his graduate student thesis effort. We seek to further develop this tool with greater capabilities for rendering these data, improve its functional interface and allow for a wider variety of file input formats. Completion of this tool will open up the SPDF space physics data collection to a new community of researchers and students now excluded from space physics research. Development and evaluation will be guided by a user group of space scientists (sighted and visually-impaired) and experts in adaptive technologies from the National Federation of the Blind (NFB).","2006-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9H77M2D8","journalArticle","1997","Fouad, Hesham; Hahn, James K.; Ballas, James A.","Perceptuallly based scheduling algorithms for real-time synthesis of complex sonic environments","","","","","http://hdl.handle.net/1853/50751","In this paper, we present a technique for managing overload conditions that occur when computational resources are not sufficient to evaluate all the active sound sources in a Virtual Environment. A real-time scheduling strategy is introduced which degrades less important sound sources so that resource constraints are met. Finally, scheduling algorithms are considered based on their effect on listeners' perception of the resultant sound quality.","1997-11","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P8ZA7RHE","journalArticle","2002","Zotkin, Dmitry N.; Duraiswami, R.; Davis, L. S.","Customizable auditory displays","","","","","http://hdl.handle.net/1853/51348","High-quality virtual audio scene rendering is a must for emerging virtual and augmented reality applications, for perceptual user interfaces,and sonification of data. Personalization of HRTF is necessary in applications where perceptual realism and correct elevation perception is critical. We describe algorithms for creation of virtual auditory spaces by rendering cues that arise from anatomical scattering, environmental scattering, and dynamical effects. We use a novel way of personalizing the head related transfer functions (HRTFs) from a database, based on anatomical measurements.Details of algorithms for HRTF interpolation, room impulse response creation, HRTF selection from a database, and audio scene presentation are presented. Our system runs in real time on an office PC without specialized DSP hardware.","2002-07","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CC5YSWJY","journalArticle","2005","Flowers, John H.","Thirteen years of reflection on auditory graphing: Promises, pitfalls, and potential new directions","","","","","http://hdl.handle.net/1853/50188","While developments in sound production hardware now make the creation of auditory graphs possible for casual users of personal computers, some of the same pitfalls to effective auditory display development that arose in the early 1990's continue to impede effective applications of this promising technology. Most of these pitfalls stem from lack of adequate understanding about key properties of auditory perception and attention and from inappropriate generalizations of existing data visualization practices. At the same time, however, we now know about some strategies that appear to work and offer promise for making sonification a useful and accepted tool for data exploration and decision making. The present paper summarizes several selected examples in each of these categories, along some suggestions for future research directions.","2005-07","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","Thirteen years of reflection on auditory graphing","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YVEQBGFI","journalArticle","2006","Watson, M.","Scalable earcons: Bridging the gap between intermittent and continuous auditory displays","","","","","http://hdl.handle.net/1853/50645","The development of blood pressure earcons for patient monitoring is used to illustrate how data that varies from intermittent to continuous sampling can be integrated into a single auditory display design. A scalable-earcon structure that extends the concept of hierarchies of earcons is proposed for representing blood pressure information. This structure allows the earcon to be used to produce intermittent earcons or a continuous sonification of blood pressure measurements. The results of two blood pressure earcon studies indicate that people can use the earcon to elicit large amounts of information with few major errors","2006-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","Scalable earcons","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3HUYGUU9","journalArticle","2017","Paté, Arthur; Holtzman, Benjamin; Waldhauser, Felix; Repetto, Douglas; Paisley, John","Human and Machine Listening of Seismic Data","","","","","http://hdl.handle.net/1853/58368","Geothermal energy mining consists of injecting cold water into hot rocks in order to create micro-fractures allowing heat to be extracted and converted into electrical energy. This water injection can trigger several rock fracture processes. Seismologists are facing the challenge of identifying and understanding these fracture processes in order to maximize heat extraction and minimize induced seismicity. Our assumption is that each fracture process is characterized by spectro-temporal features and patterns that are not picked up by current signal processing methods used in seismology, but can be identified by the human auditory system and/or by machine learning. We present here a pluridisciplinary methodology aimed at addressing this problem, combining machine learning, auditory display and sound perception.","2017-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZEPHU8YG","journalArticle","2003","Best, Virginia; Schaik, Andre van; Carlile, Simon","Two-point discrimination in auditory displays","","","","","http://hdl.handle.net/1853/50502","In this paper we describe work which characterises the effect of spatial factors on the segregation of concurrent sound sources. The results inform the operational requirements of virtual auditory displays required to render multiple, concurrent sound sources in terms of (i) minimum spacing between sources and (ii) identification of the principal acoustic directional cues exploited by the auditory system for source segregation. Experiments using various broadband sound sources (white noise, click trains, spoken words) indicate that the extent of actual separation required for reliable segregation of concurrent stimuli varies as a function of location. The pattern of location dependence indicates that the auditory system is principally exploiting binaural differences for sound segregation. Monaural spectral cues, while essential for high fidelity spatialisation, seem to play a much less minor role in segregation under these conditions. However, spectral cues are likely to be useful when competing stimuli have distinct temporal structures or are not fully coincident in time.","2003-07","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HZVTCEE2","book","2010","Ritchey, Paul; Muse, Lindsey; Nguyen, Harry; Burks, Ricky; Peres, S. Camille","Effective Design of Auditory Displays: Comparing Various Octave Ranges of Pitch and Panning","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49883","There is a large volume of research on designing effective visual displays, however there is little empirical research informing basic design on auditory displays. With the decreasing size of hardware (e.g., hand-held devices) and the increasing amount of software available, auditory displays are viable option for communicating data in places that have limited space for visual displays and for eye-busy environments. Auditory graphs are auditory displays that map quantified data to acoustic dimensions, such as pitch and panning, to represent changes in data. In the present study, we investigate the octave range of pitch that most effectively represents the data in an auditory graph, as well as the effects of utilizing the acoustic dimension panning to give participants added temporal context. Significant results were found that support the use of panning. A significant interaction between the reported maximum temperatures and octave range, as well as a significant main effect was found for the type of statistic participants were asked to report (minimum value, maximum value, and average value), these results are discussed","2010-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","Effective Design of Auditory Displays","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W8VRLIXB","journalArticle","2004","Guillaume, A.; Drake, C.; Blancard, C.; Chastres, V.; Pellieux, L.","How long does it take to identify everyday sounds","","","","","http://hdl.handle.net/1853/50857","Previous studies of alarm design have concluded that the faster a mental representation of the cause of the alarm is activated, the quicker the adapted reaction. In order to select sounds that are quick to identify, an experiment was carried out using a gated stimulus paradigm with 117 everyday sounds. Almost half of the sounds were identified in less than 150 ms, including both classical alarms and sounds from other categories of everyday sounds. Thus it should possible to identify acoustic properties of each category of alarms within an integrated alarm system in order to improve discrimination among them.","2004-07","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YLCPI6ZF","journalArticle","2006","Cabrera, D.; Ferguson, S.","Considerations arising from the development of auditory alerts for air traffic control consoles","","","","","http://hdl.handle.net/1853/50427","Previously, the authors reported in detail on the development of a set of auditory alerts for the air traffic control consoles now used throughout Australia [1]. The present paper briefly describes these alerts again, but focuses on the issues raised and lessons learnt in the development and evaluation process. It also presents preliminary results from a review, conducted seven months after implementation. The alerts are to be presented for discussion in the poster session.","2006-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QH544RW7","journalArticle","2005","Nickerson, Louise Valgerour; Stockman, Tony","Sonically exposing the desktop space to non-visual users: an experiment in overview information presentation","","","","","http://hdl.handle.net/1853/50182","The vast majority of computer interfaces do not translate well onto non-visual displays (e.g. for blind users, wearable/mobile computing, etc). Screen readers are the most prevalent aural technology to expose graphical user interfaces to the visually impaired. However, they eliminate many of the advantages of direct manipulation and WYSIWYG applications. While the use of sound in interfaces has become more prevalent due to advancement in sound cards for computers, it is still primarily for alerts and status-reporting. The use of sound can be expanded to enhance or replace a GUI by providing a 3D auditory environment. However, users of this environment would need a reliable and effective method of navigation. Little is known of the usability of a system based on sound identification and localisation. In this work, we describe an experiment which will examine users' ability to navigate a 3D auditory environment based on these concepts.","2005-07","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","Sonically exposing the desktop space to non-visual users","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EL46D8GK","journalArticle","2006","McGookin, D.; Brewer, S.","Contextual audio in haptic graph browsing","","","","","http://hdl.handle.net/1853/50587","This paper presents a “think-aloud” study investigating the ability of visually impaired participants to make comparisons between haptic and audio line graphs. Graphs with two data series were presented. One data series was explored with a PHANTOM haptic device, whilst the other was sonified using one of two data - sound mappings. The results show that participants can make comparisons between the two lines. However, there is some cross-modal interference which makes it difficult to extract detailed information about the data series presented in audio.","2006-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5SJTWIPB","journalArticle","2006","Hetzler, S. M.; Tardiff, R. M.","Two tools for integrating sonifications into calculus instruction","","","","","http://hdl.handle.net/1853/50689","Two sonification tools are presented for use in calculus instruction. The first is a web-based tool for teaching students to interpret sonifications. The other is a spreadsheet-based tool that uses sonification to support and reinforce graphical and numeric representations of functions. We also illustrate how the tools could be used, and present data on the usability of the tools and the ability of students to interpret our sonifications.","2006-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PQYV387Z","journalArticle","2006","Brock, D.; Martinson, E.","Exploring the utility of giving robots auditory perspective-taking abilities","","","","","http://hdl.handle.net/1853/50496","This paper reports on work in progress to develop a computational auditory perspective taking system for a robot. Auditory perspective taking is construed as the ability to reason about inferred or posited factors that affect an addressee's perspective as a listener for the purpose of presenting auditory information in an appropriate and effective manner. High-level aspects of this aural interaction skill are discussed, and a prototype adaptive auditory display, implemented in the context of a robotic information kiosk, is described and critiqued. Additionally, a sketch of the design and goals of a user study planned for later this year is given. A demonstration of the prototype system will accompany the presentation of this research in the poster session.","2006-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QHHQ55ZX","journalArticle","2003","Pihkala, Kari; Lokki, Tapio","Extending SMIL with 3D audio","","","","","http://hdl.handle.net/1853/50460","This paper describes how SMIL can be extended to support 3D audio in a similar fashion than AABIFS does it for MPEG-4. The SMIL 2D layout is extended with an extra dimension to support a 3D space. New audio elements are positioned in the 3D space, whilst a listener element defines a listening point. Similarly to AABIFS perceptual modeling approach, an environment element describes environmental parameters for audio elements. These extensions enable interactive 3D audio capabilities in SMIL. In addition, any XML based rendering language could be extended with 3D audio capabilities by using a similar approach.","2003-07","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RNRA25I4","journalArticle","2003","Thornton, Chad; Kolb, Anthony; Gemperle, Francine; Hirsch, Tad","Audiocentric interface design: A building blocks approach","","","","","http://hdl.handle.net/1853/50445","The advent of mobile, wearable, and ubiquitous computing presents opportunities for audiocentric interfaces that use sound as the primary or only means of displaying information to users whose eyes are otherwise engaged. While interface designers have a wealth of technological capabilities at their disposal for capturing, storing, transmitting, and displaying sound, there is a lack of appropriate resources to inform and inspire the design of compelling new audiocentric interfaces. This paper presents work towards developing guidelines for audio interface designers by developing a suite of interface “building blocks:” common interface elements that can be incorporated into the design of complex interfaces. Several audio progress meters and experiments in directing user focus in a spatialized audio environment are discussed.","2003-07","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","Audiocentric interface design","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MSJEM248","journalArticle","2011","Nemeth, Geza; Olaszy, Gabor; Csapo, Tamas Gabor","Spemoticons: Text to Speech Based Emotional Auditory Cues","","","","","http://hdl.handle.net/1853/51915","There are various methods of providing auditory cues in human-computer user interfaces. The two basic traditional methods are the application of real-life sounds (auditory icons) and artificially generated audio signals (earcons). Recently in- between solutions have been developed based on text-to-speech (TTS) technology. Spearcons are speeded-up versions of TTS output of a particular text-template while spindex cues are generated as auditory index items from the first letter of menu list elements. Auditory emoticons are the non-verbal human sound based audible equivalents of emoticons. Auditory emoticons are the non-verbal human sound based audible equivalents of emoticons. However we are not aware of any attempt for generating auditory emotional and intentional state representation (comparable to emoticon characters) based on a TTS solution. We denote these meaningless cues as spemoticons. The interactive development environment of our TTS system is applied as a modification tool for generating spemoticons. The intensity, duration and pitch structure of the generated speech is manipulated. An experimental sound inventory of 44 elements was compiled and tested by 54 adult subjects for the selection of spemoticons.","2011-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","Spemoticons","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5WWCVVD8","journalArticle","2014","Frimalm, Ronja; Fagerlönn, Johan; Lindberg, Stefan; Sirkka, Anna","How Many Auditory Icons, in a Control Room Environment, Can You Learn?","","","","","http://hdl.handle.net/1853/52084","Previous research has shown that auditory icons can be effective warnings. The aim of this study was to determine the number of auditory icons that can be learned, in a control room context. The participants in the study consisted of 14 control room operators and 15 people who were not control room operators. The participants were divided into three groups. Prior to the testing the three groups practiced on 10, 20 and 30 different sounds. Each group was tested using the sounds that they had practiced. The results support the potential for learning and recalling a large number of auditory icons, as many as 30. The results also show that sounds with similar characteristics are easily confused.","2014-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3TFIPM3U","journalArticle","2011","Grond, Florian; Kramer, Oliver; Hermann, Thomas","Interactive Sonification Monitoring in Evolutionary Optimization","","","","","http://hdl.handle.net/1853/51701","This case study introduces interactive sonification to evolu- tionary strategies (ES) for global optimization. We briefly describe the specific strengths of sonification as a tool for monitoring, the emerging trend of interactive sonification, and what it can add to the field of evolutionary computa- tion. Then we line out the background of ES as optimiza- tion heuristics, briefly explain the algorithmic procedure of ES and discuss the need to intervene during optimization runs and the current shortcomings in appropriate user feed- back. This motivates the development of an auditory closed loop setup that brings the expertise of interactive sonifica- tion to the field of monitoring ES algorithms. Further, we present considerations for the sound design and the detailed mapping of parameters from the ES to sound properties. Fi- nally, we discuss the various implemented modes of interac- tion and their significance for the optimization through ES.","2011-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R5QYBXAC","journalArticle","2003","McGookin, David K.; Brewster, Stephen A.","An investigation into the identification of concurrently presented earcons","","","","","http://hdl.handle.net/1853/50437","In this paper we describe an experiment investigating the ability of participants to identify multiple, concurrently playing structured sounds, called earcons. Several different sets of earcons were compared, one “state of the art” set based on the guidelines of Brewster [1], and other sets of earcons modified to take account of auditory scene analysis principles. The effect of the number of concurrently playing earcons on identification was also investigated, with instances of 1, 2, 3 and 4 concurrently playing earcons tested. Overall, performance was low, with less than two earcons being successfully identified in any condition. However it was found that both staggering the onset times of each earcon, as well as presenting each earcon with a unique timbre, had a significantly positive effect on identification.","2003-07","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IU62H7JW","journalArticle","2002","Lokki, T.; Pulkki, V.; Savioja, L.","The effect of early reflections on perceived timbre - analyzed with an auditory model","","","","","http://hdl.handle.net/1853/51388","In this paper the effect of early reflections to perceived timbre is studied. We apply room acoustic modeling to obtain simulated impulse responses which are explored. The timbre of modeled impulse responses is predicted with an analysis method motivated by auditory perception. Such analyses are utilized to determine guidelines for the required set of early reflections to be rendered for a high quality auralization. The results suggest that many orders of reflections have to be searched to guarantee that all possible reflection paths before a certain time stamp are found.","2002-07","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VIFITDVV","journalArticle","2003","Turner, Phil; McGregor, Iain; Turner, Susan; Carroll, Fiona","Evaluating soundscapes as a means of creating a sense of place","","","","","http://hdl.handle.net/1853/50462","We report an empirical study into the creation of, and response to, a soundscape of a computer centre. We contrast the use of presence questionnaires, as means of assessing the sense of being-there, against a more phenomenological approach. We conclude that we have been able to create a strong sense of place but uncovered a number of experimental / procedural issues.","2003-07","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"47QPCTKX","journalArticle","2017","Ferranti, Marcelo Pedruzzi; Spitz, Rejane","Sounding Objects: An Overview Towards Sound Methods and Techniques to Explore Sound Within a Design Process","","","","","http://hdl.handle.net/1853/58355","Sound is a neglected subject of today’s products and services. The new technologies changed the way we interact with the people, objects and the world around us, thus, designers should aim at all senses, contemplating a multi sensorial experience. In this scenario sound becomes an important aspect to be considered during the project phase in a design process. Sound becomes part of the product identity and expression, the way the product talks to us. To foster this scenario designers should be aware of the possibilities and attributes of sound and how to explore them in a creative way. In this short paper we investigated published articles, workshops and publications to collect sound methods and techniques to be used into a design process. As a result, we proposed twenty essential sound methods that could be applied in a design thinking context. This is an ongoing research, part of a thesis experiment, since further methods and refinement could be added in the future.","2017-06","2023-07-13 06:25:54","2023-07-13 06:25:54","2023-07-13","","","","","","","Sounding Objects","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GHN8MRKB","journalArticle","2016","St Pierre, Marc; Droumeva, Milena","Sonifying for Public Engagement: A Context-Based Model for Sonifying Air Pollution Data","","","","","http://hdl.handle.net/1853/56580","In this paper we report on a unique and contextually-sensitive approach to sonification of a subset of climate data: urban air pollution for four Canadian cities. Similarly to other datadriven models for sonification and auditory display, this model details an approach to data parameter mappings, however we specifically consider the context of a public engagement initiative and a reception by an 'everyday' listener, which informs our design. Further, we present an innovative model for FM index-driven sonification that rests on the notion of 'harmonic identities' for each air pollution data parameter sonified, allowing us to sonify more datasets in a perceptually 'economic' way. Finally, we briefly discuss usability and design implications and outline future work.","2016-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","Sonifying for Public Engagement","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6NGA5P8B","journalArticle","2002","Wang, K.; Sundareswran, V.; Tam, C.; Bangayan, P.; Zhorik, P.","Efficient and effective use of low-cost 3D audio systems","","","","","http://hdl.handle.net/1853/51359","Commercial, off-the-shelf (COTS) 3D sound cards offer a readily available low-cost option to consumers, researchers, and developers of 3D auditory displays. However, drawbacks of current 3D sound cards include computing platform support limitations, Application Programming Interface (API) complexity, vendor instabilities, and the lack of individualized Head-Related Transfer Functions (HRTFs). To address these issues, we have developed a client/server system utilizing COTS 3D sound cards and have investigated a method of visualfeedback training for 3D sound localization.","2002-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4HCBIUVB","journalArticle","2006","Nees, Michael A.; Walker, Bruce N.","Relative intensity of auditory context for auditory graph design","","","","","http://hdl.handle.net/1853/50632","A study examined the role of relative intensity levels for auditory context in auditory graph design. Auditory graphs were designed with auditory context equally as loud as sonified data, context 9 dB more intense than data, or context 9 dB less intense than data. For a point estimation task, participants who experienced auditory graphs with more intense context performed significantly better than participants who experienced graphs with data and context equally loud. Mean differences suggest that making the context either more intense or less intense than the data improved performance as compared to the equally loud condition. We suggest that differences in the intensity of context relative to data facilitate perceptual separation of the auditory streams and thus promote ease of use with auditory graphs. Sound examples are included, and implications for auditory graph design are discussed.","2006-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SEDI7PK9","journalArticle","2003","van de Doel, Kees","Soundview: Sensing color images by kinesthetic audio","","","","","http://hdl.handle.net/1853/50489","An experimental system called SoundView has been developed, which allows the exploration of a color image through touch and hearing. The image is mapped onto a virtual surface with a fine-grained color dependent roughness texture. The user explores the image by moving a pointer device over the image. The pointer acts like a virtual gramophone needle, and the sound produced depends on the motion as well as on the color of the area explored. The roughness texture is obtained by constructing a mapping of three dimensional color space onto a three dimensional sound space. The mapping tries to achieve maximal alignment of the color and sound spaces by preserving the perceptual metrical and topological structure of color space, as well as by incorporating common associations between sound and color.","2003-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","Soundview","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NTP43AK6","journalArticle","2013","Tsiros, Augoustinos","A Multidimensional Sketching interface for Corpus Based Concatenative Synthesis","","","","","http://hdl.handle.net/1853/51680","This paper presents Morpheme, a multidimensional interface that allows real-time control of concatenative synthesis through the act of sketching on a digital canvas. Morpheme extracts textural, spatial and volumetric features from a sketch developed by a practitioner and associates these to audio features for retrieval of audio units and to synthesis parameter for signal processing. Two mappings between audio and visual features were developed based on findings from previous studies that examined audio and visual feature correlation. One of the mappings is achromatic as the features extracted from the sketch are mainly volumetric and spatial, while the second mapping is chromatic as the features extracted from the sketch are based on color attributes. A number of simple algorithms are discussed that were developed to address three problems: (i) to estimate high level visual feature, (ii) set constrains in the audio corpus to improve the selection algorithm and the exploration of the corpus, and (iii) automatically adjust the weights to improve the efficacy of the selection algorithm in assessing similarity, by optimizing the algorithm in a corpus depended manner.","2013-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2SCVV6FH","journalArticle","2011","Barrass, Stephen","Phsyical Sonification Dataforms","","","","","http://hdl.handle.net/1853/51741","Physical Sonification Dataforms are physical objects constructed from digital datasets to produce sounds. This paper reports on a series of three experiments that establish and verify the theory of Physical Sonification. This experiments use the HRTF data provided as a sonification challenge at ICAD 2011. The dataset is a spatial array of spectral filters measured from the left and right ears of a dummy head. The proof of concept is a coin-like metal disc cnstructed from the data that can be struck or scraped to produce a sound. The second iteration is shaped like a bell to produce a more sustained and pitched sound. The third experiment compares a Control with Test Bells constructed from left and right HRTFs. The timbre of the Control is categorically different from the Left and Right Bells which are strangely dissonant. Spectrograms of the Left and Right Bells show a superposition of doubled harmonics. These results suggest that the sound of the Bell could characterize a HRTF dataset in a way that could be useful for classification and recognition of HRTF datasets from different people.","2011-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FRAMFCEZ","journalArticle","2008","Tajadura-Jimenez, Ana; Valjamae, Aleksander; Kitagawa, Norimichi; Vastfjall, Daniel","Affective Multimodal Displays: Acoustic Spectra Modulates Perception of Auditory-Tactile Signals","","","","","http://hdl.handle.net/1853/49869","Emotional events may interrupt ongoing cognitive processes and automatically grab attention, modulating the subsequent perceptual processes. Hence, emotional eliciting stimuli might effectively be used in warning applications, where a fast and accurate response from users is required. In addition, conveying information through an optimum multisensory combination can lead to a further enhancement of user responses. In the present study we investigated the emotional response to sounds differing in their acoustic spectra, and their influence on speeded detection of auditory-somatosensory stimuli. Higher sound frequencies resulted in an increase in emotional arousal. We suggest that emotional processes might be responsible for the different auditory-somatosensory integration patterns observed for low and high frequency sounds. The presented results might have important implications for the design of auditory and multisensory warning interfaces.","2008-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","Affective Multimodal Displays","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7B7I4PD5","journalArticle","2005","Baier, Gerold; Hermann, Thomas; Lara, Oscar Manuel; Miller, Markus","Using sonification to detect weak cross-correlations in coupled excitable systems","","","","","http://hdl.handle.net/1853/50195","We study cross-correlations in irregularly spiking systems. A single system displays spiking sequences that resemble a stochastic (Poisson) process. Linear coupling between two systems leaves the inter-spike interval distribution qualitatively unchanged but induces cross-correlations between the units. For strong coupling this leads to synchronization as expected but for weak coupling, both a good statistic and sonification reveal the presence of “motifs”, preferred short firing sequences which are due to the deterministic spiking mechanism. We argue that the use of sonification for time series analysis is superior in the case where intrinsic non-stationarity of an experiment cannot be ruled out.","2005-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MDW6TTWL","journalArticle","2007","MacVeigh, Ryan; Jacobson, Daniel","Increasing the Dimensionality of a Geographic Information System (GIS) Using Auditory Display","","","","","http://hdl.handle.net/1853/50005","This paper describes a way to incorporate sound into a raster based classified image. Methods for determining the sound location, amplitude, type and how to create a layer to store the information are described. Hurdles are discussed and suggestions of how to overcome them are presented. As humans we rely on our senses to help us navigate the world. Sight, sound, touch, taste, and smell; they all help us perceive our environment. Although we sometimes take vision for granted, all our other senses play as important of a role in our daily lives. Even with all these senses at our disposal, the conventional GIS very uncommonly do much more than convey their information visually. We demonstrate an auditory display with a sample implementation using a classified raster image, commonly used in a GIS analysis. This was achieved using a spatial sonification algorithm initially created in a Java environment. The ultimate aim of this work is to develop an interactive mapping technology that fully incorporates auditory display, over a variety of platforms and applications. Such a tool would have the potential be of great benefit for displaying multivariate information in complex information displays.","2007-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G9M7THF6","journalArticle","2002","Saitou, T.; Unoki, M.; Akagi, M.","Extraction of F0 dynamic characteristics and development of F0 control model in singing voice","","","","","http://hdl.handle.net/1853/51357","Fundamental frequency (F0) control models, which can cope with F0 dynamic characteristics related to singing-voice perception, are required to construct natural singing-voice synthesis systems. This paper discusses the importance of F0 dynamic characteristics in singing voices and demonstrates how much it influence on singing voice perception through psychoacoustic experiments. This paper, then, proposes an F0 control model that can generate F0 fluctuations in singing voices, and a singing-voice synthesis method. The results showthat F0 contour including fluctuations: Overshoot, Vibrato, Preparation, and Fine-fluctuation, affects singing voice perception, and the proposed synthesis method can generate natural singing voices by controlling these F0 fluctuations.","2002-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FYEQFPBJ","journalArticle","2008","Morland, Cameron; Mountain, David","Design of a Sonar System for Visually Impaired Humans","","","","","http://hdl.handle.net/1853/49898","Constraints and features useful for an effective and easy to learn human sonar device are described. These include matching spatial cues generated by the device to those in the world. Techniques used by natural echolocators, including specifications of signal type, emitter, and receiver are briefly reviewed, as is techniques of converting ultrasonic signals to the audible range and techniques for externalizing sounds. Finally, a prototype sonar system designed while considering these ideas is described.","2008-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7TFAITR2","book","2009","Brazil, Eoin; Fernstrom, Mikael; Bowers, John","Exploring concurrent auditory icon recognition","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51296","This poster explores and deepens existing studies into the identi- fication of concurrently presented auditory icons. The motivation for this work was to gain a better understanding of auditory icons where several are played together simultaneously. A set of de- scriptors for everyday sounds were collected from participants and classified into action and object categories. The exploration con- sidered the hypothesis that when auditory icons did not have the same object or action descriptors that the identifications of the au- ditory icons would improve. This was studied in conditions where three, six, and nine sounds were simultaneously presented. These conditions had two distinct sub-categories, the first category used a prior classification of sounds to ensure no sound pair in the con- dition had the same action or object properties. The second sub- category used random selection of the sounds meaning that similar sound could exist within the particular condition. A onset-to-onset gap of 300 ms between sounds being presented was used. The re- sults supports earlier findings and showed that distinguishing be- tween object and action properties of auditory icons did improve their identification accuracy. It was found that prior classification allows listeners to achieve close to a 10% identification improve- ment in accuracy.","2009-05","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X8C9IZLA","journalArticle","2011","Constantinescu, Angela; Schultz, Tanja","Redundancy Versus Complexity in Auditory Displays for Object Localization - A Pilot Study","","","","","http://hdl.handle.net/1853/51742","In user interfaces, redundancy is often an indication of good design. Several studies [1,2,3], showed that when visual, haptic or other display types were combined with an auditory display, the result was an enhanced user experience and increase in performance. Research about redundancy within the auditory display alone, however, seems to be inconclusive. A pilot study was set up to test whether redundancy in auditory mappings supports object localization or rather renders the task inefficient by adding unnecessary complexity. The study used three sound parameters: pan, pitch and tempo in a combination of three sonification schemes: pan alone, pan and pitch and all three, in order to convey to the user information about the position of an object in a 2 dimensional space. Preliminary results showed that the third sonification scheme (with most redundancy) yielded the best user performance, and was also rated best by five out of seven users.","2011-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IINYJ489","journalArticle","2016","Peres, S. Camille; Verona, Daniel","A Task-Analysis-Based Evaluation of Sonification Designs for Two sEMG Tasks","","","","","http://hdl.handle.net/1853/56584","This paper presents a brief description of surface electromyography (sEMG), what it can be used for, as well as some of the problems associated with visual displays of sEMG data. Sonifications of sEMG data have shown potential for certain applications in data monitoring and movement training, however there are still challenges related to the design of these sonifications that need to be addressed. Our previous research has shown that different sonification designs resulted in better listener performance for different sEMG evaluation tasks (e.g. identifying muscle activation time vs. muscle exertion level). Based on this finding, we speculated that sonifications may benefit from being designed to be task-specific, and that integrating a task analysis into the sonification design process may help sonification designers identify intuitive and meaningful sonification designs. This paper presents a brief introduction to what a task analysis is, provides an example of how a task analysis can be used to inform sonification design, and outlines future research into a task-analysis-based approach to sonification design.","2016-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PGP9E4VL","journalArticle","1998","Crease, Murray; Brewster, Stephen","Making progress with sounds - the design & evaluation of an audio progress bar","","","","","http://hdl.handle.net/1853/50725","This paper describes an experiment to investigate the effectiveness of adding sound to progress bars. Progress bars have usability problems because they present temporal information graphically and if the user wants to keep abreast of this information, he/she must constantly visually scan the progress bar. The addition of sounds to a progress bar allows users to monitor the state of the progress bar without using their visual focus. Nonspeech sounds called earcons were used to indicate the current state of the task as well as the completion of the download. Results showed a significant reduction in the time taken to perform the task in the audio condition. The participants were aware of the state of the progress bar without having to remove the visual focus from their foreground task.","1998-11","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VQQG2JXC","journalArticle","2007","McCormick, Carina M.; Flowers, John H.","Perceiving the Relationship Between Discrete and Continuous Data: A Comparison of Sonified Data Display Formats","","","","","http://hdl.handle.net/1853/50023","This study compared the effectiveness of two auditory display designs for conveying the relationship between discrete and continuous data. Participants judged the relationship between simulated data representing “sea temperature,” (a continuous variable) and “storm occurrence” (a categorical variable) by rating the strength of covariation between these variables and qualitatively describing the relationship for one of two types of auditory displays. One format integrated the representation of storms and sea temperature into a single pitch-varying “stream” by signaling storms occurrence by momentary amplitude and timbre changes. The other format presented the storm occurrence information as atonal percussive events separate from the pitchvarying stream that represented temperature. While both formats led to statistically equivalent proportions of verbal descriptions of the temperature-storm relationships present in the simulated data samples, the integrated display produced higher correlations between ratings of the strength of the temperature-storm relationship and the actual storm-temperature covariation present within each data sample.","2007-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","Perceiving the Relationship Between Discrete and Continuous Data","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7BEFTKHK","book","2009","Walker, Bruce N.; Davison, Benjamin K.","Measuring the use of sound in everyday software","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51402","Members of the ICAD community might contend that auditory interfaces and even just well-designed sound in computer interfaces could be used more often than is currently the case. However, it is not entirely clear where, when, and how sound is actually being employed in everyday software. We discuss the development of a long-term research project aimed at identifying and categorizing sound use in software. Our mixed- methods approach explores software artifacts from three perspectives: detailed program behavior, source code word count of audio terms, and audio infrastructure. These complementary approaches could provide a deeper understanding of sound use today and, we hope, lead to predicting, guiding, and improving the future trajectory of its use.","2009-05","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XFEL7DXK","journalArticle","2004","Mauney, Bradley S.; Walker, Bruce N.","Creating functional and livable soundscapes for peripheral monitoring of dynamic data","","","","","http://hdl.handle.net/1853/50847","Sonifications must be studied in order to match listener expectancies about data representation in the form of sound. In this study, a system was designed and implemented for dynamically rendering sonifications of simulated real-time data from the stock market. The system read and parsed the stock data then operated unit generators and mixers through a predefined sound mapping to create a `soundscape' of complementary ecological sounds. The sound mapping consisted of a threshold-based model in which a percentage change in price value was mapped to an ecological sound to be played whenever that threshold or gradient had been reached. The system also provided a generic mechanism for fading and transitioning between gradients. The prototype system was presented to stock trader test subjects in their work-listening environment for evaluation as a stand-alone system and in comparison to their preferred tools.","2004-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TLJQ84MR","journalArticle","2011","Zoon, Hanna; Bakker, Saskia; Eggen, Berry","Chronoroom Clock: Peripheral Time Awareness Through Sound Localization","","","","","http://hdl.handle.net/1853/51578","Our auditory perception skills enable us to selectively place one auditory channel in the center of our attention while monitoring others in the periphery. Furthermore, we are able to accurately localize sound sources. In this paper, we present Chronoroom Clock, an auditory display that unobtrusively provides information about the current time based on the direction the audio is coming from, enabling people to monitor it in the periphery of their attention. Evaluation of a prototype version of this design indicates that the used audio may indeed shift to the periphery of the user’s attention.","2011-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","Chronoroom Clock","","","","","","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7ZCRSW8D","book","2009","Sciabica, J. F.; Bezat, M. C.; Roussarie, V.; Kronland-Martinet, R.; Ystad, S.","Towards the timbre modeling of interior car sound","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51418","Quality investigations and design of interior car sounds constitute an important challenge for the car industry. Such sounds are complex and time-varying, inducing considerable timbre variations depending on the driving conditions. An interior car sound is indeed a mixture between several sound sources, with two main contributions, i.e. the engine noise, the aerodynamic and tire-road noise. That’s why masking phenomena between these two components should be considered when studying perceptive attributes of interior car sounds. Additive synthesis is used to simulate the harmonic engine noise. Nevertheless, this synthesis is controlled by a large number of parameters and no relation between these parameters and their perceptive relevance has been clearly identified. By combining sensory analysis and signal analysis associated with an auditory model, we can find a relation between a reduced number of signal parameters and perceptive attributes. This study develops a method to simplify the timbre description of interior card sounds and presents the first results of auditory model application on such sounds.","2009-05","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6GVWF3ZX","book","2010","Fouad, Hesham; Wersényi, György","Listening Tests and Evaluation of Simulated Sound Fields Using VibeStudio Designer","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49893","This paper presents the results of a user-based evaluation of localization accuracy, distance perception as well as room size perception for headphone and loudspeaker based auditory displays. A total of 50 participants listened to four auditory scenes created with VRSonic’s VibeStation application. Each scene was rendered using two methods: loudspeaker panning over a 5.0 loudspeaker array and headphone-based spatial sound reproduction using Head Related Transfer Functions (HRTFs). The four scenes were designed to each test a specific aspect of spatial hearing. Scene 1 tested for localization of fixed sources. Scene 2 was used to examine room size perception. Scene 3 was used to test distance perception and Scene 4 tested for localization of moving sources and listener. The participants responded to questions related to the location of each sound they heard as well as transitions between two room sizes and free field. The results of the current study show that the system setup including hardware and software performs as expected and offers a user-friendly way for virtual audio simulation.","2010-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TVEJJP8H","journalArticle","2005","Shajahan, Peer; Irani, Pourang","Manipulating synthetic voice parameters for navigation in hierarchical structures","","","","","http://hdl.handle.net/1853/50171","Auditory interfaces commonly use synthetic speech for conveying information. In many instances the information being conveyed is hierarchically structured, such as menus. In this paper, we describe the results of one experiment that was designed to investigate the use of multiple synthetic voices for representing hierarchical information. A hierarchy of 27 nodes was created (in which 2 of the nodes were not shown to the participants during the training session). A between subjects design (N-16) was conducted to evaluate the effect of multiple synthetic voices on recall rates. Two different forms of training were provided. Participant's tasks involved identifying the position of nodes in the hierarchy by listening to the synthetic voice. The results suggest that 84.38% of the participants recalled the position of the nodes accurately. The results also indicate that multiple synthetic voices can be used to facilitate navigation hierarchies. Overall, this study suggests that it is possible to use synthetic voices to represent hierarchies.","2005-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UPAE3UAN","journalArticle","2004","Hermann, T.; Baier, G.","The sonification of rhythms in human electroencephalogram","","","","","http://hdl.handle.net/1853/50919","We use sonification of temporal information extracted from scalp EEG to characterize the dynamic properties of rhythms in certain frequency bands. Sonification proves particularly useful in the simultaneous monitoring of several EEG channels. Our results suggest sonification as an important tool in the analysis of multivariate data with subtle correlation differences.","2004-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DKDMTD3U","journalArticle","2001","Evreinov, Grigori","Spotty: Imaging sonification based on spot-mapping and tonal volume","","","","","http://hdl.handle.net/1853/50643","A basic question at image sonification is the image segmentation. A cognitive model of visual processing in a greater degree could define possible ways of sound mapping. For instance, the scanpath theory suggests that a top-down internal cognitive model, of what we see, drives the sequences of rapid eye movements and fixations or glances that so efficiently travel over scene or picture of interest. The scanpath theory may be applied at sonification of visual image. But it is necessary to solve, what is more important in each stage of the image recognition process: the scan trajectory or the optical characteristics of its extreme positions? That is to say, what is dominant - scanpath or the spot of glance? I hope a solution of these questions will allow to develop new tools for VR applications as well as to continue designing of visualization system for blind people on a basis of blind-eye tracking.","2001-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","Spotty","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EUJGXI39","journalArticle","2001","Lokki, Tapio; Jarvelainen, Hanna","Subjective evaluation of auralization of physics-based room acoustics modeling","","","","","http://hdl.handle.net/1853/50647","This paper describes the results of subjective evaluation of auralization by listening tests. The task was to compare real-head recorded and auralized sound samples. The evaluation process as well as the creation of soundtracks are briefly reviewed. The listening test procedure is presented along with the analyzed results of a case study. They show that with a simple room geometry (a lecture room) reliable and natural sounding auralization is possible with physics-based room acoustic modeling. However, there are still some modeling problems which are discussed as well as the guidelines for future work in evaluation.","2001-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NRXEDWPC","journalArticle","2005","Holmes, Jason","Interacting with an information space using sound: Accuracy and patterns","","","","","http://hdl.handle.net/1853/50162","Human auditory perception is suited to receiving and interpreting information from the environment but this knowledge has not been used extensively in designing computer-based information exploration tools. It is not known how accurate humans can be in navigating an auditory display. Furthermore, it is not known if listeners will conform to known pattern search techniques in a search task using sound alone. An auditory display was created using PD (Pure Data), a graphical programming language used primarily to manipulate digital sound. The visual interface for the auditory display was a blank window. The auditory interface was based on ground level ozone concentration data. When the cursor is moved around in this window, the sound generated changes based on the underlying data value at any given point. An experiment was conducted to determine how accurately subjects were able to locate the highest concentration level using the auditory display. The four attributes of sound tested were frequencysine waveform, frequency-sawtooth waveform, loudness and tempo. Results indicate that sonic display of data yields less resolution than visual. It is also shown that people will generally utilize recognizable search patterns when exploring the information space.","2005-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","Interacting with an information space using sound","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"96KLYERR","journalArticle","2002","Smith, Daniel R.; Walker, Bruce N.","Tick-marks, axes, and labels: The effects of adding context to auditory graphs","","","","","http://hdl.handle.net/1853/51392","As the use of sonification expands, researchers and designers continue to employ techniques for adding context (such as tick marks, axes, or labels) whose benefit remains unquantified. This study examined the effect of several such techniques on the perceivability of an auditory graph. In Block 1, participants listened to a simple auditory graph, which had no added context (such as tick marks, axes, and labels), and answered trend analysis and point estimation questions about the information presented by the graph. In Block 2, participants repeated the process but the graph was augmented by 1 of 6 types of added context. The data revealed differences in perceivability between conditions for both trend analysis and the point estimation task, and an explainable ordering of error levels based on the amount and type of information provided by a particular contextual setting.","2002-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","Tick-marks, axes, and labels","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XCVJLMTS","journalArticle","2005","Ronkainen, Sami; Hakkila, Jonna; Pasanen, Leena","Effect of aesthetics on audio-enhanced graphical buttons","","","","","http://hdl.handle.net/1853/50155","In this paper, two different auditory feedback schemes related to graphical buttons are compared to each other and to a visual- only condition. The results show that aesthetically pleasing auditory design is clearly preferred among the users, and can lead to performance benefits over not only a design with no auditory enhancements, but also a design with aesthetically less pleasing auditory enhancements.","2005-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6I2CDW4Z","journalArticle","2016","Brock, Derek; Wasylyshyn, Christina; McClimens, Brian","Word Spotting in a Multichannel Virtual Auditory Display at Normal and Accelerated Rates of Speech","","","","","http://hdl.handle.net/1853/56578","The demands of concurrent radio communications in Navy shipboard command centers contribute to the problem of operator information overload and impede personnel optimization goals for new platforms. Motivations for serializing this task and human performance research with virtual, multichannel, rate-accelerated speech in support of this idea are briefly reviewed, and the results of a recent listening study in which participants carried out a Navyrelevant word-spotting task in this context are reported.","2016-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T7TTSCK5","journalArticle","2004","Lindsay, Jeff; Walker, Bruce N.","Auditory navigation performance is affected by waypoint capture radius","","","","","http://hdl.handle.net/1853/50829","Non-speech audio navigation systems can be very effective mobility aids for persons with either temporary or permanent vision loss. Sound design has certainly been shown to be important in such devices [e.g. 1]. In this study we consider the added factor of capture radius. The capture radius of an auditory beacon is defined as the range at which the system considers a user to have reached the waypoint where the beacon is located. 108 participants successfully navigated paths through a virtual world using only nonspeech beacon cues. Performance differed across the capture radius conditions. Further, there was a speed-accuracy tradeoff, which complicates the design decision process. Implications of these results for the design of auditory navigation aids are discussed, as are other ongoing and future studies.","2004-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F6UM8RQL","journalArticle","2007","Nickerson, Louise Valgerour; Stockman, Tony; Thiebaut, Jean-Baptiste","Sonifying the London Underground Real-Time-Disruption Map","","","","","http://hdl.handle.net/1853/50036","In mobile computing, there is a need for interfaces that better suit the context of use. Auditory interfaces have the potential to address the limitations of small screens and support eyes-free tasks. In order to fill this gap, we must develop more fluid and usable auditory interfaces. A key aspect of this is understanding the process of designing overviews. In this work, we describe a conceptual strategy for providing an overview of disruptions in the London Underground: The approach adopted is based on what information is perceived as most crucial to the user.","2007-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L76S36TL","journalArticle","2002","Menzies, D.","Scene management for modelled audio objects in interactive worlds","","","","","http://hdl.handle.net/1853/51381","The acoustic behaviour of natural objects and their interactions can be accurately recreated in interactive world simulations. A realistic simulation may contain many such objects, any combination of which may be interacting at a given time. High and low level structures are presented for managing this complexity efficiently and with flexibility, based upon an existing PC system.","2002-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PIAAUPSZ","journalArticle","2002","Hiipakka, J.; Lorho, G.; Holm, J.","Auditory navigation cues for a small 2D grid: A case study of the memory game","","","","","http://hdl.handle.net/1853/51336","This paper presents an investigation on auditory navigation in a two-dimensional grid. A sonification of the memory game is proposed and utilized for evaluating user's ability to interact with objects in a small 2-D grid based on sound only. After a short review of the basic game, we start from the idea of replacing each picture under a card by a sound. Subsequently, we present the necessary sonification steps to obtain an eyes-free version of the game. We also propose some slight modifications to the rules of the game to make this audio version more attractive for gaming. This game is used as a case study for auditory user interface testing focusing on auditory navigation in the grid.","2002-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","Auditory navigation cues for a small 2D grid","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YQIBGZKY","journalArticle","2002","Tohyama, M.","Sinusoidal and envelope modulation modeling of signals - a signal theoretic approach to acoustic events rendering","","","","","http://hdl.handle.net/1853/51387","This study investigates a signal theoretic approach to rendering acoustic events. Acoustic events modeling language (AEML) is an essential part of acoustic events rendering using an acoustic-data stream based on structured audio representation. This article mainly describes sinusoidal and envelope-modulation modeling for intelligible speech modification and reverberation signals rendering. The sinusoidal modeling is useful for constructing intelligible speech using only a few dominant components, and narrow-band envelopes such as 1/4- octave-band-speech envelopes are the key to representation of speech intelligibility. Envelope modulation modeling using the dominant sinusoidal carriers enables modification of the talker's pitch and speech-rate without sacrificing intelligibility. The narrowband envelope can be estimated by clustered linespectrum modeling (CLSM) based on the LSE-criterion in the frequency domain. Sinusoidal modeling with a decaying envelope is also a key technology for reverberation sound rendering based on the modal statistics of a reverberation field.","2002-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6ELPKN4R","journalArticle","2002","Nesbitt, K. V.; Barrass, S.","Evaluation of a multimodal sonification and visualisation of depth of market stock data","","","","","http://hdl.handle.net/1853/51355","Day traders make minute by minute decisions from depth of market stock data read from a table. A visualisation has been designed to help traders make better decisions from extra context and history in the display. An expert review of the visualisation identified the need for more dynamic information and higher resolution in critical areas. We designed a sonification to complement the visualisation with this information in a multimodal display. We evaluated the visualisation, sonification and multimodal displays with 15 non-experts. The subjects predicted price movements significantly better than chance, at 61.3% correct with the visualisation, 70% with the sonification and 70% with the multimodal display. The prediction of downward movements was significantly better than upward with a best of 83% for downward with the sonification, compared with 65.5% for upward with the multimodal display. There was no effect due to the order of experience with the different displays. The results suggest that the multimodal display is redundant for downward movements and complementary for upward. The subjects commented that the sonification provides recent trends while the visualisation provides context.","2002-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FQ4UXRRB","journalArticle","2007","Frauenberger, C.; Stockman, T.; Bourguet, M. L.","Pattern Design in the Context Space A Methodological Framework for Auditory Display Design","","","","","http://hdl.handle.net/1853/50020","Common practice in the design of auditory display is hardly ever based on any structured design methodology. This leaves audio being widely underused or used inappropriately and inefficiently. We analyse the current status of research in this context and develop requirements for a methodological framework for auditory display design. Based on these requirements, we have created a framework of methods to capture, transfer and apply design knowledge based on design patterns - paco ad. We present the context space as the organising principle to conceptualise the design space facilitating the matching of design knowledge with solutions and the workflow. Finally, we elaborate on how we intend to evaluate the framework and how it can be supported by tools.","2007-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FSE6MD9G","journalArticle","2004","Li, Z.; Duraiswami, R.; Gumerov, N. A.","Capture and recreation of higher order 3D sound fields via reciprocity","","","","","http://hdl.handle.net/1853/50834","We propose a unified and simple approach for capturing and recreating 3D sound fields by exploring the reciprocity principle that is satisfied between the two processes. Our approach makes the system easy to build, and practical. Using this approach, we can capture the 3D sound field by a spherical microphone array and recreate it using a spherical loudspeaker array, and ensure that the recreated sound field matches the recorded field up to a high order of spherical harmonics. A design example and simulation results are presented. For some regular or semi-regular microphone layouts, we design an efficient parallel implementation of the multi-directional spherical beamformer by using the rotational symmetries of the beampattern and of the spherical microphone array. This can be implemented in either software or hardware. A simple design example is presented to demonstrate the idea. It can be easily adapted for other regular or semi-regular layouts of microphones.","2004-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YXE8NPLB","journalArticle","2017","Sardesai, Ruta R.; Gable, Thomas M.; Walker, Bruce N.","Introducing Multimodal Sliding Index: Qualitative Feedback, Perceived Workload, and Driving Performance with an Auditory Enhanced Menu Navigation Method","","","","","http://hdl.handle.net/1853/58361","Using auditory menus on a mobile device has been studied in depth with standard flicking, as well as wheeling and tapping interactions. Here, we introduce and evaluate a new type of interaction with auditory menus, intended to speed up movement through a list. This multimodal “sliding index” was compared to use of the standard flicking interaction on a phone, while the user was also engaged in a driving task. The sliding index was found to require less mental workload than flicking. What’s more, the way participants used the sliding index technique modulated their preferences, including their reactions to the presence of audio cues. Follow-on work should study how sliding index use evolves with practice.","2017-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","Introducing Multimodal Sliding Index","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3FHRUHNG","journalArticle","2016","Martens, William L.; Poronnik, Philip; Saunders, Darren","Hypothesis-Driven Sonification of Proteomic Data Distributions Indicating Neurodegredation in Amyotrophic Lateral Sclerosis","","","","","http://hdl.handle.net/1853/56571","Three alternative sonifications of proteomic data distributions were compared as a means to indicate the neuropathology associated with Amyotrophic Lateral Sclerosis (ALS) via auditory display (through exploration of the differentiation of induced pluripotent stem cell derived neurons). Pure visual displays of proteomic data often result in ""visual overload"" such that detailed or subtle data important to describe ALS neurodegradation may be glossed over, and so three competing approaches to the sonification of proteomic data were designed to capitalize upon human auditory capacities that complement the visual capacities engaged by more conventional graphic representations. The auditory displays resulting from hypothesis-driven design of three alternative sonifications were evaluated by naïve listeners, who were instructed to listen for differences between the sonifications produce from proteomic data associated with three different types of cells. One of the sonifications was based upon the hypothesis that auditory sensitivity to regularities and irregularities in spatio-temporal patterns in the data could be heard through spatial distribution of sonification components. The design of a second sonification was based upon the hypothesis that variation in timbral components might create a distinguishable sound for each of three types of cells. A third sonification was based upon the hypothesis that redundant variation in both spatial and timbral components would be even more powerful as a means for identifying spatio-temporal patterns in the dynamic, multidimensional data generated in current proteomic studies of ALS.","2016-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"26DJRUU2","journalArticle","1997","Evans, Michael J.; Tew, Anthony I.; Angus, James A. S.","Spatial audio teleconferencing - which way is better","","","","","http://hdl.handle.net/1853/50752","This paper examines the two basic philosophies of spatial audio reproduction, with reference to their application to teleconferencing services. Sound Field Simulation, as exemplified by multiple loudspeakers techniques such as Ambisonics, encodes information about a remote or virtual sound field, and allows the reproduction of that field across a listening space. Alternatively, an application might employ Perceptual Synthesis, in which measured or simulated sound localisation cues (e.g. Head-Related Transfer Function (HRTF) data) are imposed on the signals reproduced over headphones or a suitably set-up pair of loudspeakers. The relative merits and drawbacks of each approach are discussed in terms of cost, implementation logistics, flexibility, specification and, critically, perceived performance.","1997-11","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2JF8TA4Y","journalArticle","2007","Brazil, Eoin; Fernstrom, Mikael","Investigating Ambient Auditory Information Systems","","","","","http://hdl.handle.net/1853/50002","This paper discusses an exploration using a concurrent auditory displays for awareness and lightweight interactions. The design of this type of system and comparisons to existing awareness tools are discussed. The auditory display system in this exploration was designed to explore, using concurrent auditory icons, the issue of group awareness. The sounds used in this auditory display where selected based on their identification derived from individual's personal constructs using the Repertory Grid Technique. The system was designed to create a `soundscape' of concurrent ecological sounds mapped to the individual's availability and to the group activities, respectively. In this paper we present an auditory display using auditory icons to create an interactive soundscape that support opportunistic interactions and awareness. Presence and activity are conveyed by changes in the soundscape. Our goal in this work is to explore the potential of this type of system for supporting awareness and lightweight interactions.","2007-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VZ5G8JNE","book","2009","Dicke, Christina; Aaltonen, Viljakaisa; Billinghurst, Mark","Occurrence of simulator sickness in spatial sound spaces and 3d auditory displays","","978-87-7606-033-6","","","http://hdl.handle.net/1853/51406","This paper describes an investigation into the effect of movement patterns in a spatial sound space on the perceived amount of simulator sickness, the pleasantness of the experience, and the perceived workload. Our user study indicates that predictable left to right movements lead to a perceived unpleasantness that is significantly higher than the unpleasantness experienced for unpredictable or no movements at all. Approx. 48 percent of all participants showed mild to moderate symptoms of simulator sickness, with a trend towards stronger symptoms for the left to right movements. Our data suggest that neither of the movement patterns has an effect on the perceived cognitive load for simple tasks.","2009-05","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YEW2TRKJ","journalArticle","2002","van den Doel, K.; Pai, D. K.; Adam, T.; Kortchmar, L.; Pichora-Fuller, K.","Measurements of the perceptual quality of contact sound models","","","","","http://hdl.handle.net/1853/51367","We describe and test methods to construct modal resonance models for solid objects, suitable for the real-time synthesis of soundeffects in simulation and animation. Measurements on typical everyday objects such as a metal vase or a bowl result in several hundred modes, of which only a small fraction is perceptually relevant. We have proposed several heuristics, inspired by psycho acoustical data, to select the modes by perceptual relevance and to order them so that one can increase the quality by adding more modes, at the price of additional computational complexity (progressive synthesis). The resulting synthetic sounds are tested on human subjects in order to determine the quality of the sounds relative to the target sound which they are designed to approximate. The resulting data is used to verify and tune the mode selection methodologies, and to increase our understanding of what determines the subjective quality of a synthetic sound effect.","2002-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NS6GNYBJ","journalArticle","2018","Chabot, Samuel; Lee, Wendy; Elder, Rebecca; Braasch, Jonas","Using a multimodal immersive environment to investigate perceptions in augmented virtual reality systems","","","","","http://hdl.handle.net/1853/60077","The Collaborative-Research Augmented Immersive Virtual Environment Laboratory at Rensselaer is a state-of-the-art space that offers users the capabilities of multimodality and immersion. Realistic and abstract sets of data can be explored in a variety of ways, even in large group settings. This paper discusses the motivations of the immersive experience and the advantages over smaller scale and single-modality expressions of data. One experiment focuss on the influence of immersion on perceptions of architectural renderings. Its findings suggest disparities between participants’ judgment when viewing either two-dimensional printouts or the immersive CRAIVE-Lab screen. The advantages of multimodality are discussed in an experiment concerning abstract data exploration. Various auditory cues for aiding in visual data extraction were tested for their affects on participants’ speed and accuracy of information extraction. Finally, artificially generated auralizations are paired with recreations of realistic spaces to analyze the influences of immersive visuals on the perceptions of sound fields. One utilized method for creating these sound fields is a geometric ray-tracing model, which calculates the auditory streams of each individual loudspeaker in the lab to create a cohesive sound field representation of the visual space.","2018-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6689S2YF","journalArticle","2008","Borss, Christian; Silzle, Andreas; Martin, Rainer","Internet-Based Interactive Auditory Virtual Environment Generators","","","","","http://hdl.handle.net/1853/49936","In this paper we investigate general design considerations and practical implementation aspects for Internet-based interactive auditory virtual environments (I-AVE) for the post-PC era. An implementation of such an AVE generator as a web service allows for platform independent “AVE services” for mobile devices almost “anywhere on any device” using a standard web browser. We propose a client-server architecture which computes the acoustic signals on a high-performance server and provides low-latency audio streaming from the server to the client.","2008-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WTYC4GZN","journalArticle","2001","Houben, Mark MJ; Kohlrausch, Armin; Hermes, Dik","Auditory cues determining the perception of the size and speed of rolling balls","","","","","http://hdl.handle.net/1853/50512","This study investigates the auditory perception of the size and the speed of rolling balls. Prior experiments showed that subjects can discriminate differences in size and speed of wooden rolling balls on the basis of recorded sounds. Recorded sounds were manipulated by merging the temporal characteristics of one sound with the spectral characteristics of another. Perception experiments showed that when subjects had to choose the larger ball from two sounds, they had a preference for the spectral content of a large ball. If subjects had to choose the faster out of two sounds, they preferred the spectral content of a small ball, and, to a lesser degree, the spectral content of a fast rolling ball. The temporal cues in the sounds were of minor importance for the range of stimuli used in this experiment, possibly because sounds with much amplitude modulation and bouncing were excluded from the experiments.","2001-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IT94NWF2","journalArticle","2002","Savioja, L.; Lokki, T.; Huopaniemi, J.","Auralization applying the parametric room acoustic modeling technique - the div auralization system","","","","","http://hdl.handle.net/1853/51340","The primary goal of this paper is to give a general view on room acoustic modeling and auralization, and especially to describe the current status of the DIVA auralization system. We have been building the system for several years, and it has evolved a lot during that time. It is a room acoustic modeling and auralization system suitable for both real-time and non-realtime acoustic rendering, and it is designed for research purposes. It applies the parametric room impulse response rendering technique described in the article. In this paper we review the architecture and design principles of the system. A description of recent advances is given and results of perceptual evaluations are presented.","2002-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2PBMFYW2","journalArticle","2021","Cantrell, Stanley J.; Walker, Bruce N.; Moseng, Øystein","Highcharts Sonification Studio: an online, open-source, extensible, and accessible data sonification tool","","","","","http://hdl.handle.net/1853/66348","The Highcharts Sonification Studio is the culmination of a multi-year collaboration between Highsoft — the creators of Highcharts — and the Georgia Tech Sonification Lab to develop an extensible, accessible, online spreadsheet and multimodal graphing platform for the auditory display, assistive technology, and STEM education communities. The Highcharts Sonification Studio leverages the advances in auditory display and sonification research, as well as over 20 years of experience gained through research and development of the original Sonification Sandbox. We discuss the iterative design and evaluation process of the Highcharts Sonification Studio to ensure usability and accessibility, highlight opportunities for growth of the tool, and its use for research, art, and education within the ICAD community and beyond.","2021-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","Highcharts Sonification Studio","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P6ZXWPY3","journalArticle","2002","Rivenez, M.; Drake, C.; Guillaume, A.; Sebastien, D.","Listening to environmental scenes in real time","","","","","http://hdl.handle.net/1853/51363","Our perception of auditory environmental scenes depends on both the context in which the sources occur and the way in which we listen to them (if we are focused on some sources or not). This study investigates processing differences for a single source depending on its context of occurrence. A tone-detection paradigm adapted to an everyday listening context compares the ability to detect tones in a focused stream within three 10- second non-focused streams: an environmental scene, a white noise and a silence. We predict fluctuations in detection times as a function of the number of streams in the context and of the changes occurring in the non-focused stream.","2002-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UFG3X5LU","journalArticle","2006","Honda, A.; Shibata, H.; Gyoba, J.; Iwaya, Y.; Suzuki, Y.","Transfer effects of playing a virtual three-dimensional auditory game: Influences on the performance in a communication task and a collision avoidance task","","","","","http://hdl.handle.net/1853/50690","We investigated the transfer effects of playing an auditory game with a virtual auditory display (VAD) on various auditory skills in daily life situations. To measure those effects, all blindfolded participants performed a communication task and a collision avoidance task on the first day. They were asked to perform the same tasks two weeks later. Participants of the training condition were asked to play the VAD game for seven days (30 min/day) for two weeks, whereas the control group did not play the game within that period. Results of playing the VAD game revealed that the number of face-contacts in the communication task increased significantly. In contrast, no difference was detectable in the subjectively rated levels of tension during the communication task between the two conditions. Furthermore, results showed that playing the VAD game altered the participants' avoidance behaviors. Therefore, we can conclude that the effects of playing the VAD game transfer to communication behaviors in social interaction and to avoidance behaviors from approaching objects in a real environment.","2006-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","Transfer effects of playing a virtual three-dimensional auditory game","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CV6QG5DI","journalArticle","2007","Rath, Matthias","Auditory Velocity Information in a Balancing Task","","","","","http://hdl.handle.net/1853/49980","Within the general context of auditory perception of ecological information a previously rather less studied aspect is the one of the convection of continuous dynamic physical attributes. The study focuses on velocity information in a scenario of interactive control, the one of balancing a virtual ball on a tiltable track. In a target reaching experiment control movements and performance times are measured and recorded under different conditions of auditory feedback in addition to a wide–screen graphical display. The presence and relevance of auditory perception of velocity information can be inferred from analysis of experimental results and conclusions can be drawn concerning the design of auditory feedback of ecological or rather abstract nature.","2007-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F7TQHVHI","journalArticle","2017","Chabot, Samuel; Braasch, Jonas","An Immersive Virtual Environment for Congruent Audio-Visual Spatialized Data Sonifications","","","","","http://hdl.handle.net/1853/58381","The use of spatialization techniques in data sonification provides system designers with an additional tool for conveying information to users. Oftentimes, spatialized data sets are meant to be experienced by a single or few users at a time. Projects at Rensselaer's Collaborative-Research Augmented Immersive Virtual Environment Laboratory allow even large groups of collaborators to work within a shared virtual environment system. The lab provides an equal emphasis on the visual and audio system, with a nearly 360° panoramic display and 128-loudspeaker array housed behind the acoustically-transparent screen. The space allows for dynamic switching between immersions in recreations of physical scenes and presentations of abstract or symbolic data. Content creation for the space is not a complex process-the entire display is essentially a single desktop and straight-forward tools such as the Virtual Microphone Control allow for dynamic real-time spatialization. With the ability to target individual channels in the array, audio-visual congruency is achieved. The loudspeaker array creates a high-spatial density soundfield within which users are able to freely explore due to the virtual elimination of a so-called “sweet-spot.”","2017-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KKWNWEVZ","journalArticle","2002","de Cheveigne, A.","Scalable metadata for search, sonification and display","","","","","http://hdl.handle.net/1853/51377","This paper argues for the need - and usefulness - of scalable content-based metadata. Scalability is here defined as the conjunction of two properties: arbitrary resolution, and convertibility between resolutions. The need follows directly from the projected exponential trend of media data size, that equally affects metadata. In addition to addressing this need, scalable metadata are useful because they are hierarchical in nature, and incorporate statistics effective for search (in automatic media handling systems) or sonification and display (in interactive media handling systems). Scalable metadata are built upon a small number of statistical operations that offer the right scalability properties: extrema (min, max), mean, variance, covariance, histogram, etc. These statistics are used alone or in combination to produce summary descriptions with a resolution tailored to the needs and constraints of the application. They can also be understood as parametrizations of the distributions of full-resolution descriptor values that they summarize. As such, they support inference mechanisms upon to build search and matching algorithms. For interactive applications, scalable content-based descriptors can be used to produce visual displays that support zooming and navigation within multimedia collections of arbitrary size, under the assistance of visual and auditory feedback.","2002-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FVWL5LPR","journalArticle","2012","Gonzalez, Christian; Lewis, Bridget A.; Baldwin, Carryl L.","Revisiting pulse rate, frequency and perceived urgency: have relationships changed and why?","","","2168-5126","","http://hdl.handle.net/1853/44420","As the presence of technology within vehicles increases, alerts and warnings need to not only be salient, but also correctly mapped to events of varying time sensitivity. The goal of this research was to reevaluate key auditory parameters within a driving context that have been shown to exhibit specific relationships with perceived urgency. We examined two commonly manipulated alert parameters: frequency and pulse rate. In accordance with the existing literature, we used previously validated psychophysical techniques to describe how changes in stimuli are reflected in changes of perceived urgency ratings across four different experimental conditions. Our findings indicate that pulse rate may be a more reliable and robust parameter for conveying levels of urgency than frequency. Our results also suggest that the relationship between pulse rate and perceived urgency may have weakened since the psychophysical work of the early 1990’s. Auditory alert designers wishing to convey varying levels of urgency within the variable environment of the vehicle cockpit may be more successful utilizing pulse rate than frequency.","2012-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","Revisiting pulse rate, frequency and perceived urgency","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WSALCT32","book","2010","Boyd, Jeffrey; Godbout, Andrew","Corrective Sonic Feedback for Speed Skating: A Case Study","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49865","We present a system that provides real-time audio feedback to athletes performing repetitive, periodic movements. The system synchronizes the temporal signal from a sensor placed on the athletes body with a model signal. The audio feedback tells the athlete how well they are synchronized with the model, and whether or not they are deviating from the model at critical points in the periodic motion. Because the feedback is continuous and in real-time, the athlete is able to correct their motion in response to the sounds they hear. The system uses simple, inexpensive instrumentation (the entire system costs less than $500) and avoids the uses of expensive and inconvenient motion capture systems. We demonstrate the effectiveness of the system with a case study featuring a speed skater that had developed a significant anomaly in his technique.","2010-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","Corrective Sonic Feedback for Speed Skating","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PRQB7VW9","journalArticle","2002","Cabrera, D.; Gilfillan, D.","Auditory distance perception of speech in the presence of noise","","","","","http://hdl.handle.net/1853/51331","This study examines the effects of background noise, actual source distance, and room reverberation on the perceived distance of a single phrase of recorded speech reproduced at a naturalistic sound pressure level. A simple rectangular room was used for stimulus generation, wherein binaural recordings were made with source-receiver distances between 0.9 m and 5.1 m, reverberation times between 0.7 s and 5.7 s, and effective continuous background noise levels between 30 dBA and 66 dBA. Subjects, wearing headphones, judged the distance of the speech source in these recordings. The three independent variables of physical distance, reverberation time and background noise level each had a positive effect on perceived distance. Previous studies, using noise targets, have found the presence of background noise to reduce perceived distance. One possible explanation for this discrepancy is that auditory distance cues for speech are weighted differently to those of arbitrary signals, such as noise.","2002-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DLUB47XG","journalArticle","2014","Schmitz, Gerd; Kroeger, Daniela; Effenberg, Alfred O.","A mobile sonification system for stroke rehabilitation","","","","","http://hdl.handle.net/1853/52045","Growing evidence suggests that sonification supports movement perception as well as motor functions. It is hypothesized that real-time sonification supports movement control in patients with sensorimotor dysfunctions efficiently by intermodal substitution of sensory loss. The present article describes a sonification system for the upper extremities that might be used in neuromotor rehabilitation after stroke. A keyfeature of the system is mobility: Arm movements are captured by intertial sensors that transmit their data wirelessly to a portable computer. Hand position is computed in an egocentric reference frame and mapped onto four acoustic parameters. A pilot feasibility study with acute stroke patients resulted in significant effects and is encouraging with respect to ambulatory use.","2014-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NTP783LR","journalArticle","2014","Wenzel, Elizabeth M.; Godfroy-Cooper, Martine; Miller, Joel D.","Spatial Auditory Displays: Substitution and Complemenarity to Visual Displays","","","","","http://hdl.handle.net/1853/52061","The primary goal of this research was to compare the performance in localization of stationary targets during a simulated extra-vehicular exploration of a planetary surface. Three different types of displays were tested for aiding orientation and localization: a 3D spatial auditory display, a 2D North-up visual map, and the combination of the two in a bimodal display. Localization performance was compared under four different environmental conditions combining high and low levels of visibility and ambiguity. In a separate experiment using a similar protocol, the impact of visual workload on performance was also investigated contrasting high (Dual-Task paradigm) and low workload (Single Orientation task). A synergistic presentation of the visual and auditory information (bimodal display) lead to a significant improvement in performance (higher percent correct orientation and localization, shorter decision and localization times) compared to either unimodal condition, in particular when the visual environmental conditions were degraded. Preliminary data using the dual-task paradigm suggest that the performance with displays utilizing auditory cues was less affected by the extra demands of additional visual workload than a visual-only display.","2014-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","Spatial Auditory Displays","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CHXL96EI","journalArticle","2013","Neumann, Alexander; Hermann, Thomas","Interactive sonification of collaborative ar-based planning tasks for enhancing joint attention","","","","","http://hdl.handle.net/1853/51641","This paper introduces a novel sonification-based interaction support for cooperating users in an Augmented Reality setting. When using head-mounted AR displays, the field of view is limited which causes users to miss important activities such as object interactions or deictic references of their interaction partner to (re-)establish joint attention. We introduce an interactive sonification which makes object manipulations of both interaction partners mutually transparent by sounds that convey information about the kind of activity, and can optionally even identify the object itself. In this paper we focus on the sonification method, interaction design and sound design, and we furthermore render the sonification both from sensor data (e.g. object tracking) and manual annotations. As a spin-off of our approach we propose this method further for the enhancement of interaction observation, data analysis, and multimodal annotation in interactional linguistics and conversation analysis.","2013-07","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"36P8NSKS","journalArticle","2019","Podwinska, Zuzanna; Fazenda, Bruno M.; Davies, William J.","Testing spatial aspects of auditory salience","","","","","http://hdl.handle.net/1853/61515","Auditory salience describes the extent to which sounds attract the listener's attention. So far, there have not been any published studies testing if the location of sound relative to the listener influences its salience. In fact, not many experiments in general test auditory attention in a fully spatialised setting, with sounds in front and behind the listener. We modified two experimental methods from the literature so that they can be used to test spatial salience - one based on oddball detection and artificially created sounds, the other based on self-reported attention tracking in a more ecologically valid scenario. Each of these methods has its advantages and each presents different challenges. However, they both seem to indicate that high frequency sounds arriving from the back are slightly less salient. We believe this result could likely be explained by loudness differences.","2019-06","2023-07-13 06:25:55","2023-07-13 06:25:55","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2RC8K6WR","journalArticle","2001","Wenzel, Elizabeth M.","Effect of increasing system latency on localization of virtual sounds with short and long duration","","","","","http://hdl.handle.net/1853/50618","In a virtual acoustic environment, the total system latency (TSL) refers to the time elapsed from the transduction of an event or action, such as movement of the head, until the consequences of that action cause the equivalent change in the virtual sound source. This paper reports on the impact of increasing TSL on localization accuracy when head motion is enabled. A previous study [1] investigated long duration stimuli of 8 s to provide subjects with substantial opportunity for exploratory head movements. Those data indicated that localization was generally accurate, even with a latency as great as 500 ms. In contrast, Sandvad [2] has observed deleterious effects on localization with latencies as small as 96 ms when using stimuli of shorter duration ( 1.5 to 2.5 s). In an effort to investigate stimuli more comparable to Sandvad [2], the present study repeated the experimental conditions of [1] but with a stimulus duration of 3 s. Five subjects estimated the location of 12 virtual sound sources (individualized head-related transfer functions) with latencies of 33.8, 100.4, 250.4 or 500.3 ms in an absolute judgement paradigm. Subjects also rated the perceived latency on each trial. Comparison of the data for the 3 and 8 ms duration stimuli indicates that localization accuracy as a function of latency is moderately affected by the overall duration of the sound. For example, for the 8-s stimuli, frontback confusions were minimal and increased only slightly with increasing latency. For the 3-s stimuli, the increase in front-back confusions with latency was more pronounced, particularly for the longest latency tested (500 ms). Mean latency ratings indicated that latency had to be at least 250 ms to be readily perceived. The fact that accuracy was generally comparable for the shortest and longest latencies suggests that listeners are able to ignore latency during active localization, even though delays of this magnitude produce an obvious spatial “slewing” of the source such that it is no longer stabilized in space. There is some suggestion that listeners are less able to compensate for latency with the short duration stimuli, although the effect is not as pronounced as in [2].","2001-07","2023-07-13 06:25:56","2023-07-13 06:25:56","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"36U6IN99","journalArticle","2007","Hermann, Thomas; Bunte, Kerstin; Ritter, Helge","Relevance-Based Interactive Optimization of Sonification","","","","","http://hdl.handle.net/1853/50026","This paper presents a novel approach for the interactive optimization of sonification parameters. In a closed loop, the system automatically generates modified versions of an initial (or previously selected) sonification via gradient ascend or evolutionary algorithms. The human listener directs the optimization process by providing relevance feedback about the perceptual quality of these propositions. In summary, the scheme allows users to bring in their perceptual capabilities without burdening them with computational tasks. It also allows for continuous update of exploration goals in the course of an exploration task. Finally, Interactive Optimization is a promising novel paradigm for solving the mapping problems and for a user-centred design of auditory display. The paper gives a full account on the technique, and demonstrates the optimization at hand of synthetic and real-world data sets.","2007-06","2023-07-13 06:25:56","2023-07-13 06:25:56","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"69QTJFFT","book","2010","Raman, Parameswaran Raman; Davison, Benjamin K.; Jeon, Myounghoon; Walker, Bruce N.","Reducing Repetitive Development Tasks in Auditory Menu Displays with the Auditory Menu Library","","978-0-9670904-3-6","","","http://hdl.handle.net/1853/49911","This paper explores the process of auditory menus research. Several parts are tedious tasks which must be repeated for minor changes to the experiment. Fortunately many of these parts can be automated with software. We present the Auditory Menu Library (AML), a tool for simplifying experiment construction. The AML provides a cross-platform, configuration-based turnkey solution to studies involving auditory menus.","2010-06","2023-07-13 06:25:56","2023-07-13 06:25:56","2023-07-13","","","","","","","","","","","","Georgia Institute of Technology","","en_US","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PJ7K9BER","journalArticle","2014","Nawfal, Ismael; Atkins, Joshua","Binaural Reproduction Over Loudspeakers Using a Modified Target Response","","","","","http://hdl.handle.net/1853/52101","Crosstalk cancellation (XTC) is a technique that can be used to play binaural content, typically meant for headphone playback, over two or more loudspeakers. Though effective at creating a binaural spatial sound field at the listening position, many XTC algorithms introduce spectral coloration, suffer from spatial robustness issues and create filters that are unrealizable in practice. Past approaches to dealing with this issue rely heavily on regularization. In this work we propose a new topology for loudspeaker binaural rendering (LBR) that performs better than conventional techniques without the need for regularization commonly associated with crosstalk cancellation based binaural renderers (XTC-BR). We then explore the use of a proposed LBR in the context of multiple output channels. A method is investigated to further optimize the filter design process by selecting an appropriate modeling delay and filter length using methods practiced in XTC filter design.","2014-06","2023-07-13 06:25:56","2023-07-13 06:25:56","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZBV23FYY","journalArticle","2006","Brewster, S.; Kildal, J.","Providing a size-independent overview of non-visual tables","","","","","http://hdl.handle.net/1853/50607","Obtaining an overview is an important first step in the analysis of data sets, which cannot be easily done nonvisually with current accessibility tools. We present TableVis, a multimodal interface to obtain overview information from numerical data tables non-visually, with the use of an interactive sonification technique controlled from a tangible physical device (a tablet). An experimental study with TableVis is reported, in which it is found that performance is highly insensitive to the size of the data set being explored, for a broad range of data set sizes.","2006-06","2023-07-13 06:25:56","2023-07-13 06:25:56","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BCVL9TES","journalArticle","2019","Schwarz, Sebastian; Ziemer, Tim","A psychoacoustic sound design for pulse oximetry","","","","","http://hdl.handle.net/1853/61504","Oxygen saturation monitoring of neonates is a demanding task, as oxygen saturation (SpO2) has to be maintained in a particular range. However, auditory displays of conventional pulse oximeters are not suitable for informing a clinician about deviations from a target range. A psychoacoustic sonification for neonatal oxygen saturation monitoring is presented. It consists of a continuous Shepard tone at its core. In a laboratory study it was tested if participants (N = 6) could differentiate between seven ranges of oxygen saturation using the proposed sonification. On average participants could identify in 84% of all cases the correct SpO2 range. Moreover, detection rates differed significantly between the seven ranges and as a function of the magnitude of SpO2 change between two consecutive values. Possible explanations for these findings are discussed and implications for further improvements of the presented sonification are proposed.","2019-06","2023-07-13 06:25:56","2023-07-13 06:25:56","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H223CIMJ","journalArticle","2008","Brock, Derek; McClimens, Brian; Trafton, J. Gregory; McCurry, Malcolm; Perzanowski, Dennis","Evaluating Listeners' Attention to and Comprehension of Spatialized Concurrent and Serial Talkers at Normal and a Synthetically Faster Rate of Speech","","","","","http://hdl.handle.net/1853/49902","Concurrent voice communications workload has been identified as a pivotal issue for desired reductions in the size of Navy watchstanding teams on future platforms. Without effective augmenting technologies, real increases in current per-person communications monitoring requirements will lead to unacceptable reductions in operator performance. A proposal to buffer voice communications and monitor them serially at synthetically increased rates of speech has recently been put forward as an alternative to concurrent monitoring. However, any decrements in listening performance associated with temporal scaling must be weighed against the costs of current practices. A comparative study reported here examines measures of auditory attention and comprehension in different multitalker contexts using long blocks of continuous speech. In four conditions, listeners respectively heard two and four concurrent talkers and four serial talkers (i.e., one at a time) speaking normally and 75% faster. With only a few exceptions, all pairwise differences between measures were significant. Performance in the faster serial condition was lower than in the normal serial condition, but was found to be greater than in either of the concurrent conditions by a substantial margin.","2008-06","2023-07-13 06:25:56","2023-07-13 06:25:56","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: International Community for Auditory Display</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"685JICN3","journalArticle","2019","Biggs, Brandon; Coughlan, James M.; Coppin, Peter","Design and evaluation of an audio game-inspired auditory map interface","","","","","http://hdl.handle.net/1853/61525","This study evaluated a web-based auditory map prototype built utilizing conventions found in audio games and presents findings from a set of tasks participants performed with the prototype. The prototype allowed participants to use their own computer and screen reader, contrary to most studies, which restrict use to a single platform and a self-voicing feature (providing a voice that talks by default). There were three major findings from the tasks: the interface was extremely easy to learn and navigate, participants all had unique navigational styles and preferred using their own screen reader, and participants needed user interface features that made it easier to understand and answer questions about spatial properties and relationships. Participants gave an average task load score of 39 from the NASA Task Load Index and gave a confidence level of 46/100 for actually using the prototype to physically navigate.","2019-06","2023-07-13 06:25:56","2023-07-13 06:25:56","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8N6CHCZ6","journalArticle","2007","Rouben, Anna; Terveen, Loren","Speech and Non-Speech Audio: Navigational Information and Cognitive Load","","","","","http://hdl.handle.net/1853/50039","Cell phones and other mobile devices let people receive information anywhere, anytime. Navigation information – directions and distance to a destination, interesting nearby locations, etc. – is especially promising. However, there are challenges to delivering information on a cell phone, particularly with a GUI. GUIs aren't ideal when a person's visual attention is elsewhere, e.g., scanning for landmarks, assessing safety, etc. And they don't work at all for blind people, who particularly need navigation assistance. Our work responds to this challenge. We investigate the use of two non-visual techniques for delivering navigation information, speech and sonification [[3], . We conducted an experiment to compare user performance with and preference for the two techniques, in both single task (navigate to a target) and dual task (navigate to a target and respond to an auditory stimulus) conditions. Users performed better with and preferred sonification in both conditions. We discuss the implications of these results for the design of navigation aids.","2007-06","2023-07-13 06:25:56","2023-07-13 06:25:56","2023-07-13","","","","","","","Speech and Non-Speech Audio","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DAH66CAC","journalArticle","2012","Brock, Derek; Peres, S. Camille; McClimens, Brian","Evaluating listeners’ attention to and comprehension of serialy interleaved, rate-accelerated speech","","","2168-5126","","http://hdl.handle.net/1853/44411","In Navy command operations, individual watchstanders must often concurrently monitor two or more channels of spoken communications at a time, which in turn can undermine information awareness and decision performance. Recent basic work on this operational challenge has shown that a virtual auditory display solution, in which competing messages are presented one at a time at faster rates of speech, can achieve large and significant improvements on diminished measures of listening performance observed in concurrent monitoring at normal speaking rates with equivalent materials. In the third of a series of experiments developed to address performance questions the parameters of this framework raise for listeners, dependent measures of attention and comprehension were compared in a two factor design that manipulated how serial turns among four talkers were organized and their rate of speech. Although both factors had significant impacts on performance, the resulting measures remained substantially higher than performance in concurrent talker conditions.","2012-06","2023-07-13 06:25:56","2023-07-13 06:25:56","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RUGBAC74","journalArticle","2001","Hermann, T.; Hansen, M. H.; Ritter, H.","Sonification of markov chain monte carlo simulations","","","","","http://hdl.handle.net/1853/50646","Markov chain Monte Carlo (McMC) simulation is a popular computational tool for making inferences from complex, high-dimensional probability densities. Given a particular target density p, the idea behind this technique is to simulate a Markov chain that has p as its stationary distribution. To be successful, the chain needs to be run long enough so that the distribution of the current draw is close to the target density. Unfortunately, very few diagnostic tools exist to monitor characteristics of the chain. In this paper, we present a new approach to render sonifications of McMC simulations. The proposed method consists of several auditory streams which provide information about the behavior of the Markov chain. In particular, we focus on uncovering modes in the target density function. In addition to monitoring, we have found our sonification to be an effective means for understanding the structure of high-dimensional densities. We have also applied our method to the exploratory analysis of highdimensional data sets. In this case, we take as our target p a nonparametric density estimate obtained from the data. In this paper, we present a detailed description of our sonification design and illustrate its performance on test cases consisting of both synthetic and real-world data sets. Sound examples are also given.","2001-07","2023-07-13 06:25:56","2023-07-13 06:25:56","2023-07-13","","","","","","","","","","","","","","en_US","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PHJWJNTG","journalArticle","2019","Băcilă, Bogdan Ioan; Lee, Hyunkook","Subjective elicitation of listener-perspective-dependent spatial attributes in a rerverberant room, using the repertory grid technique","","","","","http://hdl.handle.net/1853/61535","Spatial impression is a widely researched topic in concert hall acoustics and spatial audio display. In order to provide the listener with plausible spatial impression in virtual and augmented reality applications, especially in the 6 Degrees of Freedom (6DOF) context, it is first important to understand how humans perceive various acoustical cues from different listening perspectives in a real space. This paper presents a fundamental subjective study conducted on the perception of spatial impression for multiple listener positions and orientations. An in-situ elicitation test was carried out using the repertory grid technique in a reverberant concert hall. Cluster analysis revealed a number of conventional spatial attributes such as source width, environmental width and envelopment. However, reverb directionality and echo perception were also found to be salient spatial properties associated with changes in the listener's position and head orientation.","2019-06","2023-07-13 06:25:56","2023-07-13 06:25:56","2023-07-13","","","","","","","","","","","","","","en","","","","","","","","<p>Publisher: Georgia Institute of Technology</p>","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BNILJI26","journalArticle","2008","Kessous, Loic; Jacquemin, Christian; Filatriau, Jean-Julien","Real-Time Sonification of Physiological Data in an Artistic Performance Context","","","","","http://hdl.handle.net/1853/49943","This paper presents an approach for real-time sonification of physiological measurements and its extension to artistic creation. Three sensors where used to measure heart pulse, breathing, and thoracic volume expansion. A different sound process based on sound synthesis and digital audio effects was used for each sensor. We designed the system in order to produce three different streams clearly separables and to allow listeners to perceive as clearly as possible the physiological phenomena. The data were measured in the context of an artistic performance. Because the first purpose of this sonification is to participate to an artistic project we tried to produce an interesting sound results from an aesthetic point of view, but at the same time we tried to keep an auditory display highly correlated to the data flows.","2008-06","2023-07-13 06:43:21","2023-07-13 06:43:21","2023-07-13 06:43:21","","","","","","","","","","","","","","en_US","","","","","repository.gatech.edu","","Publisher: International Community for Auditory Display","","/Users/minsik/Zotero/storage/4YBUR2RX/Kessous et al. - 2008 - Real-Time Sonification of Physiological Data in an.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""