"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"2LEH3WNS","journalArticle","2020","Liew, Kongmeng; Lindborg, PerMagnus","A Sonification of Cross-Cultural Differences in Happiness-Related Tweets","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20715","","2020","2023-07-12 05:56:50","2023-07-12 05:56:50","","25–33","","1/2","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MTMX7Z4C","journalArticle","2018","Axon, Louise; Goldsmith, Michael; Creese, Sadie","Sonification Mappings: Estimating Effectiveness, Polarities and Scaling in an Online Experiment","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19875","Sonification is a technique to present data arrays as sound, thereby taking advantage of the human ability to hear patterns that might otherwise not be apparent. Mappings from parameters of data to parameters of sound form the basis of parameter-mapping sonification. The choice of mappings and their design can influence both the utility of the sonification system and the ability of users to interpret the sounds. In this article the authors demonstrate the use of a time-efficient methodology with an experimental online platform for assessing mappings. Experiments explored the effectiveness of various mappings, and the discussions explore the implications of each approach. Based on the responses of 100 participants in an online Magnitude Estimation experiment, the effectiveness of 16 data-sound mappings was explored. Results showed that mappings involving certain sound parameters were generally effective, while those using other sound parameters varied in their effectiveness. In some cases the ability to interpret mappings and the polarities with which they were perceived varied among individuals using them. The mappings that used the tempo parameter were generally perceived effectively, while those using other sound parameters varied. Exploratory observations suggest that differences among participants might be related to different levels of musical experience.","2018","2023-07-12 05:56:55","2023-07-19 03:37:30","","1016–1032","","12","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H97AVHSQ","journalArticle","2012","Bujacz, Michal; Skulimowski, Piotr; Strumillo, Pawel","Naviton—A Prototype Mobility Aid for Auditory Presentation of Three-Dimensional Scenes to the Visually Impaired","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16374","To augment the task of navigation and orientation of blind individuals, a new travel aid uses 3D scene sonification to present information about the environment using nonverbal audio. The model is composed of two classes of objects: obstacles and planes. The algorithm uses scene image segmentation, personalized spatial audio, musical tones, and sonar-like sound patterns. Individually measured head-related transfer functions were used to provide users with the illusion of sounds originating from the locations of sonified scene elements. Using a segmented and parametric description overcomes the sensory mismatch between visual and auditory perception. In a pilot study using both blind and sighted volunteers, subjects were able to utilize the prototype for spatial orientation and obstacle avoidance after a few minutes of training, attaining 90% accuracy in estimating the direction and depth of obstacles.","2012","2023-07-12 05:57:01","2023-07-19 03:45:35","","696–708","","9","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"52FJ88PV","journalArticle","2018","Roma, Gerard; Xambó, Anna; Freeman, Jason","User-independent Accelerometer Gesture Recognition for Participatory Mobile Music","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19582","With the widespread use of smartphones that have multiple sensors and sound processing capabilities, there is a great potential for increased audience participation in music performances. This paper proposes a framework for participatory mobile music based on mapping arbitrary accelerometer gestures to sound synthesizers. The authors describe Handwaving, a system based on neural networks for real-time gesture recognition and sonification on mobile browsers. Based on a multiuser dataset, results show that training with data from multiple users improves classification accuracy, supporting the use of the proposed algorithm for user-independent gesture recognition. This illustrates the relevance of user-independent training for multiuser settings, especially in participatory music. The system is implemented using web standards, which makes it simple and quick to deploy software on audience devices in live performance settings.","2018","2023-07-12 05:57:04","2023-07-19 04:41:39","","430–438","","6","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L4RYTSW8","journalArticle","2018","Stolfi, Ariane; Sokolovskis, Janis; Goródscy, Fábio; Iazzetta, Fernando; Barthet, Mathieu","Audio Semantics: Online Chat Communication in Open Band Participatory Music Performances","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19868","","2018","2023-07-12 05:57:06","2023-07-12 05:57:06","","910–921","","11","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7747JFUY","journalArticle","2012","Vogt, Katharina; Höldrich, Robert","Translating Sonifications","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16636","Over the last few decades there have been numerous explorations of sonification, a concept that may be loosely defined as communicating nonaudio information as sound. As with any developing field, there comes a time when a formal structure is needed to provide a framework for understanding the collection of ad hoc experiments. To make the mapping between data and sound more explicit and less prone to misunderstandings, a sonification operator has been suggested. The authors created “notation modules” to formulate this mapping for various fields. An example of a specific sonification operator in the field of physics is given. Nine subjects from research were used in a study to evaluate the experience of this formalism.","2012","2023-07-12 05:57:09","2023-07-19 10:53:28","","926–935","","11","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B5J42F5S","journalArticle","2018","Yang, Jiajun; Hermann, Thomas","Interactive Mode Explorer Sonification Enhances Exploratory Cluster Analysis","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19712","Exploratory Data Analysis (EDA) refers to the process of detecting patterns of data when explicit knowledge of such patterns within the data is missing. Because EDA predominantly employs data visualization, it remains challenging to visualize high-dimensional data. To minimize the challenge, some information can be shifted into the auditory channel using humans’ highly developed listening skills. This paper introduces Mode Explorer, a new sonification model that enables continuous interactive exploration of datasets with regards to their clustering. The method was shown to be effective in supporting users in the more accurate assessment of cluster mass and number of clusters. While the Mode Explorer sonification aimed to support cluster analysis, the ongoing research has the goal of establishing a more general toolbox of sonification models, tailored to uncover different structural aspects of high-dimensional data. The principle of extending the data display to the auditory domain is applied by augmenting interactions with 2D scatter plots of high-dimensional data with information about the probability density function.","2018","2023-07-12 05:57:12","2023-07-19 10:58:24","","703–711","","9","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DLTF7BGB","journalArticle","2012","Grosshauser, T.; Bläsing, B.; Spieth, C.; Hermann, T.","Wearable Sensor-Based Real-Time Sonification of Motion and Foot Pressure in Dance Teaching and Training","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16369","As with tasks involved with motion and gesture, teaching dance can take advantage of auditory displays that map specific dance steps into their acoustic counterparts. Wearable sensors based on acoustic “fingerprints” accompany the dance movements in real-time. This kind of audio feedback has a positive influence on motor movement and perception. For example, joint angles, weight distribution, and energy of jumps are easily recognized through sound. With practice, a student can hear if a complex movement was correctly executed. The auditory system can hear complex patterns of rapid motion, especially aspects of a dance that are not easily seen.","2012","2023-07-12 05:57:18","2023-07-19 04:03:00","","580–589","","7/8","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KIEWRMIL","journalArticle","2014","Sanz, Pablo Revuelta; Mezcua, Belén Ruiz; Pena, José M. Sánchez; Walker, Bruce N.","Scenes and Images into Sounds: A Taxonomy of Image Sonification Methods for Mobility Applications","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17130","","2014","2023-07-12 05:57:20","2023-07-12 05:57:20","","161–171","","3","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MX8LVIVI","journalArticle","2020","Hansen, Brian; Burchett, Joseph N.; Forbes, Angus G.","Quasar Spectroscopy Sound: Analyzing Intergalactic and Circumgalactic Media via Data Sonification","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21000","In this paper, we present sonification approaches to support research in astrophysics, using sound to enhance the exploration of the intergalactic medium and the circumgalactic medium. Astrophysicists often analyze matter in these media using a technique called absorption line spectroscopy. Our sonification approaches convey key spectral features identified via this technique, including the presence and width of spectral absorption lines within a region of the Universe, the relationship of a particular redshift location with respect to the absorption peak of a spectral absorption line, and the density of gas at various regions of the Universe. In addition, we introduce Quasar Spectroscopy Sound, a novel software tool that enables researchers to perform these sonification techniques on cosmological data sets, potentially accelerating the discovery and classification of matter in the intergalactic medium and circumgalactic medium.","2020","2023-07-12 05:57:23","2023-07-19 04:03:28","","865–875","","11","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4WIDVZ83","journalArticle","2012","Barrass, Stephen","Digital Fabrication of Acoustic Sonifications","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16375","","2012","2023-07-12 05:57:25","2023-07-12 05:57:25","","709–715","","9","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9LWYPV9W","journalArticle","2012","Grond, Florian; Kramer, Oliver; Hermann, Thomas","Balancing Salience and Unobtrusiveness in Auditory Monitoring of Evolutionary Optimization","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16364","","2012","2023-07-12 05:57:28","2023-07-12 05:57:28","","531–539","","7/8","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7I344SKX","journalArticle","2012","Peres, S. Camille","A Comparison of Sound Dimensions for Auditory Graphs: Pitch Is Not So Perfect","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16367","","2012","2023-07-12 05:57:31","2023-07-12 05:57:31","","561–567","","7/8","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CMAKZTKN","journalArticle","2018","Burloiu, Grigore; Mihai, Valentin; Damian, Stefan","Layered Motion and Gesture Sonification in an Interactive Installation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19860","SoundThimble is an interactive sound installation based on the relationship between human motion and virtual objects in 3D space. A Vicon infrared motion-capture system and custom software are used to track, interpret, and sonify the movement and gestures of a performer relative to a virtual object. The authors explore the resulting possibilities for layered sonification dynamics and extended perception and expression in internal tests as well as in a public demo. Experimental evaluation reveals an average object search time of around 60 s, as well as thresholding ranges for effective gesture spotting. The underlying software platform is open source and portable to similar hardware systems, leaving room for extension and variation. This paper presents the pilot application of the proposed framework. Audience members entering the tracking area shift among the roles of game player, sonic performer, and composer/arranger, according to an iterative interaction schema. The central vehicle in all three layers is the “sound-thimble” itself, a virtual object with particular spatial, sonic, and interaction attributes.","2018","2023-07-12 05:57:34","2023-07-19 03:45:55","","770–778","","10","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D6AW5V3G","journalArticle","2014","Evreinova, Tatiana V.; Evreinov, Grigori; Raisamo, Roope","An Exploration of Volumetric Data in Auditory Space","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=17131","","2014","2023-07-12 05:57:37","2023-07-12 05:57:37","","172–187","","3","62","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AVGQN2AI","journalArticle","2012","Parseihian, Gaëtan; Katz, Brian F. G.","Morphocons: A New Sonification Concept Based on Morphological Earcons","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16355","Sonification, a form of auditory display, is a means of mapping arbitrary information such as the distance to an obstacle or changes in temperature into sound. For the visually impaired, sonification can make an important contribution to increasing autonomy. In contrast to earcons (the audio analog of icons), which map a unique sound to a particular meaning, morphocons are short audio units that are used to construct a sonic grammar based on temporal-frequency patterns, rather than fixed sound samples. For example, a rhythmic repetition can be used to modify or add meaning to any base sound sample. Results indicate that both blind and sighted subjects were able to perceive temporal variations of acoustic parameters as an abstract form, independent of the base sound sample, allowing the extraction of consistent category information from a range of different customizable sounds.","2012","2023-07-12 05:57:39","2023-07-19 04:37:12","","409–418","","6","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N9HGRJVK","journalArticle","2012","Schaffert, Nina; Gehret, Reiner; Mattes, Klaus","Modeling the Rowing Stroke Cycle Acoustically","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16366","Because elite athletes require an unconscious and automated sense of time, and because sound is especially appropriate for conveying timing information, acoustic feedback can be especially useful in training of rowers. In the context of human movement, rhythm is a time accurate sequence of motor actions. Rhythm and synchronization are inseparable within a moving context. An auditory feedback signal based on boat acceleration helps rowers control their activities, and this sonified data can be stored in an audio file for later training and analysis. The improved sensitivity to the time-critical nature of the rowing cycle yielded an improved synchronization among the crew, as well as an improvement of individual athlete’s rowing technique.","2012","2023-07-12 05:57:41","2023-07-19 04:47:05","","551–560","","7/8","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XCSAILK9","journalArticle","2022","Lindetorp, Hans; Falkenberg, Kjetil","Evaluating Web Audio for Learning, Accessibility, and Distribution","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22019","","2022","2023-07-12 05:57:44","2023-07-12 05:57:44","","951–961","","11","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G5XEQ86V","journalArticle","2012","Stewart, Rebecca; Sandler, Mark","Spatial Auditory Display in Music Search and Browsing Applications","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16637","User interfaces for searching and browsing collections of music often use nonaudio for presenting information about the contents of the collection. This study reviews the literature to unify the various ways in which auditory spatialization can be used to augment the presentation of data. The authors examined 22 user interfaces that use such concepts as auditory icons, perceived location, amplitude panning, and a usability evaluation. Commonalities among the designs are discussed including the chosen spatialization approaches and evaluation methods.","2012","2023-07-12 05:57:46","2023-07-19 04:49:19","","936–946","","11","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TT5VK6U7","journalArticle","2012","Terasawa, Hiroko; Berger, Jonathan; Makino, Shoji","In Search of a Perceptual Metric for Timbre: Dissimilarity Judgments among Synthetic Sounds with MFCC-Derived Spectral Envelopes","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16372","Because the spectral envelope of a sound is a crucial aspect of timbre perception, the authors propose a quantitative model of spectral envelope perception using a set of orthogonal basis functions, analogous to the three primary colors in vision. The goal is find a quantitative mapping between the physical description of the spectral envelope and its perception. This allows for a meaningful and reliable way of controlling timbre in sonification. This paper presents a quantitative metric to describe the multidimensionality of spectral envelope perception, i.e., the perception that is specifically related to the spectral element of timbre. Mel-frequency cepstral coefficients (MFCC) were chosen as a metric for spectral envelope perception because of their linearity, orthogonality, and multidimensionality. Quantitative data from two experiments illustrate the linear relationship between the subjective perception of spectrally-varied synthetic sounds and the MFCC.","2012","2023-07-12 05:57:49","2023-07-19 04:52:08","","674–685","","9","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V4ZRWKSC","journalArticle","2023","Dupré, Théophile; Denjean, Sébastien; Aramaki, Mitsuko; Kronland-Martinet, Richard","Spatial Integration of Dynamic Auditory Feedback in Electric Vehicle Interior","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22142","With the development of electric motor vehicles, the domain of automotive sound design addresses new issues and is now concerned with creating suitable and pleasant soundscapes inside the vehicle. For instance, the absence of predominant engine sound changes the driver perception of the dynamic of the car. Previous studies proposed relevant sonification strategies to augment the interior sound environment by bringing back vehicle dynamics with synthetic auditory cues. Yet, users report a lack of blending with the existing soundscape. In this study, the authors analyze acoustical and perceptual spatial characteristics of the car soundscape and show that the spatial attributes of sound sources are fundamental to improve the perceptual coherency of the global environment.","2023","2023-07-12 05:57:52","2023-07-19 03:54:01","","349–362","","6","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LUSB87RU","journalArticle","2012","Jeon, Myounghoon; Gupta, Siddharth; Davison, Benjamin K.; Walker, Bruce N.","Auditory Menus Are Not Just Spoken Visual Menus: A Case Study of “Unavailable” Menu Items","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16362","","2012","2023-07-12 05:57:54","2023-07-12 05:57:54","","505–518","","7/8","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N62DXSB5","journalArticle","2012","Metatla, Oussama; Bryan-Kinns, Nick; Stockman, Tony","The Effects of Using Headphones and Speakers on Collaboration in an Audio-Only Workspace","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16365","The means by which audio was delivered (headphones or loudspeakers) in a shared workplace environment influenced the dynamics of collaborations. In an experiment designed to show the influence of audio delivery, pairs of sighted individuals used audio as the sole means for communicating with one another while editing a shared diagram. The choice of working style affects how collaborators attend to the sounds present in a collaborative space, which in turn influences how they structure and organize their interactions. That in turn determines which information is relevant, dynamically changing according to how collaborators choose to work with sounds. Another conclusion was that the mere physical presence of audio in a shared space does not necessary imply that it is being attended to by those hearing it.","2012","2023-07-12 05:57:59","2023-07-19 04:31:26","","540–550","","7/8","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AN972NCJ","journalArticle","2021","Bown, Oliver; Ferguson, Sam; Dos Santos, Augusto Dias Pereira; Mikolajczyk, Kurt","Supporting Creative Practice in Wireless Distributed Sound Installations Given Technical Constraints","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21470","In this paper we present creative practice-led research into building large, scalable “multiplicitous media” artworks in which many networked devices control lights and speakers and are coordinated over Wi-Fi to create holistic artistic and environmental experiences. We discuss competing constraints, in particular the creative constraints associated with the challenge of coding complex multi-device behaviors, maximizing creative freedom and simplifying complex engineering and design decisions. Based on recent experience building multi-device digital installation works, we propose an approach, the “broadcast-first recipe,” that aims to simplify the space of creative possibilities, with a trade-off between expressive power and creative efficiency that we argue is worth adopting. We examine this approach in light of hard technical constraints such as central processing unit (CPU)and Wi-Fi bandwidth budgets, which we discuss in a concrete example. We consider how the effectiveness of the proposed approach could be further leveraged in the provision of support tools.","2021","2023-07-12 05:58:01","2023-07-19 03:44:24","","757–767","","10","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CBM3DHYM","journalArticle","2013","Lech, Michal; Kostek, Bozena","Testing A Novel Gesture-Based Mixing Interface","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16822","With a digital audio workstation, in contrast to the traditional mouse-keyboard computer interface, hand gestures can be used to mix audio with eyes closed. Mixing with a visual representation of audio parameters during experiments led to broadening the panorama and a more intensive use of shelving equalizers. Listening tests proved that the use of hand gestures produces mixes that are aesthetically as good as those obtained using a mouse, keyboard, and MIDI controller. The human and artistic factor is an essential part of the art, which includes the way in which sound tools are controlled. Alternative means of control are part of sound art.","2013","2023-07-12 05:58:04","2023-07-19 04:18:07","","301–313","","5","61","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SPCP6MJC","journalArticle","2022","Baratè, Adriano; Ludovico, Luca A.","Web MIDI API: State of the Art and Future Perspectives","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22016","The Web MIDI API is intended to connect a browser app with Musical Instrument Digital Interface (MIDI) devices and make them interact. Such an interface deals with exchanging MIDI messages between a browser app and an external MIDI system, either physical or virtual. The standardization by the World Wide Web (W3C) Consortium started about 10 years ago, with a first public draft published on October 2012, and the process is not over yet. Because this technology can pave the way for innovative applications in musical and extra-musical fields, the present paper aims to unveil the main features of the API, remarking its advantages and drawbacks and discussing several applications that could take benefit from its adoption.","2022","2023-07-12 05:58:06","2023-07-19 03:38:49","","918–925","","11","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VRVQR38Y","journalArticle","2018","Liang, Beici; Fazekas, György; Sandler, Mark","Measurement, Recognition, and Visualization of Piano Pedaling Gestures and Techniques","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19584","When playing the piano, pedaling is one of the important techniques that lead to expressive performance, comprising not only the onset and offset information that composers often indicate in the score, but also gestures related to the musical interpretation by performers. This research examines pedaling gestures and techniques on the sustain pedal from the perspective of measurement, recognition, and visualization. Pedaling gestures can be captured by a dedicated measurement system where the sensor data is simultaneously recorded alongside the piano sound under normal playing conditions. Recognition is comprised of two separate tasks on the sensor data: pedal onset/offset detection and classification by technique. The onset and offset times of each pedaling technique were computed using signal processing algorithms. Based on features extracted from every segment when the pedal is pressed, the task of classifying the segments by pedaling technique was undertaken using machine-learning methods. High accuracy was obtained by cross validation. The recognition results can be represented using novel pedaling notations and visualized in an audio-based score-following application.","2018","2023-07-12 05:58:09","2023-07-19 04:20:07","","448–456","","6","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WYLKJ84E","journalArticle","2007","Short, Kevin M.; Garcia, Ricardo A.; Daniels, Michelle L.","Multichannel Audio Processing Using a Unified-Domain Representation","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=14155","[Engineering Report] The unified-domain representation for synchronized multichannel audio streams is introduced. This lossless and invertible transformation describes multiple streams of audio as a single-frequency-domain magnitude component multiplied by a complex matrix encoding the spatial and phase relationship information for each channel. Unified-domain analysis and signal-processing techniques for applications such as high-resolution frequency analysis, sound source separation, spatial psychoacoustic models, and low-bit-rate audio coding are presented.","2007","2023-07-12 05:58:11","2023-07-19 04:48:15","","156–165","","3","55","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QWZXW2RY","journalArticle","2020","Kritsis, Kosmas; Garoufis, Christos; Zlatintsi, Athanasia; Bouillon, Manuel; Acosta, Carlos; Martín-Albo, Daniel; Piechaud, Robert; Maragos, Petros; Katsouros, Vassilis","iMuSciCA Workbench: Web-based Music Activities For Science Education","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20990","","2020","2023-07-12 05:58:14","2023-07-12 05:58:14","","738–746","","10","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QVFYILHA","journalArticle","2018","Xambó, Anna; Roma, Gerard; Shah, Pratik; Tsuchiya, Takahiko; Freeman, Jason; Magerko, Brian","Turn-taking and Online Chatting in Remote and Co-located Collaborative Music Live Coding","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19391","This paper looks into co-located and remote turn-taking and online chatting in collaborative music live coding (CMLC) using the web-based computer science education platform EarSketch. Duo and trio live coding are considered from an autoethnographic stance. An online survey with six practitioners in live coding and collaboration complements the autoethnographic findings. It was identified that turn-taking in duo and trio live coding was more promising in an education context than in performance. It is expected that turn-taking and online chatting in CMLC, among small groups of two, three, or four people can be useful in the classroom for pedagogical purposes. The role of a chat window is important as a tool for supporting communication in CMLC, but the proposal of semantic hashtags should be reconsidered as a tailorable vocabulary adapted to the needs of each group and perhaps linked to a notification system that facilitates the collaboration. From the four use cases based on trio/duo versus co-located/remote situations, it was discovered that a co-located trio live coding mediated by a turn-taking mechanism can be more interesting for group dynamics because the roles of a driver and two navigators can specialize and adapt easily during the musical improvisation act, while combining both verbal and nonverbal communication.","2018","2023-07-12 05:58:16","2023-07-19 10:57:58","","253–266","","4","66","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6LRBWTCF","journalArticle","2020","Xambó, Anna; Støckert, Robin; Jensenius, Alexander Refsum; Saue, Sigurd","Learning to Code Through Web Audio: A Team-Based Learning Approach","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20989","In this article, we discuss the challenges and opportunities provided by teaching programming using web audio technologies and adopting a team-based learning (TBL) approach among a mix of colocated and remote students, mostly novices in programming. The course has been designed for cross-campus teaching and teamwork, in alignment with the two-city master's program in which it has been delivered. We present the results and findings from (1) students' feedback; (2) software complexity metrics; (3) students' blog posts; and (4) teacher's reflections. We found that the nature of web audio as a browser-based environment, coupled with the collaborative nature of the course, was suitable for improving the students' level of confidence about their abilities in programming. This approach promoted the creation of group course projects of a certain level of complexity, based on the students' interests and programming levels. We discuss the challenges of this approach, such as supporting smooth cross-campus interactions and assuring students' preknowledge in web technologies (HTML, CSS, and JavaScript) for an optimal experience. We conclude by envisioning the scalability of this course to other distributed and remote learning scenarios in academic and professional settings. This is in line with the foreseen future scenario of cross-site interaction mediated through code.","2020","2023-07-12 05:58:19","2023-07-19 10:58:07","","727–737","","10","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DS2UTRIB","journalArticle","2004","Härmä, Aki; Jakka, Julia; Tikander, Miikka; Karjalainen, Matti; Lokki, Tapio; Hiipakka, Jarmo; Lorho, Gaëtan","Augmented Reality Audio for Mobile and Wearable Appliances","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=13010","","2004","2023-07-12 05:58:21","2023-07-12 05:58:21","","618–639","","6","52","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WHD5HJKS","journalArticle","2023","Meyer-Kahlen, Nils; Kastemaa, Miranda; Schlecht, Sebastian J.; Lokki, Tapio","Measuring Motion-to-Sound Latency in Virtual Acoustic Rendering Systems","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22145","Few studies that employ virtual acoustic rendering systems accurately specify motion-to-sound latency. To make such assessments more common, we present two methods for latency measurements using either impulsive or periodic movements. The methods only require hardware available in every acoustics lab: a small microphone and a loudspeaker. We provide open-source tools that implement analysis according to the methods. The methods are evaluated on a high-quality optical tracking system. In addition, three small trackers based on inertial measurement units were tested. The results show the reliability of the method for the optical system and the difficulties in defining the latency of inertial measurement unit-based trackers.","2023","2023-07-12 05:58:23","2023-07-19 04:31:34","","390–398","","6","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UW8WNJ2T","journalArticle","2017","Andreopoulou, Areti; Roginska, Agnieszka","Database Matching of Sparsely Measured Head-Related Transfer Functions","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=19171","","2017","2023-07-12 05:58:25","2023-07-12 05:58:25","","552–561","","7/8","65","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WST47GAB","journalArticle","2021","Rovithis, Emmanouel; Moustakas, Nikolaos; Vogklis, Konstantinos; Drossos, Konstantinos; Floros, Andreas","Design Recommendations for a Collaborative Game of Bird Call Recognition Based on Internet of Sound Practices","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21544","","2021","2023-07-12 05:58:27","2023-07-12 05:58:27","","956–966","","12","69","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VGL4E6SQ","journalArticle","2012","Schönstein, David; Katz, Brian F.G.","Variability in Perceptual Evaluation of HRTFs","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16552","Because appropriate head-related transfer functions (HRTFs) are key to binaural rendering, an evaluation is required to assess processing steps when individual HRTFs are not available. This study involving six subjects showed significant response variability in perceptual evaluations of HRTFs when subjects were asked to judge six sets of HRTFs, including individual HRTFs, with three different attributes. Insufficient reproducibility is problematic when trying to select nonindividual HRTFs. In order to minimize the effect of learning, adequate training should be provided. By using attribute evaluations and assessor selection, this study offers a methodology that might be used to produce consistent evaluations in commercial binaural syntheses.","2012","2023-07-12 05:58:29","2023-07-19 04:47:13","","783–793","","10","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T3SD48RF","journalArticle","2020","Vindrola, Lucas; Melon, Manuel; Chamard, Jean-Christophe; Gazengel, Bruno","Pressure Matching With Forced Filters for Personal Sound Zones Application","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20997","This paper presents a rethinking of the Pressure Matching Method (PM) used in the generation of Personal Sound Zones when the responses of some filters are already known. They are then imposed in the calculation, resulting in a Forced Pressure Matching method. This new formulation is implemented to control two zones—a reproduction zone and dark zone—in a two-seat configuration aimed toward the Transportation industry. Due to variations in transportation acoustic environments, the computational time is added to the metrics typically used in the Personal Sound Zones literature (such as acoustic contrast, effort, error, etc.), foreseeing the need of an adaptive system. Perfect Dirac delta functions were forced as filters of the loudspeakers closest to the reproduction zone. The new formulation achieved the same acoustic contrast, effort, and reproduction error very similar to that of the conventional PMbut calculated the filters 24% faster.","2020","2023-07-12 05:58:31","2023-07-19 10:53:18","","832–842","","11","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SJDFIAWM","journalArticle","2020","Salmon, François; Hendrickx, Étienne; Épain, Nicolas; Paquier, Mathieu","The Influence of Vision on Perceived Differences Between Sound Spaces","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20888","Few studies have investigated the influence of visual cues on sound space perception beyond the influence of visual cues on sound source position. Previous studies suggest that the perception of late reflections is not affected by the visual impression of a room; however, only a limited number of spatial sound attributes were investigated. In the present paper, audiovisual interactions were examined without making assumptions on the number and nature of perceptual dimensions involved in the perception of sound space. In a virtual environment that employed a Head Mounted Display and dynamic binaural playback, subjects were asked to judge the perceived dissimilarity between sound spaces while watching the same visual stimulus. Pairwise comparisons were repeated using multiple visual conditions, including an audio-only condition. One sound source, a male voice reciting a poem, was considered in the listening test. It appeared that the visual modality did not impact the perceived differences between sound spaces.","2020","2023-07-12 05:58:33","2023-07-19 04:46:07","","522–531","","7/8","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PUAEAMK6","journalArticle","2023","Ois Salmon, Franç; Changenet, Frédéric; Colas, Tom; Verron, Charles; Paquier, Mathieu","A Comparative Study of Multichannel Microphone Arrays Used in Classical Music Recording","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22149","With the growing advent of object-based audio productions, a major challenge for sound recordists is to determine multichannel microphone arrays that are suitable on several sound reproduction systems. Various multichannel 3D microphone arrays have been designed for the production of immersive content and it seems necessary to assess their qualities on several playback systems. This study concerns the subjective evaluation of six multichannel microphone arrays used for the recording of classical music: Decca Tree, ESMA-3D, MMAD, 2L-Cube, and first-order and second-order ambisonic microphone arrays. Subjects evaluated the sound recordings according to four perceptual attributes (precision of localization, envelopment, spectral quality, and preference) as well as on two reproduction systems (a 5.1.4 multichannel loudspeaker setup and a dynamic binaural playback). As observed previously with stereophonic reproduction, results showed that coincident systems can provide a good localization accuracy but can lack in the sensation of envelopment by reverberation. Moreover, they are more likely to be perceived differently under different rendering conditions. The greatest sense of envelopment was produced by ESMA-3D for the two rendering conditions. No particular system was preferred by the subjects for creating a mix with spot microphones.","2023","2023-07-12 05:58:36","2023-07-19 04:35:33","","441–454","","7/8","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZW6GHTVB","journalArticle","2015","McGregor, Iain Peter; Cunningham, Stuart","Comparative Evaluation of Radio and Audio Logo Sound Designs","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=18048","","2015","2023-07-12 05:58:39","2023-07-12 05:58:39","","876–888","","11","63","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F88FPUWY","journalArticle","2022","Engel, Isaac; Alon, David L.; Scheumann, Kevin; Crukley, Jeff; Mehra, Ravish","On the Differences in Preferred Headphone Response for Spatial and Stereo Content","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=21564","When reproducing spatial audio over headphones, ensuring that these have a flat frequency response is important to produce an accurate rendering. However, previous studies suggest that, when reproducing nonspatial content such as stereo music, the headphone response should resemble that of a loudspeaker system in a listening room (e.g., the so-called Harman target). It is not yet clear whether a pair of headphones calibrated in such way would be preferred by listeners for spatial audio reproduction too. This study investigates how listeners' preference regarding headphone frequency response differs in the cases of stereo and spatial audio content reproduction, rendered using individual binaural room impulse responses. Three listening tests that evaluate seven different target headphone responses, two headphones, and two reproduction bandwidths are presented with over 20 listeners per test. Results suggest that a flat headphone response is preferred when listening to spatial audio content, whereas the Harman target was preferred for stereo content. This effect was found to be stronger when user-specific equalization was used and was not significantly affected by the choice of headphone or reproduction bandwidth.","2022","2023-07-12 05:58:49","2023-07-19 03:54:35","","271–283","","4","70","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NVLUVV7N","journalArticle","2023","Vidal, Adrien; Herzog, Philippe; Lambourg, Christophe; Chatron, Jacques","Comparison of Transaural Configurations Inside Usual Rooms","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=22041","","2023","2023-07-12 05:58:53","2023-07-12 05:58:53","","202–215","","4","71","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FDIATTI7","journalArticle","2012","Wersényi, György","Virtual Localization by Blind Persons","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=16368","In order for blind people to better use personal computers, an auditory virtual environment can be used to present information that might otherwise be available only with vision. Auditory objects can be spatial placed in the virtual environment if the user can successfully identify their location. In contrast to sighted subjects, blind subjects were better at detecting movements in the horizontal plane around the head, localizing static frontal audio sources, and orientation in a 2-D virtual audio display. On the other hand, sighted subjects performed better identifying ascending sound sources in the vertical plane and detecting static sources in the back.","2012","2023-07-12 05:58:55","2023-07-19 10:55:26","","568–579","","7/8","60","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U2DDK77J","journalArticle","2020","Poirier-Quinot, David; Katz, Brian F.G.","Assessing the Impact of Head-Related Transfer Function Individualization on Task Performance: Case of a Virtual Reality Shooter Game","J. Audio Eng. Soc","","","","http://www.aes.org/e-lib/browse.cfm?elib=20731","","2020","2023-07-12 05:58:57","2023-07-12 05:58:57","","248–260","","4","68","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""